# Unit06ï½œCross-Validation èˆ‡æ¨¡å‹é¸æ“‡ï¼ˆå«åŒ–å·¥å®‰å…¨é‚Šç•Œæ¡ˆä¾‹ï¼‰

**èª²ç¨‹åç¨±**ï¼šåŒ–å·¥è³‡æ–™ç§‘å­¸èˆ‡æ©Ÿå™¨å­¸ç¿’å¯¦å‹™ï¼ˆCHE-AI-101ï¼‰  
**æœ¬å ‚èª²å®šä½**ï¼šå¾ã€Œä¸€æ¬¡åˆ‡åˆ†ã€é€²éšåˆ°ã€Œæ›´ç©©å¥çš„è©•ä¼°èˆ‡é¸æ¨¡ã€ï¼šCVã€Grid Searchï¼Œä¸¦ç”¨åŒ–å·¥å®‰å…¨é‚Šç•Œåšæ¡ˆä¾‹ã€‚  

---

## 1. é€²éšæ¦‚å¿µï¼šäº¤å‰é©—è­‰ (Cross-Validation)

åœ¨åŒ–å·¥å¯¦é©—ä¸­ï¼Œæ•¸æ“šå¾€å¾€å¾ˆçè²´ä¸”æ•¸é‡ç¨€å°‘ï¼ˆSmall Dataï¼‰ã€‚å¦‚æœåªåˆ‡ä¸€æ¬¡ Train/Testï¼Œå‰›å¥½åˆ‡åˆ°æ¯”è¼ƒã€Œç°¡å–®ã€æˆ–ã€Œæ¥µç«¯ã€çš„æ¸¬è©¦è³‡æ–™ï¼Œè©•ä¼°çµæœå¯èƒ½ä¸æº–ã€‚

**ç†è«–è©³è§£ï¼šK-Fold Cross-Validation**

ç‚ºäº†é™ä½è©•ä¼°çš„è®Šç•°æ•¸ (Variance)ï¼Œæˆ‘å€‘æ¡ç”¨ $K$-Fold äº¤å‰é©—è­‰ã€‚
1.  å°‡åŸå§‹è³‡æ–™é›† $D$ éš¨æ©Ÿåˆ†å‰²æˆ $K$ å€‹äº’æ–¥çš„å­é›† (Folds)ï¼š$D_1, D_2, \dots, D_K$ã€‚
2.  é€²è¡Œ $K$ æ¬¡è¿­ä»£ï¼Œæ¯æ¬¡å–ç¬¬ $k$ å€‹å­é›† $D_k$ ä½œç‚º**é©—è­‰é›† (Validation Set)**ï¼Œå…¶é¤˜ $K-1$ å€‹å­é›†ä½œç‚º**è¨“ç·´é›†**ã€‚
3.  è¨ˆç®— $K$ æ¬¡çš„è©•ä¼°æŒ‡æ¨™ $E_k$ (ä¾‹å¦‚ Accuracy)ï¼Œæœ€çµ‚çš„æ¨¡å‹æ•ˆèƒ½ç‚ºå¹³å‡å€¼ï¼š

$$ E_{CV} = \frac{1}{K} \sum_{k=1}^{K} E_k $$

é€™ç¨®æ–¹æ³•ç¢ºä¿äº†**æ¯ä¸€ç­†è³‡æ–™éƒ½æœ‰æ©Ÿæœƒè¢«ç•¶ä½œæ¸¬è©¦è³‡æ–™**ï¼Œä¸”æ°å¥½ä¸€æ¬¡ã€‚é€™å°æ–¼å°æ¨£æœ¬æ•¸æ“šï¼ˆå¦‚åŒ–å·¥æ‰¹æ¬¡å¯¦é©—æ•¸æ“šï¼‰ç‰¹åˆ¥é‡è¦ï¼Œèƒ½æä¾›æ›´å¯ä¿¡çš„æ³›åŒ–èª¤å·®ä¼°è¨ˆã€‚

### 1.1 å¯¦å‹™ä¸Šä½ è©²ç”¨å“ªç¨® CVï¼Ÿï¼ˆåˆ¥å†åªç”¨ `cv=5`ï¼‰

åœ¨çœŸå¯¦åŒ–å·¥è³‡æ–™è£¡ï¼Œã€Œé©—è­‰ç­–ç•¥ã€å¾€å¾€æ¯”ã€Œæ¨¡å‹é¸å“ªå€‹ã€æ›´é‡è¦ã€‚è«‹å…ˆåˆ¤æ–·ä½ çš„è³‡æ–™å±¬æ–¼å“ªå€‹ä¸–ç•Œï¼š

| ä½ çš„è³‡æ–™é•·ä»€éº¼æ¨£ | å¸¸è¦‹æƒ…å¢ƒï¼ˆåŒ–å·¥ï¼‰ | å»ºè­°åˆ‡æ³• | Scikit-learn å»ºè­° |
|---|---|---|---|
| **i.i.d.**ï¼ˆæ¨£æœ¬äº’ç›¸ç¨ç«‹ï¼‰ | å¯¦é©—æ¢ä»¶éš¨æ©ŸæŠ½æ¨£ã€Titanic é€™ç¨®è¡¨æ ¼è³‡æ–™ | `train_test_split(..., stratify=y)` + `StratifiedKFold` | `StratifiedKFold` |
| **æœ‰ã€Œæ‰¹æ¬¡/æ´»å‹•/è¨­å‚™ã€ç¾¤çµ„** | æ‰¹æ¬¡åæ‡‰ã€Campaignã€ç”Ÿç”¢ç·š/æ©Ÿå°å·®ç•°ã€é…æ–¹ Lot | **åŒä¸€å€‹ Batch ä¸èƒ½è¢«æ‹†é–‹** | `GroupKFold` / `GroupShuffleSplit` |
| **æ™‚é–“åºåˆ—**ï¼ˆæœ‰å…ˆå¾Œé †åºï¼‰ | DCS/SCADA é€£çºŒè³‡æ–™ã€è»Ÿæ„Ÿæ¸¬å™¨ã€RUL | **ä¸èƒ½ shuffle**ï¼Œè¦ç”¨ã€Œéå»é æ¸¬æœªä¾†ã€ | `TimeSeriesSplit`ï¼ˆæˆ–è‡ªè¨‚ Walk-forwardï¼‰ |

> å£è¨£ï¼š**åŒä¸€æ‰¹/åŒä¸€å°/åŒä¸€å¤©**å¦‚æœåŒæ™‚å‡ºç¾åœ¨ Train èˆ‡ Valï¼Œæº–ç¢ºç‡å¸¸å¸¸æ˜¯ã€Œå‡çš„ã€ã€‚

### 1.1.5 âš ï¸ åŒ–å·¥æ™‚åºæ•¸æ“šçš„è‡´å‘½é™·é˜±ï¼šKFold vs TimeSeriesSplit

**é‡è¦è­¦å‘Š**ï¼šåŒ–å·¥è£½ç¨‹æ•¸æ“šçµ•å¤§å¤šæ•¸å…·æœ‰æ™‚åºæ€§ï¼ˆæº«åº¦ã€å£“åŠ›ã€æµé‡éš¨æ™‚é–“è®ŠåŒ–ï¼‰ï¼Œè‹¥éŒ¯èª¤ä½¿ç”¨ KFold æœƒé€ æˆ**æœªä¾†è³‡è¨Šæ´©æ¼ï¼ˆFuture Information Leakageï¼‰**ï¼Œå°è‡´æ¨¡å‹æ€§èƒ½è¢«åš´é‡é«˜ä¼°ï¼

#### å•é¡Œæ ¹æº

| æ–¹æ³• | æ•¸æ“šåˆ†å‰²æ–¹å¼ | å•é¡Œ |
|------|------------|------|
| **KFoldï¼ˆéŒ¯èª¤ï¼‰** | éš¨æ©Ÿæ‰“äº‚æ•¸æ“šå¾Œåˆ‡åˆ† | è¨“ç·´é›†åŒ…å«ã€Œæœªä¾†ã€æ•¸æ“šï¼Œæ¸¬è©¦é›†åŒ…å«ã€Œéå»ã€æ•¸æ“š |
| **TimeSeriesSplitï¼ˆæ­£ç¢ºï¼‰** | ä¿è­‰è¨“ç·´é›†æ°¸é åœ¨æ¸¬è©¦é›†ä¹‹å‰ | ç¬¦åˆå¯¦éš›éƒ¨ç½²æƒ…å¢ƒï¼ˆç”¨éå»é æ¸¬æœªä¾†ï¼‰ |

#### å¯¦éš›ç¯„ä¾‹ï¼šåæ‡‰å™¨æº«åº¦æ§åˆ¶

```python
from sklearn.model_selection import TimeSeriesSplit, KFold

# ç”Ÿæˆæ™‚åºæ•¸æ“šï¼ˆæ¨¡æ“¬åæ‡‰å™¨æº«åº¦èˆ‡è½‰åŒ–ç‡é—œä¿‚ï¼‰
# é—œéµï¼šåŒ…å«æ™‚é–“è¶¨å‹¢é …ï¼ˆæ—©æœŸæ•¸æ“šå°æ™šæœŸé æ¸¬ç„¡æ•ˆï¼‰
temperature = 350 + 0.1 * time_idx + ...
conversion_rate = 50 + 0.5 * temperature + 0.05 * time_idx + ...

# âŒ éŒ¯èª¤åšæ³•ï¼šKFold
kf_wrong = KFold(n_splits=5, shuffle=True, random_state=42)
scores_kfold = cross_val_score(model, X, y, cv=kf_wrong)
# çµæœï¼šæº–ç¢ºç‡ 0.920ï¼ˆéåº¦æ¨‚è§€ï¼ï¼‰

# âœ… æ­£ç¢ºåšæ³•ï¼šTimeSeriesSplit  
tscv = TimeSeriesSplit(n_splits=5)
scores_tscv = cross_val_score(model, X, y, cv=tscv)
# çµæœï¼šæº–ç¢ºç‡ 0.750ï¼ˆçœŸå¯¦æ³›åŒ–èƒ½åŠ›ï¼‰

# ğŸ“‰ æ€§èƒ½å·®ç•°ï¼š17 å€‹ç™¾åˆ†é»ï¼
```

#### TimeSeriesSplit çš„åˆ†å‰²æ–¹å¼

![æ™‚åºåˆ†å‰²ç¤ºæ„](P2_Unit06_Results/05_timeseries_split.png)

**é—œéµç‰¹æ€§**ï¼š
- **Fold 1**ï¼šè¨“ç·´ [0-60]ï¼Œæ¸¬è©¦ [60-120]
- **Fold 2**ï¼šè¨“ç·´ [0-120]ï¼Œæ¸¬è©¦ [120-180]
- **Fold 3**ï¼šè¨“ç·´ [0-180]ï¼Œæ¸¬è©¦ [180-240]
- **Fold 4**ï¼šè¨“ç·´ [0-240]ï¼Œæ¸¬è©¦ [240-300]

æ¯ä¸€æŠ˜çš„æ¸¬è©¦é›†éƒ½åœ¨è¨“ç·´é›†**æ™‚é–“ä¹‹å¾Œ**ï¼Œå®Œå…¨é¿å…æœªä¾†è³‡è¨Šæ´©æ¼ã€‚

#### åŒ–å·¥æ™‚åºæ•¸æ“šçš„ CV ç­–ç•¥ç¸½çµ

| æ•¸æ“šé¡å‹ | æ¨è–¦æ–¹æ³• | åŸå›  |
|---------|---------|------|
| **æ‰¹æ¬¡æ•¸æ“šï¼ˆBatchï¼‰** | GroupKFold | é¿å…åŒæ‰¹æ¬¡æ•¸æ“šæ´©æ¼ |
| **æ™‚åºæ•¸æ“šï¼ˆTime Seriesï¼‰** | TimeSeriesSplit | ä¿è­‰è¨“ç·´åœ¨æ¸¬è©¦ä¹‹å‰ |
| **ä¸€èˆ¬åˆ†é¡æ•¸æ“š** | StratifiedKFold | ç¶­æŒé¡åˆ¥æ¯”ä¾‹å¹³è¡¡ |
| **ä¸€èˆ¬å›æ­¸æ•¸æ“š** | KFold | ç°¡å–®éš¨æ©Ÿåˆ†å‰² |

#### å¸¸è¦‹éŒ¯èª¤èˆ‡è§£æ±ºæ–¹æ¡ˆ

**âš ï¸ éŒ¯èª¤ç¯„ä¾‹**ï¼š
```python
# éŒ¯èª¤ï¼å°æ™‚åºæ•¸æ“šä½¿ç”¨ shuffle
kf = KFold(n_splits=5, shuffle=True)  # â† æ‰“äº‚æ™‚åº
grid_search = GridSearchCV(model, params, cv=kf)
```

**âœ… æ­£ç¢ºç¯„ä¾‹**ï¼š
```python
# æ­£ç¢ºï¼ä½¿ç”¨ TimeSeriesSplit
tscv = TimeSeriesSplit(n_splits=5)
grid_search = GridSearchCV(
    estimator=model,
    param_grid=params,
    cv=tscv,  # â† ä¿æŒæ™‚åº
    scoring='neg_mean_squared_error'
)
grid_search.fit(X_timeseries, y_timeseries)
```

#### å¯¦å‹™å»ºè­°

1. **è­˜åˆ¥æ•¸æ“šé¡å‹**ï¼šåˆ†ææ•¸æ“šæ˜¯å¦æœ‰æ™‚é–“æˆ³è¨˜æˆ–é †åºä¾è³´
2. **æª¢è¦–æ™‚é–“è¶¨å‹¢**ï¼šç¹ªè£½ç›®æ¨™è®Šæ•¸éš¨æ™‚é–“çš„è®ŠåŒ–åœ–
3. **é¸æ“‡æ­£ç¢º CV**ï¼šæ™‚åºæ•¸æ“šå‹™å¿…ä½¿ç”¨ `TimeSeriesSplit`
4. **é©—è­‰å‡è¨­**ï¼šæ¯”è¼ƒ KFold èˆ‡ TimeSeriesSplit çš„æ€§èƒ½å·®ç•°ï¼Œè‹¥å·®è·å¤§è¡¨ç¤ºå­˜åœ¨æ™‚é–“è¶¨å‹¢

**ğŸ¯ è¨˜ä½**ï¼šåŒ–å·¥è£½ç¨‹æ•¸æ“šå¹¾ä¹éƒ½æ˜¯æ™‚åºæ•¸æ“šï¼Œé è¨­ä½¿ç”¨ `TimeSeriesSplit` æ˜¯æ›´å®‰å…¨çš„é¸æ“‡ï¼

---

### 1.2 é¿å… Data Leakageï¼šåŒ–å·¥å ´åŸŸæœ€å¸¸è¸©çš„ 6 å€‹å‘

1. **æ‰¹æ¬¡è¢«æ‹†é–‹**ï¼šåŒä¸€å€‹ Batch çš„æ—©æ®µåœ¨ Trainã€æ™šæ®µåœ¨ Valï¼Œæ¨¡å‹ç­‰æ–¼ã€Œçœ‹éç­”æ¡ˆã€  
2. **æ™‚åºè¢«æ‰“äº‚**ï¼šå°æ™‚åºæ•¸æ“šä½¿ç”¨ `shuffle=True` çš„ KFold â†’ ç”¨æœªä¾†é æ¸¬éå»ï¼ˆæœ€å¸¸è¦‹éŒ¯èª¤ï¼‰
3. **å…ˆå…¨è³‡æ–™åšæ¨™æº–åŒ–/ç¼ºå€¼å¡«è£œ/ç‰¹å¾µé¸æ“‡**ï¼šå†åš CVï¼ˆæ‡‰è©²æ”¾é€² `Pipeline` è®“å®ƒåœ¨æ¯æŠ˜å…§ fitï¼‰  
4. **ç”¨åˆ°æœªä¾†è³‡è¨Š**ï¼šä¾‹å¦‚ç”¨ `t+10min` çš„æº«åº¦ç‰¹å¾µå»é æ¸¬ `t` çš„å“è³ªï¼ˆå°¤å…¶è»Ÿæ„Ÿæ¸¬å™¨ï¼‰  
5. **Target leakage**ï¼šç‰¹å¾µä¸­æ··å…¥ã€Œå¯¦é©—å®¤æ¸¬åˆ°çš„ y çš„ proxyã€ï¼ˆä¾‹å¦‚ç”¢å“åˆ†æå„€è¼¸å‡ºï¼‰  
6. **ç”¨ Test set èª¿åƒ**ï¼šTest çš„å­˜åœ¨æ„ç¾©æ˜¯ã€Œæœ€å¾Œä¸€æ¬¡ã€çš„é©—æ”¶ï¼Œä¸æ˜¯èª¿åˆ°æœ€å¥½  
7. **åœ¨ CV ä¹‹å‰åšé‡æŠ½æ¨£/SMOTE**ï¼šæœƒè®“åˆæˆæ¨£æœ¬è·¨æŠ˜æ´©æ¼ï¼ˆæ‡‰è©²æ”¾é€²æŠ˜å…§æµç¨‹ï¼‰

### 1.3 Nested CVï¼šé¸æ¨¡ä¸è¦å·çœ‹ï¼ˆå¤–å±¤è©•ä¼°ã€å…§å±¤èª¿åƒï¼‰

ç•¶ä½ åŒæ™‚åšäº†ã€Œèª¿åƒ/æŒ‘æ¨¡å‹ã€èˆ‡ã€Œå ±å‘Šæ•ˆèƒ½ã€ï¼Œæœ€å®¹æ˜“ä¸å°å¿ƒæŠŠé©—è­‰é›†ç•¶æˆæ¸¬è©¦é›†ã€‚è§£æ³•æ˜¯ **Nested CV**ï¼š

- **å¤–å±¤ CV**ï¼šåªè² è²¬ã€Œä¼°è¨ˆæ³›åŒ–èª¤å·®ã€â†’ ä½ è¦å ±å‘Šçš„åˆ†æ•¸åœ¨é€™è£¡ç®—  
- **å…§å±¤ CV**ï¼šåªè² è²¬ã€Œé¸æœ€å¥½çš„è¶…åƒæ•¸/æ¨¡å‹ã€â†’ ä½ å¯ä»¥ Grid/Random Search æ”¾åœ¨é€™è£¡  

å ±å‘Šæ–¹å¼å»ºè­°ï¼š
- å¤–å±¤æ¯æŠ˜åˆ†æ•¸ï¼ˆå¹³å‡ Â± æ¨™æº–å·®ï¼‰ï¼Œå†åŠ ä¸Šã€Œæœ€çµ‚ä¸€æ¬¡çš„ç¨ç«‹ Testã€ä½œç‚ºé©—æ”¶  
- æœ€çµ‚éƒ¨ç½²æ¨¡å‹ï¼šç”¨ã€Œå…¨è¨“ç·´è³‡æ–™ã€+ å…§å±¤æ‰¾åˆ°çš„æœ€ä½³è¨­å®šé‡æ–°è¨“ç·´

### ç¨‹å¼å¯¦ä½œèˆ‡çµæœè§£è®€

```python
from sklearn.model_selection import cross_val_score

# æ±ºç­–æ¨¹æ¨¡å‹
from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier(max_depth=3)

# K-Fold äº¤å‰é©—è­‰ï¼Œé€™è£¡ä»¥ K=5 ç‚ºä¾‹
scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')

# é¡¯ç¤ºæ¯ä¸€æŠ˜çš„æº–ç¢ºç‡
for i, score in enumerate(scores, 1):
    print(f"Fold {i}: {score:.4f}")

# å¹³å‡æº–ç¢ºç‡
print(f"Mean Accuracy: {scores.mean():.4f}")
```

**åŸ·è¡Œçµæœåˆ†æï¼š**
- **å¹³å‡æº–ç¢ºç‡ (Mean Accuracy)**ï¼šé€™ä»£è¡¨æ¨¡å‹åœ¨æœªè¦‹éè³‡æ–™ä¸Šçš„é æœŸè¡¨ç¾ã€‚
- **æ¨™æº–å·® (Standard Deviation)**ï¼šä»£è¡¨æ¨¡å‹è¡¨ç¾çš„æ³¢å‹•ç¨‹åº¦ã€‚
  - è‹¥æ•¸å€¼å¾ˆå°ï¼ˆä¾‹å¦‚ < 2%ï¼‰ï¼Œä»£è¡¨æ¨¡å‹å¾ˆ**ç©©å®š (Stable)**ï¼Œä¸ç®¡è³‡æ–™æ€éº¼åˆ‡ï¼Œè¡¨ç¾éƒ½å·®ä¸å¤šã€‚
  - è‹¥æ•¸å€¼å¾ˆå¤§ï¼Œä»£è¡¨æ¨¡å‹å°è¨“ç·´è³‡æ–™çš„é¸å–éå¸¸æ•æ„Ÿï¼Œå¯èƒ½æœ‰ **Overfitting** æˆ–è³‡æ–™é‡ä¸è¶³çš„å•é¡Œã€‚

---


## 2. æ¨¡å‹é¸æ“‡ç­–ç•¥èˆ‡æ±ºç­–æµç¨‹

### 2.1 æ¨¡å‹é¸æ“‡æ±ºç­–æ¨¹ï¼ˆModel Selection Decision Treeï¼‰

åœ¨åŒ–å·¥å°ˆæ¡ˆä¸­ï¼Œé¸æ“‡åˆé©çš„æ©Ÿå™¨å­¸ç¿’æ¨¡å‹éœ€è¦è€ƒæ…®æ•¸æ“šç‰¹æ€§ã€æ¥­å‹™éœ€æ±‚èˆ‡éƒ¨ç½²é™åˆ¶ã€‚ä»¥ä¸‹æä¾›ä¸€å€‹å¯¦ç”¨çš„æ±ºç­–æµç¨‹ï¼š

```
ğŸ“Š æ•¸æ“šé¡å‹åˆ¤æ–·
â”‚
â”œâ”€ ğŸ¯ åˆ†é¡å•é¡Œï¼ˆSurvived, FaultType, FlowRegime...ï¼‰
â”‚   â”œâ”€ ç‰¹å¾µå°‘ (< 10) + éœ€è¦å¯è§£é‡‹æ€§
â”‚   â”‚   â†’ Decision Tree / Logistic Regression
â”‚   â”œâ”€ ç‰¹å¾µå¤š (> 20) + è¿½æ±‚æº–ç¢ºç‡
â”‚   â”‚   â†’ Random Forest / XGBoost / Gradient Boosting
â”‚   â”œâ”€ é¡åˆ¥ä¸å¹³è¡¡ï¼ˆImbalancedï¼‰
â”‚   â”‚   â†’ èª¿æ•´ class_weight æˆ–ä½¿ç”¨ SMOTE + Ensemble
â”‚   â””â”€ éœ€è¦æ©Ÿç‡è¼¸å‡ºï¼ˆç”¨æ–¼é¢¨éšªè©•ä¼°ï¼‰
â”‚       â†’ Logistic Regression / Random Forest (predict_proba)
â”‚
â””â”€ ğŸ“ˆ å›æ­¸å•é¡Œï¼ˆTemperature, Yield, Viscosity...ï¼‰
    â”œâ”€ ç·šæ€§é—œä¿‚ï¼ˆç›¸é—œä¿‚æ•¸ > 0.7ï¼‰
    â”‚   â”œâ”€ ç„¡å¤šé‡å…±ç·šæ€§ â†’ Linear Regression
    â”‚   â”œâ”€ æœ‰å¤šé‡å…±ç·šæ€§ â†’ Ridge Regression
    â”‚   â””â”€ éœ€è¦ç‰¹å¾µé¸æ“‡ â†’ Lasso Regression
    â”œâ”€ éç·šæ€§é—œä¿‚ + å°‘é‡äº¤äº’ä½œç”¨
    â”‚   â†’ Polynomial Regression (degree 2-3)
    â”œâ”€ æ™‚åºæ•¸æ“š + æ»¯å¾Œæ•ˆæ‡‰ï¼ˆLag featuresï¼‰
    â”‚   â†’ Gradient Boosting + TimeSeriesSplit
    â””â”€ ç‰©ç†ç´„æŸï¼ˆVLE, Arrhenius...ï¼‰
        â†’ Physics-informed ML / Scipy curve_fit
```

### 2.2 åŒ–å·¥å°ˆæ¡ˆçš„æ¨¡å‹é¸æ“‡è€ƒé‡

| è€ƒé‡å› ç´  | å„ªå…ˆé¸æ“‡ | åŸå›  |
|---------|---------|------|
| **å¯è§£é‡‹æ€§** | Decision Tree, Linear Model | éœ€è¦å‘ç¾å ´/ä¸»ç®¡èªªæ˜è¦å‰‡ï¼ˆSOPï¼‰ |
| **å°æ¨£æœ¬ (n < 100)** | Regularized Linear Model | é¿å…éæ“¬åˆï¼Œæ³›åŒ–èƒ½åŠ›è¼ƒä½³ |
| **é«˜ç¶­åº¦ (p > n)** | Lasso / ElasticNet | è‡ªå‹•ç‰¹å¾µé¸æ“‡ |
| **æ™‚åºç›¸ä¾** | Gradient Boosting + TSCV | æ•æ‰æ™‚é–“æ¨¡å¼ |
| **æ‰¹æ¬¡æ•ˆæ‡‰** | Random Forest + GroupKFold | å°ç•°è³ªæ€§ç©©å¥ |
| **å³æ™‚æ¨è«–** | Decision Tree (depth â‰¤ 5) | è¨ˆç®—å¿«é€Ÿï¼Œé©åˆé‚Šç·£è£ç½® |
| **æ¥µé«˜æº–ç¢ºç‡è¦æ±‚** | Ensemble (RF / XGBoost) | çŠ§ç‰²è§£é‡‹æ€§æ›å–æ€§èƒ½ |

### 2.3 æ¨¡å‹é¸æ“‡å¯¦å‹™æµç¨‹

```python
# Step 1: å»ºç«‹ Baselineï¼ˆæ°¸é å…ˆåšï¼ï¼‰
from sklearn.dummy import DummyClassifier
baseline = DummyClassifier(strategy='most_frequent')
baseline.fit(X_train, y_train)
baseline_acc = baseline.score(X_test, y_test)
print(f"Baseline (always predict majority): {baseline_acc:.3f}")

# Step 2: ç°¡å–®æ¨¡å‹ï¼ˆå¯è§£é‡‹æ€§å„ªå…ˆï¼‰
from sklearn.tree import DecisionTreeClassifier
simple_model = DecisionTreeClassifier(max_depth=3)
simple_model.fit(X_train, y_train)
simple_acc = simple_model.score(X_test, y_test)
print(f"Simple Decision Tree: {simple_acc:.3f}")

# Step 3: è¤‡é›œæ¨¡å‹ï¼ˆæ€§èƒ½å„ªå…ˆï¼‰
from sklearn.ensemble import RandomForestClassifier
complex_model = RandomForestClassifier(n_estimators=100)
complex_model.fit(X_train, y_train)
complex_acc = complex_model.score(X_test, y_test)
print(f"Random Forest: {complex_acc:.3f}")

# Step 4: æ±ºç­–åˆ¤æ–·
# è‹¥ simple_acc èˆ‡ complex_acc å·®è· < 2%ï¼Œå„ªå…ˆé¸æ“‡ç°¡å–®æ¨¡å‹
# è‹¥å·®è· > 5%ï¼Œä¸”å°ˆæ¡ˆå…è¨±é»‘ç›’æ¨¡å‹ï¼Œé¸æ“‡è¤‡é›œæ¨¡å‹
```

**æ±ºç­–æº–å‰‡**ï¼š
1. **Baseline æª¢æŸ¥**ï¼šæ–°æ¨¡å‹å¿…é ˆæ˜é¡¯å„ªæ–¼éš¨æ©ŸçŒœæ¸¬
2. **ç°¡å–®å„ªå…ˆ**ï¼šè‹¥ç°¡å–®æ¨¡å‹å¤ ç”¨ï¼ˆæº–ç¢ºç‡å·®è· < 2-3%ï¼‰ï¼Œé¸å®ƒ
3. **æˆæœ¬æ¬Šè¡¡**ï¼šè€ƒæ…®ç¶­è­·æˆæœ¬ã€è§£é‡‹æˆæœ¬ã€è¨ˆç®—æˆæœ¬
4. **é¢¨éšªè©•ä¼°**ï¼šé«˜é¢¨éšªæ‡‰ç”¨ï¼ˆå®‰å…¨ã€å“è³ªï¼‰å„ªå…ˆé¸å¯è§£é‡‹æ¨¡å‹

---

## 3. æ¨¡å‹å„ªåŒ–ï¼šè¶…åƒæ•¸èª¿æ•´ (Hyperparameter Tuning)

æ±ºç­–æ¨¹æœ‰è¨±å¤šã€Œæ—‹éˆ•ã€å¯ä»¥èª¿æ•´ï¼Œé€™äº›ç¨±ç‚º **è¶…åƒæ•¸ (Hyperparameters)**ã€‚èª¿æ•´è¶…åƒæ•¸çš„éç¨‹ï¼Œæœ¬è³ªä¸Šæ˜¯åœ¨æ¬Šè¡¡ **åå·® (Bias)** èˆ‡ **è®Šç•° (Variance)**ã€‚

- **`max_depth` (æœ€å¤§æ·±åº¦)**ï¼š
    - **å¤ªæ·± (High Complexity)**ï¼šæ¨¡å‹æœƒæ­»è¨˜ç¡¬èƒŒè¨“ç·´è³‡æ–™çš„é›œè¨Šï¼Œå°è‡´ **Overfitting (High Variance)**ã€‚
    - **å¤ªæ·º (Low Complexity)**ï¼šæ¨¡å‹ç„¡æ³•æ•æ‰è³‡æ–™çš„è¦å¾‹ï¼Œå°è‡´ **Underfitting (High Bias)**ã€‚
- **`min_samples_split`**ï¼šä¸€å€‹ç¯€é»è‡³å°‘è¦æœ‰å¹¾å€‹æ¨£æœ¬æ‰å…è¨±å†åˆ†è£‚ã€‚æ•¸å€¼è¶Šå¤§ï¼Œæ¨¡å‹è¶Šä¿å®ˆï¼ˆè¶Šä¸å®¹æ˜“éæ“¬åˆï¼‰ã€‚

### 3.1 ä½ åœ¨èª¿çš„å…¶å¯¦æ˜¯ã€Œå·¥ç¨‹æ±ºç­–ã€ï¼šæˆæœ¬ã€é™åˆ¶ã€èˆ‡å¯ç¶­è­·æ€§

åœ¨åŒ–å·¥å°ˆæ¡ˆè£¡ï¼Œè¶…åƒæ•¸èª¿æ•´ä¸åªæ˜¯è¿½æ±‚åˆ†æ•¸ï¼Œé‚„åŒ…å«ï¼š
- **éŒ¯èª¤æˆæœ¬**ï¼šæ¼å ± (FN) æ¯”èª¤å ± (FP) åš´é‡å—ï¼Ÿï¼ˆå®‰å…¨/å“è³ªå¸¸å¸¸æ˜¯ï¼‰  
- **éƒ¨ç½²é™åˆ¶**ï¼šDCS/é‚Šç·£è£ç½®æ¨è«–æ™‚é–“ã€è¨˜æ†¶é«”ã€å¯è§£é‡‹æ€§è¦æ±‚ï¼ˆSOP èƒ½ä¸èƒ½å¯«å‡ºä¾†ï¼‰  
- **è³‡æ–™ç¨€å°‘**ï¼šå°æ¨£æœ¬æ™‚ï¼Œéåº¦èª¿åƒå¾ˆå®¹æ˜“æŠŠé›œè¨Šç•¶è¦å¾‹  

è‹¥æœå°‹ç©ºé–“å¾ˆå¤§ï¼Œé™¤äº† Grid Search ä¹Ÿå¯ç”¨ï¼š
- **RandomizedSearchCV**ï¼šå›ºå®šè¨ˆç®—é ç®—ä¸‹å¸¸å¸¸æ›´æœ‰æ•ˆç‡  
-ï¼ˆé€²éšï¼‰**Bayesian Optimization**ï¼šåœ¨ Part 5 æœƒæ›´å®Œæ•´è«‡ã€Œå¦‚ä½•ç”¨æ›´å°‘å¯¦é©—æ‰¾åˆ°æœ€ä½³é»ã€  

**ç¶²æ ¼æœç´¢ (Grid Search)**ï¼š
é€™å°±åƒåŒ–å·¥è£½ç¨‹ä¸­ï¼Œæˆ‘å€‘éœ€è¦å°‹æ‰¾æœ€ä½³çš„æ“ä½œè¦–çª— (Operating Window)ã€‚
å‡è¨­æˆ‘å€‘æœ‰å…©å€‹åƒæ•¸ $\theta_1$ å’Œ $\theta_2$ï¼ŒGrid Search æœƒçª®èˆ‰æ‰€æœ‰è¨­å®šçš„çµ„åˆ $(\theta_1^{(i)}, \theta_2^{(j)})$ï¼Œä¸¦é€é Cross-Validation æ‰¾å‡ºä½¿ $E_{CV}$ æœ€å°çš„é‚£çµ„åƒæ•¸ $\theta^*$ï¼š

$$ \theta^* = \underset{\theta \in \Theta}{\text{argmin}} \frac{1}{K} \sum_{k=1}^{K} L(D_k, f(D_{\setminus k}; \theta)) $$

**åŸ·è¡Œçµæœåˆ†æï¼š**
- **æœ€ä½³åƒæ•¸ (Best Params)**ï¼šGrid Search å‘Šè¨´æˆ‘å€‘åœ¨å“ªå€‹åƒæ•¸çµ„åˆä¸‹ï¼Œæ¨¡å‹åœ¨é©—è­‰é›†ä¸Šçš„å¹³å‡è¡¨ç¾æœ€å¥½ã€‚
  - ä¾‹å¦‚ï¼Œå¦‚æœ `max_depth` é¸äº† 3 æˆ– 5 è€Œä¸æ˜¯ 10ï¼Œé€™å°è­‰äº†ã€Œæ¨¹ä¸ç”¨å¤ªæ·±ä¹Ÿèƒ½æœ‰å¾ˆå¥½çš„è¡¨ç¾ã€ï¼Œå¤ªæ·±åè€Œå®¹æ˜“éæ“¬åˆã€‚
  - é€™å°±åƒæˆ‘å€‘åœ¨åŒ–å·¥å» åšå¯¦é©—è¨­è¨ˆ (DoE)ï¼Œæ‰¾åˆ°äº†åæ‡‰çš„æœ€ä½³æº«åº¦èˆ‡å£“åŠ›è¨­å®šé»ã€‚

### 3.2 å¯¦å‹™äº¤ä»˜ï¼šModel Selection Reportï¼ˆä½ è¦äº¤çµ¦ä¸»ç®¡/ç¾å ´çš„é‚£ä»½ï¼‰

å»ºè­°ä½ åœ¨æ¯æ¬¡é¸æ¨¡çµæŸå¾Œï¼Œç•™ä¸‹ä¸€ä»½ã€Œå¯è¢«å¯©æŸ¥ã€çš„äº¤ä»˜ï¼š
- **è³‡æ–™æè¿°**ï¼šä¾†æºã€æ™‚é–“ç¯„åœã€Batch/Campaign å®šç¾©ã€ç¼ºå€¼ç­–ç•¥  
- **é©—è­‰ç­–ç•¥**ï¼ši.i.d / Group / TimeSeriesï¼Œç‚ºä»€éº¼é€™æ¨£åˆ‡  
- **æ¨¡å‹å€™é¸**ï¼šBaselineï¼ˆä¾‹å¦‚å¸¸æ•¸/ç·šæ€§æ¨¡å‹ï¼‰ vs ä½ çš„æ¨¡å‹  
- **è©•ä¼°æŒ‡æ¨™**ï¼šå¹³å‡ Â± æ¨™æº–å·®ã€æ··æ·†çŸ©é™£ï¼ˆè‹¥åˆ†é¡ï¼‰ã€ä»¥åŠéŒ¯èª¤æˆæœ¬æˆ–é–€æª»è¨­å®š  
- **é¢¨éšªèˆ‡é™åˆ¶**ï¼šå¤–æ¨ç¯„åœã€è³‡æ–™æ¼‚ç§»å¯èƒ½ã€ä¸å¯ç”¨æƒ…å¢ƒ  


## 4. åŒ–å·¥å°ˆé¡Œè£œå……ï¼šåæ‡‰å™¨ç•°å¸¸åµæ¸¬ (Reactor Fault Detection)

é™¤äº† Titanic ç”Ÿå­˜é æ¸¬ï¼Œæ±ºç­–æ¨¹éå¸¸é©åˆç”¨æ–¼åŒ–å·¥è£½ç¨‹çš„ **ç•°å¸¸åµæ¸¬ (Fault Detection)**ã€‚

**æƒ…å¢ƒèˆ‡ç†è«–åˆ†æ**ï¼š
å‡è¨­æˆ‘å€‘æœ‰ä¸€å€‹é€£çºŒæ”ªæ‹Œæ§½åæ‡‰å™¨ (CSTR)ï¼Œç›£æ§è®Šæ•¸ç‚º **æº«åº¦ (T)** èˆ‡ **å£“åŠ› (P)**ã€‚
- ç•¶ $T > 180^\circ C$ æ™‚ï¼Œåæ‡‰å¤±æ§ã€‚
- ç•¶ $T > 160^\circ C$ ä¸” $P > 8 \text{ bar}$ æ™‚ï¼Œå£“åŠ›éå¤§å°è‡´æ´©æ¼é¢¨éšªã€‚

**æ±ºç­–é‚Šç•Œ (Decision Boundary)**ï¼š
æ±ºç­–æ¨¹çš„ä¸€å€‹é‡è¦å¹¾ä½•ç‰¹æ€§æ˜¯ï¼šå®ƒçš„æ±ºç­–é‚Šç•Œæ˜¯ç”±**å‚ç›´æ–¼ç‰¹å¾µè»¸çš„è¶…å¹³é¢ (Hyperplanes orthogonal to axes)** æ‰€çµ„æˆçš„ã€‚
é€™æ„å‘³è‘—æ±ºç­–æ¨¹éå¸¸æ“…é•·å­¸ç¿’åƒ `IF T > 180` é€™æ¨£çš„è¦å‰‡ï¼ˆå‚ç›´åˆ‡åˆ†ï¼‰ï¼Œé€™èˆ‡åŒ–å·¥æ“ä½œè¦ç¨‹ (SOP) çš„é‚è¼¯å®Œå…¨ä¸€è‡´ã€‚

ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¦‚æœæ˜¯é‚è¼¯æ–¯è¿´æ­¸ (Logistic Regression) æˆ– SVMï¼Œå®ƒå€‘å¯èƒ½æœƒç•«å‡ºä¸€æ¢æ–œç·šæˆ–æ›²ç·šï¼Œé›–ç„¶æ•¸å­¸ä¸Šå¯èƒ½æº–ç¢ºï¼Œä½†åœ¨è§£é‡‹çµ¦ç¾å ´æ“ä½œå“¡è½æ™‚ï¼ˆä¾‹å¦‚ï¼šã€Œç•¶ 0.5å€æº«åº¦ + 0.3å€å£“åŠ› > 100 æ™‚å±éšªã€ï¼‰ï¼Œç›´è§€æ€§è¼ƒå·®ã€‚

æˆ‘å€‘æ”¶é›†æ­·å²æ“ä½œæ•¸æ“šï¼Œæ¨™è¨˜ã€Œæ­£å¸¸ã€èˆ‡ã€Œç•°å¸¸ã€ï¼Œè¨“ç·´æ±ºç­–æ¨¹ä¾†å­¸ç¿’é€™å€‹ **å®‰å…¨æ“ä½œé‚Šç•Œ (Safety Boundary)**ã€‚æ±ºç­–æ¨¹ç•«å‡ºçš„é‚Šç•Œï¼Œå¯ä»¥ç›´æ¥å°æ‡‰åˆ°æ“ä½œè¦ç¨‹ä¸­çš„ SOP æ¢æ¬¾ï¼Œé€™åœ¨è£½ç¨‹å®‰å…¨ç®¡ç† (PSM) ä¸­éå¸¸æœ‰åƒ¹å€¼ã€‚

### å¯¦ä½œçµæœèˆ‡åœ–å½¢åˆ†æ

![Reactor Decision Boundary](P2_Unit06_Results/04_reactor_boundary.png)
*(åœ–ï¼šåæ‡‰å™¨ç•°å¸¸åµæ¸¬çš„æ±ºç­–é‚Šç•Œï¼Œç´…è‰²ç‚ºå±éšªå€ï¼Œè—è‰²ç‚ºå®‰å…¨å€)*

**åœ–å½¢è©³ç´°è§£æï¼š**

1.  **å¹¾ä½•ç‰¹æ€§**ï¼š
    - è«‹æ³¨æ„ç´…è‰²èˆ‡è—è‰²äº¤ç•Œçš„ç·šæ¢ï¼Œå®ƒå€‘å…¨éƒ¨éƒ½æ˜¯ **æ°´å¹³ç·š** æˆ– **å‚ç›´ç·š**ã€‚é€™æ˜¯æ±ºç­–æ¨¹çš„ç‰¹æ€§ï¼šå®ƒåªèƒ½åš `X > Value` é€™ç¨®å–®ä¸€è®Šæ•¸çš„åˆ‡åˆ†ã€‚
    - **å‚ç›´ç·š (T â‰ˆ 180)**ï¼šå°æ‡‰åˆ°æˆ‘å€‘è¨­å®šçš„è¦å‰‡ `T > 180`ã€‚æ¨¡å‹æˆåŠŸæŠ“åˆ°äº†é€™å€‹ä¸»è¦çš„é«˜æº«é™åˆ¶ã€‚
    - **Lå‹ç¼ºå£ (T > 160 & P > 8)**ï¼šåœ¨åœ–çš„å³ä¸Šæ–¹ï¼ˆè—è‰²å®‰å…¨å€çš„å³ä¸Šè§’ï¼‰ï¼Œå¯ä»¥çœ‹åˆ°ä¸€å€‹æ˜é¡¯çš„çŸ©å½¢ç¼ºå£è¢«ç´…è‰²å±éšªå€ä½”æ“šã€‚é€™æ˜¯æ¨¡å‹å­¸æœƒäº†ç¬¬äºŒæ¢è¦å‰‡ï¼šå³ä½¿æº«åº¦æ²’åˆ° 180 (åœ¨ 160~180 ä¹‹é–“)ï¼Œå¦‚æœå£“åŠ›éé«˜ (> 8)ï¼Œä¾ç„¶æ˜¯å±éšªçš„ã€‚

2.  **åŒ–å·¥æ„ç¾©**ï¼š
    - é€™å¼µåœ–å°±æ˜¯ä¸€å¼µ **æ“ä½œè¦–çª— (Operating Window)** åœ–ã€‚
    - é€éæ©Ÿå™¨å­¸ç¿’ï¼Œæˆ‘å€‘ä¸éœ€è¦äººå·¥å»ç•«é€™äº›ç·šï¼Œæ¨¡å‹æœƒè‡ªå‹•å¾æ­·å²æ•¸æ“šä¸­æ‰¾å‡ºé€™äº›å®‰å…¨é‚Šç•Œã€‚å¦‚æœæœªä¾†æœ‰æ–°çš„æ“ä½œé»è½åœ¨ç´…è‰²å€åŸŸï¼Œç³»çµ±å°±å¯ä»¥ç™¼å‡ºè­¦å ±ã€‚

---


## 5. èª²å¾Œç·´ç¿’ï¼šæµé«”æµå‹•æ¨¡å¼åˆ†é¡ (Flow Regime Classification)

ç‚ºäº†è®“åŒå­¸ç·´ç¿’ **å¤šé¡åˆ¥åˆ†é¡ (Multi-class Classification)**ï¼Œæˆ‘å€‘æº–å‚™äº†ä¸€å€‹æµé«”åŠ›å­¸çš„æ¡ˆä¾‹ã€‚

**èƒŒæ™¯**ï¼š
åœ¨åŒ–å·¥ç®¡è·¯è¨­è¨ˆä¸­ï¼Œæ°£æ¶²é›™ç›¸æµ (Gas-Liquid Two-Phase Flow) çš„æµå‹•æ¨¡å¼å°æ–¼å£“é™è¨ˆç®—èˆ‡ç†±å‚³æ•ˆç‡è‡³é—œé‡è¦ã€‚å¸¸è¦‹çš„æ¨¡å¼åŒ…å«ï¼š
- **Bubble Flow (æ³¡ç‹€æµ)**ï¼šæ¶²é«”ç‚ºä¸»ï¼Œæ°£é«”ä»¥å°æ°£æ³¡åˆ†æ•£å…¶ä¸­ã€‚
- **Slug Flow (æ “ç‹€æµ)**ï¼šæ°£æ³¡èšé›†æˆå¤§æ°£å¡ï¼Œæµå‹•ä¸ç©©å®šã€‚
- **Annular Flow (ç’°ç‹€æµ)**ï¼šæ°£é«”åœ¨ä¸­å¿ƒé«˜é€Ÿæµå‹•ï¼Œæ¶²é«”æ²¿ç®¡å£æˆè†œç‹€ã€‚

**ä»»å‹™**ï¼š
åœ¨ Notebook çš„æœ€å¾Œï¼Œæˆ‘å€‘æä¾›äº†ä¸€çµ„æ¨¡æ“¬æ•¸æ“šï¼ŒåŒ…å«æ°£é«”è¡¨è§€æµé€Ÿ ($V_{sg}$) èˆ‡æ¶²é«”è¡¨è§€æµé€Ÿ ($V_{sl}$)ã€‚
è«‹å˜—è©¦ï¼š
1.  å»ºç«‹æ±ºç­–æ¨¹æ¨¡å‹ã€‚
2.  ç•«å‡ºæ±ºç­–é‚Šç•Œåœ–ï¼Œè§€å¯Ÿæ¨¡å‹å¦‚ä½•å€åˆ†é€™ä¸‰ç¨®æµå‹•æ¨¡å¼ã€‚

> **æç¤º**ï¼šé€™èˆ‡åæ‡‰å™¨ç•°å¸¸åµæ¸¬éå¸¸åƒï¼Œåªæ˜¯æ¨™ç±¤ $y$ è®Šæˆäº† 0, 1, 2 ä¸‰ç¨®ã€‚

---

**[Next Unit]**  
æ¥ä¸‹ä¾†åœ¨ **Unit 07**ï¼Œæˆ‘å€‘å°‡é€²å…¥åŒ–å·¥æ ¸å¿ƒæ‡‰ç”¨ï¼šä½¿ç”¨ **Scipy èˆ‡ Regression** æŠ€è¡“ä¾†è§£æ±º **ç†±åŠ›å­¸åƒæ•¸æ“¬åˆ (Thermodynamic Fitting)** çš„å•é¡Œã€‚

---

## æœ¬å ‚èª²ç¨‹å¼æ¼”ç·´

è«‹é–‹å•Ÿä¸¦å®Œæˆï¼š`Part_2/Unit06_CV_Model_Selection.ipynb`  
