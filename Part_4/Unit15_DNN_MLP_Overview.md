# Unit 15: æ·±åº¦ç¥ç¶“ç¶²è·¯(DNN)èˆ‡å¤šå±¤æ„ŸçŸ¥æ©Ÿ(MLP)æ¦‚è¿°

## èª²ç¨‹ç›®æ¨™
- ç†è§£æ·±åº¦ç¥ç¶“ç¶²è·¯(DNN)èˆ‡å¤šå±¤æ„ŸçŸ¥æ©Ÿ(MLP)çš„åŸºæœ¬æ¦‚å¿µèˆ‡æ•¸å­¸åŸç†
- å­¸æœƒä½¿ç”¨TensorFlow/Keraså»ºç«‹ã€è¨“ç·´ã€è©•ä¼°DNNæ¨¡å‹
- æŒæ¡æ¨¡å‹å„ªåŒ–æŠ€å·§èˆ‡è¶…åƒæ•¸èª¿æ•´æ–¹æ³•
- äº†è§£DNNåœ¨åŒ–å·¥é ˜åŸŸçš„æ‡‰ç”¨å ´æ™¯

---

## 1. DNNèˆ‡MLPåŸºç¤ç†è«–

### 1.1 ä»€éº¼æ˜¯ç¥ç¶“ç¶²è·¯?

**äººå·¥ç¥ç¶“ç¶²è·¯(Artificial Neural Network, ANN)** æ˜¯ä¸€ç¨®å—ç”Ÿç‰©ç¥ç¶“ç³»çµ±å•Ÿç™¼çš„æ©Ÿå™¨å­¸ç¿’æ¨¡å‹ã€‚å®ƒé€éæ¨¡æ“¬ç¥ç¶“å…ƒä¹‹é–“çš„é€£æ¥èˆ‡è¨Šè™Ÿå‚³éï¼Œä¾†å­¸ç¿’è¼¸å…¥èˆ‡è¼¸å‡ºä¹‹é–“çš„è¤‡é›œé—œä¿‚ã€‚

**å¤šå±¤æ„ŸçŸ¥æ©Ÿ(Multi-Layer Perceptron, MLP)** æ˜¯æœ€åŸºæœ¬çš„å‰é¥‹å¼ç¥ç¶“ç¶²è·¯(Feedforward Neural Network)ï¼Œç”±å¤šå±¤ç¥ç¶“å…ƒçµ„æˆ:
- **è¼¸å…¥å±¤(Input Layer)**: æ¥æ”¶åŸå§‹ç‰¹å¾µæ•¸æ“š
- **éš±è—å±¤(Hidden Layers)**: é€²è¡Œç‰¹å¾µè½‰æ›èˆ‡å­¸ç¿’
- **è¼¸å‡ºå±¤(Output Layer)**: ç”¢ç”Ÿæœ€çµ‚é æ¸¬çµæœ

**æ·±åº¦ç¥ç¶“ç¶²è·¯(Deep Neural Network, DNN)** æ˜¯æŒ‡å…·æœ‰**å¤šå€‹éš±è—å±¤**çš„ç¥ç¶“ç¶²è·¯ã€‚ç•¶éš±è—å±¤æ•¸é‡å¢åŠ æ™‚ï¼Œç¶²è·¯èƒ½å¤ å­¸ç¿’æ›´è¤‡é›œã€æ›´æŠ½è±¡çš„ç‰¹å¾µè¡¨ç¤ºã€‚

### 1.2 æ­·å²ç™¼å±•

- **1943**: McCulloch & Pitts æå‡ºç¬¬ä¸€å€‹ç¥ç¶“å…ƒæ•¸å­¸æ¨¡å‹
- **1958**: Rosenblatt ç™¼æ˜æ„ŸçŸ¥æ©Ÿ(Perceptron)
- **1986**: Rumelhart ç­‰äººæå‡ºåå‘å‚³æ’­æ¼”ç®—æ³•(Backpropagation)
- **2006**: Hinton æå‡ºæ·±åº¦å­¸ç¿’(Deep Learning)æ¦‚å¿µ
- **2012**: AlexNet åœ¨ImageNetç«¶è³½ä¸­å¤§æ”¾ç•°å½©ï¼Œé–‹å•Ÿæ·±åº¦å­¸ç¿’æ™‚ä»£

### 1.3 ç¥ç¶“å…ƒæ•¸å­¸æ¨¡å‹

å–®ä¸€ç¥ç¶“å…ƒçš„é‹ç®—å¯è¡¨ç¤ºç‚º:

$$
z = \sum_{i=1}^{n} w_i x_i + b = \mathbf{w}^T \mathbf{x} + b
$$

$$
a = f(z)
$$

å…¶ä¸­:
- $x_i$ : è¼¸å…¥ç‰¹å¾µ
- $w_i$ : æ¬Šé‡(weight)
- $b$ : åå·®(bias)
- $z$ : åŠ æ¬Šç¸½å’Œ
- $f$ : æ¿€æ´»å‡½æ•¸(activation function)
- $a$ : ç¥ç¶“å…ƒè¼¸å‡º(activation)

### 1.4 å‰å‘å‚³æ’­(Forward Propagation)

å°æ–¼ä¸€å€‹å…·æœ‰ $L$ å±¤çš„ç¥ç¶“ç¶²è·¯ï¼Œå‰å‘å‚³æ’­éç¨‹ç‚º:

**ç¬¬ä¸€å±¤(è¼¸å…¥å±¤åˆ°ç¬¬ä¸€å€‹éš±è—å±¤)**:
$$
\mathbf{z}^{[1]} = \mathbf{W}^{[1]} \mathbf{x} + \mathbf{b}^{[1]}
$$
$$
\mathbf{a}^{[1]} = f^{[1]}(\mathbf{z}^{[1]})
$$

**ç¬¬ $l$ å±¤(ä¸€èˆ¬åŒ–)**:
$$
\mathbf{z}^{[l]} = \mathbf{W}^{[l]} \mathbf{a}^{[l-1]} + \mathbf{b}^{[l]}
$$
$$
\mathbf{a}^{[l]} = f^{[l]}(\mathbf{z}^{[l]})
$$

**è¼¸å‡ºå±¤**:
$$
\hat{y} = \mathbf{a}^{[L]}
$$

å…¶ä¸­:
- $\mathbf{W}^{[l]}$ : ç¬¬ $l$ å±¤çš„æ¬Šé‡çŸ©é™£
- $\mathbf{b}^{[l]}$ : ç¬¬ $l$ å±¤çš„åå·®å‘é‡
- $f^{[l]}$ : ç¬¬ $l$ å±¤çš„æ¿€æ´»å‡½æ•¸

### 1.5 æå¤±å‡½æ•¸(Loss Function)

æå¤±å‡½æ•¸ç”¨æ–¼è¡¡é‡æ¨¡å‹é æ¸¬å€¼èˆ‡çœŸå¯¦å€¼ä¹‹é–“çš„å·®ç•°:

**å›æ­¸å•é¡Œå¸¸ç”¨æå¤±å‡½æ•¸**:

1. **å‡æ–¹èª¤å·®(Mean Squared Error, MSE)**:
$$
L_{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

**é©ç”¨å ´æ™¯**:
- é æ¸¬é€£çºŒæ•¸å€¼çš„å›æ­¸å•é¡Œ
- å°å¤§èª¤å·®æ•æ„Ÿ,å› ç‚ºèª¤å·®è¢«å¹³æ–¹æ”¾å¤§
- é©åˆç›®æ¨™è®Šæ•¸åˆ†å¸ƒè¼ƒç‚ºå‡å‹»çš„æƒ…æ³

**å„ªé»**: æ•¸å­¸æ€§è³ªè‰¯å¥½,å¯å¾®åˆ†,æ¢¯åº¦è¨ˆç®—ç°¡å–®  
**ç¼ºé»**: å°ç•°å¸¸å€¼éå¸¸æ•æ„Ÿ

2. **å¹³å‡çµ•å°èª¤å·®(Mean Absolute Error, MAE)**:
$$
L_{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
$$

**é©ç”¨å ´æ™¯**:
- å›æ­¸å•é¡Œ,ç‰¹åˆ¥æ˜¯å­˜åœ¨ç•°å¸¸å€¼çš„æ•¸æ“š
- å°æ‰€æœ‰èª¤å·®ä¸€è¦–åŒä»(ä¸æœƒæ”¾å¤§å¤§èª¤å·®)
- é©åˆéœ€è¦æ›´ç©©å¥(robust)æ¨¡å‹çš„å ´æ™¯

**å„ªé»**: å°ç•°å¸¸å€¼ä¸æ•æ„Ÿ  
**ç¼ºé»**: åœ¨0é»ä¸å¯å¾®,å„ªåŒ–è¼ƒå›°é›£

3. **Huber Loss**:
$$
L_{\delta}(y, \hat{y}) = \begin{cases}
\frac{1}{2}(y - \hat{y})^2 & \text{if } |y - \hat{y}| \leq \delta \\
\delta |y - \hat{y}| - \frac{1}{2}\delta^2 & \text{otherwise}
\end{cases}
$$

**é©ç”¨å ´æ™¯**:
- çµåˆMSEå’ŒMAEçš„å„ªé»
- å°èª¤å·®æ™‚ä½¿ç”¨MSE(å¹³æ»‘),å¤§èª¤å·®æ™‚ä½¿ç”¨MAE(ç©©å¥)
- é©åˆå·¥æ¥­æ•¸æ“šä¸­å«æœ‰å™ªéŸ³å’Œç•°å¸¸å€¼çš„æƒ…æ³

**åˆ†é¡å•é¡Œå¸¸ç”¨æå¤±å‡½æ•¸**:

1. **äºŒå…ƒäº¤å‰ç†µ(Binary Crossentropy)**:
$$
L_{BCE} = -\frac{1}{n} \sum_{i=1}^{n} [y_i \log(\hat{y}_i) + (1-y_i) \log(1-\hat{y}_i)]
$$

**é©ç”¨å ´æ™¯**:
- **äºŒå…ƒåˆ†é¡å•é¡Œ** (å¦‚:æ˜¯/å¦ã€æ­£å¸¸/ç•°å¸¸)
- è¼¸å‡ºå±¤ä½¿ç”¨Sigmoidæ¿€æ´»å‡½æ•¸
- æ¨™ç±¤ç‚º0æˆ–1

**ç¯„ä¾‹**: åŒ–å·¥è¨­å‚™æ•…éšœé æ¸¬ã€ç”¢å“åˆæ ¼èˆ‡å¦åˆ¤å®š

2. **é¡åˆ¥äº¤å‰ç†µ(Categorical Crossentropy)**:
$$
L_{CCE} = -\frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{C} y_{ij} \log(\hat{y}_{ij})
$$

**é©ç”¨å ´æ™¯**:
- **å¤šé¡åˆ¥åˆ†é¡å•é¡Œ** (C > 2)
- è¼¸å‡ºå±¤ä½¿ç”¨Softmaxæ¿€æ´»å‡½æ•¸
- æ¨™ç±¤ç‚ºone-hotç·¨ç¢¼æ ¼å¼

**ç¯„ä¾‹**: ç”¢å“å“è³ªç­‰ç´šåˆ†é¡(A/B/Cç´š)ã€åŒ–å­¸åæ‡‰é¡å‹è­˜åˆ¥

3. **ç¨€ç–é¡åˆ¥äº¤å‰ç†µ(Sparse Categorical Crossentropy)**:
$$
L_{SCCE} = -\frac{1}{n} \sum_{i=1}^{n} \log(\hat{y}_{i,c_i})
$$

å…¶ä¸­ $c_i$ æ˜¯ç¬¬ $i$ å€‹æ¨£æœ¬çš„çœŸå¯¦é¡åˆ¥ç´¢å¼•ã€‚

**é©ç”¨å ´æ™¯**:
- **å¤šé¡åˆ¥åˆ†é¡å•é¡Œ**
- æ¨™ç±¤ç‚ºæ•´æ•¸æ ¼å¼(0, 1, 2, ..., C-1),è€Œéone-hotç·¨ç¢¼
- ç¯€çœè¨˜æ†¶é«”,é©åˆé¡åˆ¥æ•¸é‡å¾ˆå¤šçš„æƒ…æ³

### æå¤±å‡½æ•¸é¸æ“‡æŒ‡å—

| å•é¡Œé¡å‹ | æ¨è–¦æå¤±å‡½æ•¸ | è¼¸å‡ºå±¤æ¿€æ´»å‡½æ•¸ |
|---------|------------|---------------|
| å›æ­¸(ä¸€èˆ¬) | MSE | Linear / ä¸æŒ‡å®š |
| å›æ­¸(æœ‰ç•°å¸¸å€¼) | MAE æˆ– Huber | Linear / ä¸æŒ‡å®š |
| äºŒå…ƒåˆ†é¡ | Binary Crossentropy | Sigmoid |
| å¤šé¡åˆ¥åˆ†é¡(one-hot) | Categorical Crossentropy | Softmax |
| å¤šé¡åˆ¥åˆ†é¡(æ•´æ•¸æ¨™ç±¤) | Sparse Categorical Crossentropy | Softmax |


### 1.6 åå‘å‚³æ’­(Backpropagation)

åå‘å‚³æ’­æ¼”ç®—æ³•ç”¨æ–¼è¨ˆç®—æå¤±å‡½æ•¸å°æ¯å€‹åƒæ•¸çš„æ¢¯åº¦ï¼Œä½¿ç”¨**éˆå¼æ³•å‰‡(Chain Rule)**å¾è¼¸å‡ºå±¤å¾€å›è¨ˆç®—:

$$
\frac{\partial L}{\partial \mathbf{W}^{[l]}} = \frac{\partial L}{\partial \mathbf{z}^{[l]}} \cdot \frac{\partial \mathbf{z}^{[l]}}{\partial \mathbf{W}^{[l]}} = \delta^{[l]} \cdot (\mathbf{a}^{[l-1]})^T
$$

$$
\frac{\partial L}{\partial \mathbf{b}^{[l]}} = \delta^{[l]}
$$

å…¶ä¸­ $\delta^{[l]}$ æ˜¯ç¬¬ $l$ å±¤çš„èª¤å·®é …ã€‚

### 1.7 æ¢¯åº¦ä¸‹é™èˆ‡åƒæ•¸æ›´æ–°

ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•æ›´æ–°åƒæ•¸:

$$
\mathbf{W}^{[l]} := \mathbf{W}^{[l]} - \alpha \frac{\partial L}{\partial \mathbf{W}^{[l]}}
$$

$$
\mathbf{b}^{[l]} := \mathbf{b}^{[l]} - \alpha \frac{\partial L}{\partial \mathbf{b}^{[l]}}
$$

å…¶ä¸­ $\alpha$ æ˜¯å­¸ç¿’ç‡(learning rate)ã€‚

---

## 2. æ¿€æ´»å‡½æ•¸(Activation Functions)

æ¿€æ´»å‡½æ•¸ç‚ºç¥ç¶“ç¶²è·¯å¼•å…¥éç·šæ€§ï¼Œä½¿å…¶èƒ½å¤ å­¸ç¿’è¤‡é›œçš„å‡½æ•¸é—œä¿‚ã€‚

### 2.1 å¸¸ç”¨æ¿€æ´»å‡½æ•¸

#### 2.1.1 ReLU (Rectified Linear Unit)
$$
f(x) = \max(0, x) = \begin{cases} x & \text{if } x > 0 \\ 0 & \text{if } x \leq 0 \end{cases}
$$

**å„ªé»**:
- è¨ˆç®—ç°¡å–®ã€é€Ÿåº¦å¿«
- æœ‰æ•ˆç·©è§£æ¢¯åº¦æ¶ˆå¤±å•é¡Œ
- ä½¿ç¶²è·¯å…·æœ‰ç¨€ç–æ€§

**ç¼ºé»**:
- å¯èƒ½å‡ºç¾"ç¥ç¶“å…ƒæ­»äº¡"å•é¡Œ(dying ReLU)

**é©ç”¨å ´æ™¯**: éš±è—å±¤çš„é¦–é¸æ¿€æ´»å‡½æ•¸

#### 2.1.2 Leaky ReLU
$$
f(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha x & \text{if } x \leq 0 \end{cases}
$$

å…¶ä¸­ $\alpha$ é€šå¸¸è¨­ç‚º 0.01ã€‚

**å„ªé»**: è§£æ±ºReLUçš„ç¥ç¶“å…ƒæ­»äº¡å•é¡Œ

#### 2.1.3 Sigmoid
$$
f(x) = \frac{1}{1 + e^{-x}}
$$

**ç‰¹æ€§**:
- è¼¸å‡ºç¯„åœ: (0, 1)
- å¯è§£é‡‹ç‚ºæ©Ÿç‡

**ç¼ºé»**:
- å®¹æ˜“å‡ºç¾æ¢¯åº¦æ¶ˆå¤±
- è¼¸å‡ºä¸æ˜¯ä»¥é›¶ç‚ºä¸­å¿ƒ

**é©ç”¨å ´æ™¯**: äºŒå…ƒåˆ†é¡çš„è¼¸å‡ºå±¤

#### 2.1.4 Tanh (é›™æ›²æ­£åˆ‡)
$$
f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

**ç‰¹æ€§**:
- è¼¸å‡ºç¯„åœ: (-1, 1)
- ä»¥é›¶ç‚ºä¸­å¿ƒ

**ç¼ºé»**: ä»æœ‰æ¢¯åº¦æ¶ˆå¤±å•é¡Œ

**é©ç”¨å ´æ™¯**: éš±è—å±¤(ä½†ReLUé€šå¸¸æ›´å¥½)

#### 2.1.5 Softmax
$$
f(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{C} e^{x_j}}
$$

**ç‰¹æ€§**:
- è¼¸å‡ºç¸½å’Œç‚º1
- å¯è§£é‡‹ç‚ºæ©Ÿç‡åˆ†å¸ƒ

**é©ç”¨å ´æ™¯**: å¤šé¡åˆ¥åˆ†é¡çš„è¼¸å‡ºå±¤

#### 2.1.6 Linear (ç·šæ€§)
$$
f(x) = x
$$

**é©ç”¨å ´æ™¯**: å›æ­¸å•é¡Œçš„è¼¸å‡ºå±¤

### 2.2 æ¿€æ´»å‡½æ•¸é¸æ“‡æŒ‡å—

| å±¤é¡å‹ | å•é¡Œé¡å‹ | æ¨è–¦æ¿€æ´»å‡½æ•¸ |
|--------|----------|--------------|
| éš±è—å±¤ | ä¸€èˆ¬æƒ…æ³ | ReLU |
| éš±è—å±¤ | é¿å…dying ReLU | Leaky ReLU |
| è¼¸å‡ºå±¤ | äºŒå…ƒåˆ†é¡ | Sigmoid |
| è¼¸å‡ºå±¤ | å¤šé¡åˆ¥åˆ†é¡ | Softmax |
| è¼¸å‡ºå±¤ | å›æ­¸ | Linear (æˆ–ä¸æŒ‡å®š) |

---

## 3. DNN/MLPæ‡‰ç”¨å ´æ™¯

### 3.1 é©åˆä½¿ç”¨DNN/MLPçš„æƒ…å¢ƒ

1. **éç·šæ€§é—œä¿‚è¤‡é›œ**: è¼¸å…¥èˆ‡è¼¸å‡ºä¹‹é–“å­˜åœ¨é«˜åº¦éç·šæ€§é—œä¿‚
2. **ç‰¹å¾µäº¤äº’ä½œç”¨**: ç‰¹å¾µä¹‹é–“æœ‰è¤‡é›œçš„äº¤äº’ä½œç”¨
3. **å¤§é‡æ•¸æ“š**: æœ‰è¶³å¤ çš„è¨“ç·´æ•¸æ“šæ”¯æŒæ·±åº¦æ¨¡å‹
4. **ç‰¹å¾µå·¥ç¨‹å›°é›£**: é›£ä»¥æ‰‹å‹•è¨­è¨ˆæœ‰æ•ˆç‰¹å¾µæ™‚ï¼ŒDNNå¯è‡ªå‹•å­¸ç¿’

### 3.2 åŒ–å·¥é ˜åŸŸæ‡‰ç”¨æ¡ˆä¾‹

#### 3.2.1 è£½ç¨‹åƒæ•¸å„ªåŒ–
- **æ‡‰ç”¨**: é æ¸¬åæ‡‰å™¨æº«åº¦ã€å£“åŠ›ã€æµé‡ç­‰æ“ä½œæ¢ä»¶å°ç”¢å“å“è³ªçš„å½±éŸ¿
- **å„ªå‹¢**: å¯è™•ç†å¤šè®Šæ•¸ã€éç·šæ€§çš„è£½ç¨‹é—œä¿‚

#### 3.2.2 ç”¢å“å“è³ªé æ¸¬
- **æ‡‰ç”¨**: æ ¹æ“šåŸæ–™æˆåˆ†èˆ‡è£½ç¨‹æ¢ä»¶é æ¸¬æœ€çµ‚ç”¢å“æ€§è³ª
- **ç¯„ä¾‹**: ç´…é…’å“è³ªé æ¸¬ã€èšåˆç‰©æ€§è³ªé æ¸¬

#### 3.2.3 è¨­å‚™æ•…éšœè¨ºæ–·
- **æ‡‰ç”¨**: é€éæ„Ÿæ¸¬å™¨æ•¸æ“šé æ¸¬è¨­å‚™ç•°å¸¸æˆ–æ•…éšœ
- **å„ªå‹¢**: å¯å­¸ç¿’è¤‡é›œçš„æ™‚é–“åºåˆ—æ¨¡å¼

#### 3.2.4 åˆ†é›¢ç¨‹åºæ¨¡æ“¬
- **æ‡‰ç”¨**: è’¸é¤¾å¡”ã€èƒå–å¡”ç­‰åˆ†é›¢è¨­å‚™çš„å¿«é€Ÿæ¨¡æ“¬
- **å„ªå‹¢**: æ¯”å‚³çµ±æ•¸å€¼æ¨¡æ“¬å¿«é€Ÿï¼Œé©åˆå³æ™‚æ§åˆ¶

#### 3.2.5 ç’°å¢ƒæ’æ”¾é æ¸¬
- **æ‡‰ç”¨**: é æ¸¬ç‡ƒç‡’ç¨‹åºçš„æ±¡æŸ“ç‰©æ’æ”¾é‡
- **ç¯„ä¾‹**: NOxã€SOxã€CO2æ’æ”¾é æ¸¬

#### 3.2.6 ç¤¦æ¥­æµ®é¸éç¨‹
- **æ‡‰ç”¨**: é æ¸¬ç¤¦çŸ³æµ®é¸éç¨‹çš„çŸ½çŸ³æ¿ƒåº¦
- **å„ªå‹¢**: å¯æ•´åˆå¤šç¨®æ„Ÿæ¸¬å™¨æ•¸æ“šé€²è¡Œå³æ™‚é æ¸¬

### 3.3 DNNçš„å„ªå‹¢èˆ‡é™åˆ¶

**å„ªå‹¢**:
- å¼·å¤§çš„éç·šæ€§å»ºæ¨¡èƒ½åŠ›
- è‡ªå‹•ç‰¹å¾µå­¸ç¿’
- å¯è™•ç†é«˜ç¶­åº¦æ•¸æ“š
- æ“´å±•æ€§å¥½

**é™åˆ¶**:
- éœ€è¦å¤§é‡è¨“ç·´æ•¸æ“š
- è¨ˆç®—è³‡æºéœ€æ±‚é«˜
- æ¨¡å‹å¯è§£é‡‹æ€§è¼ƒå·®(é»‘ç›’æ¨¡å‹)
- å®¹æ˜“éæ“¬åˆ
- è¶…åƒæ•¸èª¿æ•´è¤‡é›œ

---

## 4. TensorFlow/Kerasæ¡†æ¶ä»‹ç´¹

### 4.1 TensorFlowèˆ‡Kerasç°¡ä»‹

**TensorFlow** æ˜¯Googleé–‹ç™¼çš„é–‹æºæ·±åº¦å­¸ç¿’æ¡†æ¶ï¼Œæä¾›å®Œæ•´çš„æ©Ÿå™¨å­¸ç¿’ç”Ÿæ…‹ç³»çµ±ã€‚

**Keras** æ˜¯é«˜éšç¥ç¶“ç¶²è·¯APIï¼Œç¾å·²æ•´åˆé€²TensorFlow 2.x (tf.keras)ï¼Œæä¾›:
- ç°¡æ½”æ˜“ç”¨çš„ä»‹é¢
- æ¨¡çµ„åŒ–è¨­è¨ˆ
- æ˜“æ–¼æ“´å±•
- æ”¯æ´å¤šç¨®å¾Œç«¯

### 4.2 ç’°å¢ƒå®‰è£

```bash
# å®‰è£TensorFlow (åŒ…å«Keras)
pip install tensorflow

# æˆ–å®‰è£ç‰¹å®šç‰ˆæœ¬
pip install tensorflow==2.15.0

# é©—è­‰å®‰è£
python -c "import tensorflow as tf; print(tf.__version__)"
```

### 4.3 åŸºæœ¬å°å…¥

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
import numpy as np
import matplotlib.pyplot as plt
```

---

## 5. ä½¿ç”¨Keraså»ºç«‹DNNæ¨¡å‹

### 5.1 æ¨¡å‹æ¶æ§‹: Sequential vs Functional API

Kerasæä¾›å…©ç¨®å»ºç«‹æ¨¡å‹çš„æ–¹å¼:

#### 5.1.1 Sequential API (åºåˆ—æ¨¡å‹)

é©ç”¨æ–¼**å–®è¼¸å…¥ã€å–®è¼¸å‡ºã€ç·šæ€§å †ç–Š**çš„ç°¡å–®æ¨¡å‹:

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# å»ºç«‹Sequentialæ¨¡å‹
model = Sequential([
    Dense(64, activation='relu', input_shape=(10,)),
    Dense(32, activation='relu'),
    Dense(1)
])

# æˆ–ä½¿ç”¨addæ–¹æ³•é€å±¤æ·»åŠ 
model = Sequential()
model.add(Dense(64, activation='relu', input_shape=(10,)))
model.add(Dense(32, activation='relu'))
model.add(Dense(1))
```

#### 5.1.2 Functional API (å‡½æ•¸å¼API)

é©ç”¨æ–¼**å¤šè¼¸å…¥ã€å¤šè¼¸å‡ºã€è¤‡é›œé€£æ¥**çš„æ¨¡å‹:

```python
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense

# å®šç¾©è¼¸å…¥
inputs = Input(shape=(10,))

# å®šç¾©éš±è—å±¤
x = Dense(64, activation='relu')(inputs)
x = Dense(32, activation='relu')(x)

# å®šç¾©è¼¸å‡º
outputs = Dense(1)(x)

# å»ºç«‹æ¨¡å‹
model = Model(inputs=inputs, outputs=outputs)
```

### 5.2 å¸¸ç”¨å±¤(Layers)

#### 5.2.1 Dense Layer (å…¨é€£æ¥å±¤)

**åŠŸèƒ½**: å¯¦ç¾å…¨é€£æ¥çš„ç¥ç¶“ç¶²è·¯å±¤

```python
from tensorflow.keras.layers import Dense

layer = Dense(
    units=64,              # ç¥ç¶“å…ƒæ•¸é‡
    activation='relu',     # æ¿€æ´»å‡½æ•¸
    use_bias=True,         # æ˜¯å¦ä½¿ç”¨åå·®
    kernel_initializer='glorot_uniform',  # æ¬Šé‡åˆå§‹åŒ–æ–¹æ³•
    bias_initializer='zeros',             # åå·®åˆå§‹åŒ–æ–¹æ³•
    kernel_regularizer=None,              # æ¬Šé‡æ­£å‰‡åŒ–
    bias_regularizer=None,                # åå·®æ­£å‰‡åŒ–
    activity_regularizer=None             # è¼¸å‡ºæ­£å‰‡åŒ–
)
```

**åƒæ•¸èªªæ˜**:
- `units`: è©²å±¤ç¥ç¶“å…ƒæ•¸é‡
- `activation`: æ¿€æ´»å‡½æ•¸ ('relu', 'sigmoid', 'tanh', 'softmax', 'linear', None)
- `kernel_initializer`: æ¬Šé‡åˆå§‹åŒ–ç­–ç•¥

#### 5.2.2 Dropout Layer (éš¨æ©Ÿå¤±æ´»å±¤)

**åŠŸèƒ½**: è¨“ç·´æ™‚éš¨æ©Ÿå°‡éƒ¨åˆ†ç¥ç¶“å…ƒè¼¸å‡ºè¨­ç‚º0ï¼Œé˜²æ­¢éæ“¬åˆ

```python
from tensorflow.keras.layers import Dropout

layer = Dropout(rate=0.5)  # å¤±æ´»æ¯”ä¾‹
```

**ä½¿ç”¨æ™‚æ©Ÿ**:
- æ¨¡å‹å‡ºç¾éæ“¬åˆæ™‚
- é€šå¸¸æ”¾åœ¨Denseå±¤ä¹‹å¾Œ
- å…¸å‹dropout rate: 0.2 ~ 0.5

**ç¯„ä¾‹**:
```python
model = Sequential([
    Dense(128, activation='relu', input_shape=(10,)),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(1)
])
```

#### 5.2.3 BatchNormalization Layer (æ‰¹æ¬¡æ­£è¦åŒ–å±¤)

**åŠŸèƒ½**: å°æ¯å€‹batchçš„è¼¸å…¥é€²è¡Œæ¨™æº–åŒ–ï¼ŒåŠ é€Ÿè¨“ç·´ä¸¦æé«˜ç©©å®šæ€§

```python
from tensorflow.keras.layers import BatchNormalization

layer = BatchNormalization()
```

**å„ªé»**:
- åŠ å¿«è¨“ç·´é€Ÿåº¦
- å…è¨±ä½¿ç”¨æ›´é«˜çš„å­¸ç¿’ç‡
- æ¸›å°‘å°åˆå§‹åŒ–çš„æ•æ„Ÿåº¦
- å…·æœ‰è¼•å¾®çš„æ­£å‰‡åŒ–æ•ˆæœ
- ç·©è§£æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸å•é¡Œ

**é©ç”¨æ™‚æ©Ÿèˆ‡å•é¡Œç¨®é¡**:

1. **æ·±åº¦ç¶²è·¯** (å±¤æ•¸ > 10å±¤):
   - BatchNormå¹«åŠ©æ¢¯åº¦åœ¨æ·±å±¤ç¶²è·¯ä¸­é †åˆ©å‚³æ’­
   - ç‰¹åˆ¥é©åˆåœ–åƒè­˜åˆ¥ã€è‡ªç„¶èªè¨€è™•ç†ç­‰è¤‡é›œä»»å‹™

2. **è¨“ç·´ä¸ç©©å®š**:
   - æå¤±æ›²ç·šæ³¢å‹•åŠ‡çƒˆ
   - æ¢¯åº¦çˆ†ç‚¸æˆ–æ¶ˆå¤±
   - å°å­¸ç¿’ç‡éæ–¼æ•æ„Ÿ

3. **éœ€è¦æ›´å¿«æ”¶æ–‚**:
   - è¨“ç·´æ™‚é–“å—é™çš„å ´æ™¯
   - å¤§è¦æ¨¡æ•¸æ“šé›†

**ä½¿ç”¨ä½ç½®**: é€šå¸¸æ”¾åœ¨Denseå±¤èˆ‡æ¿€æ´»å‡½æ•¸ä¹‹é–“

```python
model = Sequential([
    Dense(128, input_shape=(10,)),
    BatchNormalization(),
    Activation('relu'),
    Dense(64),
    BatchNormalization(),
    Activation('relu'),
    Dense(1)
])
```

### âš ï¸ å·¥æ¥­æ•¸æ“šå›æ­¸ä»»å‹™ä¸­çš„BatchNormalizationä½¿ç”¨å»ºè­°

**å•é¡Œ**: å¦‚æœè¼¸å…¥æ•¸æ“šå·²ç¶“ä½¿ç”¨StandardScaleré€²è¡Œæ¨™æº–åŒ–,æ˜¯å¦é‚„éœ€è¦BatchNormalization?

**ç­”æ¡ˆ**: **è¦–æƒ…æ³è€Œå®š**

#### æƒ…æ³1: æ·ºå±¤ç¶²è·¯ (â‰¤ 3-4å±¤) + å·²æ¨™æº–åŒ–æ•¸æ“š
**å»ºè­°**: **ä¸éœ€è¦BatchNormalization**

**ç†ç”±**:
- è¼¸å…¥æ•¸æ“šå·²æ¨™æº–åŒ–,ç¬¬ä¸€å±¤çš„è¼¸å…¥åˆ†å¸ƒå·²ç¶“è‰¯å¥½
- æ·ºå±¤ç¶²è·¯æ¢¯åº¦å‚³æ’­å•é¡Œä¸æ˜é¡¯
- å¢åŠ BatchNormæœƒå¢åŠ è¨ˆç®—æˆæœ¬å’Œæ¨¡å‹è¤‡é›œåº¦
- åœ¨å°æ•¸æ“šé›†ä¸Šå¯èƒ½ç”¢ç”Ÿéæ“¬åˆ

```python
# æ·ºå±¤ç¶²è·¯ç¯„ä¾‹ (æ•¸æ“šå·²æ¨™æº–åŒ–)
model = Sequential([
    Dense(64, activation='relu', input_shape=(n_features,)),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dense(1)
])
```

#### æƒ…æ³2: æ·±å±¤ç¶²è·¯ (> 4å±¤) + å·²æ¨™æº–åŒ–æ•¸æ“š
**å»ºè­°**: **å»ºè­°ä½¿ç”¨BatchNormalization**

**ç†ç”±**:
- å³ä½¿è¼¸å…¥æ¨™æº–åŒ–äº†,æ·±å±¤ç¶²è·¯ä¸­é–“å±¤çš„åˆ†å¸ƒä»å¯èƒ½shift
- BatchNormåœ¨æ¯ä¸€å±¤éƒ½é‡æ–°æ¨™æº–åŒ–,ç©©å®šå„å±¤åˆ†å¸ƒ
- å¹«åŠ©æ¢¯åº¦å‚³æ’­,åŠ é€Ÿè¨“ç·´

```python
# æ·±å±¤ç¶²è·¯ç¯„ä¾‹ (æ•¸æ“šå·²æ¨™æº–åŒ–,ä»ä½¿ç”¨BatchNorm)
model = Sequential([
    Dense(128, input_shape=(n_features,)),
    BatchNormalization(),
    Activation('relu'),
    
    Dense(64),
    BatchNormalization(),
    Activation('relu'),
    
    Dense(32),
    BatchNormalization(),
    Activation('relu'),
    
    Dense(16, activation='relu'),
    Dense(1)
])
```

#### æƒ…æ³3: æ•¸æ“šæœªæ¨™æº–åŒ–
**å»ºè­°**: **å¼·çƒˆå»ºè­°ä½¿ç”¨StandardScaler + BatchNormalization**

**ç†ç”±**:
- å…ˆç”¨StandardScaleræ¨™æº–åŒ–è¼¸å…¥ç‰¹å¾µ(å¿…è¦æ­¥é©Ÿ)
- å†ç”¨BatchNormç©©å®šè¨“ç·´éç¨‹
- å…©è€…ä½œç”¨ä¸åŒ,å¯ä»¥äº’è£œ

### æœ€ä½³å¯¦è¸å»ºè­°

| ç¶²è·¯æ·±åº¦ | æ•¸æ“šæ˜¯å¦æ¨™æº–åŒ– | æ˜¯å¦ä½¿ç”¨BatchNorm | èªªæ˜ |
|---------|--------------|-----------------|------|
| æ·ºå±¤(â‰¤4å±¤) | æ˜¯ | å¯é¸ | é€šå¸¸ä¸éœ€è¦,é™¤éè¨“ç·´ä¸ç©©å®š |
| æ·ºå±¤(â‰¤4å±¤) | å¦ | å»ºè­°ä½¿ç”¨ | å…ˆStandardScaler,å¯é¸BatchNorm |
| æ·±å±¤(>4å±¤) | æ˜¯ | å»ºè­°ä½¿ç”¨ | ç©©å®šå„å±¤åˆ†å¸ƒ,åŠ é€Ÿæ”¶æ–‚ |
| æ·±å±¤(>4å±¤) | å¦ | å¼·çƒˆå»ºè­° | StandardScaler + BatchNorméƒ½éœ€è¦ |

**åŒ–å·¥å·¥æ¥­æ‡‰ç”¨ç¶“é©—**:
- è£½ç¨‹æ•¸æ“šå›æ­¸(å¦‚æº«åº¦ã€å£“åŠ›é æ¸¬): æ·ºå±¤ç¶²è·¯ + StandardScaleré€šå¸¸å·²è¶³å¤ 
- è¤‡é›œéç·šæ€§ç³»çµ±(å¦‚è’¸é¤¾å¡”å¤šè®Šæ•¸æ§åˆ¶): æ·±å±¤ç¶²è·¯ + BatchNormæ•ˆæœæ›´å¥½
- å°æ•¸æ“šé›†(<1000æ¨£æœ¬): è¬¹æ…ä½¿ç”¨BatchNorm,å¯èƒ½å°è‡´éæ“¬åˆ


#### 5.2.4 Activation Layer (æ¿€æ´»å±¤)

**åŠŸèƒ½**: å–®ç¨å®šç¾©æ¿€æ´»å‡½æ•¸å±¤

```python
from tensorflow.keras.layers import Activation

layer = Activation('relu')
```

**ç­‰åƒ¹å¯«æ³•**:
```python
# æ–¹æ³•1: åœ¨Denseä¸­æŒ‡å®š
Dense(64, activation='relu')

# æ–¹æ³•2: ä½¿ç”¨å–®ç¨çš„Activationå±¤
Dense(64)
Activation('relu')
```

### 5.3 æ¬Šé‡åˆå§‹åŒ–ç­–ç•¥

åˆé©çš„æ¬Šé‡åˆå§‹åŒ–å¯ä»¥åŠ é€Ÿè¨“ç·´ä¸¦é¿å…æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸å•é¡Œã€‚

| åˆå§‹åŒ–æ–¹æ³• | èªªæ˜ | é©ç”¨æ¿€æ´»å‡½æ•¸ |
|-----------|------|--------------|
| `glorot_uniform` (Xavier Uniform) | é è¨­å€¼ï¼Œå‡å‹»åˆ†å¸ƒ | Sigmoid, Tanh, Softmax |
| `glorot_normal` (Xavier Normal) | å¸¸æ…‹åˆ†å¸ƒ | Sigmoid, Tanh, Softmax |
| `he_uniform` | å‡å‹»åˆ†å¸ƒ | ReLU, Leaky ReLU |
| `he_normal` | å¸¸æ…‹åˆ†å¸ƒ | ReLU, Leaky ReLU |
| `zeros` | å…¨éƒ¨åˆå§‹åŒ–ç‚º0 | åå·®é … |
| `ones` | å…¨éƒ¨åˆå§‹åŒ–ç‚º1 | - |

**ä½¿ç”¨ç¯„ä¾‹**:
```python
from tensorflow.keras.initializers import HeNormal

model = Sequential([
    Dense(64, activation='relu', 
          kernel_initializer=HeNormal(),
          input_shape=(10,)),
    Dense(32, activation='relu',
          kernel_initializer=HeNormal()),
    Dense(1)
])
```

### 5.4 æ­£å‰‡åŒ–(Regularization)

é˜²æ­¢éæ“¬åˆçš„æŠ€è¡“ã€‚

#### 5.4.1 L1/L2æ­£å‰‡åŒ–

```python
from tensorflow.keras.regularizers import l1, l2, l1_l2

# L2æ­£å‰‡åŒ– (Ridge)
Dense(64, activation='relu', kernel_regularizer=l2(0.01))

# L1æ­£å‰‡åŒ– (Lasso)
Dense(64, activation='relu', kernel_regularizer=l1(0.01))

# L1+L2æ­£å‰‡åŒ– (Elastic Net)
Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))
```

---

## 6. æ¨¡å‹ç·¨è­¯ (Model Compilation)

ç·¨è­¯æ¨¡å‹æ™‚éœ€è¦æŒ‡å®š**å„ªåŒ–å™¨**ã€**æå¤±å‡½æ•¸**å’Œ**è©•ä¼°æŒ‡æ¨™**ã€‚

### 6.1 model.compile() æ–¹æ³•

```python
model.compile(
    optimizer='adam',           # å„ªåŒ–å™¨
    loss='mse',                 # æå¤±å‡½æ•¸
    metrics=['mae', 'mse']      # è©•ä¼°æŒ‡æ¨™
)
```

### 6.2 å„ªåŒ–å™¨(Optimizers)

å„ªåŒ–å™¨æ±ºå®šå¦‚ä½•æ ¹æ“šæ¢¯åº¦æ›´æ–°æ¬Šé‡ã€‚

#### 6.2.1 Adam (Adaptive Moment Estimation)
**æ¨è–¦é¦–é¸**ï¼Œçµåˆäº†Momentumå’ŒRMSpropçš„å„ªé»ã€‚

```python
from tensorflow.keras.optimizers import Adam

optimizer = Adam(
    learning_rate=0.001,    # å­¸ç¿’ç‡
    beta_1=0.9,             # ä¸€éšçŸ©ä¼°è¨ˆçš„æŒ‡æ•¸è¡°æ¸›ç‡
    beta_2=0.999,           # äºŒéšçŸ©ä¼°è¨ˆçš„æŒ‡æ•¸è¡°æ¸›ç‡
    epsilon=1e-07           # æ•¸å€¼ç©©å®šæ€§å¸¸æ•¸
)

model.compile(optimizer=optimizer, loss='mse')
```

**å„ªé»**:
- è‡ªé©æ‡‰å­¸ç¿’ç‡
- å°è¶…åƒæ•¸ä¸æ•æ„Ÿ
- é©ç”¨æ–¼å¤§å¤šæ•¸å•é¡Œ

#### 6.2.2 SGD (Stochastic Gradient Descent)

```python
from tensorflow.keras.optimizers import SGD

optimizer = SGD(
    learning_rate=0.01,
    momentum=0.9,           # å‹•é‡
    nesterov=True           # æ˜¯å¦ä½¿ç”¨Nesterovå‹•é‡
)
```

#### 6.2.3 RMSprop

```python
from tensorflow.keras.optimizers import RMSprop

optimizer = RMSprop(
    learning_rate=0.001,
    rho=0.9
)
```

#### 6.2.4 AdaGrad, Adadelta, Nadam ç­‰

```python
from tensorflow.keras.optimizers import AdaGrad, Adadelta, Nadam
```

**å„ªåŒ–å™¨é¸æ“‡å»ºè­°**:
- **é¦–é¸**: Adam (é©ç”¨å¤§å¤šæ•¸æƒ…æ³)
- **éœ€è¦æ›´å¥½æ³›åŒ–**: SGD with momentum
- **RNNå•é¡Œ**: RMSprop æˆ– Adam

### 6.3 æå¤±å‡½æ•¸(Loss Functions)

æå¤±å‡½æ•¸æ ¹æ“šå•é¡Œé¡å‹é¸æ“‡,ä»¥ä¸‹åˆ—å‡ºKerasä¸­å¸¸ç”¨æå¤±å‡½æ•¸åŠå…¶æ•¸å­¸å…¬å¼ã€‚

#### 6.3.1 å›æ­¸å•é¡Œ

**1. Mean Squared Error (MSE) - å‡æ–¹èª¤å·®**

**æ•¸å­¸å…¬å¼**:
$$
L_{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

**Kerasä½¿ç”¨**:
```python
# å‡æ–¹èª¤å·® (Mean Squared Error)
model.compile(optimizer='adam', loss='mse')
model.compile(optimizer='adam', loss='mean_squared_error')
from tensorflow.keras.losses import MeanSquaredError
model.compile(optimizer='adam', loss=MeanSquaredError())
```

**ç‰¹æ€§**: å°å¤§èª¤å·®æ‡²ç½°é‡,æ¢¯åº¦èˆ‡èª¤å·®æˆæ­£æ¯”

---

**2. Mean Absolute Error (MAE) - å¹³å‡çµ•å°èª¤å·®**

**æ•¸å­¸å…¬å¼**:
$$
L_{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
$$

**Kerasä½¿ç”¨**:
```python
# å¹³å‡çµ•å°èª¤å·® (Mean Absolute Error)
model.compile(optimizer='adam', loss='mae')
model.compile(optimizer='adam', loss='mean_absolute_error')
from tensorflow.keras.losses import MeanAbsoluteError
model.compile(optimizer='adam', loss=MeanAbsoluteError())
```

**ç‰¹æ€§**: å°ç•°å¸¸å€¼ç©©å¥,æ¢¯åº¦ç‚ºå¸¸æ•¸

---

**3. Huber Loss**

**æ•¸å­¸å…¬å¼**:
$$
L_{\delta}(y, \hat{y}) = \begin{cases}
\frac{1}{2}(y - \hat{y})^2 & \text{if } |y - \hat{y}| \leq \delta \\
\delta (|y - \hat{y}| - \frac{1}{2}\delta) & \text{otherwise}
\end{cases}
$$

**Kerasä½¿ç”¨**:
```python
# Huber Loss (å°ç•°å¸¸å€¼è¼ƒä¸æ•æ„Ÿ)
from tensorflow.keras.losses import Huber
model.compile(optimizer='adam', loss=Huber(delta=1.0))
```

**ç‰¹æ€§**: çµåˆMSEå’ŒMAEå„ªé»,å°èª¤å·®ç”¨MSE(å¹³æ»‘),å¤§èª¤å·®ç”¨MAE(ç©©å¥)

---

**4. Mean Squared Logarithmic Error (MSLE) - å‡æ–¹å°æ•¸èª¤å·®**

**æ•¸å­¸å…¬å¼**:
$$
L_{MSLE} = \frac{1}{n} \sum_{i=1}^{n} (\log(y_i + 1) - \log(\hat{y}_i + 1))^2
$$

**Kerasä½¿ç”¨**:
```python
from tensorflow.keras.losses import MeanSquaredLogarithmicError
model.compile(optimizer='adam', loss='msle')
model.compile(optimizer='adam', loss=MeanSquaredLogarithmicError())
```

**é©ç”¨å ´æ™¯**: ç›®æ¨™è®Šæ•¸ç¯„åœå¾ˆå¤§,é—œæ³¨ç›¸å°èª¤å·®è€Œéçµ•å°èª¤å·®

#### 6.3.2 äºŒå…ƒåˆ†é¡å•é¡Œ

**Binary Crossentropy - äºŒå…ƒäº¤å‰ç†µ**

**æ•¸å­¸å…¬å¼**:
$$
L_{BCE} = -\frac{1}{n} \sum_{i=1}^{n} [y_i \log(\hat{y}_i) + (1-y_i) \log(1-\hat{y}_i)]
$$

å…¶ä¸­:
- $y_i \in \{0, 1\}$ : çœŸå¯¦æ¨™ç±¤
- $\hat{y}_i \in (0, 1)$ : é æ¸¬æ©Ÿç‡(Sigmoidè¼¸å‡º)

**Kerasä½¿ç”¨**:
```python
# äºŒå…ƒäº¤å‰ç†µ
model.compile(optimizer='adam', loss='binary_crossentropy')
from tensorflow.keras.losses import BinaryCrossentropy
model.compile(optimizer='adam', loss=BinaryCrossentropy())
```

**é…åˆä½¿ç”¨**: è¼¸å‡ºå±¤ä½¿ç”¨`Sigmoid`æ¿€æ´»å‡½æ•¸

#### 6.3.3 å¤šé¡åˆ¥åˆ†é¡å•é¡Œ

**1. Categorical Crossentropy - é¡åˆ¥äº¤å‰ç†µ**

**æ•¸å­¸å…¬å¼**:
$$
L_{CCE} = -\frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{C} y_{ij} \log(\hat{y}_{ij})
$$

å…¶ä¸­:
- $C$ : é¡åˆ¥æ•¸é‡
- $y_{ij}$ : one-hotç·¨ç¢¼æ¨™ç±¤ (ç¬¬iå€‹æ¨£æœ¬å±¬æ–¼ç¬¬jé¡æ™‚ç‚º1,å¦å‰‡ç‚º0)
- $\hat{y}_{ij}$ : é æ¸¬æ©Ÿç‡(Softmaxè¼¸å‡º)

**Kerasä½¿ç”¨**:
```python
# é¡åˆ¥äº¤å‰ç†µ (æ¨™ç±¤ç‚ºone-hotç·¨ç¢¼)
model.compile(optimizer='adam', loss='categorical_crossentropy')
from tensorflow.keras.losses import CategoricalCrossentropy
model.compile(optimizer='adam', loss=CategoricalCrossentropy())
```

**æ¨™ç±¤æ ¼å¼**: `[[0, 0, 1], [1, 0, 0], [0, 1, 0]]` (one-hot)

---

**2. Sparse Categorical Crossentropy - ç¨€ç–é¡åˆ¥äº¤å‰ç†µ**

**æ•¸å­¸å…¬å¼**:
$$
L_{SCCE} = -\frac{1}{n} \sum_{i=1}^{n} \log(\hat{y}_{i,c_i})
$$

å…¶ä¸­:
- $c_i$ : ç¬¬iå€‹æ¨£æœ¬çš„çœŸå¯¦é¡åˆ¥ç´¢å¼•
- $\hat{y}_{i,c_i}$ : è©²æ¨£æœ¬åœ¨çœŸå¯¦é¡åˆ¥ä¸Šçš„é æ¸¬æ©Ÿç‡

**Kerasä½¿ç”¨**:
```python
# ç¨€ç–é¡åˆ¥äº¤å‰ç†µ (æ¨™ç±¤ç‚ºæ•´æ•¸)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')
from tensorflow.keras.losses import SparseCategoricalCrossentropy
model.compile(optimizer='adam', loss=SparseCategoricalCrossentropy())
```

**æ¨™ç±¤æ ¼å¼**: `[2, 0, 1]` (æ•´æ•¸ç´¢å¼•)

**å·®ç•°**: èˆ‡Categorical Crossentropyæ•¸å­¸ä¸Šç­‰åƒ¹,åƒ…æ¨™ç±¤æ ¼å¼ä¸åŒ

### æå¤±å‡½æ•¸é¸æ“‡å¿«é€Ÿåƒè€ƒ

| å•é¡Œé¡å‹ | æå¤±å‡½æ•¸ | æ•¸å­¸ç‰¹æ€§ |
|---------|---------|---------|
| å›æ­¸(ä¸€èˆ¬) | MSE | å°å¤§èª¤å·®æ•æ„Ÿ |
| å›æ­¸(æœ‰ç•°å¸¸å€¼) | MAE æˆ– Huber | ç©©å¥æ€§å¼· |
| å›æ­¸(å¤§ç¯„åœç›®æ¨™å€¼) | MSLE | é—œæ³¨ç›¸å°èª¤å·® |
| äºŒå…ƒåˆ†é¡ | Binary Crossentropy | æ©Ÿç‡è§£é‡‹æ¸…æ™° |
| å¤šé¡åˆ¥(one-hot) | Categorical Crossentropy | æ¨™æº–å¤šé¡åˆ¥æå¤± |
| å¤šé¡åˆ¥(æ•´æ•¸æ¨™ç±¤) | Sparse Categorical Crossentropy | ç¯€çœè¨˜æ†¶é«” |


### 6.4 è©•ä¼°æŒ‡æ¨™(Metrics)

#### ä»€éº¼æ˜¯è©•ä¼°æŒ‡æ¨™?

è©•ä¼°æŒ‡æ¨™(Metrics)ç”¨æ–¼**ç›£æ§å’Œè©•ä¼°æ¨¡å‹æ€§èƒ½**,ä½†**ä¸æœƒå½±éŸ¿æ¨¡å‹è¨“ç·´éç¨‹å’Œæœ€çµ‚çµæœ**ã€‚

> [!IMPORTANT]
> **å¸¸è¦‹èª¤è§£**: è¨±å¤šåˆå­¸è€…èª¤ä»¥ç‚ºåœ¨`metrics`ä¸­æ·»åŠ æŒ‡æ¨™æœƒå½±éŸ¿æ¨¡å‹è¨“ç·´å’Œåƒæ•¸æ›´æ–°ã€‚
> 
> **æ­£ç¢ºç†è§£**:
> - âœ… **Loss Function**: æ±ºå®šæ¨¡å‹å¦‚ä½•å­¸ç¿’å’Œæ›´æ–°åƒæ•¸  
> - âœ… **Metrics**: åƒ…ç”¨æ–¼è©•ä¼°å’Œç›£æ§,ä¸å½±éŸ¿è¨“ç·´

#### Metricsçš„ä½œç”¨

1. **è¨“ç·´éç¨‹ç›£æ§**: åœ¨è¨“ç·´æ™‚é¡¯ç¤ºé¡å¤–çš„è©•ä¼°æŒ‡æ¨™
2. **æ¨¡å‹æ¯”è¼ƒ**: ä½¿ç”¨å¤šå€‹æŒ‡æ¨™å…¨é¢è©•ä¼°æ¨¡å‹æ€§èƒ½
3. **æ—©åœåˆ¤æ–·**: å¯ä½œç‚ºEarlyStoppingçš„ç›£æ§æŒ‡æ¨™
4. **çµæœè¨˜éŒ„**: ä¿å­˜åœ¨Historyç‰©ä»¶ä¸­ä¾›å¾ŒçºŒåˆ†æ

#### ä½¿ç”¨ç¯„ä¾‹

**å›æ­¸å•é¡ŒæŒ‡æ¨™**:
```python
# å›æ­¸æŒ‡æ¨™
model.compile(
    optimizer='adam',
    loss='mse',                                  # è¨“ç·´å„ªåŒ–ç›®æ¨™ (å½±éŸ¿è¨“ç·´)
    metrics=['mae', 'mse', 'RootMeanSquaredError']  # åƒ…ç›£æ§ (ä¸å½±éŸ¿è¨“ç·´)
)
```

**åˆ†é¡å•é¡ŒæŒ‡æ¨™**:
```python
# åˆ†é¡æŒ‡æ¨™
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',                  # è¨“ç·´å„ªåŒ–ç›®æ¨™
    metrics=['accuracy', 'Precision', 'Recall', 'AUC']  # åƒ…ç›£æ§
)
```

**è‡ªè¨‚æŒ‡æ¨™**:
```python
# è‡ªè¨‚æŒ‡æ¨™
from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError

model.compile(
    optimizer='adam',
    loss='mse',
    metrics=[MeanAbsoluteError(name='MAE'),
             RootMeanSquaredError(name='RMSE')]
)
```

#### Loss vs Metrics çš„é—œéµå€åˆ¥

| é …ç›® | Loss Function | Metrics |
|------|--------------|---------|
| **æ•¸é‡** | å¿…é ˆæŒ‡å®š1å€‹ | å¯æŒ‡å®š0å€‹æˆ–å¤šå€‹ |
| **ä½œç”¨** | è¨ˆç®—æ¢¯åº¦,æ›´æ–°æ¬Šé‡ | åƒ…è©•ä¼°æ€§èƒ½ |
| **å½±éŸ¿è¨“ç·´** | âœ… æ˜¯ | âŒ å¦ |
| **é¡¯ç¤º** | è¨“ç·´å’Œé©—è­‰ | è¨“ç·´å’Œé©—è­‰ |
| **å„²å­˜** | Historyç‰©ä»¶ | Historyç‰©ä»¶ |

#### å¯¦éš›é‹ä½œç¯„ä¾‹

```python
# æ¨¡å‹ç·¨è­¯
model.compile(
    optimizer='adam',
    loss='mse',           # ç”¨æ–¼è¨ˆç®—æ¢¯åº¦ä¸¦æ›´æ–°æ¬Šé‡
    metrics=['mae']       # åƒ…ç”¨æ–¼é¡¯ç¤º,ä¸å½±éŸ¿è¨“ç·´
)

# è¨“ç·´
history = model.fit(X_train, y_train, epochs=100, validation_split=0.2)

# historyç‰©ä»¶ä¸­æœƒè¨˜éŒ„:
# - history.history['loss']      : è¨“ç·´é›†MSE (å½±éŸ¿è¨“ç·´)
# - history.history['val_loss']  : é©—è­‰é›†MSE (å½±éŸ¿è¨“ç·´)
# - history.history['mae']       : è¨“ç·´é›†MAE (åƒ…ç›£æ§)
# - history.history['val_mae']   : é©—è­‰é›†MAE (åƒ…ç›£æ§)
```

#### å¸¸ç”¨Metricsåˆ—è¡¨

**å›æ­¸Metrics**:
- `mae` / `MeanAbsoluteError`: å¹³å‡çµ•å°èª¤å·®
- `mse` / `MeanSquaredError`: å‡æ–¹èª¤å·®
- `RootMeanSquaredError`: å‡æ–¹æ ¹èª¤å·®
- `MeanAbsolutePercentageError`: å¹³å‡çµ•å°ç™¾åˆ†æ¯”èª¤å·®

**åˆ†é¡Metrics**:
- `accuracy` / `BinaryAccuracy` / `CategoricalAccuracy`: æº–ç¢ºç‡
- `Precision`: ç²¾ç¢ºç‡
- `Recall`: å¬å›ç‡
- `AUC`: ROCæ›²ç·šä¸‹é¢ç©
- `F1Score`: F1åˆ†æ•¸


### 6.5 æ¨¡å‹æ‘˜è¦èˆ‡è¦–è¦ºåŒ–

#### 6.5.1 model.summary() - æ–‡å­—æ‘˜è¦

æŸ¥çœ‹æ¨¡å‹æ¶æ§‹ã€åƒæ•¸æ•¸é‡:

```python
model.summary()
```

**è¼¸å‡ºç¯„ä¾‹**:
```
Model: "sequential"
_________________________________________________________________
Layer (type)                Output Shape              Param #   
=================================================================
dense (Dense)               (None, 64)                704       
dense_1 (Dense)             (None, 32)                2080      
dense_2 (Dense)             (None, 1)                 33        
=================================================================
Total params: 2,817
Trainable params: 2,817
Non-trainable params: 0
_________________________________________________________________
```

**åƒæ•¸è¨ˆç®—**:
- Denseå±¤åƒæ•¸æ•¸é‡ = (è¼¸å…¥ç‰¹å¾µæ•¸ + 1) Ã— ç¥ç¶“å…ƒæ•¸
- ç¬¬ä¸€å±¤: (10 + 1) Ã— 64 = 704
- ç¬¬äºŒå±¤: (64 + 1) Ã— 32 = 2,080
- ç¬¬ä¸‰å±¤: (32 + 1) Ã— 1 = 33

#### 6.5.2 plot_model() - åœ–å½¢åŒ–è¦–è¦ºåŒ–

ä½¿ç”¨`plot_model()`å°‡æ¨¡å‹æ¶æ§‹ç¹ªè£½æˆåœ–ç‰‡,æ›´ç›´è§€åœ°ç†è§£ç¶²è·¯çµæ§‹ã€‚

**åŸºæœ¬ä½¿ç”¨**:
```python
from tensorflow.keras.utils import plot_model

# ç¹ªè£½æ¨¡å‹æ¶æ§‹ä¸¦ä¿å­˜ç‚ºåœ–ç‰‡
plot_model(
    model, 
    to_file='model_architecture.png',  # è¼¸å‡ºæª”æ¡ˆåç¨±
    show_shapes=True,                  # é¡¯ç¤ºæ¯å±¤çš„è¼¸å‡ºå½¢ç‹€
    show_layer_names=True              # é¡¯ç¤ºå±¤åç¨±
)
```

**å®Œæ•´åƒæ•¸èªªæ˜**:
```python
plot_model(
    model,
    to_file='model.png',               # åœ–ç‰‡ä¿å­˜è·¯å¾‘
    show_shapes=True,                  # æ˜¯å¦é¡¯ç¤ºè¼¸å‡ºå½¢ç‹€ (å»ºè­°True)
    show_dtype=False,                  # æ˜¯å¦é¡¯ç¤ºæ•¸æ“šé¡å‹
    show_layer_names=True,             # æ˜¯å¦é¡¯ç¤ºå±¤åç¨± (å»ºè­°True)
    rankdir='TB',                      # æ’åˆ—æ–¹å‘: 'TB'(ä¸Šåˆ°ä¸‹)æˆ–'LR'(å·¦åˆ°å³)
    expand_nested=False,               # æ˜¯å¦å±•é–‹åµŒå¥—æ¨¡å‹
    dpi=96,                            # åœ–ç‰‡è§£æåº¦
    show_layer_activations=False       # æ˜¯å¦é¡¯ç¤ºæ¿€æ´»å‡½æ•¸(TF 2.9+)
)
```

**é—œéµåƒæ•¸è©³è§£**:

1. **show_shapes** (å»ºè­°é–‹å•Ÿ):
   - `True`: é¡¯ç¤ºæ¯å±¤çš„è¼¸å‡ºå½¢ç‹€,å¹«åŠ©ç†è§£æ•¸æ“šæµ
   - `False`: åªé¡¯ç¤ºå±¤åç¨±

2. **rankdir** (æ’åˆ—æ–¹å‘):
   - `'TB'` (Top to Bottom): å¾ä¸Šåˆ°ä¸‹,é©åˆæ·±å±¤ç¶²è·¯
   - `'LR'` (Left to Right): å¾å·¦åˆ°å³,é©åˆè¼ƒå¯¬çš„ç¶²è·¯

3. **dpi** (è§£æåº¦):
   - é è¨­96,å¯èª¿é«˜åˆ°150-300ä»¥ç²å¾—æ›´æ¸…æ™°çš„åœ–ç‰‡
   - æ•¸å€¼è¶Šé«˜æª”æ¡ˆè¶Šå¤§

**ä½¿ç”¨ç¯„ä¾‹**:
```python
# ç¯„ä¾‹1: åŸºæœ¬è¦–è¦ºåŒ–
plot_model(model, to_file='model_basic.png')

# ç¯„ä¾‹2: è©³ç´°è¦–è¦ºåŒ– (æ¨è–¦)
plot_model(
    model,
    to_file='model_detailed.png',
    show_shapes=True,
    show_layer_names=True,
    rankdir='TB',
    dpi=150
)

# ç¯„ä¾‹3: æ©«å‘æ’åˆ—
plot_model(
    model,
    to_file='model_horizontal.png',
    show_shapes=True,
    rankdir='LR'
)
```

**åœ¨Jupyter Notebookä¸­ç›´æ¥é¡¯ç¤º**:
```python
from IPython.display import Image, display
from tensorflow.keras.utils import plot_model

# ç¹ªè£½ä¸¦é¡¯ç¤ºæ¨¡å‹
plot_model(model, to_file='model.png', show_shapes=True)
display(Image('model.png'))
```

**æ³¨æ„äº‹é …**:
1. **éœ€è¦å®‰è£graphviz**:
   ```bash
   # å®‰è£Pythonå¥—ä»¶
   pip install pydot graphviz
   
   # å®‰è£ç³»çµ±å¥—ä»¶ (Windows)
   # ä¸‹è¼‰ä¸¦å®‰è£: https://graphviz.org/download/
   # ä¸¦å°‡binç›®éŒ„åŠ å…¥ç³»çµ±PATH
   
   # å®‰è£ç³»çµ±å¥—ä»¶ (Linux)
   sudo apt-get install graphviz
   
   # å®‰è£ç³»çµ±å¥—ä»¶ (Mac)
   brew install graphviz
   ```

2. **å¦‚æœç„¡æ³•å®‰è£graphviz**:
   - ä½¿ç”¨`model.summary()`ä½œç‚ºæ›¿ä»£
   - æˆ–ä½¿ç”¨TensorBoardçš„æ¨¡å‹åœ–åŠŸèƒ½

**æ¯”è¼ƒ: summary() vs plot_model()**

| ç‰¹æ€§ | model.summary() | plot_model() |
|------|----------------|--------------|
| è¼¸å‡ºå½¢å¼ | æ–‡å­— | åœ–ç‰‡ |
| è¦–è¦ºåŒ– | è¡¨æ ¼å½¢å¼ | æµç¨‹åœ– |
| å®‰è£è¦æ±‚ | ç„¡ | éœ€è¦graphviz |
| é©ç”¨å ´æ™¯ | å¿«é€ŸæŸ¥çœ‹åƒæ•¸ | ç†è§£æ¶æ§‹ |
| ç°¡å ±å±•ç¤º | è¼ƒä¸é©åˆ | é©åˆ |


---

## 7. æ¨¡å‹è¨“ç·´ (Model Training)

### 7.1 model.fit() æ–¹æ³•

```python
history = model.fit(
    x=X_train,                          # è¨“ç·´ç‰¹å¾µ
    y=y_train,                          # è¨“ç·´æ¨™ç±¤
    batch_size=32,                      # æ‰¹æ¬¡å¤§å°
    epochs=100,                         # è¨“ç·´è¼ªæ•¸
    verbose=1,                          # é¡¯ç¤ºæ¨¡å¼
    validation_split=0.2,               # é©—è­‰é›†åˆ†å‰²æ¯”ä¾‹
    # validation_data=(X_val, y_val),  # æˆ–ç›´æ¥æä¾›é©—è­‰é›†
    callbacks=[callback1, callback2],   # å›èª¿å‡½æ•¸åˆ—è¡¨
    shuffle=True                        # æ˜¯å¦æ¯è¼ªæ‰“äº‚æ•¸æ“š
)
```

### 7.2 é‡è¦åƒæ•¸èªªæ˜

#### 7.2.1 batch_size (æ‰¹æ¬¡å¤§å°)

**å®šç¾©**: æ¯æ¬¡æ¢¯åº¦æ›´æ–°ä½¿ç”¨çš„æ¨£æœ¬æ•¸é‡

**é¸æ“‡å»ºè­°**:
- å°batch (8-32): è¨“ç·´ç©©å®šä½†è¼ƒæ…¢ï¼Œæ³›åŒ–èƒ½åŠ›å¯èƒ½è¼ƒå¥½
- ä¸­batch (32-128): **æ¨è–¦ç¯„åœ**
- å¤§batch (128-256): è¨“ç·´å¿«ä½†å¯èƒ½æ³›åŒ–è¼ƒå·®

**è¨˜æ†¶é«”é™åˆ¶**: è¼ƒå¤§batchéœ€è¦æ›´å¤šGPUè¨˜æ†¶é«”

#### 7.2.2 epochs (è¨“ç·´è¼ªæ•¸)

**ç™¼éŸ³**: /'epÉ’ks/ (eh-poks),ä¸æ˜¯"ee-pocks"  
**å®šç¾©**: å®Œæ•´éæ­·æ•´å€‹è¨“ç·´é›†çš„æ¬¡æ•¸

**é¸æ“‡å»ºè­°**:
- è¨­å®šè¼ƒå¤§å€¼(å¦‚100-500)
- æ­é…EarlyStoppingè‡ªå‹•åœæ­¢

### ğŸ”‘ Batch Size, Iteration, Epoch é—œä¿‚è©³è§£

åˆå­¸è€…å¸¸å¸¸æ··æ·†é€™ä¸‰å€‹æ¦‚å¿µ,ä»¥ä¸‹ç”¨å¯¦ä¾‹èªªæ˜å®ƒå€‘çš„é—œä¿‚ã€‚

#### åŸºæœ¬æ¦‚å¿µ

å‡è¨­æˆ‘å€‘æœ‰:
- **è¨“ç·´æ•¸æ“šç¸½é‡**: 1000ç­†
- **batch_size**: 32
- **epochs**: 10

#### è¨ˆç®—é—œä¿‚

**1. Iteration (è¿­ä»£)**:
- **å®šç¾©**: è™•ç†ä¸€å€‹batchä¸¦é€²è¡Œä¸€æ¬¡åƒæ•¸æ›´æ–°
- **è¨ˆç®—**: æ¯å€‹epochçš„iterations = è¨“ç·´æ•¸æ“šç¸½é‡ Ã· batch_size
- **æœ¬ä¾‹**: 1000 Ã· 32 = **31.25 â†’ 32 iterations** (å‘ä¸Šå–æ•´)

**2. Epoch (è¼ª)**:
- **å®šç¾©**: å®Œæ•´éæ­·æ•´å€‹è¨“ç·´é›†ä¸€æ¬¡
- **æœ¬ä¾‹**: è¨­å®š10 epochs

**3. ç¸½æ›´æ–°æ¬¡æ•¸**:
- **è¨ˆç®—**: Total updates = iterations Ã— epochs
- **æœ¬ä¾‹**: 32 Ã— 10 = **320æ¬¡åƒæ•¸æ›´æ–°**

#### å®Œæ•´è¨“ç·´æµç¨‹åœ–

```
è¨“ç·´æ•¸æ“š: 1000ç­†
batch_size: 32
epochs: 10

Epoch 1:
  â”œâ”€ Iteration 1: è™•ç†æ¨£æœ¬ 1-32    â†’ æ›´æ–°æ¬Šé‡ (ç¬¬1æ¬¡)
  â”œâ”€ Iteration 2: è™•ç†æ¨£æœ¬ 33-64   â†’ æ›´æ–°æ¬Šé‡ (ç¬¬2æ¬¡)
  â”œâ”€ Iteration 3: è™•ç†æ¨£æœ¬ 65-96   â†’ æ›´æ–°æ¬Šé‡ (ç¬¬3æ¬¡)
  â”‚  ...
  â””â”€ Iteration 32: è™•ç†æ¨£æœ¬ 993-1000 â†’ æ›´æ–°æ¬Šé‡ (ç¬¬32æ¬¡)

Epoch 2:
  â”œâ”€ Iteration 1: è™•ç†æ¨£æœ¬ 1-32    â†’ æ›´æ–°æ¬Šé‡ (ç¬¬33æ¬¡)
  â”œâ”€ Iteration 2: è™•ç†æ¨£æœ¬ 33-64   â†’ æ›´æ–°æ¬Šé‡ (ç¬¬34æ¬¡)
  â”‚  ...
  â””â”€ Iteration 32: è™•ç†æ¨£æœ¬ 993-1000 â†’ æ›´æ–°æ¬Šé‡ (ç¬¬64æ¬¡)

...

Epoch 10:
  â””â”€ Iteration 32: è™•ç†æ¨£æœ¬ 993-1000 â†’ æ›´æ–°æ¬Šé‡ (ç¬¬320æ¬¡)
```

#### æ•¸å­¸å…¬å¼

$$
\text{Iterations per Epoch} = \left\lceil \frac{\text{Training Samples}}{\text{batch\_size}} \right\rceil
$$

$$
\text{Total Updates} = \text{Iterations per Epoch} \times \text{epochs}
$$

#### ä¸åŒbatch_sizeçš„å½±éŸ¿

| batch_size | Iterations/Epoch | ç¸½æ›´æ–°æ¬¡æ•¸ (10 epochs) | ç‰¹æ€§ |
|-----------|------------------|---------------------|------|
| 8 | 125 | 1250 | æ›´æ–°é »ç¹,æ¢¯åº¦å™ªéŸ³å¤§,æ³›åŒ–å¥½ |
| 32 | 32 | 320 | **å¹³è¡¡æ¨è–¦** |
| 128 | 8 | 80 | æ›´æ–°å°‘,è¨“ç·´å¿«,å¯èƒ½æ¬ æ“¬åˆ |
| 1000 (å…¨batch) | 1 | 10 | æ¯epochåªæ›´æ–°ä¸€æ¬¡(ä¸æ¨è–¦) |

#### å¯¦ç”¨å»ºè­°

**é¸æ“‡batch_sizeçš„è€ƒé‡**:
1. **è¨˜æ†¶é«”é™åˆ¶**: GPUè¨˜æ†¶é«”ä¸è¶³æ™‚é™ä½batch_size
2. **æ•¸æ“šé›†å¤§å°**:
   - å°æ•¸æ“šé›† (<1000): batch_size=16-32
   - ä¸­æ•¸æ“šé›† (1000-10000): batch_size=32-64
   - å¤§æ•¸æ“šé›† (>10000): batch_size=64-128
3. **è¨“ç·´ç©©å®šæ€§**: batchå¤ªå°æœƒå°è‡´æ¢¯åº¦ä¼°è¨ˆä¸æº–ç¢º

**ç›£æ§è¨“ç·´é€²åº¦**:
```python
# Kerasæœƒè‡ªå‹•é¡¯ç¤ºiterationé€²åº¦
Epoch 1/10
32/32 [==============================] - 2s 50ms/step - loss: 0.5430
```
- `32/32` è¡¨ç¤ºå®Œæˆäº†32å€‹iterationsä¸­çš„32å€‹
- æ¯å€‹iterationè™•ç†32ç­†æ•¸æ“š (å‡è¨­batch_size=32)

#### å¸¸è¦‹éŒ¯èª¤è§€å¿µ

âŒ **èª¤è§£1**: "epochsè¶Šå¤šè¶Šå¥½"  
âœ… **æ­£ç¢º**: éå¤šepochsæœƒéæ“¬åˆ,æ‡‰æ­é…EarlyStopping

âŒ **èª¤è§£2**: "batch_sizeè¶Šå¤§è¨“ç·´è¶Šå¥½"  
âœ… **æ­£ç¢º**: é©ä¸­çš„batch_sizeå¹³è¡¡è¨“ç·´é€Ÿåº¦èˆ‡æ³›åŒ–èƒ½åŠ›

âŒ **èª¤è§£3**: "iterationå’Œepochæ˜¯åŒä¸€å€‹æ±è¥¿"  
âœ… **æ­£ç¢º**: 1 epoch = å¤šå€‹ iterations


#### 7.2.3 validation_split vs validation_data

**validation_split**:
```python
# å¾è¨“ç·´æ•¸æ“šæœ«å°¾åˆ†å‰²20%ä½œç‚ºé©—è­‰é›†
model.fit(X_train, y_train, validation_split=0.2)
```

**validation_data**:
```python
# ç›´æ¥æä¾›é©—è­‰é›†
model.fit(X_train, y_train, validation_data=(X_val, y_val))
```

**å»ºè­°**: ä½¿ç”¨`validation_data`å¯æ›´å¥½æ§åˆ¶æ•¸æ“šåˆ†å‰²

#### 7.2.4 verbose (é¡¯ç¤ºæ¨¡å¼)

- `0`: ä¸é¡¯ç¤ºè¨“ç·´éç¨‹
- `1`: é¡¯ç¤ºé€²åº¦æ¢ (é è¨­)
- `2`: æ¯è¼ªé¡¯ç¤ºä¸€è¡Œ

### 7.3 Callbacks (å›èª¿å‡½æ•¸)

Callbacksåœ¨è¨“ç·´éç¨‹ä¸­çš„ç‰¹å®šæ™‚é–“é»åŸ·è¡Œç‰¹å®šæ“ä½œã€‚

#### 7.3.1 EarlyStopping (æ—©åœ)

**åŠŸèƒ½**: ç•¶é©—è­‰æŒ‡æ¨™ä¸å†æ”¹å–„æ™‚è‡ªå‹•åœæ­¢è¨“ç·´

```python
from tensorflow.keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(
    monitor='val_loss',      # ç›£æ§çš„æŒ‡æ¨™
    patience=10,             # å®¹å¿å¤šå°‘è¼ªæ²’æœ‰æ”¹å–„
    restore_best_weights=True,  # æ¢å¾©æœ€ä½³æ¬Šé‡
    verbose=1,
    mode='min'               # 'min'è¡¨ç¤ºæŒ‡æ¨™è¶Šå°è¶Šå¥½, 'max'è¡¨ç¤ºè¶Šå¤§è¶Šå¥½
)

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=500,
    callbacks=[early_stopping]
)
```

**å„ªé»**:
- é˜²æ­¢éæ“¬åˆ
- ç¯€çœè¨“ç·´æ™‚é–“
- è‡ªå‹•æ‰¾åˆ°æœ€ä½³è¨“ç·´è¼ªæ•¸

#### 7.3.2 ModelCheckpoint (æ¨¡å‹æª¢æŸ¥é»)

**åŠŸèƒ½**: åœ¨è¨“ç·´éç¨‹ä¸­è‡ªå‹•ä¿å­˜æ¨¡å‹

```python
from tensorflow.keras.callbacks import ModelCheckpoint

checkpoint = ModelCheckpoint(
    filepath='best_model.keras',  # ä¿å­˜è·¯å¾‘
    monitor='val_loss',           # ç›£æ§æŒ‡æ¨™
    save_best_only=True,          # åªä¿å­˜æœ€ä½³æ¨¡å‹
    save_weights_only=False,      # Falseä¿å­˜å®Œæ•´æ¨¡å‹, Trueåªä¿å­˜æ¬Šé‡
    mode='min',
    verbose=1
)

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=100,
    callbacks=[checkpoint]
)
```

#### 7.3.3 ReduceLROnPlateau (å‹•æ…‹èª¿æ•´å­¸ç¿’ç‡)

**åŠŸèƒ½**: ç•¶è¨“ç·´åœæ»¯æ™‚é™ä½å­¸ç¿’ç‡

```python
from tensorflow.keras.callbacks import ReduceLROnPlateau

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,              # å­¸ç¿’ç‡ç¸®æ¸›å€æ•¸
    patience=5,              # å®¹å¿è¼ªæ•¸
    min_lr=1e-7,             # æœ€å°å­¸ç¿’ç‡
    verbose=1
)
```

#### 7.3.4 TensorBoard (è¨“ç·´è¦–è¦ºåŒ–)

**åŠŸèƒ½**: ä½¿ç”¨TensorBoardè¨˜éŒ„è¨“ç·´éç¨‹

```python
from tensorflow.keras.callbacks import TensorBoard
import datetime

log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = TensorBoard(
    log_dir=log_dir,
    histogram_freq=1,        # è¨˜éŒ„æ¬Šé‡åˆ†å¸ƒçš„é »ç‡
    write_graph=True,        # è¨˜éŒ„æ¨¡å‹åœ–
    update_freq='epoch'      # æ›´æ–°é »ç‡
)

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=100,
    callbacks=[tensorboard_callback]
)
```

**å•Ÿå‹•TensorBoard**:
```bash
tensorboard --logdir=logs/fit
```

åœ¨ç€è¦½å™¨é–‹å•Ÿ `http://localhost:6006/`

#### 7.3.5 çµ„åˆä½¿ç”¨å¤šå€‹Callbacks

```python
callbacks = [
    EarlyStopping(
        monitor='val_loss',
        patience=20,
        restore_best_weights=True
    ),
    ModelCheckpoint(
        filepath='best_model.keras',
        monitor='val_loss',
        save_best_only=True
    ),
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=10,
        min_lr=1e-7
    ),
    TensorBoard(log_dir='logs')
]

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=500,
    batch_size=32,
    callbacks=callbacks
)
```

### 7.4 Historyç‰©ä»¶

`model.fit()`è¿”å›ä¸€å€‹`History`ç‰©ä»¶ï¼Œè¨˜éŒ„äº†è¨“ç·´éç¨‹ä¸­çš„æŒ‡æ¨™ã€‚

```python
# æŸ¥çœ‹å¯ç”¨çš„æŒ‡æ¨™
print(history.history.keys())
# è¼¸å‡º: dict_keys(['loss', 'mae', 'val_loss', 'val_mae'])

# å­˜å–è¨“ç·´æå¤±
train_loss = history.history['loss']

# å­˜å–é©—è­‰æå¤±
val_loss = history.history['val_loss']
```

---

## 8. è¨“ç·´éç¨‹è¦–è¦ºåŒ–

### 8.1 ç¹ªè£½æå¤±æ›²ç·š

```python
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 4))

# æå¤±æ›²ç·š
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Model Loss')
plt.legend()
plt.grid(True)

# æŒ‡æ¨™æ›²ç·š (ä¾‹å¦‚MAE)
plt.subplot(1, 2, 2)
plt.plot(history.history['mae'], label='Training MAE')
plt.plot(history.history['val_mae'], label='Validation MAE')
plt.xlabel('Epoch')
plt.ylabel('MAE')
plt.title('Model MAE')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()
```

### 8.2 åˆ¤æ–·éæ“¬åˆèˆ‡æ¬ æ“¬åˆ

**éæ“¬åˆ(Overfitting)**:
- è¨“ç·´æå¤±æŒçºŒä¸‹é™
- é©—è­‰æå¤±é–‹å§‹ä¸Šå‡
- è¨“ç·´èˆ‡é©—è­‰æå¤±å·®è·å¤§

**æ¬ æ“¬åˆ(Underfitting)**:
- è¨“ç·´èˆ‡é©—è­‰æå¤±éƒ½å¾ˆé«˜
- å…©è€…å·®è·å°ä½†éƒ½ç„¡æ³•ä¸‹é™

**è‰¯å¥½æ“¬åˆ**:
- è¨“ç·´èˆ‡é©—è­‰æå¤±éƒ½ä¸‹é™
- å…©è€…å·®è·å°ä¸”è¶¨æ–¼ç©©å®š

### 8.3 ä½¿ç”¨TensorBoardé€²éšç›£æ§

TensorBoardæä¾›è±å¯Œçš„è¦–è¦ºåŒ–åŠŸèƒ½:

**1. æå¤±èˆ‡æŒ‡æ¨™æ›²ç·š**:
- å¯¦æ™‚ç›£æ§è¨“ç·´èˆ‡é©—è­‰æŒ‡æ¨™
- æ”¯æ´å¹³æ»‘è™•ç†

**2. æ¨¡å‹æ¶æ§‹åœ–**:
- è¦–è¦ºåŒ–ç¶²è·¯çµæ§‹
- æŸ¥çœ‹å¼µé‡ç¶­åº¦èˆ‡é€£æ¥é—œä¿‚

**3. æ¬Šé‡èˆ‡æ¢¯åº¦åˆ†å¸ƒ**:
- ç›£æ§åƒæ•¸åˆ†å¸ƒè®ŠåŒ–
- æª¢æ¸¬æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸

**4. è¶…åƒæ•¸èª¿æ•´**:
- æ¯”è¼ƒä¸åŒè¶…åƒæ•¸è¨­å®šçš„æ•ˆæœ

**è¨­å®šç¯„ä¾‹**:
```python
# åœ¨ä¸åŒè¶…åƒæ•¸ä¸‹è¨“ç·´
for learning_rate in [1e-2, 1e-3, 1e-4]:
    for batch_size in [16, 32, 64]:
        log_dir = f"logs/lr{learning_rate}_bs{batch_size}"
        
        model = create_model()
        model.compile(
            optimizer=Adam(learning_rate=learning_rate),
            loss='mse',
            metrics=['mae']
        )
        
        model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=100,
            batch_size=batch_size,
            callbacks=[TensorBoard(log_dir=log_dir)],
            verbose=0
        )
```

---

## 9. æ¨¡å‹è©•ä¼° (Model Evaluation)

### 9.1 model.evaluate() æ–¹æ³•

```python
# åœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°
test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)

print(f'Test Loss: {test_loss:.4f}')
print(f'Test MAE: {test_mae:.4f}')
```

**è¿”å›å€¼**:
- ç¬¬ä¸€å€‹å€¼: æå¤±å‡½æ•¸å€¼
- å¾ŒçºŒå€¼: metricsä¸­æŒ‡å®šçš„æŒ‡æ¨™

### 9.2 è©³ç´°è©•ä¼°æŒ‡æ¨™è¨ˆç®—

```python
# é€²è¡Œé æ¸¬
y_pred = model.predict(X_test)

# è¨ˆç®—å„ç¨®æŒ‡æ¨™
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f'MAE:  {mae:.4f}')
print(f'MSE:  {mse:.4f}')
print(f'RMSE: {rmse:.4f}')
print(f'RÂ²:   {r2:.4f}')
```

---

## 10. æ¨¡å‹é æ¸¬ (Model Prediction)

### 10.1 model.predict() æ–¹æ³•

```python
# å°æ¸¬è©¦é›†é€²è¡Œé æ¸¬
predictions = model.predict(X_test)

# å°å–®ä¸€æ¨£æœ¬é æ¸¬
single_sample = X_test[0:1]  # ä¿æŒ2Då½¢ç‹€
prediction = model.predict(single_sample)
print(f'Prediction: {prediction[0][0]:.4f}')
print(f'Actual: {y_test[0]:.4f}')
```

### 10.2 é æ¸¬çµæœè¦–è¦ºåŒ–

```python
# å›æ­¸å•é¡Œ: çœŸå¯¦å€¼ vs é æ¸¬å€¼æ•£é»åœ–
plt.figure(figsize=(8, 8))
plt.scatter(y_test, predictions, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], 
         [y_test.min(), y_test.max()], 
         'r--', lw=2, label='Perfect Prediction')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs Predicted')
plt.legend()
plt.grid(True)
plt.show()

# æ®˜å·®åœ–
residuals = y_test - predictions
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.scatter(predictions, residuals, alpha=0.5)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.title('Residual Plot')
plt.grid(True)

plt.subplot(1, 2, 2)
plt.hist(residuals, bins=30, edgecolor='black')
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.title('Residual Distribution')
plt.grid(True)

plt.tight_layout()
plt.show()
```

---

## 11. æ¨¡å‹ä¿å­˜èˆ‡è¼‰å…¥

### 11.1 ä¿å­˜å®Œæ•´æ¨¡å‹

Kerasæä¾›å…©ç¨®ä¸»è¦çš„æ¨¡å‹ä¿å­˜æ ¼å¼:**Kerasæ ¼å¼ (.keras)** å’Œ **HDF5æ ¼å¼ (.h5)**ã€‚

#### 11.1.1 Kerasæ ¼å¼ (æ¨è–¦,TensorFlow 2.xé è¨­)

**æª”æ¡ˆæ ¼å¼**: `.keras` (å–®ä¸€æª”æ¡ˆ,å¯¦éš›ä¸Šæ˜¯zipå£“ç¸®æª”)

**ä¿å­˜æ–¹å¼**:
```python
# ä¿å­˜æ¨¡å‹
model.save('my_model.keras')

# æˆ–ä¸æŒ‡å®šå‰¯æª”å(æœƒè‡ªå‹•ä½¿ç”¨.keras)
model.save('my_model')

# è¼‰å…¥æ¨¡å‹
from tensorflow.keras.models import load_model
loaded_model = load_model('my_model.keras')

# é©—è­‰è¼‰å…¥çš„æ¨¡å‹
predictions = loaded_model.predict(X_test)
```

**ä¿å­˜å…§å®¹**:
- âœ… æ¨¡å‹æ¶æ§‹ (å±¤çš„é…ç½®èˆ‡é€£æ¥)
- âœ… æ¨¡å‹æ¬Šé‡ (æ‰€æœ‰å±¤çš„åƒæ•¸)
- âœ… å„ªåŒ–å™¨ç‹€æ…‹ (optimizerçš„å…§éƒ¨è®Šæ•¸)
- âœ… ç·¨è­¯é…ç½® (loss, metrics, optimizerè¨­å®š)
- âœ… è¨“ç·´é…ç½® (å¦‚æœèª¿ç”¨é`model.fit()`)

**å„ªé»**:
- **TensorFlow 2.xå®˜æ–¹æ¨è–¦æ ¼å¼**
- æ›´å¥½çš„è·¨å¹³å°å…¼å®¹æ€§
- æ”¯æ´è‡ªè¨‚å°è±¡çš„åºåˆ—åŒ–
- å¯ç›´æ¥ç”¨æ–¼TensorFlow Servingéƒ¨ç½²
- æª”æ¡ˆçµæ§‹æ¸…æ™°(zipæ ¼å¼,å¯è§£å£“æŸ¥çœ‹)
- æ”¯æ´å¤§å‹æ¨¡å‹(>2GB)

**ç¼ºé»**:
- ä¸å‘å¾Œå…¼å®¹TensorFlow 1.x
- æª”æ¡ˆç¨å¤§æ–¼HDF5æ ¼å¼

#### 11.1.2 HDF5æ ¼å¼ (èˆŠç‰ˆ,ä»æ”¯æ´ä½†ä¸æ¨è–¦æ–°å°ˆæ¡ˆä½¿ç”¨)

**æª”æ¡ˆæ ¼å¼**: `.h5` æˆ– `.hdf5` (HDF5 binaryæ ¼å¼)

**ä¿å­˜æ–¹å¼**:
```python
# ä¿å­˜ç‚ºHDF5æ ¼å¼
model.save('my_model.h5')

# è¼‰å…¥HDF5æ¨¡å‹
from tensorflow.keras.models import load_model
loaded_model = load_model('my_model.h5')
```

**ä¿å­˜å…§å®¹**:
- âœ… æ¨¡å‹æ¶æ§‹
- âœ… æ¨¡å‹æ¬Šé‡
- âœ… å„ªåŒ–å™¨ç‹€æ…‹
- âœ… ç·¨è­¯é…ç½®

**å„ªé»**:
- å‘å¾Œå…¼å®¹TensorFlow 1.x
- æª”æ¡ˆè¼ƒå°
- å»£æ³›æ”¯æ´,è¨±å¤šå·¥å…·å¯è®€å–HDF5æ ¼å¼

**ç¼ºé»**:
- **å®˜æ–¹å·²ä¸æ¨è–¦ç”¨æ–¼æ–°å°ˆæ¡ˆ**
- å°è‡ªè¨‚å°è±¡æ”¯æ´è¼ƒå·®
- å¤§å‹æ¨¡å‹(>2GB)å¯èƒ½æœ‰å•é¡Œ
- Windowsä¸Šé•·æª”åå¯èƒ½æœ‰å•é¡Œ

### h5 vs keras æ ¼å¼è©³ç´°æ¯”è¼ƒ

| ç‰¹æ€§ | Kerasæ ¼å¼ (.keras) | HDF5æ ¼å¼ (.h5) |
|------|-------------------|----------------|
| **å®˜æ–¹æ¨è–¦** | âœ… æ˜¯ (TF 2.x) | âŒ å¦ (èˆŠæ ¼å¼) |
| **æª”æ¡ˆé¡å‹** | ZIPå£“ç¸®æª” | HDF5 binary |
| **å–®ä¸€æª”æ¡ˆ** | âœ… æ˜¯ | âœ… æ˜¯ |
| **ä¿å­˜å…§å®¹** | æ¶æ§‹+æ¬Šé‡+å„ªåŒ–å™¨+é…ç½® | æ¶æ§‹+æ¬Šé‡+å„ªåŒ–å™¨+é…ç½® |
| **TF 1.xå…¼å®¹** | âŒ å¦ | âœ… æ˜¯ |
| **TF 2.xå…¼å®¹** | âœ… æ˜¯ | âœ… æ˜¯ |
| **è‡ªè¨‚å±¤/å°è±¡** | âœ… å®Œæ•´æ”¯æ´ | âš ï¸ æœ‰é™æ”¯æ´ |
| **å¤§å‹æ¨¡å‹(>2GB)** | âœ… æ”¯æ´ | âš ï¸ å¯èƒ½æœ‰å•é¡Œ |
| **TF Serving** | âœ… åŸç”Ÿæ”¯æ´ | âš ï¸ éœ€è½‰æ› |
| **æª”æ¡ˆå¤§å°** | ç¨å¤§ | ç¨å° |
| **è·¨å¹³å°** | âœ… å„ªç§€ | âš ï¸ Windowsé•·æª”åå•é¡Œ |
| **æª”æ¡ˆçµæ§‹** | å¯è§£å£“æŸ¥çœ‹ | éœ€å°ˆç”¨å·¥å…· |

### é¸æ“‡å»ºè­°

**ä½¿ç”¨ Kerasæ ¼å¼ (.keras) ç•¶**:
- âœ… æ–°å°ˆæ¡ˆ (å¼·çƒˆæ¨è–¦)
- âœ… ä½¿ç”¨TensorFlow 2.x
- âœ… æœ‰è‡ªè¨‚å±¤æˆ–æå¤±å‡½æ•¸
- âœ… éœ€è¦éƒ¨ç½²åˆ°TensorFlow Serving
- âœ… æ¨¡å‹å¤§æ–¼2GB

**ä½¿ç”¨ HDF5æ ¼å¼ (.h5) ç•¶**:
- âš ï¸ éœ€è¦èˆ‡TensorFlow 1.xå…¼å®¹
- âš ï¸ ç¶­è­·èˆŠå°ˆæ¡ˆ
- âš ï¸ æª”æ¡ˆå¤§å°æ¥µåº¦æ•æ„Ÿ

**æœ€ä½³å¯¦è¸**:
```python
# æ¨è–¦: ä½¿ç”¨.kerasæ ¼å¼
model.save('my_best_model.keras')

# å¦‚æœéœ€è¦å…¼å®¹æ€§,å¯åŒæ™‚ä¿å­˜å…©ç¨®æ ¼å¼
model.save('model_keras_format.keras')  # ä¸»è¦æ ¼å¼
model.save('model_h5_format.h5')        # å‚™ç”¨æ ¼å¼
```


### 11.2 åƒ…ä¿å­˜æ¬Šé‡

```python
# ä¿å­˜æ¬Šé‡
model.save_weights('model_weights.h5')

# è¼‰å…¥æ¬Šé‡(éœ€å…ˆå»ºç«‹ç›¸åŒæ¶æ§‹çš„æ¨¡å‹)
new_model = create_model()  # å»ºç«‹ç›¸åŒæ¶æ§‹
new_model.load_weights('model_weights.h5')
```

### 11.3 ä¿å­˜æ¨¡å‹æ¶æ§‹

```python
# ä¿å­˜ç‚ºJSON
json_config = model.to_json()
with open('model_architecture.json', 'w') as json_file:
    json_file.write(json_config)

# å¾JSONè¼‰å…¥
from tensorflow.keras.models import model_from_json
with open('model_architecture.json', 'r') as json_file:
    json_config = json_file.read()
new_model = model_from_json(json_config)
```

### 11.4 ä¿å­˜è¨“ç·´æ­·å²

è¨“ç·´æ­·å²(`history.history`)æ˜¯ä¸€å€‹Pythonå­—å…¸,è¨˜éŒ„äº†è¨“ç·´éç¨‹ä¸­çš„æ‰€æœ‰æŒ‡æ¨™ã€‚å¯ä»¥ä½¿ç”¨**pickle**æˆ–**joblib**ä¾†ä¿å­˜ã€‚

#### 11.4.1 ä½¿ç”¨Pickleä¿å­˜ (Pythonå…§å»º)

**Pickle** æ˜¯Pythonæ¨™æº–åº«çš„åºåˆ—åŒ–å·¥å…·ã€‚

```python
import pickle

# ä¿å­˜history
with open('training_history.pkl', 'wb') as file:
    pickle.dump(history.history, file)

# è¼‰å…¥history
with open('training_history.pkl', 'rb') as file:
    history_dict = pickle.load(file)

# æŸ¥çœ‹è¼‰å…¥çš„æ­·å²
print(history_dict.keys())  # dict_keys(['loss', 'val_loss', 'mae', 'val_mae'])
```

**å„ªé»**:
- Pythonå…§å»º,ç„¡éœ€é¡å¤–å®‰è£
- é©åˆå°å‹å°è±¡
- å»£æ³›æ”¯æ´

**ç¼ºé»**:
- å°å¤§å‹numpyæ•¸çµ„æ•ˆç‡è¼ƒä½
- ä¸æ”¯æ´å£“ç¸®
- è·¨Pythonç‰ˆæœ¬å¯èƒ½æœ‰å…¼å®¹æ€§å•é¡Œ

#### 11.4.2 ä½¿ç”¨Joblibä¿å­˜ (æ¨è–¦ç”¨æ–¼numpyæ•¸çµ„)

**Joblib** æ˜¯scikit-learnæ¨è–¦çš„æŒä¹…åŒ–å·¥å…·,å°numpyæ•¸çµ„å„ªåŒ–æ›´å¥½ã€‚

**å®‰è£**:
```bash
pip install joblib
```

**ä½¿ç”¨æ–¹å¼**:
```python
import joblib

# ä¿å­˜history
joblib.dump(history.history, 'training_history.joblib')

# è¼‰å…¥history
history_dict = joblib.load('training_history.joblib')

# ä½¿ç”¨å£“ç¸®(æ¨è–¦,å¯å¤§å¹…æ¸›å°æª”æ¡ˆå¤§å°)
joblib.dump(history.history, 'training_history_compressed.joblib', compress=3)
# compressåƒæ•¸: 0-9, æ•¸å­—è¶Šå¤§å£“ç¸®ç‡è¶Šé«˜ä½†é€Ÿåº¦è¶Šæ…¢,æ¨è–¦3-5
```

**å„ªé»**:
- **å°numpyæ•¸çµ„å„ªåŒ–,é€Ÿåº¦å¿«**
- **æ”¯æ´å£“ç¸®,ç¯€çœç©ºé–“**
- è¨˜æ†¶é«”æ•ˆç‡é«˜
- æ›´å¥½çš„è·¨Pythonç‰ˆæœ¬å…¼å®¹æ€§

**ç¼ºé»**:
- éœ€è¦é¡å¤–å®‰è£
- å°énumpyå°è±¡å„ªå‹¢ä¸æ˜é¡¯

### Pickle vs Joblib è©³ç´°æ¯”è¼ƒ

| ç‰¹æ€§ | Pickle | Joblib |
|------|--------|--------|
| **å®‰è£** | âœ… Pythonå…§å»º | âš ï¸ éœ€å®‰è£ (`pip install joblib`) |
| **numpyæ•¸çµ„é€Ÿåº¦** | âš ï¸ è¼ƒæ…¢ | âœ… å¿« (å„ªåŒ–é) |
| **æª”æ¡ˆå¤§å°** | âš ï¸ è¼ƒå¤§ | âœ… æ”¯æ´å£“ç¸®,å¯å¤§å¹…ç¸®å° |
| **è¨˜æ†¶é«”æ•ˆç‡** | ä¸€èˆ¬ | âœ… é«˜ (å¤§å‹æ•¸çµ„) |
| **é€šç”¨å°è±¡** | âœ… æ”¯æ´æ‰€æœ‰Pythonå°è±¡ | âœ… æ”¯æ´æ‰€æœ‰Pythonå°è±¡ |
| **è·¨ç‰ˆæœ¬å…¼å®¹** | âš ï¸ å¯èƒ½æœ‰å•é¡Œ | âœ… è¼ƒå¥½ |
| **ä½¿ç”¨å ´æ™¯** | å°å‹Pythonå°è±¡ | numpyæ•¸çµ„ã€å¤§å‹æ•¸æ“š |
| **sklearnæ¨è–¦** | - | âœ… æ˜¯ |

### å¯¦éš›æ¯”è¼ƒç¯„ä¾‹

```python
import pickle
import joblib
import numpy as np
import time
import os

# æ¨¡æ“¬ä¸€å€‹è¼ƒå¤§çš„history (å…¸å‹DNNè¨“ç·´100 epochs)
history_data = {
    'loss': np.random.rand(100).tolist(),
    'val_loss': np.random.rand(100).tolist(),
    'mae': np.random.rand(100).tolist(),
    'val_mae': np.random.rand(100).tolist()
}

# Pickleä¿å­˜
start = time.time()
with open('history_pickle.pkl', 'wb') as f:
    pickle.dump(history_data, f)
pickle_time = time.time() - start
pickle_size = os.path.getsize('history_pickle.pkl')

# Joblibä¿å­˜ (ç„¡å£“ç¸®)
start = time.time()
joblib.dump(history_data, 'history_joblib.joblib')
joblib_time = time.time() - start
joblib_size = os.path.getsize('history_joblib.joblib')

# Joblibä¿å­˜ (å£“ç¸®)
start = time.time()
joblib.dump(history_data, 'history_joblib_compressed.joblib', compress=3)
joblib_compressed_time = time.time() - start
joblib_compressed_size = os.path.getsize('history_joblib_compressed.joblib')

print(f"Pickle      - æ™‚é–“: {pickle_time:.4f}s, å¤§å°: {pickle_size} bytes")
print(f"Joblib      - æ™‚é–“: {joblib_time:.4f}s, å¤§å°: {joblib_size} bytes")
print(f"Joblibå£“ç¸®  - æ™‚é–“: {joblib_compressed_time:.4f}s, å¤§å°: {joblib_compressed_size} bytes")
```

### é¸æ“‡å»ºè­°

**ä½¿ç”¨ Pickle ç•¶**:
- historyæ•¸æ“šå¾ˆå° (<100 epochs)
- ä¸æƒ³å®‰è£é¡å¤–å¥—ä»¶
- ç°¡å–®å¿«é€Ÿçš„åŸå‹é–‹ç™¼

**ä½¿ç”¨ Joblib ç•¶**:
- âœ… historyæ•¸æ“šè¼ƒå¤§ (>100 epochs)
- âœ… è¨“ç·´æ™‚é–“å¾ˆé•·,æ•¸æ“šå¾ˆå¤š
- âœ… éœ€è¦é »ç¹ä¿å­˜/è¼‰å…¥
- âœ… ç£ç¢Ÿç©ºé–“æœ‰é™ (ä½¿ç”¨å£“ç¸®)
- âœ… å·²å®‰è£scikit-learn (joblibæœƒè‡ªå‹•å®‰è£)

### æœ€ä½³å¯¦è¸

```python
import joblib

# æ¨è–¦: ä½¿ç”¨joblib withé©åº¦å£“ç¸®
joblib.dump(
    history.history, 
    'training_history.joblib',
    compress=3  # å¹³è¡¡å£“ç¸®ç‡èˆ‡é€Ÿåº¦
)

# è¼‰å…¥
history_dict = joblib.load('training_history.joblib')

# è¦–è¦ºåŒ–è¼‰å…¥çš„æ­·å²
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history_dict['loss'], label='Training Loss')
plt.plot(history_dict['val_loss'], label='Validation Loss')
plt.legend()
plt.title('Loss History')

plt.subplot(1, 2, 2)
plt.plot(history_dict['mae'], label='Training MAE')
plt.plot(history_dict['val_mae'], label='Validation MAE')
plt.legend()
plt.title('MAE History')

plt.show()
```

### å®Œæ•´ç¯„ä¾‹:åŒæ™‚ä¿å­˜æ¨¡å‹å’Œæ­·å²

```python
import joblib
from datetime import datetime

# è¨“ç·´å¾Œä¿å­˜
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# 1. ä¿å­˜æ¨¡å‹ (ä½¿ç”¨.kerasæ ¼å¼)
model.save(f'model_{timestamp}.keras')

# 2. ä¿å­˜è¨“ç·´æ­·å² (ä½¿ç”¨joblibå£“ç¸®)
joblib.dump(
    history.history,
    f'history_{timestamp}.joblib',
    compress=3
)

print(f"æ¨¡å‹å’Œæ­·å²å·²ä¿å­˜,æ™‚é–“æˆ³: {timestamp}")

# è¼‰å…¥
model_loaded = load_model(f'model_{timestamp}.keras')
history_loaded = joblib.load(f'history_{timestamp}.joblib')
```


---

## 12. TensorFlow/Keras vs sklearn MLPRegressor/MLPClassifier

### 12.1 ä¸»è¦å·®ç•°æ¯”è¼ƒ

| ç‰¹æ€§ | TensorFlow/Keras | sklearn MLP |
|------|------------------|-------------|
| **éˆæ´»æ€§** | æ¥µé«˜ï¼Œå¯è‡ªè¨‚å„ç¨®å±¤èˆ‡æ¶æ§‹ | æœ‰é™ï¼Œåƒ…åŸºæœ¬MLP |
| **æ¨¡å‹è¦æ¨¡** | æ”¯æ´å¤§å‹æ·±åº¦ç¶²è·¯ | é©åˆä¸­å°å‹ç¶²è·¯ |
| **GPUæ”¯æ´** | åŸç”Ÿæ”¯æ´ | ä¸æ”¯æ´ |
| **è¨“ç·´æ§åˆ¶** | ç²¾ç´°æ§åˆ¶(callbacks, è‡ªè¨‚è¨“ç·´å¾ªç’°) | åŸºæœ¬æ§åˆ¶ |
| **éƒ¨ç½²** | æ”¯æ´å¤šç¨®éƒ¨ç½²æ–¹æ¡ˆ | æœ‰é™ |
| **å­¸ç¿’æ›²ç·š** | è¼ƒé™¡å³­ | è¼ƒå¹³ç·© |
| **APIé¢¨æ ¼** | Keras API | Scikit-learn API |
| **é©ç”¨å ´æ™¯** | å¤§è¦æ¨¡ã€è¤‡é›œæ·±åº¦å­¸ç¿’ | å¿«é€ŸåŸå‹ã€å°å‹å•é¡Œ |

### 12.2 sklearn MLPç¯„ä¾‹

```python
from sklearn.neural_network import MLPRegressor, MLPClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# å›æ­¸å•é¡Œ
mlp_reg = MLPRegressor(
    hidden_layer_sizes=(64, 32),   # éš±è—å±¤çµæ§‹
    activation='relu',              # æ¿€æ´»å‡½æ•¸
    solver='adam',                  # å„ªåŒ–å™¨
    alpha=0.0001,                   # L2æ­£å‰‡åŒ–åƒæ•¸
    batch_size='auto',
    learning_rate='constant',
    learning_rate_init=0.001,
    max_iter=500,                   # æœ€å¤§è¿­ä»£æ¬¡æ•¸
    random_state=42,
    early_stopping=True,            # æ—©åœ
    validation_fraction=0.1,
    n_iter_no_change=10
)

# è¨“ç·´
mlp_reg.fit(X_train, y_train)

# é æ¸¬
y_pred = mlp_reg.predict(X_test)

# è©•ä¼°
score = mlp_reg.score(X_test, y_test)  # RÂ² score
```

### 12.3 é¸æ“‡å»ºè­°

**ä½¿ç”¨TensorFlow/Kerasç•¶**:
- éœ€è¦è¤‡é›œçš„ç¶²è·¯æ¶æ§‹
- æ•¸æ“šé‡å¤§(>10,000æ¨£æœ¬)
- éœ€è¦GPUåŠ é€Ÿ
- éœ€è¦ç²¾ç´°æ§åˆ¶è¨“ç·´éç¨‹
- è¨ˆç•«éƒ¨ç½²åˆ°ç”Ÿç”¢ç’°å¢ƒ
- é€²è¡Œæ·±åº¦å­¸ç¿’ç ”ç©¶

**ä½¿ç”¨sklearn MLPç•¶**:
- å¿«é€ŸåŸå‹é©—è­‰
- æ•¸æ“šé‡å°åˆ°ä¸­ç­‰
- éœ€è¦èˆ‡å…¶ä»–sklearnå·¥å…·æ•´åˆ
- ä¸éœ€è¦GPU
- åå¥½ç°¡æ½”çš„sklearn API

---

## 13. å®Œæ•´å·¥ä½œæµç¨‹ç¯„ä¾‹

### 13.1 æ¨™æº–DNNå›æ­¸æµç¨‹

```python
# 1. å°å…¥å¿…è¦å¥—ä»¶
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

# 2. æº–å‚™æ•¸æ“š (æ­¤è™•ä½¿ç”¨æ¨¡æ“¬æ•¸æ“š)
np.random.seed(42)
X = np.random.randn(1000, 10)
y = X[:, 0]**2 + 2*X[:, 1] - X[:, 2] + np.random.randn(1000)*0.1

# 3. åˆ†å‰²æ•¸æ“š
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.3, random_state=42
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42
)

# 4. ç‰¹å¾µç¸®æ”¾
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# 5. å»ºç«‹æ¨¡å‹
model = Sequential([
    Dense(64, activation='relu', input_shape=(10,)),
    BatchNormalization(),
    Dropout(0.3),
    
    Dense(32, activation='relu'),
    BatchNormalization(),
    Dropout(0.3),
    
    Dense(16, activation='relu'),
    Dense(1)
])

# 6. ç·¨è­¯æ¨¡å‹
model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='mse',
    metrics=['mae']
)

# 7. æŸ¥çœ‹æ¨¡å‹çµæ§‹
model.summary()

# 8. è¨­å®šcallbacks
callbacks = [
    EarlyStopping(
        monitor='val_loss',
        patience=20,
        restore_best_weights=True,
        verbose=1
    ),
    ModelCheckpoint(
        filepath='best_model.keras',
        monitor='val_loss',
        save_best_only=True,
        verbose=1
    )
]

# 9. è¨“ç·´æ¨¡å‹
history = model.fit(
    X_train_scaled, y_train,
    validation_data=(X_val_scaled, y_val),
    epochs=200,
    batch_size=32,
    callbacks=callbacks,
    verbose=1
)

# 10. è¦–è¦ºåŒ–è¨“ç·´éç¨‹
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Model Loss')
plt.legend()
plt.grid(True)

plt.subplot(1, 2, 2)
plt.plot(history.history['mae'], label='Training MAE')
plt.plot(history.history['val_mae'], label='Validation MAE')
plt.xlabel('Epoch')
plt.ylabel('MAE')
plt.title('Model MAE')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

# 11. è©•ä¼°æ¨¡å‹
test_loss, test_mae = model.evaluate(X_test_scaled, y_test, verbose=0)
print(f'\nTest Loss: {test_loss:.4f}')
print(f'Test MAE: {test_mae:.4f}')

# 12. é€²è¡Œé æ¸¬
y_pred = model.predict(X_test_scaled)

# 13. è¨ˆç®—è©³ç´°æŒ‡æ¨™
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f'\nè©³ç´°è©•ä¼°æŒ‡æ¨™:')
print(f'MAE:  {mae:.4f}')
print(f'MSE:  {mse:.4f}')
print(f'RMSE: {rmse:.4f}')
print(f'RÂ²:   {r2:.4f}')

# 14. è¦–è¦ºåŒ–é æ¸¬çµæœ
plt.figure(figsize=(8, 8))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], 
         [y_test.min(), y_test.max()], 
         'r--', lw=2)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs Predicted')
plt.grid(True)
plt.show()

# 15. ä¿å­˜æ¨¡å‹
model.save('final_model.keras')

# 16. è¼‰å…¥ä¸¦é©—è­‰æ¨¡å‹
loaded_model = tf.keras.models.load_model('final_model.keras')
loaded_predictions = loaded_model.predict(X_test_scaled)
print(f'\næ¨¡å‹è¼‰å…¥é©—è­‰ - é æ¸¬ä¸€è‡´æ€§: {np.allclose(y_pred, loaded_predictions)}')
```

---

## 14. æœ€ä½³å¯¦è¸èˆ‡å»ºè­°

### 14.1 æ•¸æ“šæº–å‚™
1. **ç‰¹å¾µç¸®æ”¾**: ä½¿ç”¨`StandardScaler`æˆ–`MinMaxScaler`æ¨™æº–åŒ–è¼¸å…¥ç‰¹å¾µ
2. **æ•¸æ“šåˆ†å‰²**: è¨“ç·´é›†(70%) + é©—è­‰é›†(15%) + æ¸¬è©¦é›†(15%)
3. **æ•¸æ“šå¢å¼·**: å°æ–¼å°æ•¸æ“šé›†ï¼Œè€ƒæ…®æ•¸æ“šå¢å¼·æŠ€è¡“

### 14.2 æ¨¡å‹è¨­è¨ˆ
1. **å±¤æ•¸èˆ‡å¯¬åº¦**: å¾å°æ¨¡å‹é–‹å§‹ï¼Œé€æ­¥å¢åŠ è¤‡é›œåº¦
2. **æ¿€æ´»å‡½æ•¸**: éš±è—å±¤ä½¿ç”¨ReLUï¼Œè¼¸å‡ºå±¤æ ¹æ“šå•é¡Œé¸æ“‡
3. **æ­£å‰‡åŒ–**: ä½¿ç”¨Dropout (0.2-0.5) å’Œ L2æ­£å‰‡åŒ–é˜²æ­¢éæ“¬åˆ
4. **BatchNormalization**: åŠ é€Ÿè¨“ç·´ä¸¦æé«˜ç©©å®šæ€§

### 14.3 è¨“ç·´ç­–ç•¥
1. **å­¸ç¿’ç‡**: Adamå„ªåŒ–å™¨å¾0.001é–‹å§‹
2. **Batch Size**: é€šå¸¸32-128ä¹‹é–“
3. **Early Stopping**: è¨­å®špatience=10-20
4. **ModelCheckpoint**: ä¿å­˜é©—è­‰é›†ä¸Šæœ€ä½³æ¨¡å‹

### 14.4 èª¿è©¦æŠ€å·§
1. **éæ“¬åˆ**: å¢åŠ Dropoutã€L2æ­£å‰‡åŒ–ã€æ¸›å°‘æ¨¡å‹è¤‡é›œåº¦ã€å¢åŠ æ•¸æ“š
2. **æ¬ æ“¬åˆ**: å¢åŠ æ¨¡å‹å®¹é‡ã€è¨“ç·´æ›´å¤šè¼ªã€é™ä½æ­£å‰‡åŒ–
3. **è¨“ç·´ä¸ç©©å®š**: é™ä½å­¸ç¿’ç‡ã€ä½¿ç”¨BatchNormalizationã€æª¢æŸ¥æ•¸æ“šç¸®æ”¾
4. **æ¢¯åº¦æ¶ˆå¤±**: ä½¿ç”¨ReLUã€é©ç•¶çš„æ¬Šé‡åˆå§‹åŒ–ã€BatchNormalization

### 14.5 è¶…åƒæ•¸ì¡°ì •
å»ºè­°èª¿æ•´é †åº:
1. å­¸ç¿’ç‡ (æœ€é‡è¦)
2. ç¶²è·¯æ¶æ§‹ (å±¤æ•¸ã€ç¥ç¶“å…ƒæ•¸)
3. Batch size
4. æ­£å‰‡åŒ–åƒæ•¸ (dropout rate, L2ä¿‚æ•¸)
5. å„ªåŒ–å™¨é¸æ“‡

---

## 16. èª²å ‚ä½œæ¥­

### ä½œæ¥­ç›®æ¨™

é€éå¯¦ä½œå®Œæ•´çš„DNNå»ºæ¨¡æµç¨‹,æ·±å…¥ç†è§£æ¨¡å‹åƒæ•¸å°è¨“ç·´æ•ˆæœçš„å½±éŸ¿,ä¸¦å­¸æœƒå¦‚ä½•ç³»çµ±æ€§åœ°èª¿æ•´è¶…åƒæ•¸ä»¥ç²å¾—æœ€ä½³æ¨¡å‹æ€§èƒ½ã€‚

---

### ğŸ“‹ ä½œæ¥­ä¸€:å®Œæ•´DNNå»ºæ¨¡æµç¨‹å¯¦ä½œ (60åˆ†)

#### ä»»å‹™æè¿°

ä½¿ç”¨æä¾›çš„åŒ–å·¥è£½ç¨‹æ•¸æ“šé›†,å»ºç«‹ä¸€å€‹DNNå›æ­¸æ¨¡å‹ä¾†é æ¸¬ç”¢å“å“è³ªæŒ‡æ¨™ã€‚

#### æ•¸æ“šé›†èªªæ˜

**åŒ–å·¥åæ‡‰å™¨æº«åº¦æ§åˆ¶æ•¸æ“šé›†**
- **ç‰¹å¾µ** (8å€‹):
  - é€²æ–™æµé‡ (L/min)
  - åæ‡‰å™¨æº«åº¦ (Â°C)
  - åæ‡‰å™¨å£“åŠ› (bar)
  - å‚¬åŒ–åŠ‘æ¿ƒåº¦ (%)
  - æ”ªæ‹Œé€Ÿåº¦ (rpm)
  - å†·å»æ°´æº«åº¦ (Â°C)
  - åŸæ–™ç´”åº¦ (%)
  - åœç•™æ™‚é–“ (min)
- **ç›®æ¨™è®Šæ•¸**: ç”¢å“è½‰åŒ–ç‡ (%)
- **æ¨£æœ¬æ•¸**: 2000ç­†

#### æ•¸æ“šç”Ÿæˆç¨‹å¼ç¢¼

```python
import numpy as np
import pandas as pd

# è¨­å®šéš¨æ©Ÿç¨®å­
np.random.seed(42)

# ç”Ÿæˆç‰¹å¾µæ•¸æ“š
n_samples = 2000
data = {
    'é€²æ–™æµé‡': np.random.uniform(10, 50, n_samples),
    'åæ‡‰å™¨æº«åº¦': np.random.uniform(150, 250, n_samples),
    'åæ‡‰å™¨å£“åŠ›': np.random.uniform(5, 15, n_samples),
    'å‚¬åŒ–åŠ‘æ¿ƒåº¦': np.random.uniform(0.5, 5, n_samples),
    'æ”ªæ‹Œé€Ÿåº¦': np.random.uniform(100, 500, n_samples),
    'å†·å»æ°´æº«åº¦': np.random.uniform(15, 35, n_samples),
    'åŸæ–™ç´”åº¦': np.random.uniform(90, 99.5, n_samples),
    'åœç•™æ™‚é–“': np.random.uniform(30, 120, n_samples)
}

df = pd.DataFrame(data)

# ç”Ÿæˆç›®æ¨™è®Šæ•¸(ç”¢å“è½‰åŒ–ç‡) - è¤‡é›œéç·šæ€§é—œä¿‚
conversion_rate = (
    0.3 * df['åæ‡‰å™¨æº«åº¦'] +
    0.2 * df['å‚¬åŒ–åŠ‘æ¿ƒåº¦'] * df['åœç•™æ™‚é–“'] +
    -0.15 * (df['é€²æ–™æµé‡'] - 30)**2 +
    0.1 * np.log(df['æ”ªæ‹Œé€Ÿåº¦']) * df['åæ‡‰å™¨å£“åŠ›'] +
    0.05 * df['åŸæ–™ç´”åº¦'] * df['åæ‡‰å™¨æº«åº¦'] / 100 +
    np.random.normal(0, 5, n_samples)  # æ·»åŠ å™ªéŸ³
)/2

# é™åˆ¶è½‰åŒ–ç‡åœ¨åˆç†ç¯„åœ
conversion_rate = np.clip(conversion_rate, 0, 100)

df['ç”¢å“è½‰åŒ–ç‡'] = conversion_rate

# ä¿å­˜æ•¸æ“š
df.to_csv('reactor_data.csv', index=False, encoding='utf-8-sig')
print("æ•¸æ“šé›†å·²ç”Ÿæˆ: reactor_data.csv")
print(f"æ•¸æ“šå½¢ç‹€: {df.shape}")
print(f"\nå‰5ç­†æ•¸æ“š:\n{df.head()}")
```

#### å¿…é ˆå®Œæˆçš„æ­¥é©Ÿ (æ¯æ­¥10åˆ†)

**1. æ•¸æ“šæº–å‚™èˆ‡æ¢ç´¢ (10åˆ†)**
- è¼‰å…¥æ•¸æ“šä¸¦æª¢æŸ¥åŸºæœ¬çµ±è¨ˆè³‡è¨Š
- ç¹ªè£½ç›®æ¨™è®Šæ•¸åˆ†å¸ƒåœ–
- ç¹ªè£½è‡³å°‘3å€‹ç‰¹å¾µèˆ‡ç›®æ¨™è®Šæ•¸çš„æ•£é»åœ–
- æ•¸æ“šåˆ†å‰²:è¨“ç·´é›†70%ã€é©—è­‰é›†15%ã€æ¸¬è©¦é›†15%
- **å°Xå’ŒYéƒ½é€²è¡ŒStandardScaleræ¨™æº–åŒ–**

**2. æ¨¡å‹å»ºç«‹ (10åˆ†)**
- ä½¿ç”¨Sequential APIå»ºç«‹DNNæ¨¡å‹
- è‡³å°‘åŒ…å«3å€‹éš±è—å±¤
- ä½¿ç”¨ReLUæ¿€æ´»å‡½æ•¸
- åŠ å…¥Dropoutå±¤(rate=0.3)
- ä½¿ç”¨`model.summary()`æŸ¥çœ‹æ¨¡å‹çµæ§‹
- **åŠ åˆ†é …**: ä½¿ç”¨`plot_model()`ç¹ªè£½æ¨¡å‹æ¶æ§‹åœ–

**3. æ¨¡å‹ç·¨è­¯ (10åˆ†)**
- ä½¿ç”¨Adamå„ªåŒ–å™¨(learning_rate=0.001)
- æå¤±å‡½æ•¸:MSE
- è©•ä¼°æŒ‡æ¨™:è‡³å°‘åŒ…å«MAEå’ŒRMSE
- èªªæ˜ç‚ºä½•é¸æ“‡é€™äº›è¨­å®š

**4. æ¨¡å‹è¨“ç·´ (10åˆ†)**
- è¨­å®šè‡³å°‘2å€‹callbacks:
  - EarlyStopping (patience=20)
  - ModelCheckpoint
- epochs=200, batch_size=32
- ä½¿ç”¨validation_dataé€²è¡Œé©—è­‰
- **åŠ åˆ†é …**: åŠ å…¥TensorBoard callback

**5. è¨“ç·´éç¨‹è¦–è¦ºåŒ– (10åˆ†)**
- ç¹ªè£½è¨“ç·´èˆ‡é©—è­‰çš„Lossæ›²ç·š
- ç¹ªè£½è¨“ç·´èˆ‡é©—è­‰çš„MAEæ›²ç·š
- åˆ†ææ˜¯å¦æœ‰éæ“¬åˆæˆ–æ¬ æ“¬åˆç¾è±¡

**6. æ¨¡å‹è©•ä¼°èˆ‡é æ¸¬ (10åˆ†)**
- **é‡è¦**: å°é æ¸¬çµæœé€²è¡Œåæ¨™æº–åŒ–
- åœ¨æ¸¬è©¦é›†ä¸Šè¨ˆç®—MAEã€RMSEã€RÂ²
- ç¹ªè£½å¯¦éš›å€¼vsé æ¸¬å€¼æ•£é»åœ–
- ç¹ªè£½æ®˜å·®åœ–
- åˆ†ææ¨¡å‹æ€§èƒ½

---

### ğŸ”¬ ä½œæ¥­äºŒ:è¶…åƒæ•¸æ¢è¨èˆ‡æ¯”è¼ƒ (40åˆ†)

#### ä»»å‹™æè¿°

ç³»çµ±æ€§åœ°æ¢è¨ä¸åŒè¶…åƒæ•¸å°æ¨¡å‹æ€§èƒ½çš„å½±éŸ¿,ä¸¦æ’°å¯«åˆ†æå ±å‘Šã€‚

#### å¿…é ˆå®Œæˆçš„å¯¦é©—

**å¯¦é©—1: ç¶²è·¯æ·±åº¦å½±éŸ¿ (10åˆ†)**

æ¯”è¼ƒä»¥ä¸‹3ç¨®ç¶²è·¯æ¶æ§‹:
- **æ·ºå±¤ç¶²è·¯**: 2å±¤ [64, 32]
- **ä¸­å±¤ç¶²è·¯**: 3å±¤ [128, 64, 32]
- **æ·±å±¤ç¶²è·¯**: 4å±¤ [256, 128, 64, 32]

**è¦æ±‚**:
- å…¶ä»–åƒæ•¸ä¿æŒä¸€è‡´
- è¨˜éŒ„æ¯å€‹æ¨¡å‹çš„:
  - è¨“ç·´æ™‚é–“
  - æœ€ä½³é©—è­‰Loss
  - æ¸¬è©¦é›†MAEã€RMSEã€RÂ²
  - ç¸½åƒæ•¸é‡(å¾model.summary()ç²å–)
- è£½ä½œæ¯”è¼ƒè¡¨æ ¼
- **åˆ†æ**: å“ªç¨®æ·±åº¦æœ€é©åˆ?ç‚ºä»€éº¼?

**å¯¦é©—2: Dropout Rateå½±éŸ¿ (10åˆ†)**

æ¯”è¼ƒä»¥ä¸‹4ç¨®dropout rate:
- 0.0 (ç„¡dropout)
- 0.2
- 0.3
- 0.5

**è¦æ±‚**:
- ä½¿ç”¨ç›¸åŒçš„ç¶²è·¯æ¶æ§‹
- è¨˜éŒ„è¨“ç·´èˆ‡é©—è­‰Lossçš„å·®è·
- åˆ†æéæ“¬åˆç¨‹åº¦
- **åˆ†æ**: æœ€ä½³dropout rateæ˜¯å¤šå°‘?

**å¯¦é©—3: Batch Sizeå½±éŸ¿ (10åˆ†)**

æ¯”è¼ƒä»¥ä¸‹4ç¨®batch size:
- 16
- 32
- 64
- 128

**è¦æ±‚**:
- è¨˜éŒ„æ¯å€‹epochçš„è¨“ç·´æ™‚é–“
- è¨˜éŒ„æœ€çµ‚æ¸¬è©¦é›†æ€§èƒ½
- è¨ˆç®—æ¯å€‹epochçš„iterationsæ•¸é‡
- **åˆ†æ**: batch sizeå¦‚ä½•å½±éŸ¿è¨“ç·´é€Ÿåº¦å’Œæ¨¡å‹æ€§èƒ½?

**å¯¦é©—4: å­¸ç¿’ç‡å½±éŸ¿ (10åˆ†)**

æ¯”è¼ƒä»¥ä¸‹4ç¨®å­¸ç¿’ç‡:
- 0.0001
- 0.001
- 0.01
- 0.1

**è¦æ±‚**:
- è§€å¯Ÿè¨“ç·´æ›²ç·šçš„æ”¶æ–‚é€Ÿåº¦
- è¨˜éŒ„æ˜¯å¦å‡ºç¾è¨“ç·´ä¸ç©©å®š
- **åˆ†æ**: æœ€ä½³å­¸ç¿’ç‡æ˜¯å¤šå°‘?å­¸ç¿’ç‡éå¤§æˆ–éå°æœƒæœ‰ä»€éº¼å•é¡Œ?

---

### ğŸ“Š å¯¦é©—çµæœæ•´ç†æ ¼å¼

#### è¡¨æ ¼ç¯„ä¾‹

**å¯¦é©—1: ç¶²è·¯æ·±åº¦æ¯”è¼ƒ**

| ç¶²è·¯æ¶æ§‹ | åƒæ•¸é‡ | è¨“ç·´æ™‚é–“(s) | é©—è­‰Loss | æ¸¬è©¦MAE | æ¸¬è©¦RMSE | æ¸¬è©¦RÂ² |
|---------|--------|------------|---------|---------|----------|--------|
| [64, 32] | XXX | XX | X.XX | X.XX | X.XX | X.XX |
| [128, 64, 32] | XXX | XX | X.XX | X.XX | X.XX | X.XX |
| [256, 128, 64, 32] | XXX | XX | X.XX | X.XX | X.XX | X.XX |

**åˆ†æ**:
- æœ€ä½³æ¶æ§‹: ___
- åŸå› : ___
- è§€å¯Ÿåˆ°çš„ç¾è±¡: ___

#### è¦–è¦ºåŒ–è¦æ±‚

æ¯å€‹å¯¦é©—è‡³å°‘åŒ…å«:
1. è¨“ç·´æ›²ç·šå°æ¯”åœ–
2. æ¸¬è©¦é›†æ€§èƒ½æŸ±ç‹€åœ–
3. å¯¦éš›å€¼vsé æ¸¬å€¼æ•£é»åœ–(æœ€ä½³æ¨¡å‹)

---

### ğŸ’¡ åŠ åˆ†é …ç›® (æœ€å¤š+20åˆ†)

1. **ä½¿ç”¨TensorBoard** (+5åˆ†)
   - è¨˜éŒ„æ‰€æœ‰å¯¦é©—çš„è¨“ç·´éç¨‹
   - åœ¨å ±å‘Šä¸­å±•ç¤ºTensorBoardæˆªåœ–

2. **BatchNormalizationæ¢è¨** (+5åˆ†)
   - æ¯”è¼ƒæœ‰ç„¡BatchNormalizationçš„å·®ç•°
   - åˆ†æåœ¨å·²æ¨™æº–åŒ–æ•¸æ“šä¸Šçš„æ•ˆæœ

3. **ä¸åŒæ¿€æ´»å‡½æ•¸æ¯”è¼ƒ** (+5åˆ†)
   - æ¯”è¼ƒReLUã€LeakyReLUã€tanh
   - åˆ†æå„è‡ªçš„å„ªç¼ºé»

4. **æ¨¡å‹ä¿å­˜èˆ‡è¼‰å…¥** (+5åˆ†)
   - ä¿å­˜æœ€ä½³æ¨¡å‹(.kerasæ ¼å¼)
   - ä¿å­˜scalers(joblibæ ¼å¼)
   - å±•ç¤ºå¦‚ä½•è¼‰å…¥ä¸¦ä½¿ç”¨æ¨¡å‹é€²è¡Œæ–°é æ¸¬

---

### ğŸ“ ç¹³äº¤æ ¼å¼

#### 1. Jupyter Notebookæª”æ¡ˆ

**æª”å**: `å­¸è™Ÿ_å§“å_Unit15ä½œæ¥­.ipynb`

**å…§å®¹çµæ§‹**:
```
# Unit15 DNNèª²å ‚ä½œæ¥­
## å­¸ç”Ÿè³‡è¨Š
- å­¸è™Ÿ: ___
- å§“å: ___
- ç¹³äº¤æ—¥æœŸ: ___

## ä½œæ¥­ä¸€:å®Œæ•´DNNå»ºæ¨¡æµç¨‹
### 1. æ•¸æ“šæº–å‚™èˆ‡æ¢ç´¢
### 2. æ¨¡å‹å»ºç«‹
### 3. æ¨¡å‹ç·¨è­¯
### 4. æ¨¡å‹è¨“ç·´
### 5. è¨“ç·´éç¨‹è¦–è¦ºåŒ–
### 6. æ¨¡å‹è©•ä¼°èˆ‡é æ¸¬

## ä½œæ¥­äºŒ:è¶…åƒæ•¸æ¢è¨
### å¯¦é©—1: ç¶²è·¯æ·±åº¦å½±éŸ¿
### å¯¦é©—2: Dropout Rateå½±éŸ¿
### å¯¦é©—3: Batch Sizeå½±éŸ¿
### å¯¦é©—4: å­¸ç¿’ç‡å½±éŸ¿

## ç¸½çµèˆ‡å¿ƒå¾—
```

#### 2. å ±å‘ŠPDFæª”æ¡ˆ

**æª”å**: `å­¸è™Ÿ_å§“å_Unit15ä½œæ¥­å ±å‘Š.pdf`

**å…§å®¹**:
- æ‰€æœ‰å¯¦é©—çµæœè¡¨æ ¼
- æ‰€æœ‰è¦–è¦ºåŒ–åœ–è¡¨
- è©³ç´°åˆ†æèˆ‡è¨è«–
- å€‹äººå¿ƒå¾—èˆ‡å­¸ç¿’æ”¶ç©«

#### 3. æ¨¡å‹æª”æ¡ˆ (åŠ åˆ†é …)

- `best_model.keras`: æœ€ä½³æ¨¡å‹
- `X_scaler.joblib`: Xç‰¹å¾µscaler
- `y_scaler.joblib`: Yç›®æ¨™scaler

---

### â° ç¹³äº¤æœŸé™èˆ‡è©•åˆ†æ¨™æº–

**ç¹³äº¤æœŸé™**: èª²ç¨‹çµæŸå¾Œ2é€±å…§

**è©•åˆ†æ¨™æº–**:

| é …ç›® | é…åˆ† | è©•åˆ†é‡é» |
|------|------|---------|
| ä½œæ¥­ä¸€å®Œæˆåº¦ | 60åˆ† | æ¯å€‹æ­¥é©Ÿçš„æ­£ç¢ºæ€§èˆ‡å®Œæ•´æ€§ |
| ä½œæ¥­äºŒå¯¦é©—è¨­è¨ˆ | 30åˆ† | å¯¦é©—è¨­è¨ˆåˆç†æ€§ã€çµæœè¨˜éŒ„å®Œæ•´æ€§ |
| åˆ†æèˆ‡è¨è«– | 10åˆ† | åˆ†ææ·±åº¦ã€é‚è¼¯æ€§ã€æ´å¯ŸåŠ› |
| ç¨‹å¼ç¢¼å“è³ª | åŠ åˆ† | è¨»è§£æ¸…æ¥šã€çµæ§‹è‰¯å¥½ |
| åŠ åˆ†é …ç›® | +20åˆ† | é¡å¤–æ¢è¨èˆ‡å‰µæ–° |

**ç¸½åˆ†**: 100åˆ† + åŠ åˆ†æœ€å¤š20åˆ†

---

### ğŸ’­ æ€è€ƒé¡Œ (ä¸è¨ˆåˆ†,ä½†å»ºè­°æ€è€ƒ)

1. ç‚ºä»€éº¼åœ¨å›æ­¸ä»»å‹™ä¸­,Yæ•¸æ“šä¹Ÿéœ€è¦æ¨™æº–åŒ–?
2. å¦‚æœä¸é€²è¡Œåæ¨™æº–åŒ–ç›´æ¥è¨ˆç®—è©•ä¼°æŒ‡æ¨™,æœƒæœ‰ä»€éº¼å•é¡Œ?
3. ç‚ºä»€éº¼æ·±å±¤ç¶²è·¯ä¸ä¸€å®šæ¯”æ·ºå±¤ç¶²è·¯å¥½?
4. Dropoutå¦‚ä½•é˜²æ­¢éæ“¬åˆ?å®ƒçš„å·¥ä½œåŸç†æ˜¯ä»€éº¼?
5. å­¸ç¿’ç‡éå¤§å’Œéå°åˆ†åˆ¥æœƒå°è‡´ä»€éº¼å•é¡Œ?
6. åœ¨å·¥æ¥­æ‡‰ç”¨ä¸­,å¦‚ä½•é¸æ“‡åˆé©çš„batch size?
7. å¦‚æœæ¸¬è©¦é›†æ€§èƒ½é å·®æ–¼é©—è­‰é›†,å¯èƒ½æ˜¯ä»€éº¼åŸå› ?

---

### ğŸ“š åƒè€ƒè³‡æº

- èª²ç¨‹è¬›ç¾©: Unit15_DNN_MLP_Overview.md
- èª²ç¨‹ç¯„ä¾‹: Unit15_DNN_MLP_Overview.ipynb
- TensorFlowå®˜æ–¹æ–‡æª”: https://www.tensorflow.org/
- Keraså®˜æ–¹æ–‡æª”: https://keras.io/

---

### â“ å¸¸è¦‹å•é¡Œ

**Q1: æ•¸æ“šé›†å¤ªå¤§,è¨“ç·´å¤ªæ…¢æ€éº¼è¾¦?**
A: å¯ä»¥å…ˆç”¨è¼ƒå°çš„å­é›†(å¦‚500ç­†)é€²è¡Œå¯¦é©—,ç¢ºèªç¨‹å¼ç¢¼æ­£ç¢ºå¾Œå†ç”¨å®Œæ•´æ•¸æ“šé›†ã€‚

**Q2: å¦‚ä½•çŸ¥é“æ¨¡å‹æ˜¯å¦éæ“¬åˆ?**
A: è§€å¯Ÿè¨“ç·´LossæŒçºŒä¸‹é™ä½†é©—è­‰Lossé–‹å§‹ä¸Šå‡,æˆ–å…©è€…å·®è·éå¤§ã€‚

**Q3: å¯¦é©—çµæœä¸ç†æƒ³æ€éº¼è¾¦?**
A: é‡é»åœ¨æ–¼åˆ†æéç¨‹å’Œç†è§£åŸå› ,ä¸è¦æ±‚ä¸€å®šè¦é”åˆ°å¾ˆé«˜çš„RÂ²ã€‚

**Q4: å¯ä»¥ä½¿ç”¨å…¶ä»–æ•¸æ“šé›†å—?**
A: å¯ä»¥,ä½†å¿…é ˆæ˜¯å›æ­¸å•é¡Œ,ä¸¦åœ¨å ±å‘Šä¸­èªªæ˜æ•¸æ“šä¾†æºã€‚

**Q5: éœ€è¦ä½¿ç”¨GPUå—?**
A: ä¸éœ€è¦,é€™å€‹ä½œæ¥­åœ¨CPUä¸Šå³å¯å®Œæˆã€‚

---

## 15. ç¸½çµ

æœ¬å–®å…ƒæ¶µè“‹äº†DNN/MLPçš„å®Œæ•´çŸ¥è­˜é«”ç³»:

âœ… **ç†è«–åŸºç¤**: ç¥ç¶“ç¶²è·¯æ•¸å­¸åŸç†ã€å‰å‘å‚³æ’­ã€åå‘å‚³æ’­  
âœ… **æ¿€æ´»å‡½æ•¸**: ReLUã€Sigmoidã€Tanhã€Softmaxçš„ç‰¹æ€§èˆ‡é¸æ“‡  
âœ… **TensorFlow/Keras**: å®Œæ•´çš„æ¨¡å‹å»ºç«‹ã€è¨“ç·´ã€è©•ä¼°æµç¨‹  
âœ… **æ¨¡å‹å„ªåŒ–**: Callbacksã€æ­£å‰‡åŒ–ã€è¶…åƒæ•¸èª¿æ•´æŠ€å·§  
âœ… **å¯¦å‹™æ‡‰ç”¨**: åŒ–å·¥é ˜åŸŸæ‡‰ç”¨æ¡ˆä¾‹èˆ‡æœ€ä½³å¯¦è¸  

### ä¸‹ä¸€æ­¥å­¸ç¿’
- Unit15é™„éŒ„æ¡ˆä¾‹: å¯¦éš›åŒ–å·¥å•é¡Œæ‡‰ç”¨
  - ç‡ƒæ–™æ°£é«”æ’æ”¾é æ¸¬
  - è’¸é¤¾å¡”æ“ä½œæ§åˆ¶
  - ç´…é…’å“è³ªé æ¸¬
  - ç¤¦æ¥­æµ®é¸éç¨‹é æ¸¬

---

## åƒè€ƒè³‡æ–™

1. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
2. TensorFlowå®˜æ–¹æ–‡æª”: https://www.tensorflow.org/
3. Keraså®˜æ–¹æ–‡æª”: https://keras.io/
4. GÃ©ron, A. (2019). Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow.
5. Chollet, F. (2021). Deep Learning with Python (2nd Edition).

---

**èª²ç¨‹ç·¨è™Ÿ**: CHE-AI-114  
**æˆèª²æ•™å¸«**: èŠæ›œç¦ åŠ©ç†æ•™æˆ  
**é€¢ç”²å¤§å­¸åŒ–å­¸å·¥ç¨‹å­¸ç³»**
