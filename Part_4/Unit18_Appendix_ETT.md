# Unit18ï½œLSTM æ™‚é–“åºåˆ—é æ¸¬ï¼šåŒ–å·¥è£½ç¨‹å‹•æ…‹å»ºæ¨¡

**Part 4 - æ·±åº¦å­¸ç¿’é€²éšæ‡‰ç”¨**

> **æ•™å­¸ç›®æ¨™**ï¼šæœ¬å–®å…ƒæ·±å…¥æ¢è¨æ™‚é–“åºåˆ—é æ¸¬åœ¨åŒ–å·¥è£½ç¨‹ä¸­çš„æ‡‰ç”¨ï¼Œå¾å‚³çµ±çµ±è¨ˆæ–¹æ³•åˆ°æ·±åº¦å­¸ç¿’LSTM/GRUæ¨¡å‹ï¼Œå»ºç«‹å®Œæ•´çš„å‹•æ…‹ç³»çµ±é æ¸¬èƒ½åŠ›ã€‚

## ğŸ“š æœ¬å–®å…ƒæ ¸å¿ƒå…§å®¹

### å­¸ç¿’ç›®æ¨™

1. **ç†è§£æ™‚é–“åºåˆ—ç‰¹æ€§**ï¼šæŒæ¡è‡ªç›¸é—œæ€§ã€å­£ç¯€æ€§ã€è¶¨å‹¢ç­‰æ™‚é–“åºåˆ—åŸºæœ¬æ¦‚å¿µ
2. **åŒ–å·¥è£½ç¨‹å‹•æ…‹å»ºæ¨¡**ï¼šç†è§£è£½ç¨‹æ…£æ€§ã€æ™‚é–“å»¶é²ã€å¤šè®Šæ•¸è€¦åˆç­‰å·¥æ¥­ç‰¹æ€§
3. **LSTM/GRU æ¶æ§‹åŸç†**ï¼šæ·±å…¥ç†è§£å¾ªç’°ç¥ç¶“ç¶²è·¯çš„è¨˜æ†¶æ©Ÿåˆ¶èˆ‡æ¢¯åº¦å‚³æ’­
4. **å¤šæ­¥é æ¸¬ç­–ç•¥**ï¼šæŒæ¡ Recursiveã€Directã€Seq2Seq ç­‰é æ¸¬æ–¹æ³•
5. **æ¨¡å‹è©•ä¼°èˆ‡å°æ¯”**ï¼šå»ºç«‹å®Œæ•´çš„ baseline å°æ¯”èˆ‡ rolling backtest è©•ä¼°é«”ç³»
6. **å·¥æ¥­éƒ¨ç½²è€ƒé‡**ï¼šåœ¨ç·šé æ¸¬ã€æ¨¡å‹æ›´æ–°ã€ç•°å¸¸æª¢æ¸¬ç­‰å¯¦å‹™è­°é¡Œ

### æ•¸æ“šé›†ä»‹ç´¹ï¼šé‹çˆé‹è¡Œæ•¸æ“š

**æ•¸æ“šä¾†æº**ï¼šå·¥æ¥­é‹çˆå¤šè®Šæ•¸æ™‚é–“åºåˆ—æ•¸æ“š  
**æ¡æ¨£é »ç‡**ï¼š5ç§’ï¼ˆé™æ¡æ¨£è‡³1åˆ†é˜ï¼‰  
**ç¸½æ¨£æœ¬æ•¸**ï¼šç´„ 50,000+ æ™‚é–“é»  
**ä¸»è¦è®Šæ•¸**ï¼š

| è®Šæ•¸åç¨± | ç‰©ç†æ„ç¾© | å–®ä½ | æ§åˆ¶ç›®æ¨™ |
|---------|---------|------|---------|
| `TE_8332A.AV_0#` | é‹çˆå‡ºå£è’¸æ±½æº«åº¦ | Â°C | é æ¸¬ç›®æ¨™ |
| `ZZQBCHLL.AV_0#` | ä¸»è’¸æ±½æµé‡ | t/h | è² è·æŒ‡æ¨™ |
| `PTCA_8324.AV_0#` | çˆè†›å£“åŠ› | kPa | ç‡ƒç‡’ç‹€æ…‹ |
| `AIR_8301A.AV_0#` | ä¸€æ¬¡é¢¨é‡ | mÂ³/h | æ°§æ°£ä¾›æ‡‰ |
| `AIR_8301B.AV_0#` | äºŒæ¬¡é¢¨é‡ | mÂ³/h | ç‡ƒç‡’æ§åˆ¶ |
| `FT_8301.AV_0#` | ç‡ƒæ–™æµé‡1 | kg/h | ç†±é‡è¼¸å…¥ |
| `FT_8302.AV_0#` | ç‡ƒæ–™æµé‡2 | kg/h | ç†±é‡è¼¸å…¥ |
| `TV_8329ZC.AV_0#` | æ¸›æº«æ°´æµé‡ | t/h | æº«åº¦èª¿ç¯€ |

**åŒ–å·¥è£½ç¨‹ç‰¹æ€§**ï¼š
- **ç†±å®¹æ•ˆæ‡‰**ï¼šé‹çˆæ°´å®¹ç©å¤§ï¼Œæº«åº¦éŸ¿æ‡‰ç·©æ…¢ï¼ˆæ™‚é–“å¸¸æ•¸ ~10-30åˆ†é˜ï¼‰
- **å¤šè®Šæ•¸è€¦åˆ**ï¼šç‡ƒæ–™-é¢¨é‡-æº«åº¦-å£“åŠ›ç›¸äº’å½±éŸ¿
- **éç·šæ€§å‹•æ…‹**ï¼šä¸åŒè² è·ä¸‹çš„å‹•æ…‹ç‰¹æ€§ä¸åŒ
- **æ™‚é–“å»¶é²**ï¼šæ§åˆ¶å‹•ä½œåˆ°æº«åº¦è®ŠåŒ–æœ‰æ˜é¡¯æ»¯å¾Œ

---

## ç¬¬ä¸€ç« ï¼šæ™‚é–“åºåˆ—åŸºç¤ç†è«–

### 1.1 ä»€éº¼æ˜¯æ™‚é–“åºåˆ—ï¼Ÿ

**å®šç¾©**ï¼šæŒ‰æ™‚é–“é †åºæ’åˆ—çš„æ•¸æ“šé»åºåˆ— $\{y_1, y_2, \ldots, y_T\}$

**èˆ‡å‚³çµ±æ©Ÿå™¨å­¸ç¿’çš„å·®ç•°**ï¼š

| ç‰¹æ€§ | å‚³çµ± ML (i.i.d.) | æ™‚é–“åºåˆ— |
|-----|----------------|---------|
| æ¨£æœ¬ç¨ç«‹æ€§ | âœ“ æ¨£æœ¬ç¨ç«‹åŒåˆ†å¸ƒ | âŒ æ¨£æœ¬æœ‰æ™‚é–“ä¾è³´ |
| é †åºé‡è¦æ€§ | âŒ é †åºå¯æ‰“äº‚ | âœ“ é †åºä¸å¯æ”¹è®Š |
| è¨“ç·´/æ¸¬è©¦åŠƒåˆ† | éš¨æ©ŸåŠƒåˆ† | æ™‚é–“é †åºåŠƒåˆ† |
| é æ¸¬ç›®æ¨™ | å–®ä¸€é æ¸¬ | åºåˆ—é æ¸¬ |
| ç‰¹å¾µå·¥ç¨‹ | ç•¶å‰æ™‚åˆ»ç‰¹å¾µ | æ­·å²ç‰¹å¾µï¼ˆlag featuresï¼‰|

### 1.2 æ™‚é–“åºåˆ—çš„é—œéµæ¦‚å¿µ

#### è‡ªç›¸é—œæ€§ï¼ˆAutocorrelationï¼‰

**å®šç¾©**ï¼šæ™‚é–“åºåˆ—èˆ‡å…¶è‡ªèº«æ»¯å¾Œç‰ˆæœ¬çš„ç›¸é—œæ€§

$$
\text{ACF}(k) = \frac{\text{Cov}(y_t, y_{t-k})}{\text{Var}(y_t)} = \frac{\mathbb{E}[(y_t - \mu)(y_{t-k} - \mu)]}{\sigma^2}
$$

å…¶ä¸­ï¼š
- $k$ï¼šæ»¯å¾Œéšæ•¸ï¼ˆlagï¼‰
- $\mu$ï¼šåºåˆ—å‡å€¼
- $\sigma^2$ï¼šåºåˆ—æ–¹å·®

**ç‰©ç†æ„ç¾©ï¼ˆé‹çˆç³»çµ±ï¼‰**ï¼š
- ACF(1) é«˜ï¼šæº«åº¦ç›¸é„°æ™‚åˆ»é«˜åº¦ç›¸é—œï¼ˆç†±æ…£æ€§ï¼‰
- ACF(k) éš¨ k è¡°æ¸›ï¼šè¨˜æ†¶é€æ¼¸æ¶ˆé€€
- ACF(k) æˆªå°¾ï¼šå¯èƒ½å­˜åœ¨é€±æœŸæ€§æ“¾å‹•

#### åè‡ªç›¸é—œæ€§ï¼ˆPartial Autocorrelationï¼‰

**å®šç¾©**ï¼šå‰”é™¤ä¸­é–“æ™‚åˆ»å½±éŸ¿å¾Œçš„ç›´æ¥ç›¸é—œæ€§

$$
\text{PACF}(k) = \text{Corr}(y_t - \hat{y}_t, y_{t-k} - \hat{y}_{t-k})
$$

å…¶ä¸­ $\hat{y}_t$ æ˜¯ç”± $y_{t-1}, \ldots, y_{t-k+1}$ ç·šæ€§é æ¸¬çš„å€¼ã€‚

**æ‡‰ç”¨**ï¼šç¢ºå®š AR æ¨¡å‹çš„éšæ•¸

#### å¹³ç©©æ€§ï¼ˆStationarityï¼‰

**å®šç¾©**ï¼šçµ±è¨ˆç‰¹æ€§ä¸éš¨æ™‚é–“è®ŠåŒ–

**åš´æ ¼å¹³ç©©æ€§**ï¼ˆStrict Stationarityï¼‰ï¼š
$$
P(y_{t_1}, \ldots, y_{t_n}) = P(y_{t_1+\tau}, \ldots, y_{t_n+\tau}), \quad \forall \tau, n
$$

**å¼±å¹³ç©©æ€§**ï¼ˆWeak Stationarityï¼‰ï¼š
1. å‡å€¼æ†å®šï¼š$\mathbb{E}[y_t] = \mu, \quad \forall t$
2. æ–¹å·®æ†å®šï¼š$\text{Var}(y_t) = \sigma^2, \quad \forall t$
3. è‡ªå”æ–¹å·®åƒ…ä¾è³´æ»¯å¾Œï¼š$\text{Cov}(y_t, y_{t-k}) = \gamma_k, \quad \forall t$

**æª¢é©—æ–¹æ³•**ï¼š
- **ADF æª¢é©—ï¼ˆAugmented Dickey-Fullerï¼‰**ï¼šæª¢é©—å–®ä½æ ¹
  $$
  \Delta y_t = \alpha + \beta t + \gamma y_{t-1} + \sum_{i=1}^{p} \delta_i \Delta y_{t-i} + \epsilon_t
  $$
  - $H_0$: $\gamma = 0$ (éå¹³ç©©)
  - $H_1$: $\gamma < 0$ (å¹³ç©©)

- **KPSS æª¢é©—**ï¼šæª¢é©—å¹³ç©©æ€§ï¼ˆèˆ‡ ADF ç›¸åï¼‰

**åŒ–å·¥è£½ç¨‹çš„éå¹³ç©©æ€§**ï¼š
- **è¶¨å‹¢æ€§**ï¼šè¨­å‚™è€åŒ–ã€å‚¬åŒ–åŠ‘å¤±æ´»å°è‡´æ€§èƒ½æ¼‚ç§»
- **é€±æœŸæ€§**ï¼šç™½å¤©/å¤œé–“è² è·è®ŠåŒ–ã€å­£ç¯€æ€§åŸæ–™å·®ç•°
- **çµæ§‹æ€§çªè®Š**ï¼šå·¥è—æ”¹é€²ã€è¨­å‚™æ›´æ›

### 1.3 åŒ–å·¥è£½ç¨‹çš„æ™‚é–“åºåˆ—ç‰¹æ€§

#### å‹•æ…‹ç³»çµ±çš„æ™‚é–“å°ºåº¦

åŒ–å·¥è£½ç¨‹æ¶‰åŠå¤šå€‹æ™‚é–“å°ºåº¦ï¼š

| éç¨‹ | æ™‚é–“å°ºåº¦ | æ•¸å­¸æè¿° | æ§åˆ¶ç­–ç•¥ |
|-----|---------|---------|---------|
| å¿«é€ŸåŒ–å­¸åæ‡‰ | æ¯«ç§’-ç§’ | $\frac{dr}{dt} = k(T) \cdot c^n$ | åæ‡‰å™¨æº«åº¦æ§åˆ¶ |
| ç†±å‚³å° | ç§’-åˆ†é˜ | $\frac{\partial T}{\partial t} = \alpha \nabla^2 T$ | æ›ç†±å™¨æ§åˆ¶ |
| ç‰©æ–™ç´¯ç© | åˆ†é˜-å°æ™‚ | $\frac{dM}{dt} = F_{in} - F_{out}$ | æ¶²ä½/å£“åŠ›æ§åˆ¶ |
| å‚¬åŒ–åŠ‘å¤±æ´» | å°æ™‚-å¤© | $\frac{da}{dt} = -k_d \cdot a$ | å†ç”Ÿé€±æœŸå„ªåŒ– |

**æ™‚é–“å¸¸æ•¸ï¼ˆTime Constantï¼‰**ï¼š

ä¸€éšç³»çµ±ï¼š
$$
\tau \frac{dy}{dt} + y = K u(t)
$$

- $\tau$ï¼šæ™‚é–“å¸¸æ•¸ï¼ˆé”åˆ° 63.2% ç©©æ…‹å€¼æ‰€éœ€æ™‚é–“ï¼‰
- $K$ï¼šå¢ç›Š
- $u(t)$ï¼šè¼¸å…¥æ“¾å‹•

**é‹çˆæº«åº¦éŸ¿æ‡‰**ï¼šå…¸å‹æ™‚é–“å¸¸æ•¸ $\tau \approx 15$ åˆ†é˜

#### æ™‚é–“å»¶é²ï¼ˆDead Timeï¼‰

**å®šç¾©

**å®šç¾©**ï¼šè¼¸å…¥è®ŠåŒ–åˆ°è¼¸å‡ºéŸ¿æ‡‰ä¹‹é–“çš„ç´”å»¶é²

$$
y(t) = f(u(t - \theta))
$$

å…¶ä¸­ $\theta$ æ˜¯æ­»å€æ™‚é–“ï¼ˆdead timeï¼‰ã€‚

**åŒ–å·¥å¯¦ä¾‹**ï¼š
- **ç®¡é“è¼¸é€**ï¼šç‰©æ–™å¾æŠ•å…¥é»åˆ°æ¸¬é‡é»çš„æ™‚é–“
- **åˆ†æå„€è¡¨**ï¼šæ¨£å“æ¡é›†-å‚³è¼¸-åˆ†æçš„æ™‚é–“
- **é‹çˆç‡ƒç‡’**ï¼šç‡ƒæ–™æŠ•å…¥åˆ°ç†±é‡å‚³éè‡³æ°´çš„æ™‚é–“ï¼ˆ~5-10åˆ†é˜ï¼‰

**æ¨¡å‹è¡¨ç¤ºï¼ˆFOPDTï¼‰**ï¼š

ä¸€éšåŠ ç´”æ»¯å¾Œæ¨¡å‹ï¼ˆFirst-Order Plus Dead Timeï¼‰ï¼š
$$
G(s) = \frac{K e^{-\theta s}}{\tau s + 1}
$$

#### å¤šè®Šæ•¸è€¦åˆèˆ‡å› æœé—œä¿‚

**Granger å› æœæª¢é©—**ï¼š

è®Šæ•¸ $X$ æ˜¯å¦å° $Y$ æœ‰é æ¸¬èƒ½åŠ›ï¼š

**é™åˆ¶æ¨¡å‹**ï¼ˆåƒ…ç”¨ $Y$ çš„æ­·å²ï¼‰ï¼š
$$
Y_t = \alpha_0 + \sum_{i=1}^{p} \alpha_i Y_{t-i} + \epsilon_t
$$

**å®Œæ•´æ¨¡å‹**ï¼ˆåŠ å…¥ $X$ çš„æ­·å²ï¼‰ï¼š
$$
Y_t = \alpha_0 + \sum_{i=1}^{p} \alpha_i Y_{t-i} + \sum_{j=1}^{q} \beta_j X_{t-j} + \eta_t
$$

**æª¢é©—**ï¼š
- $H_0$: $\beta_1 = \cdots = \beta_q = 0$ (X ä¸å½±éŸ¿ Y)
- ä½¿ç”¨ F æª¢é©—æ¯”è¼ƒå…©æ¨¡å‹çš„ RSS

**é‹çˆç³»çµ±çš„å› æœéˆ**ï¼š
```
ç‡ƒæ–™æµé‡ â†’ (5-10åˆ†é˜) â†’ çˆè†›æº«åº¦ â†’ (10-20åˆ†é˜) â†’ è’¸æ±½æº«åº¦
    â†“                         â†‘
  é¢¨é‡èª¿ç¯€ â†’ (3-5åˆ†é˜) â†’ ç‡ƒç‡’æ•ˆç‡
```

---

## ç¬¬äºŒç« ï¼šå‚³çµ±æ™‚é–“åºåˆ—æ¨¡å‹

### 2.1 ARIMA æ¨¡å‹æ—

#### AR (AutoRegressive) æ¨¡å‹

**å®šç¾©**ï¼šç•¶å‰å€¼ç”±éå»å€¼ç·šæ€§çµ„åˆ

$$
y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \cdots + \phi_p y_{t-p} + \epsilon_t
$$

å…¶ä¸­ï¼š
- $p$ï¼šè‡ªå›æ­¸éšæ•¸
- $\phi_i$ï¼šè‡ªå›æ­¸ä¿‚æ•¸
- $\epsilon_t \sim \mathcal{N}(0, \sigma^2)$ï¼šç™½å™ªè²

**å¹³ç©©æ€§æ¢ä»¶**ï¼šç‰¹å¾µæ–¹ç¨‹çš„æ ¹åœ¨å–®ä½åœ“å¤–
$$
1 - \phi_1 z - \phi_2 z^2 - \cdots - \phi_p z^p = 0, \quad |z| > 1
$$

#### MA (Moving Average) æ¨¡å‹

**å®šç¾©**ï¼šç•¶å‰å€¼ç”±éå»èª¤å·®ç·šæ€§çµ„åˆ

$$
y_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \cdots + \theta_q \epsilon_{t-q}
$$

å…¶ä¸­ï¼š
- $q$ï¼šç§»å‹•å¹³å‡éšæ•¸
- $\theta_i$ï¼šç§»å‹•å¹³å‡ä¿‚æ•¸

**å¯é€†æ€§æ¢ä»¶**ï¼šç‰¹å¾µæ–¹ç¨‹çš„æ ¹åœ¨å–®ä½åœ“å¤–

#### ARIMA(p, d, q) æ¨¡å‹

**å®šç¾©**ï¼šçµåˆ ARã€å·®åˆ†ï¼ˆIï¼‰ã€MA

$$
\phi(B)(1-B)^d y_t = \theta(B) \epsilon_t
$$

å…¶ä¸­ï¼š
- $B$ï¼šå¾Œç§»ç®—å­ï¼ˆ$B y_t = y_{t-1}$ï¼‰
- $d$ï¼šå·®åˆ†éšæ•¸
- $\phi(B) = 1 - \phi_1 B - \cdots - \phi_p B^p$
- $\theta(B) = 1 + \theta_1 B + \cdots + \theta_q B^q$

**Box-Jenkins å»ºæ¨¡æµç¨‹**ï¼š
1. **è­˜åˆ¥ï¼ˆIdentificationï¼‰**ï¼š
   - ç¹ªè£½ ACF/PACF åœ–
   - ADF æª¢é©—å¹³ç©©æ€§
   - ç¢ºå®š $(p, d, q)$ éšæ•¸

2. **ä¼°è¨ˆï¼ˆEstimationï¼‰**ï¼š
   - æœ€å¤§ä¼¼ç„¶ä¼°è¨ˆï¼ˆMLEï¼‰
   - æœ€å°äºŒä¹˜æ³•ï¼ˆOLSï¼‰

3. **è¨ºæ–·ï¼ˆDiagnosticï¼‰**ï¼š
   - æ®˜å·®ç™½å™ªè²æª¢é©—ï¼ˆLjung-Boxï¼‰
   - æ®˜å·®æ­£æ…‹æ€§æª¢é©—ï¼ˆJarque-Beraï¼‰

4. **é æ¸¬ï¼ˆForecastingï¼‰**ï¼š
   - é»é æ¸¬ï¼š$\hat{y}_{T+h|T}$
   - å€é–“é æ¸¬ï¼š$\hat{y}_{T+h|T} \pm z_{\alpha/2} \cdot \sigma_h$

#### SARIMA å­£ç¯€æ€§æ¨¡å‹

**å®šç¾©**ï¼šARIMA(p,d,q)Ã—(P,D,Q)s

$$
\phi(B) \Phi(B^s) (1-B)^d (1-B^s)^D y_t = \theta(B) \Theta(B^s) \epsilon_t
$$

å…¶ä¸­ $s$ æ˜¯å­£ç¯€é€±æœŸï¼ˆå¦‚ 12 å€‹æœˆã€24 å°æ™‚ï¼‰ã€‚

**åŒ–å·¥æ‡‰ç”¨**ï¼š
- æ—¥é€±æœŸï¼šç™½å¤©/å¤œé–“è² è·è®ŠåŒ–ï¼ˆs=24å°æ™‚ï¼‰
- é€±é€±æœŸï¼šå·¥ä½œæ—¥/é€±æœ«å·®ç•°ï¼ˆs=7å¤©ï¼‰
- å¹´é€±æœŸï¼šå­£ç¯€æ€§åŸæ–™å·®ç•°ï¼ˆs=12æœˆï¼‰

### 2.2 å¤šè®Šæ•¸æ¨¡å‹ï¼šVAR

**å‘é‡è‡ªå›æ­¸æ¨¡å‹ï¼ˆVector AutoRegressionï¼‰**ï¼š

$$
\mathbf{y}_t = \mathbf{c} + \mathbf{\Phi}_1 \mathbf{y}_{t-1} + \cdots + \mathbf{\Phi}_p \mathbf{y}_{t-p} + \mathbf{\epsilon}_t
$$

å…¶ä¸­ï¼š
- $\mathbf{y}_t \in \mathbb{R}^m$ï¼šm å€‹è®Šæ•¸
- $\mathbf{\Phi}_i \in \mathbb{R}^{m \times m}$ï¼šä¿‚æ•¸çŸ©é™£
- $\mathbf{\epsilon}_t \sim \mathcal{N}(\mathbf{0}, \mathbf{\Sigma})$

**è„ˆè¡éŸ¿æ‡‰åˆ†æï¼ˆImpulse Response Functionï¼‰**ï¼š

å°è®Šæ•¸ $j$ æ–½åŠ å–®ä½è¡æ“Šï¼Œè§€å¯Ÿè®Šæ•¸ $i$ çš„å‹•æ…‹éŸ¿æ‡‰ï¼š
$$
\text{IRF}_{ij}(h) = \frac{\partial y_{i,t+h}}{\partial \epsilon_{j,t}}
$$

**é‹çˆç³»çµ±çš„ VAR æ‡‰ç”¨**ï¼š
- ç‡ƒæ–™æµé‡ â†’ çˆè†›å£“åŠ› â†’ è’¸æ±½æº«åº¦
- é¢¨é‡ â†’ æ°§å«é‡ â†’ ç‡ƒç‡’æ•ˆç‡
- æ¸›æº«æ°´ â†’ è’¸æ±½æº«åº¦ï¼ˆç›´æ¥å¿«é€Ÿï¼‰

### 2.3 å‚³çµ±æ–¹æ³•çš„å±€é™æ€§

**ç·šæ€§å‡è¨­çš„å•é¡Œ**ï¼š
- åŒ–å·¥è£½ç¨‹å¸¸æœ‰éç·šæ€§ï¼ˆå¦‚ Arrhenius åæ‡‰é€Ÿç‡ï¼‰
- ä¸åŒæ“ä½œå€åŸŸçš„å‹•æ…‹ç‰¹æ€§ä¸åŒ

**é•·æœŸä¾è³´å»ºæ¨¡å›°é›£**ï¼š
- AR(p) éœ€è¦å¾ˆå¤§çš„ $p$ æ‰èƒ½æ•æ‰é•·æœŸè¨˜æ†¶
- åƒæ•¸æ•¸é‡éš¨ $p$ ç·šæ€§å¢é•·ï¼Œå®¹æ˜“éæ“¬åˆ

**å¤–ç”Ÿè®Šæ•¸è™•ç†è¤‡é›œ**ï¼š
- ARIMAX éœ€è¦æ‰‹å‹•é¸æ“‡æ»¯å¾Œéšæ•¸
- å¤šè®Šæ•¸äº¤äº’ä½œç”¨é›£ä»¥å»ºæ¨¡

**ç„¡æ³•è‡ªå‹•ç‰¹å¾µæå–**ï¼š
- éœ€è¦é ˜åŸŸå°ˆå®¶è¨­è¨ˆæ»¯å¾Œç‰¹å¾µ
- ç„¡æ³•è‡ªå‹•ç™¼ç¾è¤‡é›œæ¨¡å¼

---

## ç¬¬ä¸‰ç« ï¼šå¾ªç’°ç¥ç¶“ç¶²è·¯åŸºç¤

### 3.1 ç°¡å–® RNN çš„åŸç†

#### åŸºæœ¬æ¶æ§‹

**æ•¸å­¸å½¢å¼**ï¼š

$$
\mathbf{h}_t = \tanh(\mathbf{W}_{hh} \mathbf{h}_{t-1} + \mathbf{W}_{xh} \mathbf{x}_t + \mathbf{b}_h)
$$
$$
\mathbf{y}_t = \mathbf{W}_{hy} \mathbf{h}_t + \mathbf{b}_y
$$

å…¶ä¸­ï¼š
- $\mathbf{h}_t \in \mathbb{R}^h$ï¼šéš±è—ç‹€æ…‹ï¼ˆè¨˜æ†¶ï¼‰
- $\mathbf{x}_t \in \mathbb{R}^d$ï¼šè¼¸å…¥
- $\mathbf{y}_t \in \mathbb{R}^k$ï¼šè¼¸å‡º
- $\mathbf{W}_{hh}, \mathbf{W}_{xh}, \mathbf{W}_{hy}$ï¼šæ¬Šé‡çŸ©é™£

**æ™‚é–“å±•é–‹ï¼ˆUnrollingï¼‰**ï¼š

```
x_1 â†’ [RNN] â†’ h_1 â†’ y_1
x_2 â†’ [RNN] â†’ h_2 â†’ y_2
        â†‘       â†‘
      h_1     h_1
```

#### BPTT (Backpropagation Through Time)

**æå¤±å‡½æ•¸**ï¼š
$$
\mathcal{L} = \sum_{t=1}^{T} \ell(y_t, \hat{y}_t)
$$

**æ¢¯åº¦è¨ˆç®—**ï¼ˆéˆå¼æ³•å‰‡ï¼‰ï¼š
$$
\frac{\partial \mathcal{L}}{\partial \mathbf{W}_{hh}} = \sum_{t=1}^{T} \sum_{k=1}^{t} \frac{\partial \mathcal{L}_t}{\partial \mathbf{h}_t} \frac{\partial \mathbf{h}_t}{\partial \mathbf{h}_k} \frac{\partial \mathbf{h}_k}{\partial \mathbf{W}_{hh}}
$$

**æ¢¯åº¦å‚³æ’­**ï¼š
$$
\frac{\partial \mathbf{h}_t}{\partial \mathbf{h}_k} = \prod_{i=k+1}^{t} \frac{\partial \mathbf{h}_i}{\partial \mathbf{h}_{i-1}} = \prod_{i=k+1}^{t} \mathbf{W}_{hh}^T \cdot \text{diag}[\tanh'(\cdot)]
$$

#### æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸å•é¡Œ

**æ¢¯åº¦æ¶ˆå¤±**ï¼š

ç•¶ $|\lambda_{\max}(\mathbf{W}_{hh})| < 1$ æ™‚ï¼š
$$
\left\| \frac{\partial \mathbf{h}_t}{\partial \mathbf{h}_k} \right\| \leq \left\| \mathbf{W}_{hh} \right\|^{t-k} \|\text{diag}[\tanh'(\cdot)]\|^{t-k} \to 0
$$

**æ¢¯åº¦çˆ†ç‚¸**ï¼š

ç•¶ $|\lambda_{\max}(\mathbf{W}_{hh})| > 1$ æ™‚ï¼Œæ¢¯åº¦æŒ‡æ•¸å¢é•·ã€‚

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
- **Gradient Clipping**ï¼š
  $$
  \mathbf{g} \leftarrow \begin{cases}
  \mathbf{g} & \text{if } \|\mathbf{g}\| \leq \theta \\
  \frac{\theta}{\|\mathbf{g}\|} \mathbf{g} & \text{otherwise}
  \end{cases}
  $$

- **æ›´å¥½çš„æ¶æ§‹**ï¼šLSTM/GRU

### 3.2 LSTM (Long Short-Term Memory)

#### æ¶æ§‹è¨­è¨ˆå‹•æ©Ÿ

**å•é¡Œ**ï¼šç°¡å–® RNN ç„¡æ³•å­¸ç¿’é•·æœŸä¾è³´

**è§£æ±ºæ€è·¯**ï¼š
- å¼•å…¥ **Cell State** $\mathbf{c}_t$ ä½œç‚º"é«˜é€Ÿå…¬è·¯"
- ç”¨ **Gate** æ©Ÿåˆ¶æ§åˆ¶ä¿¡æ¯æµå‹•
- ç·šæ€§è·¯å¾‘é¿å…æ¢¯åº¦æ¶ˆå¤±

#### LSTM æ•¸å­¸å…¬å¼

**éºå¿˜é–€ï¼ˆForget Gateï¼‰**ï¼šæ±ºå®šä¸Ÿæ£„å¤šå°‘èˆŠä¿¡æ¯
$$
\mathbf{f}_t = \sigma(\mathbf{W}_f \cdot [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_f)
$$

**è¼¸å…¥é–€ï¼ˆInput Gateï¼‰**ï¼šæ±ºå®šæ·»åŠ å¤šå°‘æ–°ä¿¡æ¯
$$
\mathbf{i}_t = \sigma(\mathbf{W}_i \cdot [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_i)
$$
$$
\tilde{\mathbf{c}}_t = \tanh(\mathbf{W}_c \cdot [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_c)
$$

**Cell State æ›´æ–°**ï¼š
$$
\mathbf{c}_t = \mathbf{f}_t \odot \mathbf{c}_{t-1} + \mathbf{i}_t \odot \tilde{\mathbf{c}}_t
$$

**è¼¸å‡ºé–€ï¼ˆOutput Gateï¼‰**ï¼šæ±ºå®šè¼¸å‡ºä»€éº¼
$$
\mathbf{o}_t = \sigma(\mathbf{W}_o \cdot [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_o)
$$
$$
\mathbf{h}_t = \mathbf{o}_t \odot \tanh(\mathbf{c}_t)
$$

å…¶ä¸­ï¼š
- $\sigma$ï¼šSigmoid å‡½æ•¸ï¼ˆè¼¸å‡º 0-1ï¼Œä½œç‚ºé–€æ§ï¼‰
- $\odot$ï¼šé€å…ƒç´ ä¹˜æ³•ï¼ˆHadamard productï¼‰
- $[\cdot, \cdot]$ï¼šæ‹¼æ¥æ“ä½œ

**åƒæ•¸ç¸½æ•¸**ï¼š
$$
\text{Params} = 4 \times (h \times (h + d) + h)
$$

å…¶ä¸­ $h$ æ˜¯éš±è—å±¤å¤§å°ï¼Œ$d$ æ˜¯è¼¸å…¥ç¶­åº¦ã€‚

#### LSTM çš„è¨˜æ†¶æ©Ÿåˆ¶

**Cell State æ¢¯åº¦**ï¼š
$$
\frac{\partial \mathbf{c}_t}{\partial \mathbf{c}_{t-1}} = \mathbf{f}_t
$$

**é—œéµå„ªå‹¢**ï¼š
- ç•¶ $\mathbf{f}_t \approx 1$ æ™‚ï¼Œæ¢¯åº¦å¹¾ä¹ç„¡æå‚³æ’­
- ä¸ç¶“éå¤šæ¬¡çŸ©é™£ä¹˜æ³•ï¼Œé¿å…æ¢¯åº¦æ¶ˆå¤±
- å¯ä»¥é¸æ“‡æ€§è¨˜æ†¶ï¼ˆéºå¿˜é–€æ§åˆ¶ï¼‰

**ç‰©ç†é¡æ¯”ï¼ˆé‹çˆç³»çµ±ï¼‰**ï¼š
- **Cell State**ï¼šç³»çµ±ç´¯ç©çš„ç†±é‡
- **éºå¿˜é–€**ï¼šç†±æå¤±ï¼ˆæ•£ç†±ã€è¼»å°„ï¼‰
- **è¼¸å…¥é–€**ï¼šæ–°å¢ç†±é‡ï¼ˆç‡ƒæ–™ç‡ƒç‡’ï¼‰
- **è¼¸å‡ºé–€**ï¼šå¯æ¸¬é‡çš„æº«åº¦ï¼ˆè¼¸å‡ºåˆ°è’¸æ±½ï¼‰

### 3.3 GRU (Gated Recurrent Unit)

#### ç°¡åŒ–çš„ Gate çµæ§‹

GRU å°‡ LSTM çš„ä¸‰å€‹é–€ç°¡åŒ–ç‚ºå…©å€‹ï¼š

**é‡ç½®é–€ï¼ˆReset Gateï¼‰**ï¼š
$$
\mathbf{r}_t = \sigma(\mathbf{W}_r \cdot [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_r)
$$

**æ›´æ–°é–€ï¼ˆUpdate Gateï¼‰**ï¼š
$$
\mathbf{z}_t = \sigma(\mathbf{W}_z \cdot [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_z)
$$

**å€™é¸éš±è—ç‹€æ…‹**ï¼š
$$
\tilde{\mathbf{h}}_t = \tanh(\mathbf{W}_h \cdot [\mathbf{r}_t \odot \mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_h)
$$

**éš±è—ç‹€æ…‹æ›´æ–°**ï¼š
$$
\mathbf{h}_t = (1 - \mathbf{z}_t) \odot \mathbf{h}_{t-1} + \mathbf{z}_t \odot \tilde{\mathbf{h}}_t
$$

**å„ªå‹¢**ï¼š
- åƒæ•¸æ›´å°‘ï¼ˆç´„ç‚º LSTM çš„ 75%ï¼‰
- è¨ˆç®—æ›´å¿«
- åœ¨è¨±å¤šä»»å‹™ä¸Šæ€§èƒ½æ¥è¿‘ LSTM

**ä½•æ™‚é¸æ“‡ GRU vs LSTM**ï¼š
- GRUï¼šæ•¸æ“šé‡è¼ƒå°ã€è¨ˆç®—è³‡æºæœ‰é™ã€åºåˆ—è¼ƒçŸ­
- LSTMï¼šæ•¸æ“šé‡å¤§ã€éœ€è¦ç²¾ç´°è¨˜æ†¶æ§åˆ¶ã€åºåˆ—è¼ƒé•·

---

## ç¬¬å››ç« ï¼šåºåˆ—é æ¸¬å»ºæ¨¡ç­–ç•¥

### 4.1 æ•¸æ“šæº–å‚™ï¼šå¾åºåˆ—åˆ°ç›£ç£å­¸ç¿’

#### Sliding Window æ–¹æ³•

**åŸå§‹åºåˆ—**ï¼š
$$
[y_1, y_2, y_3, \ldots, y_T]
$$

**è½‰æ›ç‚ºç›£ç£æ•¸æ“š**ï¼ˆçª—å£å¤§å° = Lï¼‰ï¼š

| è¼¸å…¥ç‰¹å¾µ | ç›®æ¨™ |
|---------|-----|
| $[y_1, y_2, \ldots, y_L]$ | $y_{L+1}$ |
| $[y_2, y_3, \ldots, y_{L+1}]$ | $y_{L+2}$ |
| $\vdots$ | $\vdots$ |
| $[y_{T-L}, \ldots, y_{T-1}]$ | $y_T$ |

**ä»£ç¢¼å¯¦ç¾**ï¼š
```python
def create_sequences(data, window_size):
    X, y = [], []
    for i in range(len(data) - window_size):
        X.append(data[i:i+window_size])
        y.append(data[i+window_size])
    return np.array(X), np.array(y)
```

#### å¤šè®Šæ•¸è¼¸å…¥

**ç‰¹å¾µçŸ©é™£**ï¼ˆN å€‹è®Šæ•¸ï¼‰ï¼š
$$
\mathbf{X}_t = \begin{bmatrix}
x_{1,t-L+1} & x_{2,t-L+1} & \cdots & x_{N,t-L+1} \\
x_{1,t-L+2} & x_{2,t-L+2} & \cdots & x_{N,t-L+2} \\
\vdots & \vdots & \ddots & \vdots \\
x_{1,t} & x_{2,t} & \cdots & x_{N,t}
\end{bmatrix} \in \mathbb{R}^{L \times N}
$$

**LSTM è¼¸å…¥å½¢ç‹€**ï¼š`(batch_size, time_steps, features)`

**é‹çˆç³»çµ±ç¤ºä¾‹**ï¼š
- Time steps (L) = 30ï¼ˆéå»30åˆ†é˜ï¼‰
- Features (N) = 8ï¼ˆæº«åº¦ã€æµé‡ã€å£“åŠ›ç­‰ï¼‰
- Output = 1ï¼ˆé æ¸¬æœªä¾†1åˆ†é˜æº«åº¦ï¼‰

### 4.2 å–®æ­¥é æ¸¬ vs å¤šæ­¥é æ¸¬

#### å–®æ­¥é æ¸¬ï¼ˆOne-Step Aheadï¼‰

**ç›®æ¨™**ï¼šé æ¸¬ä¸‹ä¸€å€‹æ™‚åˆ» $\hat{y}_{t+1}$

$$
\hat{y}_{t+1} = f_\theta(y_t, y_{t-1}, \ldots, y_{t-L+1})
$$

**å„ªé»**ï¼š
- æ¨¡å‹ç°¡å–®
- èª¤å·®ä¸ç´¯ç©
- é©åˆå¯¦æ™‚æ§åˆ¶

**ç¼ºé»**ï¼š
- åªèƒ½çœ‹ä¸€æ­¥ï¼Œç„¡æ³•æå‰è¦åŠƒ

#### å¤šæ­¥é æ¸¬ï¼ˆMulti-Stepï¼‰

**ç›®æ¨™**ï¼šé æ¸¬æœªä¾† H å€‹æ™‚åˆ» $\hat{y}_{t+1}, \ldots, \hat{y}_{t+H}$

**ç­–ç•¥ 1ï¼šRecursiveï¼ˆéæ­¸ï¼‰**ï¼š

```python
def predict_recursive(model, x_init, H):
    predictions = []
    x = x_init.copy()
    for h in range(H):
        y_pred = model.predict(x)
        predictions.append(y_pred)
        x = np.roll(x, -1)  # ç§»é™¤æœ€èˆŠï¼ŒåŠ å…¥é æ¸¬å€¼
        x[-1] = y_pred
    return predictions
```

**æ•¸å­¸å½¢å¼**ï¼š
$$
\hat{y}_{t+1} = f(y_t, \ldots, y_{t-L+1})
$$
$$
\hat{y}_{t+2} = f(\hat{y}_{t+1}, y_t, \ldots, y_{t-L+2})
$$
$$
\vdots
$$

**å„ªé»**ï¼šåªéœ€è¨“ç·´ä¸€å€‹æ¨¡å‹  
**ç¼ºé»**ï¼šèª¤å·®ç´¯ç©åš´é‡

**ç­–ç•¥ 2ï¼šDirectï¼ˆç›´æ¥ï¼‰**ï¼š

ç‚ºæ¯å€‹ horizon è¨“ç·´ç¨ç«‹æ¨¡å‹ï¼š
$$
\hat{y}_{t+h} = f_h(y_t, \ldots, y_{t-L+1}), \quad h = 1, \ldots, H
$$

**å„ªé»**ï¼šèª¤å·®ä¸ç´¯ç©  
**ç¼ºé»**ï¼šéœ€è¦ H å€‹æ¨¡å‹ï¼Œè¨“ç·´æˆæœ¬é«˜

**ç­–ç•¥ 3ï¼šSeq2Seqï¼ˆåºåˆ—åˆ°åºåˆ—ï¼‰**ï¼š

**Encoder-Decoder æ¶æ§‹**ï¼š
```
Encoder: [x_1, ..., x_L] â†’ context vector c
Decoder: c â†’ [y_1, ..., y_H]
```

**æ•¸å­¸å½¢å¼**ï¼š
$$
\mathbf{c} = \text{Encoder}(\mathbf{x}_{t-L+1:t})
$$
$$
[\hat{y}_{t+1}, \ldots, \hat{y}_{t+H}] = \text{Decoder}(\mathbf{c})
$$

**å„ªé»**ï¼š
- ä¸€æ¬¡æ€§è¼¸å‡ºæ•´å€‹åºåˆ—
- è€ƒæ…®è¼¸å‡ºä¹‹é–“çš„ç›¸é—œæ€§
- å¯åŠ å…¥ Attention æ©Ÿåˆ¶

**ç­–ç•¥å°æ¯”è¡¨**ï¼š

| ç­–ç•¥ | æ¨¡å‹æ•¸é‡ | èª¤å·®ç´¯ç© | è¨“ç·´è¤‡é›œåº¦ | é©ç”¨å ´æ™¯ |
|-----|---------|---------|-----------|---------|
| Recursive | 1 | é«˜ | ä½ | H è¼ƒå°ï¼Œå¿«é€Ÿéƒ¨ç½² |
| Direct | H | ç„¡ | é«˜ | H è¼ƒå¤§ï¼Œè¿½æ±‚ç²¾åº¦ |
| Seq2Seq | 1 | ä½ | ä¸­ | åºåˆ—ç›¸é—œæ€§å¼· |

### 4.3 ç‰¹å¾µå·¥ç¨‹

#### Lag Featuresï¼ˆæ»¯å¾Œç‰¹å¾µï¼‰

**ç›®æ¨™è®Šæ•¸çš„æ»¯å¾Œ**ï¼š
$$
y_t, y_{t-1}, y_{t-2}, \ldots, y_{t-L}
$$

**å¤–ç”Ÿè®Šæ•¸çš„æ»¯å¾Œ**ï¼š
$$
x_{1,t}, x_{1,t-1}, \ldots, x_{1,t-L_1}
$$
$$
x_{2,t}, x_{2,t-1}, \ldots, x_{2,t-L_2}
$$

**æ»¯å¾Œéšæ•¸é¸æ“‡**ï¼š
- ACF/PACF åˆ†æ
- é ˜åŸŸçŸ¥è­˜ï¼ˆæ™‚é–“å¸¸æ•¸ã€å»¶é²ï¼‰
- äº¤å‰é©—è­‰

#### Rolling Statisticsï¼ˆæ»¾å‹•çµ±è¨ˆï¼‰

**ç§»å‹•å¹³å‡ï¼ˆMAï¼‰**ï¼š
$$
\text{MA}_t^{(w)} = \frac{1}{w}\sum_{i=0}^{w-1} y_{t-i}
$$

**ç§»å‹•æ¨™æº–å·®ï¼ˆRolling Stdï¼‰**ï¼š
$$
\text{Std}_t^{(w)} = \sqrt{\frac{1}{w}\sum_{i=0}^{w-1} (y_{t-i} - \text{MA}_t^{(w)})^2}
$$

**æŒ‡æ•¸åŠ æ¬Šç§»å‹•å¹³å‡ï¼ˆEWMAï¼‰**ï¼š
$$
\text{EWMA}_t = \alpha y_t + (1-\alpha) \text{EWMA}_{t-1}
$$

**æ‡‰ç”¨**ï¼š
- æ•æ‰è¶¨å‹¢ï¼ˆMAï¼‰
- è­˜åˆ¥æ³¢å‹•ï¼ˆStdï¼‰
- å¹³æ»‘å™ªè²ï¼ˆEWMAï¼‰

#### æ™‚é–“ç‰¹å¾µï¼ˆTemporal Featuresï¼‰

**é€±æœŸæ€§ç·¨ç¢¼**ï¼š

å°æ–¼é€±æœŸ $T$ï¼ˆå¦‚24å°æ™‚ï¼‰ï¼š
$$
\text{sin\_hour} = \sin\left(\frac{2\pi \cdot \text{hour}}{T}\right)
$$
$$
\text{cos\_hour} = \cos\left(\frac{2\pi \cdot \text{hour}}{T}\right)
$$

**ç‚ºä»€éº¼ç”¨ sin/cos**ï¼š
- ä¿æŒé€±æœŸæ€§é€£çºŒï¼ˆ23æ™‚åˆ°0æ™‚æ˜¯é€£çºŒçš„ï¼‰
- é¿å… one-hot ç·¨ç¢¼çš„ç¨€ç–æ€§

**å…¶ä»–æ™‚é–“ç‰¹å¾µ**ï¼š
- å·¥ä½œæ—¥/é€±æœ«ï¼ˆbinaryï¼‰
- ç­æ¬¡ï¼ˆæ—©/ä¸­/æ™šï¼‰
- æ˜¯å¦ç¯€å‡æ—¥

#### Domain-Specific Featuresï¼ˆé ˜åŸŸç‰¹å¾µï¼‰

**é‹çˆç³»çµ±çš„å·¥ç¨‹ç‰¹å¾µ**ï¼š

**éé‡ç©ºæ°£ä¿‚æ•¸**ï¼š
$$
\alpha = \frac{\text{å¯¦éš›ç©ºæ°£é‡}}{\text{ç†è«–ç©ºæ°£é‡}} = f(\text{é¢¨é‡}, \text{ç‡ƒæ–™é‡})
$$

**ç†±æ•ˆç‡æŒ‡æ¨™**ï¼š
$$
\eta = \frac{Q_{\text{useful}}}{Q_{\text{input}}} = f(\text{æº«åº¦}, \text{æµé‡}, \text{å£“åŠ›})
$$

**è² è·ç‡**ï¼š
$$
\text{Load} = \frac{\text{ç•¶å‰è’¸æ±½æµé‡}}{\text{é¡å®šè’¸æ±½æµé‡}} \times 100\%
$$

---
## ç¬¬ï¿½?ç« ï¿½?æ¨¡ï¿½?è©•ä¼°?ï¿½ï¿½?ï¿½?

### 5.1 ?ï¿½ï¿½?åºï¿½?å°ˆç”¨è©•ä¼°?ï¿½ï¿½?

#### é»ï¿½?æ¸¬èª¤ï¿½?

**MAE (Mean Absolute Error)**ï¿½?
$$
\text{MAE} = \frac{1}{n}\sum_{i=1}^{n} |y_i - \hat{y}_i|
$$

**RMSE (Root Mean Squared Error)**ï¿½?
$$
\text{RMSE} = \sqrt{\frac{1}{n}\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
$$

**MAPE (Mean Absolute Percentage Error)**ï¿½?
$$
\text{MAPE} = \frac{100\%}{n}\sum_{i=1}^{n} \left|\frac{y_i - \hat{y}_i}{y_i}\right|
$$

**sMAPE (Symmetric MAPE)**ï¿½?
$$
\text{sMAPE} = \frac{100\%}{n}\sum_{i=1}^{n} \frac{|y_i - \hat{y}_i|}{(|y_i| + |\hat{y}_i|)/2}
$$

**?ï¿½ï¿½??ï¿½ï¿½?**ï¿½?
- MAEï¼šï¿½??ï¿½å¸¸?ï¿½ï¿½??ï¿½ï¿½?ï¼Œè§£?ï¿½æ€§å¼·
- RMSEï¼šæ‡²ç½°å¤§èª¤å·®ï¼Œå¸¸?ï¿½æ–¼?ï¿½ï¿½?
- MAPEï¼šç›¸å°èª¤å·®ï¿½?ï¿½?$y_i \approx 0$ ?ï¿½ï¿½?ç©©ï¿½?
- sMAPEï¼šè§£ï¿½?MAPE ?ï¿½ï¿½?ç¨±æ€§ï¿½?ï¿½?

#### ?ï¿½ï¿½??ï¿½æ¸¬æº–ç¢º??

**å®šç¾©**ï¼šï¿½?æ¸¬ï¿½??ï¿½æ–¹?ï¿½æ˜¯?ï¿½æ­£ï¿½?

$$
\text{DA} = \frac{1}{n-1}\sum_{t=2}^{n} \mathbb{1}\left\{\text{sign}(\Delta y_t) = \text{sign}(\Delta \hat{y}_t)\right\}
$$

?ï¿½ä¸­ $\Delta y_t = y_t - y_{t-1}$

**?ï¿½ç”¨**ï¿½?
- ?ï¿½åˆ¶æ±ºï¿½?ï¼šï¿½?ï¿½??ï¿½æº«?ï¿½ï¿½?
- è¶¨å‹¢?ï¿½è­¦ï¼šï¿½???ä¸‹ï¿½?è¶¨å‹¢

#### ?ï¿½?ï¿½ï¿½?æ¸¬ï¿½?ï¿½?

**Coverageï¼ˆï¿½??ï¿½ï¿½?ï¿½?*ï¿½?
$$
\text{Coverage} = \frac{1}{n}\sum_{i=1}^{n} \mathbb{1}\{y_i \in [\hat{y}_i^L, \hat{y}_i^U]\}
$$

?ï¿½æƒ³??= ç½®ä¿¡æ°´å¹³ï¼ˆï¿½? 95%ï¿½?

**Interval Widthï¼ˆï¿½??ï¿½å¯¬åº¦ï¿½?**ï¿½?
$$
\text{Width} = \frac{1}{n}\sum_{i=1}^{n} (\hat{y}_i^U - \hat{y}_i^L)
$$

è¶Šï¿½?è¶Šå¥½ï¼ˆåœ¨ä¿ï¿½?è¦†ï¿½??ï¿½ï¿½??ï¿½ï¿½?ä¸‹ï¿½?

**Winkler Score**ï¿½?
$$
\text{WS} = \frac{1}{n}\sum_{i=1}^{n} \left[\text{Width}_i + \frac{2}{\alpha}(\hat{y}_i^L - y_i)\mathbb{1}\{y_i < \hat{y}_i^L\} + \frac{2}{\alpha}(y_i - \hat{y}_i^U)\mathbb{1}\{y_i > \hat{y}_i^U}\right]
$$

ç¶œï¿½??ï¿½æ…®å¯¬åº¦?ï¿½ï¿½??ï¿½ï¿½?

### 5.2 ?ï¿½ï¿½?åºï¿½?äº¤ï¿½?é©—ï¿½?

#### ?ï¿½çµ± K-Fold ?ï¿½ï¿½?ï¿½?

**?ï¿½èª¤?ï¿½ï¿½?**ï¼šéš¨æ©Ÿï¿½???
```
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
Fold 1: Train=[1,3,5,7,9], Test=[2,4,6,8,10]  ??
```

**?ï¿½ï¿½?**ï¿½?
- ?ï¿½ï¿½??ï¿½ï¿½??ï¿½ï¿½?
- ?ï¿½æœªä¾†ï¿½?æ¸¬ï¿½??ï¿½ï¿½?data leakageï¿½?
- é«˜ä¼°æ¨¡ï¿½??ï¿½èƒ½

#### Time Series Split

**ï¿½?ï¿½ï¿½?ï¿½ï¿½?**ï¼šï¿½??ï¿½ï¿½??ï¿½ï¿½?ï¿½?
```
Fold 1: Train=[1,2,3,4], Test=[5,6]
Fold 2: Train=[1,2,3,4,5,6], Test=[7,8]
Fold 3: Train=[1,2,3,4,5,6,7,8], Test=[9,10]
```

**ï¿½?ï¿½ï¿½å¯¦ç¾**ï¿½?
```python
from sklearn.model_selection import TimeSeriesSplit

tscv = TimeSeriesSplit(n_splits=5)
for train_idx, test_idx in tscv.split(X):
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]
    # è¨“ç·´?ï¿½ï¿½?ï¿½?
```

#### Rolling Window Validation

**?ï¿½ï¿½?çª—å£å¤§ï¿½?**ï¿½?
```
Window = 1000 samples
Fold 1: Train=[1:1000], Test=[1001:1100]
Fold 2: Train=[101:1100], Test=[1101:1200]
Fold 3: Train=[201:1200], Test=[1201:1300]
```

**?ï¿½ï¿½?**ï¿½?
- æ¨¡æ“¬å¯¦ï¿½??ï¿½ç½²ï¼ˆå›ºå®šï¿½?ç·´ï¿½?å¤§ï¿½?ï¿½?
- è©•ä¼°æ¨¡ï¿½??ï¿½æ–°?ï¿½ï¿½?ä¸Šï¿½?ç©©ï¿½???

**Expanding Window**ï¿½?
```
Fold 1: Train=[1:1000], Test=[1001:1100]
Fold 2: Train=[1:1100], Test=[1101:1200]
Fold 3: Train=[1:1200], Test=[1201:1300]
```

**?ï¿½ï¿½?å»ºè­°**ï¿½?
- Rollingï¼šï¿½?å¿µï¿½?ç§»åš´?ï¿½ï¿½?å¦‚è¨­?ï¿½è€ï¿½?ï¿½?
- Expandingï¼šæ•¸?ï¿½ï¿½?ä½ˆç©©ï¿½?

### 5.3 Baseline å°ï¿½??ï¿½ï¿½?è¦ï¿½?

**?ï¿½ï¿½?éº¼ï¿½?ï¿½?Baselineï¿½?*

1. **æª¢ï¿½?æ¨¡ï¿½??ï¿½å¦?ï¿½æ­£å­¸ï¿½?**ï¿½?
   - ç°¡å–®æ¨¡ï¿½??ï¿½èƒ½å·²ï¿½?å¾ˆå¥½
   - è¤‡ï¿½?æ¨¡ï¿½??ï¿½ï¿½??ï¿½æ˜¯?ï¿½å€¼ï¿½?

2. **?ï¿½è§£?ï¿½ï¿½???ï¿½ï¿½**ï¿½?
   - Baseline å¾ˆå·® ???ï¿½ï¿½??ï¿½é›£
   - Baseline å¾ˆå¥½ ??ç°¡å–®æ¨¡ï¿½?ä¸»ï¿½?

3. **?ï¿½ç¾?ï¿½ï¿½??ï¿½ï¿½?**ï¿½?
   - ?ï¿½?ï¿½æ¨¡?ï¿½éƒ½å¾ˆå·® ???ï¿½ï¿½?è³ªï¿½??ï¿½ï¿½?
   - è¤‡ï¿½?æ¨¡ï¿½?ä¸ï¿½?ç°¡å–®æ¨¡ï¿½? ???ï¿½æ“¬??

**å¸¸ç”¨ Baseline**ï¿½?

| Baseline | ?ï¿½æ¸¬?ï¿½ï¿½? | ?ï¿½ç”¨?ï¿½æ™¯ |
|----------|---------|---------|
| Persistence | $\hat{y}_{t+1} = y_t$ | ç·©æ…¢è®Šï¿½?ç³»çµ± |
| Moving Average | $\hat{y}_{t+1} = \frac{1}{w}\sum_{i=1}^{w} y_{t-i+1}$ | å¹³ç©©åºï¿½? |
| Linear Regression | ç·šæ€§ï¿½?ï¿½?lag features | ç·šæ€§ï¿½?ï¿½?|
| Random Forest | æ¨¹æ¨¡??| ?ï¿½ï¿½??ï¿½ï¿½?ï¿½?|
| MLP | æ·ºå±¤ç¥ï¿½?ç¶²è·¯ | ??LSTM å°ï¿½? |

**?ï¿½ï¿½?ç³»çµ±??Baseline è¡¨ç¾**ï¿½?
- Persistenceï¼šRMSE ~ 2-3Â°Cï¼ˆæº«åº¦ï¿½??ï¿½ç·©?ï¿½ï¿½?
- Linear Regressionï¼šRMSE ~ 1-1.5Â°C
- Random Forestï¼šRMSE ~ 0.8-1.2Â°C
- LSTM ?ï¿½ï¿½?ï¿½? 0.8Â°C

---

## ç¬¬å…­ç« ï¿½?LSTM æ¨¡ï¿½?å¯¦ç¾?ï¿½ï¿½?ï¿½?

### 6.1 TensorFlow/Keras å¯¦ç¾

#### ?ï¿½æœ¬ LSTM æ¨¡ï¿½?

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

def build_lstm_model(input_shape, lstm_units=64, dense_units=32):
    """
    input_shape: (time_steps, n_features)
    """
    model = keras.Sequential([
        layers.LSTM(lstm_units, 
                   return_sequences=False,  # ?ï¿½æ­¥?ï¿½æ¸¬
                   input_shape=input_shape),
        layers.Dropout(0.2),
        layers.Dense(dense_units, activation='relu'),
        layers.Dropout(0.2),
        layers.Dense(1)  # ?ï¿½ï¿½??ï¿½ï¿½?ï¿½?
    ])
    
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='mse',
        metrics=['mae']
    )
    
    return model
```

#### ?ï¿½ï¿½? LSTMï¼ˆStacked LSTMï¿½?

```python
model = keras.Sequential([
    layers.LSTM(128, return_sequences=True, input_shape=input_shape),
    layers.Dropout(0.2),
    layers.LSTM(64, return_sequences=True),
    layers.Dropout(0.2),
    layers.LSTM(32, return_sequences=False),
    layers.Dropout(0.2),
    layers.Dense(16, activation='relu'),
    layers.Dense(1)
])
```

**return_sequences**ï¿½?
- `True`ï¼šè¼¸?ï¿½ï¿½??ï¿½ï¿½??ï¿½æ­¥?ï¿½éš±?ï¿½ï¿½??ï¿½ï¿½??ï¿½æ–¼?ï¿½ï¿½?LSTMï¿½?
- `False`ï¼šåªè¼¸å‡º?ï¿½å¾Œï¿½??ï¿½æ­¥ï¼ˆç”¨?ï¿½ï¿½?æ¸¬ï¿½?

#### ?ï¿½ï¿½? LSTMï¼ˆBidirectionalï¿½?

```python
model = keras.Sequential([
    layers.Bidirectional(
        layers.LSTM(64, return_sequences=False),
        input_shape=input_shape
    ),
    layers.Dense(32, activation='relu'),
    layers.Dense(1)
])
```

**?ï¿½ç”¨?ï¿½æ™¯**ï¿½?
- ?ï¿½ï¿½??ï¿½ï¿½?ï¼ˆå¯?ï¿½åˆ°?ï¿½ï¿½??ï¿½ï¿½?ï¿½?
- ?ï¿½å¸¸æª¢æ¸¬ï¼ˆï¿½?å¾Œï¿½??ï¿½ï¿½?è¦ï¿½?
- **ä¸é©??*ï¼šå¯¦?ï¿½ï¿½?æ¸¬ï¿½??ï¿½ï¿½??ï¿½åˆ°?ï¿½ï¿½?ï¿½?

### 6.2 è¨“ç·´ç­–ç•¥

#### Early Stopping

```python
early_stop = keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True,
    verbose=1
)

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=100,
    batch_size=32,
    callbacks=[early_stop],
    verbose=1
)
```

#### Learning Rate Scheduling

**?ï¿½æ•¸è¡°ï¿½?**ï¿½?
```python
def lr_schedule(epoch, lr):
    if epoch < 10:
        return lr
    else:
        return lr * tf.math.exp(-0.1)

lr_callback = keras.callbacks.LearningRateScheduler(lr_schedule)
```

**ReduceLROnPlateau**ï¿½?
```python
reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    min_lr=1e-6,
    verbose=1
)
```

#### Model Checkpoint

```python
checkpoint = keras.callbacks.ModelCheckpoint(
    'best_model.h5',
    monitor='val_loss',
    save_best_only=True,
    verbose=1
)
```

### 6.3 è¶…åƒæ•¸èª¿å„ª

**é—œéµè¶…åƒæ•¸**ï¼š

| è¶…åƒæ•¸ | ç¯„åœ | å½±éŸ¿ |
|-------|------|------|
| `lstm_units` | 32-256 | æ¨¡å‹å®¹é‡ |
| `num_layers` | 1-3 | æŠ½è±¡å±¤æ¬¡ |
| `dropout_rate` | 0.1-0.5 | æ­£å‰‡åŒ–å¼·åº¦ |
| `learning_rate` | 1e-4 ~ 1e-2 | æ”¶æ–‚é€Ÿåº¦ |
| `batch_size` | 16-128 | è¨“ç·´ç©©å®šæ€§ |
| `window_size` | 10-60 | æ­·å²ä¿¡æ¯é‡ |

**èª¿å„ªç­–ç•¥**ï¼š

1. **Grid Search**ï¼šçª®?ï¿½ï¿½?ï¿½?
   ```python
   from sklearn.model_selection import ParameterGrid
   
   param_grid = {
       'lstm_units': [32, 64, 128],
       'dropout_rate': [0.2, 0.3],
       'learning_rate': [0.001, 0.0001]
   }
   
   best_score = float('inf')
   for params in ParameterGrid(param_grid):
       model = build_lstm_model(**params)
       # è¨“ç·´å’Œè©•ä¼°
       score = evaluate(model, X_val, y_val)
       if score < best_score:
           best_score = score
           best_params = params
   ```

2. **Random Search**ï¼šéš¨æ©Ÿæ¡æ¨£
   ```python
   from sklearn.model_selection import RandomizedSearchCV
   
   # é©åˆé«˜ç¶­ç©ºé–“
   ```

3. **Bayesian Optimization**ï¼š
   ```python
   from keras_tuner import BayesianOptimization
   
   def build_model(hp):
       model = keras.Sequential()
       model.add(layers.LSTM(
           hp.Int('units', 32, 256, step=32),
           input_shape=input_shape
       ))
       model.add(layers.Dropout(
           hp.Float('dropout', 0.1, 0.5, step=0.1)
       ))
       model.add(layers.Dense(1))
       model.compile(
           optimizer=keras.optimizers.Adam(
               hp.Float('lr', 1e-4, 1e-2, sampling='log')
           ),
           loss='mse'
       )
       return model
   
   tuner = BayesianOptimization(
       build_model,
       objective='val_loss',
       max_trials=20
   )
   
   tuner.search(X_train, y_train, validation_data=(X_val, y_val))
   ```

### 6.4 éæ“¬åˆè¨ºæ–·èˆ‡é é˜²

**éæ“¬åˆç‰¹å¾µ**ï¼š
- è¨“ç·´èª¤å·®æŒçºŒä¸‹é™ï¼Œé©—è­‰èª¤å·®ä¸Šå‡
- æ¨¡å‹åœ¨è¨“ç·´é›†è¡¨ç¾å®Œç¾ï¼Œæ¸¬è©¦é›†å¾ˆå·®
- é æ¸¬æ›²ç·šéåº¦æ“¬åˆå™ªè²

**é é˜²æªæ–½**ï¼š

1. **å¢åŠ æ•¸æ“šé‡**ï¼š
   - æ”¶é›†æ›´å¤šæ­·å²æ•¸æ“š
   - æ•¸æ“šå¢å¼·ï¼ˆåŠ å™ªè²ã€æ™‚é–“æ‰­æ›²ï¼‰

2. **æ­£å‰‡åŒ–**ï¼š
   - L2 æ­£å‰‡åŒ–ï¼ˆæ¬Šé‡è¡°æ¸›ï¼‰
     ```python
     layers.LSTM(64, kernel_regularizer=keras.regularizers.l2(0.01))
     ```
   - Dropoutï¼šéš¨æ©Ÿä¸Ÿæ£„ç¥ç¶“å…ƒ
   - Recurrent Dropoutï¼š
     ```python
     layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2)
     ```

3. **ç°¡åŒ–æ¨¡å‹**ï¼š
   - æ¸›å°‘ LSTM å–®å…ƒæ•¸
   - æ¸›å°‘å±¤æ•¸
   - ç¸®çŸ­çª—å£å¤§å°

4. **Early Stopping**ï¼š
   - é©—è­‰èª¤å·®ä¸å†æ¸›å°æ™‚åœæ­¢

5. **Batch Normalization**ï¼š
   ```python
   model = keras.Sequential([
       layers.LSTM(64, return_sequences=True),
       layers.BatchNormalization(),
       layers.LSTM(32),
       layers.Dense(1)
   ])
   ```

---

## ç¬¬ä¸ƒç« ï¼šå·¥æ¥­éƒ¨ç½²èˆ‡åœ¨ç·šé æ¸¬

### 7.1 æ¨¡å‹éƒ¨ç½²æ¶æ§‹

**é›¢ç·šè¨“ç·´ vs åœ¨ç·šé æ¸¬**ï¼š

```
[æ­·å²æ•¸æ“š] â†’ [ç‰¹å¾µå·¥ç¨‹] â†’ [æ¨¡å‹è¨“ç·´] â†’ [æ¨¡å‹å°å‡º]
                                              â†“
[å¯¦æ™‚æ•¸æ“š] â†’ [ç‰¹å¾µæå–] â†’ [æ¨¡å‹æ¨ç†] â†’ [é æ¸¬çµæœ] â†’ [æ§åˆ¶æ±ºç­–]
```

**é—œéµçµ„ä»¶**ï¼š

1. **æ•¸æ“šç®¡é“ï¼ˆData Pipelineï¼‰**ï¼š
   ```python
   class DataPipeline:
       def __init__(self, scaler, window_size):
           self.scaler = scaler
           self.window_size = window_size
           self.buffer = deque(maxlen=window_size)
       
       def update(self, new_data):
           self.buffer.append(new_data)
       
       def get_features(self):
           if len(self.buffer) < self.window_size:
               return None
           X = np.array(self.buffer).reshape(1, self.window_size, -1)
           return self.scaler.transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)
   ```

2. **æ¨¡å‹æœå‹™ï¼ˆModel Servingï¼‰**ï¼š
   - TensorFlow Serving
   - ONNX Runtime
   - Flask/FastAPI REST API

3. **é æ¸¬ç·©å­˜ï¼ˆPrediction Cacheï¼‰**ï¼š
   - é¿å…é‡è¤‡è¨ˆç®—
   - é™ä½å»¶é²

### 7.2 æŒçºŒå­¸ç¿’èˆ‡æ¨¡å‹æ›´æ–°

**æ¦‚å¿µæ¼‚ç§»ï¼ˆConcept Driftï¼‰**ï¼š

æ•¸æ“šåˆ†ä½ˆéš¨æ™‚é–“è®ŠåŒ–ï¼š
$$
P_t(X, y) \neq P_{t+\Delta t}(X, y)
$$

**æª¢æ¸¬æ–¹æ³•**ï¼š

1. **çµ±è¨ˆæª¢é©—**ï¼š
   ```python
   from scipy.stats import ks_2samp
   
   # Kolmogorov-Smirnov test
   stat, p_value = ks_2samp(recent_errors, historical_errors)
   if p_value < 0.05:
       print("Drift detected! Retrain model.")
   ```

2. **æ€§èƒ½ç›£æ§**ï¼š
2. **æ€§èƒ½ç›£æ§**ï¼š
   ```python
   rolling_rmse = []
   window = 100
   for i in range(len(predictions)):
       if i >= window:
           rmse = np.sqrt(mean_squared_error(
               actuals[i-window:i], 
               predictions[i-window:i]
           ))
           rolling_rmse.append(rmse)
           
           if rmse > threshold:
               trigger_retrain()
   ```

**æ›´æ–°ç­–ç•¥**ï¼š

1. **å®šæœŸé‡è¨“ï¼ˆPeriodic Retrainingï¼‰**ï¼š
   - æ¯é€±/æ¯æœˆé‡æ–°è¨“ç·´
   - ä½¿ç”¨æœ€æ–°æ•¸æ“š

2. **è§¸ç™¼å¼é‡è¨“ï¼ˆTriggered Retrainingï¼‰**ï¼š
   - æ€§èƒ½ä¸‹é™è¶…éé–¾å€¼
   - æª¢æ¸¬åˆ°æ¼‚ç§»

3. **å¢é‡å­¸ç¿’ï¼ˆIncremental Learningï¼‰**ï¼š
   - åœ¨ç·šæ›´æ–°æ¬Šé‡ï¼ˆéœ€ç‰¹æ®Šæ¶æ§‹ï¼‰
   - é©åˆè³‡æºå—é™å ´æ™¯

### 7.3 ç•°å¸¸æª¢æ¸¬èˆ‡é è­¦

**é æ¸¬å€é–“ï¼ˆPrediction Intervalï¼‰**ï¼š

ä½¿ç”¨ Quantile Regression æˆ– Monte Carlo Dropoutï¼š

```python
# Monte Carlo Dropout
def mc_dropout_prediction(model, X, n_iter=100):
    predictions = []
    for _ in range(n_iter):
        pred = model(X, training=True)  # ä¿æŒ Dropout å•Ÿç”¨
        predictions.append(pred.numpy())
    
    predictions = np.array(predictions)
    mean_pred = predictions.mean(axis=0)
    std_pred = predictions.std(axis=0)
    
    lower = mean_pred - 2*std_pred
    upper = mean_pred + 2*std_pred
    
    return mean_pred, lower, upper
```

**ç•°å¸¸å®šç¾©**ï¼š
- å¯¦éš›å€¼è¶…å‡ºé æ¸¬å€é–“
- é€£çºŒå¤šæ­¥èª¤å·®éå¤§
- æ¢¯åº¦ç•°å¸¸è®ŠåŒ–

**é è­¦ç­–ç•¥**ï¼š
```python
def check_anomaly(actual, pred, lower, upper):
    if actual < lower or actual > upper:
        return 'WARNING: Value outside prediction interval'
    
    if abs(actual - pred) > 3*std_historical:
        return 'ERROR: Extreme deviation'
    
    return 'NORMAL'
```

---

## ç¬¬å…«ç« ï¼šæ¡ˆä¾‹ç ”ç©¶

### 8.1 é‹çˆè’¸æ±½æº«åº¦é æ¸¬

**å•é¡Œæè¿°**ï¼š
- é æ¸¬æœªä¾† 10 åˆ†é˜çš„è’¸æ±½æº«åº¦
- è¼¸å…¥ï¼šç‡ƒæ–™æµé‡ã€é¢¨é‡ã€å£“åŠ›ç­‰ 8 å€‹è®Šæ•¸
- æ­·å²çª—å£ï¼š60 åˆ†é˜

**æ¨¡å‹å°æ¯”**ï¼š

| æ¨¡å‹ | RMSE | MAE | è¨“ç·´æ™‚é–“ | æ¨ç†æ™‚é–“ |
|-----|------|-----|---------|---------|
| Persistence | 2.45Â°C | 1.89Â°C | - | <1ms |
| ARIMA | 1.87Â°C | 1.42Â°C | 5s | 10ms |
| Random Forest | 1.32Â°C | 0.98Â°C | 2min | 5ms |
| MLP | 1.18Â°C | 0.87Â°C | 3min | 2ms |
| LSTM | **0.94Â°C** | **0.71Â°C** | 15min | 8ms |

**é—œéµç™¼ç¾**ï¼š
1. LSTM åœ¨æ•æ‰é•·æœŸä¾è³´ä¸Šå„ªæ–¼å‚³çµ±æ¨¡å‹
2. å¤šæ­¥é æ¸¬æ™‚ï¼ŒLSTM å„ªå‹¢æ›´æ˜é¡¯ï¼ˆhorizon > 5ï¼‰
3. ç‡ƒæ–™æµé‡çš„æ»¯å¾Œæ•ˆæ‡‰ï¼ˆlag 8-12åˆ†é˜ï¼‰è¢« LSTM è‡ªå‹•å­¸ç¿’

**Attention æ¬Šé‡åˆ†æ**ï¼š
- è¿‘æœŸæ™‚åˆ»ï¼ˆt-1 ~ t-5ï¼‰ï¼šæ¬Šé‡ 45%
- ä¸­æœŸæ™‚åˆ»ï¼ˆt-10 ~ t-20ï¼‰ï¼šæ¬Šé‡ 35%ï¼ˆç‡ƒæ–™æ»¯å¾Œæ•ˆæ‡‰ï¼‰
- é æœŸæ™‚åˆ»ï¼ˆt-30 ~ t-60ï¼‰ï¼šæ¬Šé‡ 20%ï¼ˆè² è·è®ŠåŒ–è¶¨å‹¢ï¼‰

### 8.2 åæ‡‰å™¨æº«åº¦æ§åˆ¶

**æŒ‘æˆ°**ï¼š
- å¼·éç·šæ€§ï¼šArrhenius å®šå¾‹
- å¿«é€Ÿå‹•æ…‹ï¼šç§’ç´šéŸ¿æ‡‰
- å®‰å…¨ç´„æŸï¼šæº«åº¦ä¸èƒ½è¶…é 350Â°C

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
1. **Seq2Seq æ¶æ§‹**ï¼šç›´æ¥é æ¸¬æœªä¾† 5 åˆ†é˜è»Œè·¡
2. **ç´„æŸæå¤±å‡½æ•¸**ï¼š
   ```python
   def constrained_loss(y_true, y_pred):
       mse = keras.losses.mse(y_true, y_pred)
       penalty = keras.backend.maximum(0, y_pred - 350) ** 2
       return mse + 10 * penalty
   ```
3. **æ¨¡å‹é æ¸¬æ§åˆ¶ï¼ˆMPCï¼‰æ•´åˆ**ï¼š
   - LSTM æä¾›å‹•æ…‹æ¨¡å‹
   - MPC æ±‚è§£æœ€å„ªæ§åˆ¶åºåˆ—

**æ€§èƒ½æŒ‡æ¨™**ï¼š
- æº«åº¦åå·®ï¼šå¹³å‡ Â±1.2Â°Cï¼ˆç›®æ¨™ Â±2Â°Cï¼‰
- è¶…æº«äº‹ä»¶ï¼šé™ä½ 85%ï¼ˆå¾ 12æ¬¡/æœˆ â†’ 2æ¬¡/æœˆï¼‰
- èƒ½è€—é™ä½ï¼š8%ï¼ˆæ›´å¹³ç©©çš„æ§åˆ¶ï¼‰

### 8.3 ç²¾é¤¾å¡”ç”¢å“è³ªé‡é æ¸¬

**Soft Sensor æ‡‰ç”¨**ï¼š
- å¯¦é©—å®¤åˆ†æå»¶é²ï¼š4å°æ™‚
- LSTM é æ¸¬å»¶é²ï¼š<1ç§’
- å¯¦ç¾åœ¨ç·šè³ªé‡æ§åˆ¶

**ç‰¹å¾µå·¥ç¨‹**ï¼š
```python
features = [
    'tower_top_temp',
    'tower_bottom_temp', 
    'reflux_ratio',
    'feed_flow',
    'feed_composition',
    # å°å‡ºç‰¹å¾µ
    'temp_gradient',     # å¡”é ‚å¡”åº•æº«å·®
    'temp_delta_15min',  # 15åˆ†é˜æº«åº¦è®ŠåŒ–
    'ma_reflux_10min'    # å›æµæ¯”æ»¾å‹•å¹³å‡
]
```

**å¤šä»»å‹™å­¸ç¿’**ï¼š
åŒæ™‚é æ¸¬è¼•çµ„åˆ†ç´”åº¦å’Œé‡çµ„åˆ†ç´”åº¦

```python
# å…±äº«ç·¨ç¢¼å™¨
encoder = keras.Sequential([
    layers.LSTM(128, return_sequences=True),
    layers.LSTM(64)
])

# å…©å€‹é æ¸¬é ­
x = encoder(inputs)
output_light = layers.Dense(1, name='light_component')(x)
output_heavy = layers.Dense(1, name='heavy_component')(x)

model = keras.Model(inputs, [output_light, output_heavy])
model.compile(
    optimizer='adam',
    loss={'light_component': 'mse', 'heavy_component': 'mse'},
    loss_weights={'light_component': 1.0, 'heavy_component': 1.0}
)
```

**æˆæœ**ï¼š
- è¼•çµ„åˆ†ç´”åº¦é æ¸¬ RÂ² = 0.94
- é‡çµ„åˆ†ç´”åº¦é æ¸¬ RÂ² = 0.91
- æ¸›å°‘é›¢ç·šåˆ†ææ¬¡æ•¸ 70%

---

## ç¬¬ä¹ç« ï¼šé€²éšä¸»é¡Œ

### 9.1 Attention Mechanism

**å‹•æ©Ÿ**ï¼šLSTM å°‡æ‰€æœ‰ä¿¡æ¯å£“ç¸®åˆ°æœ€å¾Œä¸€å€‹éš±ç‹€æ…‹ï¼Œå¯èƒ½ä¸Ÿå¤±é‡è¦ä¿¡æ¯ã€‚

**Attention æ©Ÿåˆ¶**ï¼šé¸æ“‡æ€§æ³¨æ„ä¸åŒæ™‚æ­¥

$$
\alpha_t = \frac{\exp(e_t)}{\sum_{i=1}^{T} \exp(e_i)}
$$
$$
e_t = \text{score}(\mathbf{h}_t, \mathbf{s})
$$
$$
\mathbf{c} = \sum_{t=1}^{T} \alpha_t \mathbf{h}_t
$$

å…¶ä¸­ï¼š
- $\mathbf{h}_t$ï¼šencoder åœ¨æ™‚åˆ» $t$ çš„éš±ç‹€æ…‹
- $\mathbf{s}$ï¼šdecoder çš„ç‹€æ…‹
- $\alpha_t$ï¼šattention æ¬Šé‡
- $\mathbf{c}$ï¼šcontext vector

**å¯¦ç¾**ï¼š
```python
from tensorflow.keras.layers import AdditiveAttention

encoder_outputs = layers.LSTM(64, return_sequences=True)(inputs)
query = layers.LSTM(64)(inputs)
attention = AdditiveAttention()([query, encoder_outputs])
output = layers.Dense(1)(attention)
```

### 9.2 Transformer for Time Series

**Self-Attention**ï¼š
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

**ä½ç½®ç·¨ç¢¼ï¼ˆPositional Encodingï¼‰**ï¼š
$$
PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d})
$$
$$
PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d})
$$

**å„ªå‹¢**ï¼š
- ä¸¦è¡Œè¨ˆç®—ï¼ˆæ¯” LSTM å¿«ï¼‰
- é•·è·é›¢ä¾è³´ï¼ˆç„¡æ¢¯åº¦æ¶ˆå¤±ï¼‰
- å¯è§£é‡‹æ€§ï¼ˆattention æ¬Šé‡ï¼‰

**æ‡‰ç”¨**ï¼š
- Temporal Fusion Transformer (TFT)
- Autoformer
- Informer

### 9.3 Neural ODE

**é€£çºŒæ™‚é–“å»ºæ¨¡**ï¼š

å°‡é›¢æ•£ RNN æ³›åŒ–ç‚ºé€£çºŒ ODEï¼š
$$
\frac{d\mathbf{h}(t)}{dt} = f_\theta(\mathbf{h}(t), t)
$$

**æ±‚è§£**ï¼šä½¿ç”¨ ODE solver
```python
from torchdiffeq import odeint

def ode_func(t, h):
    return model(h, t)

h_final = odeint(ode_func, h_0, t_span)
```

**å„ªå‹¢**ï¼š
- æ™‚é–“æ­¥é•·ä¸å—é™åˆ¶
- å…§å­˜é«˜æ•ˆ
- æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›

### 9.4 é›†æˆæ–¹æ³•

**çµ„åˆå¤šå€‹æ¨¡å‹**ï¼š
```python
# çµ„åˆå¤šå€‹æ¨¡å‹
predictions = []
weights = [0.3, 0.3, 0.4]  # ARIMA, RF, LSTM

pred_arima = model_arima.forecast(steps=horizon)
pred_rf = model_rf.predict(X_test)
pred_lstm = model_lstm.predict(X_test)

final_pred = (weights[0] * pred_arima + 
              weights[1] * pred_rf + 
              weights[2] * pred_lstm)
```

---

## ç¬¬åç« ï¼šç¸½çµèˆ‡æœ€ä½³å¯¦è¸

### 10.1 å»ºæ¨¡æµç¨‹ Checklist

**1. æ•¸æ“šæº–å‚™** ï¼š
- [ ] æª¢æŸ¥ç¼ºå¤±å€¼å’Œç•°å¸¸å€¼
- [ ] ç¹ªè£½æ™‚é–“åºåˆ—åœ–ï¼Œè­˜åˆ¥è¶¨å‹¢å’Œå­£ç¯€æ€§
- [ ] è¨ˆç®— ACF/PACF
- [ ] æª¢é©—å¹³ç©©æ€§ï¼ˆADF testï¼‰
- [ ] æŒ‰æ™‚é–“é †åºåŠƒåˆ†è¨“ç·´/é©—è­‰/æ¸¬è©¦é›†

**2. ç‰¹å¾µå·¥ç¨‹** ï¼š
- [ ] å‰µå»ºæ»¯å¾Œç‰¹å¾µ
- [ ] è¨ˆç®—æ»¾å‹•çµ±è¨ˆï¼ˆMA, Std, EWMAï¼‰
- [ ] æ·»åŠ æ™‚é–“ç‰¹å¾µï¼ˆhour, day of weekï¼‰
- [ ] å·¥ç¨‹ç‰¹å¾µï¼ˆç‰©ç†å…¬å¼ã€é ˜åŸŸçŸ¥è­˜ï¼‰
- [ ] æ¨™æº–åŒ–æˆ–æ­¸ä¸€åŒ–

**3. Baseline æ¨¡å‹** ï¼š
- [ ] Persistence
- [ ] Moving Average
- [ ] Linear Regression
- [ ] Random Forest / XGBoost
- [ ] MLP

**4. LSTM æ¨¡å‹** ï¼š
- [ ] è¨­è¨ˆç¶²è·¯æ¶æ§‹
- [ ] è¨­ç½®è¨“ç·´å›èª¿ï¼ˆEarly Stopping, LR Schedulerï¼‰
- [ ] è¨“ç·´æ¨¡å‹
- [ ] ç›£æ§ training/validation loss
- [ ] è¶…åƒæ•¸èª¿å„ª

**5. è©•ä¼°èˆ‡å°æ¯”** ï¼š
- [ ] è¨ˆç®—å¤šç¨®æŒ‡æ¨™ï¼ˆRMSE, MAE, MAPEï¼‰
- [ ] ç¹ªè£½é æ¸¬æ›²ç·š
- [ ] æ®˜å·®åˆ†æ
- [ ] èˆ‡ Baseline å°æ¯”
- [ ] Rolling backtest

**6. éƒ¨ç½²æº–å‚™** ï¼š
- [ ] æ¨¡å‹å°å‡ºï¼ˆSavedModel, ONNXï¼‰
- [ ] æ¨ç†é€Ÿåº¦æ¸¬è©¦
- [ ] æ¼‚ç§»æª¢æ¸¬æ©Ÿåˆ¶
- [ ] é è­¦è¦å‰‡è¨­ç½®
- [ ] æ›´æ–°ç­–ç•¥

### 10.2 å¸¸è¦‹éŒ¯èª¤èˆ‡è§£æ±º

| å•é¡Œ | åŸå›  | è§£æ±ºæ–¹æ¡ˆ |
|-----|------|---------|
| é æ¸¬å€¼å®Œå…¨æ»¯å¾Œå¯¦éš›å€¼ | æ¨¡å‹å­¸åˆ°"è¤‡è£½ä¸Šä¸€æ­¥" | å¢åŠ é æ¸¬horizonçš„æ‡²ç½°ã€æ»¯å¾Œ |
| è¨“ç·´å¾ˆå¥½æ¸¬è©¦å¾ˆå·® | éæ“¬åˆ | Dropoutã€Early Stoppingã€ç°¡åŒ–æ¨¡å‹ |
| è¨“ç·´å’Œæ¸¬è©¦éƒ½å¾ˆå·® | æ¬ æ“¬åˆ | å¢åŠ æ¨¡å‹å®¹é‡ã€æ›´å¤šç‰¹å¾µ |
| Loss ä¸æ”¶æ–‚ | å­¸ç¿’ç‡å¤ªå¤§ã€æ•¸æ“šæœªæ¨™æº–åŒ– | é™ä½LRã€æ¨™æº–åŒ–è¼¸å…¥ |
| é æ¸¬å€¼è¶¨æ–¼å¹³å‡ | æ¨¡å‹ä¿å®ˆé æ¸¬å‡å€¼ | æª¢æŸ¥æ•¸æ“šè³ªé‡ã€å¢åŠ æ¨¡å‹è¤‡é›œåº¦ |
| æ¢¯åº¦çˆ†ç‚¸ | æ¬Šé‡åˆå§‹åŒ–ä¸ç•¶ | Gradient Clippingã€Xavieråˆå§‹åŒ– |

### 10.3 å·¥æ¥­æ‡‰ç”¨å»ºè­°

**1. å¾ç°¡å–®é–‹å§‹**ï¼š
- å…ˆå»ºç«‹å¯è§£é‡‹ Baseline
- ç¢ºèª LSTM çœŸæ­£å¸¶ä¾†æ”¹é€²
- ä¸è¦ç‚ºäº†é€²æ­¥ 0.5% RMSE çŠ§ç‰²å¤ªå¤š

**2. èå…¥é ˜åŸŸçŸ¥è­˜**ï¼š
- ç‰©ç†ç´„æŸï¼ˆèƒ½é‡å®ˆæ†ã€è³ªé‡å®ˆæ†ï¼‰
- å·²çŸ¥æ™‚é–“å¸¸æ•¸å’Œå»¶é²
- å·²çŸ¥å› æœé—œä¿‚

**3. é­¯æ£’æ€§å„ªå…ˆæ–¼æº–ç¢ºæ€§**ï¼š
- åœ¨ç•°å¸¸å·¥æ³ä¸‹çš„è¡¨ç¾
- å°å‚³æ„Ÿå™¨æ•…éšœçš„å®¹éŒ¯
- æ¨¡å‹é‡è¨“çš„ç©©å®šæ€§

**4. å¯è§£é‡‹æ€§**ï¼š
- Attention æ¬Šé‡å¯è¦–åŒ–
- SHAP å€¼åˆ†æ
- èˆ‡ç‰©ç†æ¨¡å‹æ¯”å°

**5. æŒçºŒé‹ç¶­**ï¼š
- æ€§èƒ½ç›£æ§å„€è¡¨æ¿
- æ¼‚ç§»æª¢æ¸¬
- å®šæœŸé‡è¨“

### 10.4 å»¶ä¼¸é–±è®€

**ç¶“å…¸è«–æ–‡**ï¼š
1. Hochreiter & Schmidhuber (1997). "Long Short-Term Memory"
2. Cho et al. (2014). "Learning Phrase Representations using RNN Encoder-Decoder"
3. Vaswani et al. (2017). "Attention Is All You Need"

**æ™‚é–“åºåˆ—é æ¸¬**ï¼š
4. Box & Jenkins (1970). "Time Series Analysis: Forecasting and Control"
5. Makridakis et al. (2018). "Statistical and Machine Learning forecasting methods"
6. Lim et al. (2021). "Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting"

**åŒ–å·¥æ‡‰ç”¨**ï¼š
7. Fortuna et al. (2007). "Soft Sensors for Monitoring and Control of Industrial Processes"
8. Kadlec et al. (2009). "Data-driven Soft Sensors in the Process Industry"

---

## åƒè€ƒæ–‡ç»èˆ‡å»¶ä¼¸è³‡æº

### è«–æ–‡

1. **LSTMåŸç†**
   - Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780.

2. **GRU**
   - Cho, K., et al. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. EMNLP 2014.

3. **æ™‚é–“åºåˆ—æ·±åº¦å­¸ç¿’**
   - Lim, B., & Zohren, S. (2021). Time-series forecasting with deep learning: a survey. Philosophical Transactions of the Royal Society A, 379(2194), 20200209.

### é–‹ç™¼å·¥å…·

- **TensorFlow/Keras**: https://www.tensorflow.org/
- **PyTorch**: https://pytorch.org/
- **statsmodels**: https://www.statsmodels.org/
- **sktime**: https://www.sktime.org/
- **Darts**: https://unit8co.github.io/darts/

### åœ¨ç·šèª²ç¨‹

- Stanford CS231n: Deep Learning for Computer Vision
- Coursera: Sequences, Time Series and Prediction (deeplearning.ai)
- Fast.ai: Practical Deep Learning

---

**æœ¬è¬›ç¾©ç·¨å¯«æ–¼ 2025 å¹´ï¼Œå°ˆæ³¨æ–¼åŒ–å·¥è£½ç¨‹ä¸­æ™‚é–“åºåˆ—é æ¸¬çš„å®Œæ•´ç†è«–èˆ‡å¯¦è¸ã€‚**

**æˆèª²æ•™å¸«**ï¼š[æ•™å¸«å§“å]  
**èª²ç¨‹åç¨±**ï¼šåŒ–å·¥AIæ‡‰ç”¨  
**å–®å…ƒ**ï¼šUnit18 - LSTM æ™‚é–“åºåˆ—é æ¸¬  
**ç‰ˆæœ¬**ï¼šv2.0ï¼ˆ2025å¹´å…¨é¢é‡æ§‹ï¼‰
   for t in range(len(y_test)):
       pred = model.predict(X_test[t:t+1])
       error = abs(y_test[t] - pred)
       rolling_rmse.append(error)
       
       if np.mean(rolling_rmse[-100:]) > threshold:
           trigger_retrain()
   ```

**?ï¿½æ–°ç­–ç•¥**ï¿½?

1. **å®šï¿½??ï¿½ï¿½?ï¼ˆPeriodic Retrainingï¿½?*ï¿½?
   - æ¯ï¿½?æ¯ï¿½??ï¿½æ–°è¨“ç·´
   - ä½¿ç”¨?ï¿½??N å¤©æ•¸??

2. **è§¸ç™¼å¼ï¿½?è¨“ï¿½?Triggered Retrainingï¿½?*ï¿½?
   - ?ï¿½èƒ½ä¸‹ï¿½?è¶…ï¿½??ï¿½ï¿½?
   - æª¢æ¸¬?ï¿½ï¿½?ï¿½?

3. **å¢ï¿½?å­¸ï¿½?ï¼ˆIncremental Learningï¿½?*ï¿½?
   ```python
   # ä½¿ç”¨?ï¿½æ•¸?ï¿½å¾®ï¿½?
   model.fit(X_new, y_new, epochs=5, initial_epoch=previous_epochs)
   ```

4. **?ï¿½ï¿½?å­¸ï¿½?ï¼ˆEnsembleï¿½?*ï¿½?
   ```python
   # çµ„ï¿½??ï¿½æ¨¡?ï¿½ï¿½??ï¿½æ¨¡??
   pred = 0.7 * model_old.predict(X) + 0.3 * model_new.predict(X)
   ```

### 7.3 ?ï¿½å¸¸æª¢æ¸¬?ï¿½ï¿½?ï¿½?

**?ï¿½æ¸¬èª¤å·®?ï¿½ï¿½?*ï¿½?

$$
\text{Alert} = \begin{cases}
1 & \text{if } |y - \hat{y}| > k \cdot \sigma \\
0 & \text{otherwise}
\end{cases}
$$

?ï¿½ä¸­ $\sigma$ ?ï¿½æ­·?ï¿½èª¤å·®ï¿½?æ¨™ï¿½?å·®ï¿½?$k=3$ ?ï¿½å¸¸?ï¿½é–¾?ï¿½ï¿½?3-sigma æ³•ï¿½?ï¼‰ï¿½?

**?ï¿½å¸¸?ï¿½æ•¸**ï¿½?

ä½¿ç”¨?ï¿½æ¸¬ä¸ç¢ºå®šæ€§ï¿½?
$$
\text{Anomaly Score} = \frac{|y - \hat{y}|}{\sigma_{\hat{y}}}
$$

**å¤šï¿½??ï¿½è­¦**ï¿½?

| ç´šåˆ¥ | æ¢ä»¶ | ?ï¿½ï¿½? |
|-----|------|------|
| ï¿½?ï¿½ï¿½ | error < 1Â°C | ??|
| è­¦ï¿½? | 1Â°C < error < 2Â°C | è¨˜ï¿½??ï¿½ï¿½? |
| æ³¨ï¿½? | 2Â°C < error < 3Â°C | ?ï¿½çŸ¥?ï¿½ï¿½???|
| ç·Šï¿½?| error > 3Â°C | ?ï¿½ï¿½?èª¿ï¿½?/?ï¿½ï¿½? |

**è¶¨å‹¢?ï¿½è­¦**ï¿½?

æª¢æ¸¬?ï¿½ï¿½??ï¿½é›¢ï¿½?
```python
def trend_alert(errors, window=10, threshold=0.5):
    recent_errors = errors[-window:]
    if np.mean(recent_errors) > threshold:
        return "Sustained deviation detected"
    return "OK"
```

---

## ç¬¬å…«ç« ï¿½??ï¿½å·¥è£½ï¿½?æ¡ˆï¿½??ï¿½ç©¶

### 8.1 ?ï¿½ï¿½??ï¿½æ±½æº«åº¦?ï¿½æ¸¬

**?ï¿½ï¿½??ï¿½è¿°**ï¿½?
- ?ï¿½æ¸¬?ï¿½ï¿½? 10-30 ?ï¿½ï¿½??ï¿½æ±½æº«åº¦
- ?ï¿½ï¿½?èª¿æ•´æ¸›æº«æ°´ï¿½???
- ?ï¿½ï¿½?æº«åº¦è¶…ï¿½?ï¿½?50-480Â°Cï¿½?

**?ï¿½ï¿½??ï¿½ï¿½?*ï¿½?
- ?ï¿½æ¨£?ï¿½ï¿½?ï¿½? ?ï¿½ï¿½?
- è¨“ç·´?ï¿½ï¿½?ï¿½?0 å¤©ï¿½?~43,000 ï¿½?ï¿½ï¿½ï¿½?
- æ¸¬è©¦?ï¿½ï¿½?ï¿½? ï¿½?

**?ï¿½å¾µå·¥ï¿½?**ï¿½?
```python
features = [
    'TE_8332A.AV_0#',  # ?ï¿½ï¿½?ï¼šè’¸æ±½æº«ï¿½?
    'ZZQBCHLL.AV_0#',  # ?ï¿½æ±½æµï¿½?
    'PTCA_8324.AV_0#',  # ?ï¿½ï¿½?å£“ï¿½?
    'AIR_8301A.AV_0#',  # ä¸€æ¬¡é¢¨
    'FT_8301.AV_0#',    # ?ï¿½ï¿½?æµï¿½?
    'TV_8329ZC.AV_0#',  # æ¸›æº«ï¿½?
]

# æ·»ï¿½?æ»¯ï¿½??ï¿½å¾µ
for lag in [1, 5, 10, 15, 30]:
    for col in features:
        df[f'{col}_lag{lag}'] = df[col].shift(lag)

# æ·»ï¿½?æ»¾ï¿½?çµ±ï¿½?
df['temp_ma_15'] = df['TE_8332A.AV_0#'].rolling(15).mean()
df['temp_std_15'] = df['TE_8332A.AV_0#'].rolling(15).std()
```

**æ¨¡ï¿½?å°ï¿½?**ï¿½?

| æ¨¡ï¿½? | RMSE (Â°C) | MAE (Â°C) | è¨“ç·´?ï¿½ï¿½? | ?ï¿½ï¿½??ï¿½ï¿½? |
|------|----------|---------|---------|---------|
| Persistence | 2.34 | 1.82 | - | <1ms |
| Linear Reg | 1.56 | 1.21 | 2s | <1ms |
| Random Forest | 1.12 | 0.87 | 45s | 5ms |
| MLP | 0.95 | 0.74 | 3min | 2ms |
| LSTM (1-layer) | 0.82 | 0.63 | 15min | 10ms |
| LSTM (2-layer) | 0.76 | 0.59 | 25min | 15ms |

**çµï¿½?**ï¿½?
- LSTM ?ï¿½ï¿½? 32% ï¼ˆç›¸ï¿½?Random Forestï¿½?
- è¨ˆï¿½??ï¿½æœ¬?ï¿½æ¥?ï¿½ï¿½?<20ms ?ï¿½ï¿½??ï¿½ï¿½?ï¿½?
- å»ºè­°?ï¿½ç½² LSTM (1-layer) å¹³è¡¡?ï¿½èƒ½?ï¿½ï¿½???

### 8.2 ?ï¿½ï¿½??ï¿½æº«åº¦æ§??

**?ï¿½ï¿½??ï¿½è¿°**ï¿½?
- ?ï¿½ç†±?ï¿½ï¿½?ï¼Œæº«åº¦ï¿½?é«˜ï¿½??ï¿½å‰¯?ï¿½ï¿½?
- ?ï¿½?ï¿½ï¿½? 5-10 ?ï¿½ï¿½??ï¿½æ¸¬æº«åº¦è¶¨å‹¢
- èª¿æ•´?ï¿½å»æ°´ï¿½???

**?ï¿½æˆ°**ï¿½?
- é«˜åº¦?ï¿½ï¿½??ï¿½ï¿½?Arrhenius ?ï¿½ï¿½?ï¿½?
- å¤šå€‹ç©©?ï¿½ï¿½?ç©©ï¿½?/ä¸ç©©å®šï¿½?
- å¿«é€Ÿï¿½??ï¿½ï¿½??ï¿½ï¿½?å¸¸æ•¸ ~2-5?ï¿½ï¿½?ï¿½?

**ï¿½?ï¿½ï¿½?ï¿½ï¿½?**ï¿½?

1. **?ï¿½ï¿½?å»ºæ¨¡**ï¿½?
   ```python
   # ?ï¿½ï¿½?è² è·?ï¿½ï¿½?
   low_load_mask = df['load'] < 0.3
   mid_load_mask = (df['load'] >= 0.3) & (df['load'] < 0.7)
   high_load_mask = df['load'] >= 0.7
   
   model_low = train_lstm(df[low_load_mask])
   model_mid = train_lstm(df[mid_load_mask])
   model_high = train_lstm(df[high_load_mask])
   
   # ?ï¿½æ¸¬?ï¿½é¸?ï¿½ï¿½??ï¿½æ¨¡??
   def predict(X, load):
       if load < 0.3:
           return model_low.predict(X)
       elif load < 0.7:
           return model_mid.predict(X)
       else:
           return model_high.predict(X)
   ```

2. **?ï¿½ï¿½?ç´„ï¿½?**ï¿½?
   ```python
   # ?ï¿½ï¿½?å¹³è¡¡ç´„ï¿½?
   Q_reaction = k(T) * V * c^n * (-?H)
   Q_cooling = U * A * (T - T_coolant)
   
   # å°‡ç‰©?ï¿½æ¨¡?ï¿½ï¿½??ï¿½ï¿½??ï¿½è¼¸??
   df['Q_estimate'] = calculate_heat_balance(df)
   features.append('Q_estimate')
   ```

3. **æ³¨ï¿½??ï¿½ï¿½???*ï¿½?
   ```python
   # Attention LSTM
   from tensorflow.keras.layers import Attention
   
   encoder = layers.LSTM(64, return_sequences=True)(inputs)
   decoder = layers.LSTM(64, return_sequences=True)(inputs)
   attention = Attention()([decoder, encoder])
   output = layers.Dense(1)(attention)
   ```

### 8.3 ?ï¿½é¤¾å¡”ï¿½?æº«æ§??

**?ï¿½ï¿½??ï¿½è¿°**ï¿½?
- å¤šï¿½??ï¿½ç›¸äº’ï¿½??ï¿½ï¿½??ï¿½ï¿½?æ¯”ã€é€²ï¿½??ï¿½ã€ï¿½??ï¿½ï¿½??ï¿½ï¿½?
- ?ï¿½ï¿½??ï¿½å»¶?ï¿½ï¿½?~15-30 ?ï¿½ï¿½?ï¿½?
- ï¿½???ï¿½ï¿½??ï¿½ï¿½???

**?ï¿½ï¿½?æº–ï¿½?**ï¿½?
```python
# å¤šï¿½??ï¿½è¼¸??
X_vars = ['reflux_ratio', 'feed_rate', 'top_pressure', 
          'bottom_temp', 'feed_composition']

# ?ï¿½å»ºåºï¿½??ï¿½ï¿½?ï¼ˆè€ƒæ…®å»¶é²ï¿½?
def create_sequences_with_delay(df, X_vars, y_var, 
                                 window=30, delay=15):
    X, y = [], []
    for i in range(len(df) - window - delay):
        X.append(df[X_vars].iloc[i:i+window].values)
        y.append(df[y_var].iloc[i+window+delay])
    return np.array(X), np.array(y)
```

**?ï¿½ï¿½?æ¨¡ï¿½?**ï¿½?
```python
# çµ„ï¿½?å¤šå€‹æ¨¡??
predictions = []
weights = [0.3, 0.3, 0.4]  # ARIMA, RF, LSTM

pred_arima = model_arima.forecast(steps=horizon)
pred_rf = model_rf.predict(X_test)
pred_lstm = model_lstm.predict(X_test)

final_pred = (weights[0] * pred_arima + 
              weights[1] * pred_rf + 
              weights[2] * pred_lstm)
```

---

## ç¬¬ï¿½?ç« ï¿½??ï¿½ï¿½?ä¸»ï¿½?

### 9.1 Attention Mechanism

**?ï¿½ï¿½?**ï¼šLSTM å°‡ï¿½??ï¿½ä¿¡?ï¿½ï¿½?ç¸®åˆ°?ï¿½å¾Œï¿½??ï¿½ï¿½??ï¿½?ï¿½ï¿½??ï¿½èƒ½ä¸Ÿå¤±?ï¿½ï¿½?ä¿¡æ¯??

**Attention æ©Ÿåˆ¶**ï¼šï¿½??ï¿½ï¿½?æ³¨ï¿½??ï¿½ï¿½??ï¿½æ­¥

$$
\alpha_t = \frac{\exp(e_t)}{\sum_{i=1}^{T} \exp(e_i)}
$$
$$
e_t = \text{score}(\mathbf{h}_t, \mathbf{s})
$$
$$
\mathbf{c} = \sum_{t=1}^{T} \alpha_t \mathbf{h}_t
$$

?ï¿½ä¸­ï¿½?
- $\mathbf{h}_t$ï¼šencoder ?ï¿½ï¿½???$t$ ?ï¿½éš±?ï¿½ï¿½???
- $\mathbf{s}$ï¼šdecoder ?ï¿½ï¿½???
- $\alpha_t$ï¼šattention æ¬Šï¿½?
- $\mathbf{c}$ï¼šcontext vector

**å¯¦ç¾**ï¿½?
```python
from tensorflow.keras.layers import AdditiveAttention

encoder_outputs = layers.LSTM(64, return_sequences=True)(inputs)
query = layers.LSTM(64)(inputs)
attention = AdditiveAttention()([query, encoder_outputs])
output = layers.Dense(1)(attention)
```

### 9.2 Transformer for Time Series

**Self-Attention**ï¿½?
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

**ä½ç½®ç·¨ç¢¼ï¼ˆPositional Encodingï¿½?*ï¿½?
$$
PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d})
$$
$$
PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d})
$$

**?ï¿½å‹¢**ï¿½?
- ä¸¦ï¿½?è¨ˆï¿½?ï¼ˆï¿½? LSTM å¿«ï¿½?
- ?ï¿½ï¿½?ä¾è³´ï¼ˆç„¡æ¢¯åº¦æ¶ˆå¤±ï¿½?
- ?ï¿½è§£?ï¿½æ€§ï¿½?attention æ¬Šï¿½?ï¿½?

**?ï¿½ç”¨**ï¿½?
- Temporal Fusion Transformer (TFT)
- Autoformer
- Informer

### 9.3 Neural ODE

**????ï¿½ï¿½?å»ºæ¨¡**ï¿½?

å°‡é›¢??RNN ?ï¿½ç‚º??? ODEï¿½?
$$
\frac{d\mathbf{h}(t)}{dt} = f_\theta(\mathbf{h}(t), t)
$$

**æ±‚è§£**ï¼šä½¿??ODE solver
```python
from torchdiffeq import odeint

def ode_func(t, h):
    return model(h, t)

h_final = odeint(ode_func, h_0, t_span)
```

**?ï¿½å‹¢**ï¿½?
- ?ï¿½ï¿½?ä¸ï¿½??ï¿½ï¿½?ï¿½?
- ?ï¿½ï¿½??ï¿½æ•¸
- ?ï¿½å¥½?ï¿½ï¿½??ï¿½èƒ½??

---

## ç¬¬ï¿½?ç« ï¿½?ç¸½ï¿½??ï¿½ï¿½?ä½³å¯¦ï¿½?

### 10.1 å»ºæ¨¡æµï¿½? Checklist

**1. ?ï¿½ï¿½?æº–ï¿½?** ??
- [ ] æª¢æŸ¥ç¼ºå¤±?ï¿½ï¿½??ï¿½å¸¸??
- [ ] ç¹ªè£½?ï¿½ï¿½?åºï¿½??ï¿½ï¿½?è­˜åˆ¥è¶¨å‹¢?ï¿½å­£ç¯€??
- [ ] è¨ˆï¿½? ACF/PACF
- [ ] æª¢ï¿½?å¹³ç©©?ï¿½ï¿½?ADF testï¿½?
- [ ] ?ï¿½ï¿½?è¨“ç·´/é©—ï¿½?/æ¸¬è©¦?ï¿½ï¿½??ï¿½ï¿½??ï¿½ï¿½?ï¿½?

**2. ?ï¿½å¾µå·¥ï¿½?** ??
- [ ] ?ï¿½å»ºæ»¯ï¿½??ï¿½å¾µ
- [ ] è¨ˆï¿½?æ»¾ï¿½?çµ±ï¿½?ï¼ˆMA, Std, EWMAï¿½?
- [ ] æ·»ï¿½??ï¿½ï¿½??ï¿½å¾µï¼ˆhour, day of weekï¿½?
- [ ] ?ï¿½ï¿½??ï¿½å¾µï¼ˆç‰©?ï¿½å…¬å¼ï¿½?ç®—ï¿½?
- [ ] æ¨™ï¿½???æ­¸ï¿½???

**3. Baseline æ¨¡ï¿½?** ??
- [ ] Persistence
- [ ] Moving Average
- [ ] Linear Regression
- [ ] Random Forest / XGBoost
- [ ] MLP

**4. LSTM æ¨¡ï¿½?** ??
- [ ] è¨­ï¿½?ç¶²è·¯?ï¿½ï¿½?
- [ ] è¨­ç½®è¨“ç·´?ï¿½èª¿ï¼ˆEarly Stopping, LR Schedulerï¿½?
- [ ] è¨“ç·´æ¨¡ï¿½?
- [ ] ??ï¿½ï¿½ training/validation loss
- [ ] è¶…ï¿½??ï¿½èª¿??

**5. è©•ä¼°?ï¿½ï¿½?ï¿½?* ??
- [ ] è¨ˆï¿½?å¤šç¨®?ï¿½ï¿½?ï¼ˆRMSE, MAE, MAPEï¿½?
- [ ] ç¹ªè£½?ï¿½æ¸¬?ï¿½ï¿½?
- [ ] æ®˜å·®?ï¿½ï¿½?
- [ ] ??Baseline å°ï¿½?
- [ ] Rolling backtest

**6. ?ï¿½ç½²æº–ï¿½?** ??
- [ ] æ¨¡ï¿½?å°å‡ºï¼ˆSavedModel, ONNXï¿½?
- [ ] ?ï¿½ï¿½??ï¿½åº¦æ¸¬è©¦
- [ ] æ¼‚ç§»æª¢æ¸¬æ©Ÿåˆ¶
- [ ] ?ï¿½è­¦è¦ï¿½?è¨­ï¿½?
- [ ] ?ï¿½æ–°ç­–ç•¥

### 10.2 å¸¸ï¿½??ï¿½èª¤?ï¿½è§£ï¿½?

| ?ï¿½ï¿½? | ?ï¿½ï¿½? | ï¿½?ï¿½ï¿½?ï¿½ï¿½? |
|-----|------|---------|
| ?ï¿½æ¸¬?ï¿½ï¿½?æ»¯ï¿½??ï¿½å¯¦??| æ¨¡ï¿½?å­¸åˆ°"è¤‡è£½?ï¿½ï¿½?ï¿½? | å¢ï¿½??ï¿½æ¸¬horizon?ï¿½æ‡²ç½°æ»¯ï¿½?|
| è¨“ç·´å¾ˆå¥½æ¸¬è©¦å¾ˆå·® | ?ï¿½æ“¬??| Dropout?ï¿½Early Stopping?ï¿½ç°¡?ï¿½æ¨¡??|
| è¨“ç·´?ï¿½æ¸¬è©¦éƒ½å¾ˆå·® | æ¬ æ“¬??| å¢ï¿½?æ¨¡ï¿½?å®¹ï¿½??ï¿½æ›´å¤šç‰¹ï¿½?|
| Loss ä¸æ”¶??| å­¸ï¿½??ï¿½ï¿½?å¤§ã€æ•¸?ï¿½æœªæ¨™ï¿½???| ?ï¿½ï¿½?LR?ï¿½ï¿½?æº–ï¿½?è¼¸å…¥ |
| ?ï¿½æ¸¬?ï¿½ï¿½??ï¿½æ–¼å¹³ï¿½? | æ¨¡ï¿½??ï¿½æ¸¬?ï¿½ï¿½?| æª¢æŸ¥?ï¿½ï¿½?è³ªï¿½??ï¿½ï¿½??ï¿½æ¨¡?ï¿½ï¿½??ï¿½åº¦ |
| æ¢¯åº¦?ï¿½ç‚¸ | æ¬Šï¿½??ï¿½ï¿½??ï¿½ï¿½???| Gradient Clipping?ï¿½Xavier?ï¿½ï¿½???|

### 10.3 å·¥æ¥­?ï¿½ç”¨å»ºè­°

**1. å¾ç°¡?ï¿½ï¿½?ï¿½?*ï¿½?
- ?ï¿½å»ºç«‹å¯?ï¿½ï¿½? Baseline
- ç¢ºï¿½? LSTM ?ï¿½æ­£å¸¶ï¿½??ï¿½ï¿½?
- ?ï¿½ï¿½??ï¿½é€²ï¿½?RMSE ?ï¿½ï¿½?å¤šï¿½?ï¼Ÿï¿½?

**2. ?ï¿½ï¿½??ï¿½ï¿½??ï¿½ï¿½?**ï¿½?
- ?ï¿½ï¿½?ç´„ï¿½?ï¼ˆèƒ½?ï¿½ï¿½??ï¿½ã€è³ª?ï¿½ï¿½??ï¿½ï¿½?
- ?ï¿½ï¿½??ï¿½åˆ¶ï¼ˆé–¥?ï¿½?ï¿½åº¦?ï¿½ï¿½??ï¿½ï¿½??ï¿½ï¿½?
- å·²çŸ¥?ï¿½ï¿½?å¸¸æ•¸?ï¿½å»¶??

**3. é­¯ï¿½??ï¿½å„ª?ï¿½æ–¼æº–ç¢º??*ï¿½?
- ?ï¿½ç•°å¸¸å·¥æ³ï¿½??ï¿½è¡¨??
- å°å‚³?ï¿½å™¨?ï¿½ï¿½??ï¿½å®¹??
- æ¨¡ï¿½??ï¿½æ–°?ï¿½ç©©å®šï¿½?

**4. ?ï¿½è§£?ï¿½ï¿½?*ï¿½?
- Attention æ¬Šï¿½??ï¿½ï¿½???
- SHAP ?ï¿½ï¿½???
- ?ï¿½ç‰©?ï¿½æ¨¡?ï¿½ï¿½?ï¿½?

**5. ?ï¿½ï¿½???ï¿½ï¿½**ï¿½?
- ?ï¿½ï¿½??ï¿½èƒ½?ï¿½ï¿½?
- æ¼‚ç§»æª¢æ¸¬
- å®šï¿½??ï¿½ï¿½?

### 10.4 å»¶ä¼¸?ï¿½ï¿½?

**ç¶“å…¸è«–ï¿½?**ï¿½?
1. Hochreiter & Schmidhuber (1997). "Long Short-Term Memory"
2. Cho et al. (2014). "Learning Phrase Representations using RNN Encoder-Decoder"
3. Vaswani et al. (2017). "Attention Is All You Need"

**?ï¿½ï¿½?åºï¿½??ï¿½æ¸¬**ï¿½?
4. Box & Jenkins (1970). "Time Series Analysis: Forecasting and Control"
5. Makridakis et al. (2018). "Statistical and Machine Learning forecasting methods"
6. Lim et al. (2021). "Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting"

**?ï¿½å·¥?ï¿½ç”¨**ï¿½?
7. Fortuna et al. (2007). "Soft Sensors for Monitoring and Control of Industrial Processes"
8. Kadlec et al. (2009). "Data-driven Soft Sensors in the Process Industry"

---

## ?ï¿½è€ƒï¿½??ï¿½ï¿½?è³‡ï¿½?

### è«–ï¿½?

1. **LSTM?ï¿½ï¿½?**
   - Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780.

2. **GRU**
   - Cho, K., et al. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. EMNLP 2014.

3. **?ï¿½ï¿½?åºï¿½?æ·±åº¦å­¸ï¿½?**
   - Lim, B., & Zohren, S. (2021). Time-series forecasting with deep learning: a survey. Philosophical Transactions of the Royal Society A, 379(2194), 20200209.

### ?ï¿½ï¿½?å·¥å…·

- **TensorFlow/Keras**: https://www.tensorflow.org/
- **PyTorch**: https://pytorch.org/
- **statsmodels**: https://www.statsmodels.org/
- **sktime**: https://www.sktime.org/
- **Darts**: https://unit8co.github.io/darts/

### ?ï¿½ï¿½?èª²ï¿½?

- Stanford CS231n: Deep Learning for Computer Vision
- Coursera: Sequences, Time Series and Prediction (deeplearning.ai)
- Fast.ai: Practical Deep Learning

---

**?ï¿½ï¿½?ç¾©ï¿½??ï¿½æ–¼ 2025 å¹´ï¿½?å°ˆæ³¨?ï¿½ï¿½?å·¥è£½ç¨‹ï¿½??ï¿½ï¿½??ï¿½ï¿½?æ¸¬ï¿½?å®Œæ•´?ï¿½ï¿½??ï¿½å¯¦è¸ï¿½?*

**?ï¿½èª²?ï¿½å¸«**ï¼š[?ï¿½ï¿½?å§“ï¿½?]  
**èª²ï¿½??ï¿½ç¨±**ï¼šï¿½?å·¥AI?ï¿½ç”¨  
**?ï¿½ï¿½?**ï¼šUnit18 - LSTM ?ï¿½ï¿½?åºï¿½??ï¿½æ¸¬  
**?ï¿½æœ¬**ï¼šv2.0ï¿½?025å¹´ï¿½??ï¿½ï¿½?æ§‹ï¿½?
