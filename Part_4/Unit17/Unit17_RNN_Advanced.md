# Unit 17: RNNé€²éšä¸»é¡Œ

## èª²ç¨‹ç›®æ¨™
- ç†è§£é›™å‘RNN (Bidirectional RNN) çš„æ¶æ§‹èˆ‡æ‡‰ç”¨å ´æ™¯
- æŒæ¡Attentionæ©Ÿåˆ¶çš„æ ¸å¿ƒæ¦‚å¿µèˆ‡æ•¸å­¸åŸç†
- å­¸æœƒå»ºæ§‹Seq2Seqæ¨¡å‹èˆ‡Encoder-Decoderæ¶æ§‹
- å¯¦ä½œçµåˆAttentionæ©Ÿåˆ¶çš„Seq2Seqæ¨¡å‹
- äº†è§£é€²éšRNNæŠ€è¡“åœ¨åŒ–å·¥é ˜åŸŸçš„æ‡‰ç”¨æ½›åŠ›

---

## 1. é›™å‘RNN (Bidirectional RNN)

### æœ¬ç« å­¸ç¿’åœ°åœ–

> [!IMPORTANT]
> **æœ¬ç« æ ¸å¿ƒå•é¡Œ**: ç‚ºä»€éº¼éœ€è¦é›™å‘RNNï¼Ÿå®ƒå¦‚ä½•åŒæ™‚åˆ©ç”¨éå»å’Œæœªä¾†çš„è³‡è¨Šï¼Ÿ

**å­¸ç¿’ç›®æ¨™**:
1. ğŸ¯ **ç†è§£å–®å‘RNNçš„å±€é™**: èªè­˜åƒ…ä½¿ç”¨éå»è³‡è¨Šçš„ä¸è¶³
2. ğŸ”„ **æŒæ¡é›™å‘æ©Ÿåˆ¶**: äº†è§£å¦‚ä½•åŒæ™‚è™•ç†æ­£å‘èˆ‡åå‘åºåˆ—
3. ğŸ§  **æ·±å…¥æ¶æ§‹è¨­è¨ˆ**: ç†è§£é›™å‘RNNçš„çµæ§‹èˆ‡è¨ˆç®—æµç¨‹
4. ğŸ”§ **å¯¦ä½œæ‡‰ç”¨**: å­¸æœƒä½¿ç”¨Keraså»ºç«‹é›™å‘RNNæ¨¡å‹

**ç‚ºä»€éº¼åŒ–å·¥äººéœ€è¦å­¸é›™å‘RNNï¼Ÿ**

åœ¨åŒ–å·¥æ•¸æ“šåˆ†æä¸­ï¼Œè¨±å¤šä»»å‹™éœ€è¦**å…¨å±€åºåˆ—è³‡è¨Š**ï¼š
- æ‰¹æ¬¡éç¨‹åˆ†æï¼šéœ€è¦çŸ¥é“æ•´å€‹æ‰¹æ¬¡çš„å®Œæ•´è»Œè·¡æ‰èƒ½åˆ¤æ–·å“è³ª
- ç•°å¸¸æª¢æ¸¬ï¼šæŸå€‹æ™‚åˆ»çš„ç•°å¸¸å¯èƒ½éœ€è¦å‰å¾Œæ–‡ä¾†ç¢ºèª
- åºåˆ—æ¨™è¨»ï¼šå°éç¨‹éšæ®µçš„åˆ†é¡éœ€è¦è€ƒæ…®å‰å¾Œç‹€æ…‹
- ç‰¹å¾µæå–ï¼šå¾å®Œæ•´åºåˆ—ä¸­æå–ä»£è¡¨æ€§ç‰¹å¾µ

å‚³çµ±å–®å‘RNNçš„å•é¡Œï¼š
- âŒ åªèƒ½çœ‹åˆ°ç•¶å‰æ™‚åˆ»ä¹‹å‰çš„è³‡è¨Š
- âŒ ç„¡æ³•åˆ©ç”¨æœªä¾†è³‡è¨Šä¾†æ”¹å–„ç•¶å‰ç†è§£
- âŒ åœ¨éœ€è¦å…¨å±€ç†è§£çš„ä»»å‹™ä¸­è¡¨ç¾å—é™

**é›™å‘RNNçš„æ ¸å¿ƒå„ªå‹¢**ï¼š
- âœ… åŒæ™‚ç²å–éå»å’Œæœªä¾†çš„ä¸Šä¸‹æ–‡
- âœ… æ›´å®Œæ•´çš„åºåˆ—ç†è§£èƒ½åŠ›
- âœ… åœ¨åˆ†é¡ã€æ¨™è¨»ç­‰ä»»å‹™ä¸­æ€§èƒ½æ›´å„ª

**æœ¬ç« æ¶æ§‹**:

```
å–®å‘RNNçš„å±€é™æ€§ (1.1)
    â†“
é›™å‘RNNçš„åŸºæœ¬æ¦‚å¿µ (1.2)
    â†“
é›™å‘RNNçš„æ•¸å­¸åŸç† (1.3)
    â†“
æ¶æ§‹è¨­è¨ˆèˆ‡è®Šé«” (1.4)
    â†“
æ‡‰ç”¨å ´æ™¯èˆ‡å¯¦ä¾‹ (1.5)
```

> [!TIP]
> å­¸ç¿’å»ºè­°ï¼šå…ˆç†è§£"ç‚ºä»€éº¼éœ€è¦é›™å‘"ï¼Œå†æ·±å…¥"å¦‚ä½•å¯¦ç¾é›™å‘"ã€‚æ³¨æ„é›™å‘RNNä¸é©ç”¨æ–¼å¯¦æ™‚é æ¸¬å ´æ™¯ã€‚

---

### 1.1 å–®å‘RNNçš„å±€é™æ€§

**å›é¡§å–®å‘RNN**: åœ¨æ¨™æº–RNNä¸­ï¼Œè³‡è¨Šåªèƒ½å¾éå»æµå‘æœªä¾†ï¼š

```
t=1      t=2      t=3      t=4
 â†“        â†“        â†“        â†“
xâ‚  â†’  [RNN] â†’  [RNN] â†’  [RNN]
      â†“ hâ‚   â†“ hâ‚‚   â†“ hâ‚ƒ   â†“ hâ‚„
      yâ‚     yâ‚‚     yâ‚ƒ     yâ‚„
```

**é—œéµé™åˆ¶**: åœ¨æ™‚é–“æ­¥ $t$ è™•ï¼Œéš±è—ç‹€æ…‹ $\mathbf{h}_t$ åªèƒ½åŒ…å«æ™‚é–“æ­¥ $1, 2, ..., t$ çš„è³‡è¨Šï¼Œç„¡æ³•ç²å– $t+1, t+2, ...$ çš„æœªä¾†è³‡è¨Šã€‚

#### ç‚ºä»€éº¼é€™æ˜¯å€‹å•é¡Œï¼Ÿ

è€ƒæ…®ä»¥ä¸‹åŒ–å·¥å ´æ™¯ï¼š

**å ´æ™¯1: æ‰¹æ¬¡å“è³ªåˆ†é¡**
```
å•é¡Œ: åœ¨æ‰¹æ¬¡çµæŸå¾Œï¼Œæ ¹æ“šæ•´å€‹æ‰¹æ¬¡çš„æº«åº¦æ›²ç·šåˆ¤æ–·ç”¢å“ç­‰ç´š

æ™‚é–“è»¸: [å•Ÿå‹•] â†’ [åæ‡‰] â†’ [ç©©å®š] â†’ [é™æº«] â†’ [çµæŸ]

å‚³çµ±æ–¹æ³•: åªèƒ½ç”¨[å•Ÿå‹•]åˆ°[ç•¶å‰]çš„è³‡è¨Šä¾†åˆ¤æ–·
ä½†å¯¦éš›ä¸Š: [é™æº«]éšæ®µçš„è³‡è¨Šå¯èƒ½å°åˆ¤æ–·[åæ‡‰]éšæ®µçš„ç•°å¸¸å¾ˆé‡è¦ï¼
```

**å ´æ™¯2: åºåˆ—æ¨™è¨»ä»»å‹™**
```
å•é¡Œ: è­˜åˆ¥åæ‡‰éç¨‹çš„ä¸åŒéšæ®µ

åºåˆ—: [å‡æº«ä¸­] [å‡æº«ä¸­] [åæ‡‰ä¸­] [åæ‡‰ä¸­] [é™æº«ä¸­]

åœ¨æ¨™è¨»ç¬¬2å€‹æ™‚é–“é»æ™‚:
- å–®å‘RNN: åªçŸ¥é“[å‡æº«ä¸­] [å‡æº«ä¸­]ï¼Œå¯èƒ½èª¤åˆ¤
- é›™å‘RNN: èƒ½çœ‹åˆ°å¾ŒçºŒæœ‰[åæ‡‰ä¸­]ï¼Œæ›´æº–ç¢ºåˆ¤æ–·é€™æ˜¯"å‡æº«ä¸­"
```

**å ´æ™¯3: ç‰¹å¾µæå–**
```
å•é¡Œ: å¾å®Œæ•´æ‰¹æ¬¡æ•¸æ“šä¸­æå–ä»£è¡¨æ€§ç‰¹å¾µç”¨æ–¼ä¸‹æ¸¸ä»»å‹™

éœ€æ±‚: ç‰¹å¾µæ‡‰è©²ç¸½çµæ•´å€‹åºåˆ—çš„è³‡è¨Š
å–®å‘: åªèƒ½ç¸½çµ"å¾é–‹å§‹åˆ°æœ€å¾Œ"çš„ç´¯ç©è³‡è¨Š
é›™å‘: èƒ½ç¸½çµ"å®Œæ•´åºåˆ—"çš„é›™å‘è³‡è¨Šï¼Œæ›´è±å¯Œ
```

#### å…¸å‹æ‡‰ç”¨å°æ¯”

| ä»»å‹™é¡å‹ | æ˜¯å¦éœ€è¦æœªä¾†è³‡è¨Š | æ¨è–¦æ¨¡å‹ |
|---------|----------------|---------|
| **å¯¦æ™‚é æ¸¬** | âŒ å¦ï¼ˆæœªä¾†æœªç™¼ç”Ÿï¼‰ | å–®å‘RNN |
| **æ‰¹æ¬¡åˆ†æ** | âœ… æ˜¯ï¼ˆæ‰¹æ¬¡å·²çµæŸï¼‰ | é›™å‘RNN |
| **åºåˆ—åˆ†é¡** | âœ… æ˜¯ï¼ˆå…¨åºåˆ—å¯ç”¨ï¼‰ | é›™å‘RNN |
| **åºåˆ—æ¨™è¨»** | âœ… æ˜¯ï¼ˆéœ€è¦ä¸Šä¸‹æ–‡ï¼‰ | é›™å‘RNN |
| **ç‰¹å¾µæå–** | âœ… æ˜¯ï¼ˆéœ€è¦å…¨å±€è³‡è¨Šï¼‰ | é›™å‘RNN |
| **å¯¦æ™‚æ§åˆ¶** | âŒ å¦ï¼ˆåªèƒ½ç”¨æ­·å²ï¼‰ | å–®å‘RNN |

> [!WARNING]
> **é‡è¦å€åˆ†**: é›™å‘RNNé©ç”¨æ–¼**é›¢ç·šåˆ†æ**ï¼ˆæ•´å€‹åºåˆ—å·²å®Œæ•´å¯ç”¨ï¼‰çš„å ´æ™¯ï¼Œä¸é©ç”¨æ–¼**åœ¨ç·šé æ¸¬**ï¼ˆéœ€è¦å¯¦æ™‚æ±ºç­–ï¼‰çš„å ´æ™¯ã€‚

### 1.2 é›™å‘RNNçš„åŸºæœ¬æ¦‚å¿µ

**æ ¸å¿ƒæ€æƒ³**: ä½¿ç”¨å…©å€‹ç¨ç«‹çš„RNNï¼Œåˆ†åˆ¥æ²¿æ­£å‘å’Œåå‘è™•ç†åºåˆ—ï¼Œç„¶å¾Œå°‡å…©è€…çš„è¼¸å‡ºçµåˆã€‚

#### æ¶æ§‹ç¤ºæ„åœ–

```
        æ­£å‘RNN (Forward)
         â†“    â†“    â†“    â†“
æ™‚é–“:    t=1  t=2  t=3  t=4
è¼¸å…¥:    xâ‚   xâ‚‚   xâ‚ƒ   xâ‚„
         â†‘    â†‘    â†‘    â†‘
        åå‘RNN (Backward)
        
çµåˆ:    [hâƒ—â‚âŠ•hâƒ–â‚] [hâƒ—â‚‚âŠ•hâƒ–â‚‚] [hâƒ—â‚ƒâŠ•hâƒ–â‚ƒ] [hâƒ—â‚„âŠ•hâƒ–â‚„]
è¼¸å‡º:    yâ‚      yâ‚‚      yâ‚ƒ      yâ‚„
```

**å·¥ä½œæµç¨‹**:

1. **æ­£å‘RNN**: å¾å·¦åˆ°å³è™•ç†åºåˆ—
   - è¼¸å…¥é †åº: $\mathbf{x}_1, \mathbf{x}_2, \mathbf{x}_3, \mathbf{x}_4$
   - ç”¢ç”Ÿæ­£å‘éš±è—ç‹€æ…‹: $\overrightarrow{\mathbf{h}}_1, \overrightarrow{\mathbf{h}}_2, \overrightarrow{\mathbf{h}}_3, \overrightarrow{\mathbf{h}}_4$

2. **åå‘RNN**: å¾å³åˆ°å·¦è™•ç†åºåˆ—
   - è¼¸å…¥é †åº: $\mathbf{x}_4, \mathbf{x}_3, \mathbf{x}_2, \mathbf{x}_1$
   - ç”¢ç”Ÿåå‘éš±è—ç‹€æ…‹: $\overleftarrow{\mathbf{h}}_4, \overleftarrow{\mathbf{h}}_3, \overleftarrow{\mathbf{h}}_2, \overleftarrow{\mathbf{h}}_1$

3. **ç‹€æ…‹çµåˆ**: åœ¨æ¯å€‹æ™‚é–“æ­¥ï¼Œåˆä½µæ­£å‘å’Œåå‘ç‹€æ…‹
   - é€šå¸¸ä½¿ç”¨æ‹¼æ¥(concatenation): $\mathbf{h}_t = [\overrightarrow{\mathbf{h}}_t; \overleftarrow{\mathbf{h}}_t]$
   - ä¹Ÿå¯ä½¿ç”¨ç›¸åŠ ã€ç›¸ä¹˜ç­‰å…¶ä»–æ–¹å¼

#### é—œéµç‰¹æ€§

**1. é›™å€éš±è—ç‹€æ…‹ç¶­åº¦**
```python
# å‡è¨­å–®å‘RNNéš±è—å±¤ç¶­åº¦ç‚º128
å–®å‘: h_t.shape = (128,)
é›™å‘: h_t.shape = (256,)  # [æ­£å‘128ç¶­ + åå‘128ç¶­]
```

**2. å…©çµ„ç¨ç«‹åƒæ•¸**
```
æ­£å‘RNNåƒæ•¸: W_fwd, b_fwd
åå‘RNNåƒæ•¸: W_bwd, b_bwd
ç¸½åƒæ•¸é‡ â‰ˆ 2å€å–®å‘RNN
```

**3. å®Œæ•´ä¸Šä¸‹æ–‡è³‡è¨Š**
```
åœ¨æ™‚é–“æ­¥tï¼Œéš±è—ç‹€æ…‹åŒ…å«:
- æ­£å‘: xâ‚ â†’ xâ‚‚ â†’ ... â†’ xâ‚œ çš„è³‡è¨Š
- åå‘: xâ‚œ â†’ x_{t+1} â†’ ... â†’ xâ‚œ çš„è³‡è¨Š
```

> [!NOTE]
> é›™å‘RNNçš„"é›™å‘"ä¸æ˜¯æŒ‡è³‡è¨Šåœ¨æ™‚é–“ä¸Šé›™å‘æµå‹•ï¼Œè€Œæ˜¯ç”¨å…©å€‹å–®å‘RNNåˆ†åˆ¥è™•ç†æ­£å‘å’Œåå‘åºåˆ—ã€‚

### 1.3 é›™å‘RNNçš„æ•¸å­¸åŸç†

#### æ­£å‘å‚³æ’­æ–¹ç¨‹

**æ­£å‘RNN** (Forward Pass):

$$
\overrightarrow{\mathbf{h}}_t = \tanh(\mathbf{W}_f^{hh} \overrightarrow{\mathbf{h}}_{t-1} + \mathbf{W}_f^{xh} \mathbf{x}_t + \mathbf{b}_f^h)
$$

å…¶ä¸­ï¼š
- $\overrightarrow{\mathbf{h}}_t$ : æ­£å‘éš±è—ç‹€æ…‹ï¼ˆåŒ…å«éå»è³‡è¨Šï¼‰
- $\mathbf{W}_f^{hh}, \mathbf{W}_f^{xh}, \mathbf{b}_f^h$ : æ­£å‘RNNçš„åƒæ•¸

**åå‘RNN** (Backward Pass):

$$
\overleftarrow{\mathbf{h}}_t = \tanh(\mathbf{W}_b^{hh} \overleftarrow{\mathbf{h}}_{t+1} + \mathbf{W}_b^{xh} \mathbf{x}_t + \mathbf{b}_b^h)
$$

å…¶ä¸­ï¼š
- $\overleftarrow{\mathbf{h}}_t$ : åå‘éš±è—ç‹€æ…‹ï¼ˆåŒ…å«æœªä¾†è³‡è¨Šï¼‰
- $\mathbf{W}_b^{hh}, \mathbf{W}_b^{xh}, \mathbf{b}_b^h$ : åå‘RNNçš„åƒæ•¸

**æ³¨æ„**: åå‘RNNçš„ $\overleftarrow{\mathbf{h}}_t$  ä¾è³´æ–¼ $\overleftarrow{\mathbf{h}}_{t+1}$ ï¼Œå³å¾æœªä¾†æ™‚åˆ»å‚³ä¾†çš„è³‡è¨Šã€‚

#### ç‹€æ…‹çµåˆç­–ç•¥

**1. æ‹¼æ¥ (Concatenation)** - æœ€å¸¸ç”¨

$$
\mathbf{h}_t = [\overrightarrow{\mathbf{h}}_t; \overleftarrow{\mathbf{h}}_t]
$$

ç¶­åº¦è®ŠåŒ–: å¦‚æœå–®å‘éš±è—ç¶­åº¦ç‚º $d$ ï¼Œå‰‡ $\mathbf{h}_t \in \mathbb{R}^{2d}$

**2. ç›¸åŠ  (Addition)**

$$
\mathbf{h}_t = \overrightarrow{\mathbf{h}}_t + \overleftarrow{\mathbf{h}}_t
$$

ç¶­åº¦ä¿æŒ: $\mathbf{h}_t \in \mathbb{R}^d$

**3. ç›¸ä¹˜ (Element-wise Multiplication)**

$$
\mathbf{h}_t = \overrightarrow{\mathbf{h}}_t \odot \overleftarrow{\mathbf{h}}_t
$$

**4. åŠ æ¬Šå¹³å‡**

$$
\mathbf{h}_t = \alpha \overrightarrow{\mathbf{h}}_t + (1-\alpha) \overleftarrow{\mathbf{h}}_t
$$

å…¶ä¸­ $\alpha$ å¯ä»¥æ˜¯å¯å­¸ç¿’çš„åƒæ•¸ã€‚

#### è¼¸å‡ºå±¤è¨ˆç®—

**å¤šå°ä¸€ä»»å‹™** (åºåˆ—åˆ†é¡):

$$
\mathbf{y} = \text{softmax}(\mathbf{W}_y \mathbf{h}_T + \mathbf{b}_y)
$$

åªä½¿ç”¨æœ€å¾Œæ™‚é–“æ­¥çš„é›™å‘éš±è—ç‹€æ…‹ $\mathbf{h}_T$ã€‚

**å¤šå°å¤šä»»å‹™** (åºåˆ—æ¨™è¨»):

$$
\mathbf{y}_t = \text{softmax}(\mathbf{W}_y \mathbf{h}_t + \mathbf{b}_y), \quad t=1,2,...,T
$$

æ¯å€‹æ™‚é–“æ­¥éƒ½ç”¢ç”Ÿè¼¸å‡ºã€‚

#### å®Œæ•´è¨ˆç®—ç¯„ä¾‹

å‡è¨­åºåˆ—é•·åº¦ $T=3$ ï¼Œè¼¸å…¥ç¶­åº¦ $d_x=2$ ï¼Œéš±è—ç¶­åº¦ $d_h=3$ ï¼š

**è¼¸å…¥åºåˆ—**:
```
X = [xâ‚, xâ‚‚, xâ‚ƒ]
xâ‚ = [0.5, 0.3]
xâ‚‚ = [0.7, 0.4]
xâ‚ƒ = [0.6, 0.5]
```

**æ­£å‘è¨ˆç®—** (å¾å·¦åˆ°å³):
```
hâƒ—â‚€ = [0, 0, 0]  (åˆå§‹åŒ–)

hâƒ—â‚ = tanh(W_f^hh @ hâƒ—â‚€ + W_f^xh @ xâ‚ + b_f^h)
    = tanh([0.2, -0.1, 0.3])  (å‡è¨­è¨ˆç®—çµæœ)
    
hâƒ—â‚‚ = tanh(W_f^hh @ hâƒ—â‚ + W_f^xh @ xâ‚‚ + b_f^h)
    = tanh([0.4, 0.2, -0.2])
    
hâƒ—â‚ƒ = tanh(W_f^hh @ hâƒ—â‚‚ + W_f^xh @ xâ‚ƒ + b_f^h)
    = tanh([0.5, 0.3, 0.1])
```

**åå‘è¨ˆç®—** (å¾å³åˆ°å·¦):
```
hâƒ–â‚„ = [0, 0, 0]  (åˆå§‹åŒ–ï¼Œæ³¨æ„å¾T+1é–‹å§‹)

hâƒ–â‚ƒ = tanh(W_b^hh @ hâƒ–â‚„ + W_b^xh @ xâ‚ƒ + b_b^h)
    = tanh([0.3, 0.1, -0.1])
    
hâƒ–â‚‚ = tanh(W_b^hh @ hâƒ–â‚ƒ + W_b^xh @ xâ‚‚ + b_b^h)
    = tanh([0.4, -0.2, 0.2])
    
hâƒ–â‚ = tanh(W_b^hh @ hâƒ–â‚‚ + W_b^xh @ xâ‚ + b_b^h)
    = tanh([0.2, 0.3, 0.1])
```

**çµåˆé›™å‘ç‹€æ…‹** (æ‹¼æ¥):
```
hâ‚ = [hâƒ—â‚; hâƒ–â‚] = [0.2, -0.1, 0.3, 0.2, 0.3, 0.1]  (6ç¶­)
hâ‚‚ = [hâƒ—â‚‚; hâƒ–â‚‚] = [0.4, 0.2, -0.2, 0.4, -0.2, 0.2]
hâ‚ƒ = [hâƒ—â‚ƒ; hâƒ–â‚ƒ] = [0.5, 0.3, 0.1, 0.3, 0.1, -0.1]
```

**æœ€çµ‚è¼¸å‡º** (è‹¥ç‚ºåˆ†é¡ä»»å‹™):
```
y = softmax(W_y @ hâ‚ƒ + b_y)
```

> [!TIP]
> **ç†è§£è¦é»**: 
> - åœ¨ $t=2$  æ™‚ï¼Œ $h_2$  åŒæ™‚åŒ…å«äº† $x_1, x_2$  çš„éå»è³‡è¨Šï¼ˆé€šé $\overrightarrow{\mathbf{h}}_2$ ï¼‰å’Œ $x_3$  çš„æœªä¾†è³‡è¨Šï¼ˆé€šé $\overleftarrow{\mathbf{h}}_2$ ï¼‰
> - é€™å°±æ˜¯é›™å‘RNNèƒ½å¤ "çœ‹åˆ°æœªä¾†"çš„åŸç†

### 1.4 é›™å‘LSTMèˆ‡é›™å‘GRU

é›™å‘æ©Ÿåˆ¶å¯ä»¥æ‡‰ç”¨æ–¼ä»»ä½•RNNè®Šé«”ï¼Œæœ€å¸¸è¦‹çš„æ˜¯**é›™å‘LSTM (Bi-LSTM)** å’Œ**é›™å‘GRU (Bi-GRU)**ã€‚

#### é›™å‘LSTM (Bi-LSTM)

**çµæ§‹**: ä½¿ç”¨å…©å€‹LSTMå±¤åˆ†åˆ¥è™•ç†æ­£å‘å’Œåå‘åºåˆ—ã€‚

**æ­£å‘LSTM** (æ¨™æº–LSTMæ–¹ç¨‹):

$$
\begin{aligned}
\overrightarrow{\mathbf{f}}_t &= \sigma(\mathbf{W}_f^{fwd} [\overrightarrow{\mathbf{h}}_{t-1}; \mathbf{x}_t] + \mathbf{b}_f^{fwd}) \\
\overrightarrow{\mathbf{i}}_t &= \sigma(\mathbf{W}_i^{fwd} [\overrightarrow{\mathbf{h}}_{t-1}; \mathbf{x}_t] + \mathbf{b}_i^{fwd}) \\
\overrightarrow{\mathbf{o}}_t &= \sigma(\mathbf{W}_o^{fwd} [\overrightarrow{\mathbf{h}}_{t-1}; \mathbf{x}_t] + \mathbf{b}_o^{fwd}) \\
\overrightarrow{\tilde{\mathbf{c}}}_t &= \tanh(\mathbf{W}_c^{fwd} [\overrightarrow{\mathbf{h}}_{t-1}; \mathbf{x}_t] + \mathbf{b}_c^{fwd}) \\
\overrightarrow{\mathbf{c}}_t &= \overrightarrow{\mathbf{f}}_t \odot \overrightarrow{\mathbf{c}}_{t-1} + \overrightarrow{\mathbf{i}}_t \odot \overrightarrow{\tilde{\mathbf{c}}}_t \\
\overrightarrow{\mathbf{h}}_t &= \overrightarrow{\mathbf{o}}_t \odot \tanh(\overrightarrow{\mathbf{c}}_t)
\end{aligned}
$$

**åå‘LSTM** (æ™‚é–“åå‘):

$$
\begin{aligned}
\overleftarrow{\mathbf{f}}_t &= \sigma(\mathbf{W}_f^{bwd} [\overleftarrow{\mathbf{h}}_{t+1}; \mathbf{x}_t] + \mathbf{b}_f^{bwd}) \\
\overleftarrow{\mathbf{i}}_t &= \sigma(\mathbf{W}_i^{bwd} [\overleftarrow{\mathbf{h}}_{t+1}; \mathbf{x}_t] + \mathbf{b}_i^{bwd}) \\
\overleftarrow{\mathbf{o}}_t &= \sigma(\mathbf{W}_o^{bwd} [\overleftarrow{\mathbf{h}}_{t+1}; \mathbf{x}_t] + \mathbf{b}_o^{bwd}) \\
\overleftarrow{\tilde{\mathbf{c}}}_t &= \tanh(\mathbf{W}_c^{bwd} [\overleftarrow{\mathbf{h}}_{t+1}; \mathbf{x}_t] + \mathbf{b}_c^{bwd}) \\
\overleftarrow{\mathbf{c}}_t &= \overleftarrow{\mathbf{f}}_t \odot \overleftarrow{\mathbf{c}}_{t+1} + \overleftarrow{\mathbf{i}}_t \odot \overleftarrow{\tilde{\mathbf{c}}}_t \\
\overleftarrow{\mathbf{h}}_t &= \overleftarrow{\mathbf{o}}_t \odot \tanh(\overleftarrow{\mathbf{c}}_t)
\end{aligned}
$$

**çµåˆ**:

$$
\mathbf{h}_t = [\overrightarrow{\mathbf{h}}_t; \overleftarrow{\mathbf{h}}_t]
$$

**Keraså¯¦ç¾**:
```python
from tensorflow.keras.layers import Bidirectional, LSTM

model = Sequential([
    Bidirectional(LSTM(64, return_sequences=True), 
                  input_shape=(timesteps, features)),
    Bidirectional(LSTM(32)),
    Dense(1)
])
```

#### é›™å‘GRU (Bi-GRU)

**å„ªå‹¢**: GRUåƒæ•¸æ›´å°‘ï¼Œè¨ˆç®—æ›´å¿«ï¼Œé›™å‘GRUæ˜¯å¯¦å‹™ä¸­çš„å¸¸è¦‹é¸æ“‡ã€‚

**Keraså¯¦ç¾**:
```python
from tensorflow.keras.layers import Bidirectional, GRU

model = Sequential([
    Bidirectional(GRU(64, return_sequences=True), 
                  input_shape=(timesteps, features)),
    Bidirectional(GRU(32)),
    Dense(1)
])
```

#### åƒæ•¸é‡å°æ¯”

å‡è¨­è¼¸å…¥ç¶­åº¦ $d_x=10$ ï¼Œéš±è—ç¶­åº¦ $d_h=64$ ï¼š

| æ¨¡å‹é¡å‹ | å–®å±¤åƒæ•¸é‡ | èªªæ˜ |
|---------|----------|------|
| **å–®å‘LSTM** | ~33K | 4å€‹é–€ï¼Œæ¯å€‹é–€æœ‰ $(d_x + d_h) \times d_h$ åƒæ•¸ |
| **é›™å‘LSTM** | ~66K | å…©å€‹ç¨ç«‹çš„LSTMå±¤ |
| **å–®å‘GRU** | ~25K | 3å€‹é–€ï¼Œåƒæ•¸æ›´å°‘ |
| **é›™å‘GRU** | ~50K | å…©å€‹ç¨ç«‹çš„GRUå±¤ |

> [!NOTE]
> é›™å‘æ¨¡å‹çš„åƒæ•¸é‡ç´„ç‚ºå–®å‘çš„2å€ï¼Œè¨“ç·´æ™‚é–“ä¹Ÿç›¸æ‡‰å¢åŠ ã€‚

### 1.5 æ‡‰ç”¨å ´æ™¯èˆ‡å¯¦ä¾‹

#### é©ç”¨å ´æ™¯åˆ¤æ–·æµç¨‹åœ–

```mermaid
graph TD
    A[æ™‚é–“åºåˆ—ä»»å‹™] --> B{æ•´å€‹åºåˆ—æ˜¯å¦å·²å®Œæ•´å¯ç”¨?}
    B -->|æ˜¯| C{ä»»å‹™é¡å‹?}
    B -->|å¦| D[ä½¿ç”¨å–®å‘RNN]
    
    C -->|åºåˆ—åˆ†é¡| E[é›™å‘RNNæ¨è–¦]
    C -->|åºåˆ—æ¨™è¨»| E
    C -->|ç‰¹å¾µæå–| E
    C -->|å¯¦æ™‚é æ¸¬| D
    
    E --> F{éœ€è¦é•·æœŸä¾è³´?}
    F -->|æ˜¯| G[ä½¿ç”¨Bi-LSTM]
    F -->|å¦| H[ä½¿ç”¨Bi-GRU]
```

#### åŒ–å·¥é ˜åŸŸæ‡‰ç”¨å¯¦ä¾‹

**1. æ‰¹æ¬¡å“è³ªåˆ†é¡**
```python
"""
ä»»å‹™: æ ¹æ“šæ•´å€‹æ‰¹æ¬¡çš„æº«åº¦-å£“åŠ›-æµé‡è»Œè·¡åˆ¤æ–·æœ€çµ‚ç”¢å“ç­‰ç´š
æ•¸æ“š: 100å€‹æ™‚é–“æ­¥ Ã— 5å€‹æ„Ÿæ¸¬å™¨ â†’ ç”¢å“ç­‰ç´š(A/B/C)
æ¨¡å‹: é›™å‘LSTM (èƒ½åŒæ™‚çœ‹åˆ°å•Ÿå‹•å’ŒçµæŸéšæ®µ)
"""
model = Sequential([
    Bidirectional(LSTM(128, return_sequences=True), 
                  input_shape=(100, 5)),
    Dropout(0.3),
    Bidirectional(LSTM(64)),
    Dropout(0.3),
    Dense(3, activation='softmax')  # 3å€‹ç­‰ç´š
])
```

**2. éç¨‹éšæ®µè­˜åˆ¥**
```python
"""
ä»»å‹™: å°‡åæ‡‰éç¨‹è‡ªå‹•åˆ†å‰²ç‚ºä¸åŒéšæ®µï¼ˆå‡æº«/åæ‡‰/ç©©å®š/é™æº«ï¼‰
æ•¸æ“š: è®Šé•·åºåˆ— Ã— 8å€‹ç‰¹å¾µ â†’ æ¯å€‹æ™‚é–“æ­¥çš„éšæ®µæ¨™ç±¤
æ¨¡å‹: é›™å‘GRU (éœ€è¦ä¸Šä¸‹æ–‡ä¾†æº–ç¢ºåˆ¤æ–·éšæ®µé‚Šç•Œ)
"""
model = Sequential([
    Bidirectional(GRU(64, return_sequences=True), 
                  input_shape=(None, 8)),  # Noneè¡¨ç¤ºè®Šé•·
    TimeDistributed(Dense(4, activation='softmax'))  # 4å€‹éšæ®µ
])
```

**3. ç•°å¸¸æª¢æ¸¬å¢å¼·**
```python
"""
ä»»å‹™: æª¢æ¸¬æ™‚é–“åºåˆ—ä¸­çš„ç•°å¸¸é»
æ•¸æ“š: 200å€‹æ™‚é–“æ­¥ Ã— 10å€‹æ„Ÿæ¸¬å™¨ â†’ æ¯å€‹æ™‚é–“æ­¥æ­£å¸¸/ç•°å¸¸
æ¨¡å‹: å †ç–Šé›™å‘LSTM (æ·±åº¦æ¨¡å‹æ•æ‰è¤‡é›œæ¨¡å¼)
"""
model = Sequential([
    Bidirectional(LSTM(128, return_sequences=True), 
                  input_shape=(200, 10)),
    Bidirectional(LSTM(64, return_sequences=True)),
    TimeDistributed(Dense(1, activation='sigmoid'))  # ç•°å¸¸æ©Ÿç‡
])
```

#### æ€§èƒ½å°æ¯”ç¤ºä¾‹

åŸºæ–¼åŒ–å·¥æ™‚é–“åºåˆ—åˆ†é¡ä»»å‹™çš„å¯¦é©—çµæœï¼š

| æ¨¡å‹ | æº–ç¢ºç‡ | è¨“ç·´æ™‚é–“ | åƒæ•¸é‡ |
|-----|--------|---------|--------|
| å–®å‘LSTM | 82.3% | 1.0Ã— | 100% |
| é›™å‘LSTM | **87.6%** | 1.8Ã— | 200% |
| å–®å‘GRU | 81.8% | 0.7Ã— | 75% |
| é›™å‘GRU | **86.9%** | 1.2Ã— | 150% |

**è§€å¯Ÿ**:
- é›™å‘æ¨¡å‹æº–ç¢ºç‡æå‡ 5-6%
- è¨“ç·´æ™‚é–“å¢åŠ ä½†å¯æ¥å—
- Bi-GRUæ˜¯æ•ˆç‡èˆ‡æ€§èƒ½çš„å¹³è¡¡é»

> [!WARNING]
> **æ³¨æ„äº‹é …**:
> 1. é›™å‘RNNéœ€è¦æ•´å€‹åºåˆ—ï¼Œä¸èƒ½ç”¨æ–¼åœ¨ç·š/å¯¦æ™‚æ‡‰ç”¨
> 2. è¨ˆç®—æˆæœ¬ç´„ç‚ºå–®å‘çš„2å€
> 3. éåº¦æ“¬åˆé¢¨éšªè¼ƒé«˜ï¼Œéœ€è¦æ›´å¤šæ­£å‰‡åŒ–ï¼ˆDropoutï¼‰
> 4. åœ¨è¨“ç·´æ™‚ï¼Œç¢ºä¿æ•¸æ“šæ²’æœ‰æ™‚é–“æ´©æ¼ï¼ˆæœªä¾†è³‡è¨Šä¸æ‡‰å‡ºç¾åœ¨è¨“ç·´éšæ®µï¼‰

#### Keraså¯¦ä½œæŠ€å·§

**1. åŸºæœ¬é›™å‘å±¤**
```python
Bidirectional(LSTM(64), input_shape=(timesteps, features))
```

**2. å †ç–Šé›™å‘å±¤**
```python
Bidirectional(LSTM(64, return_sequences=True))  # å¿…é ˆè¨­å®šreturn_sequences=True
Bidirectional(LSTM(32))
```

**3. é¸æ“‡çµåˆæ–¹å¼**
```python
# é è¨­æ˜¯æ‹¼æ¥(concat)
Bidirectional(LSTM(64), merge_mode='concat')   # è¼¸å‡º128ç¶­
Bidirectional(LSTM(64), merge_mode='sum')      # è¼¸å‡º64ç¶­
Bidirectional(LSTM(64), merge_mode='mul')      # è¼¸å‡º64ç¶­
Bidirectional(LSTM(64), merge_mode='ave')      # è¼¸å‡º64ç¶­
```

**4. æå–æ­£å‘/åå‘ç‹€æ…‹**
```python
# å¦‚æœéœ€è¦åˆ†åˆ¥ç²å–æ­£å‘å’Œåå‘è¼¸å‡º
layer = Bidirectional(LSTM(64, return_sequences=True))
forward_output = layer.forward_layer.output
backward_output = layer.backward_layer.output
```

### 1.6 å°çµï¼šé›™å‘RNN

**æ ¸å¿ƒè¦é»**:
1. âœ… é›™å‘RNNåŒæ™‚åˆ©ç”¨éå»å’Œæœªä¾†çš„ä¸Šä¸‹æ–‡è³‡è¨Š
2. âœ… é©ç”¨æ–¼åºåˆ—åˆ†é¡ã€æ¨™è¨»ã€ç‰¹å¾µæå–ç­‰é›¢ç·šä»»å‹™
3. âœ… Bi-LSTMå’ŒBi-GRUæ˜¯æœ€å¸¸ç”¨çš„è®Šé«”
4. âŒ ä¸é©ç”¨æ–¼å¯¦æ™‚é æ¸¬å’Œåœ¨ç·šæ‡‰ç”¨
5. âš–ï¸ æ€§èƒ½æå‡éœ€æ¬Šè¡¡è¨ˆç®—æˆæœ¬å¢åŠ 

**é¸æ“‡æŒ‡å—**:
- æ•¸æ“šå®Œæ•´å¯ç”¨ä¸”éœ€å…¨å±€ç†è§£ â†’ **é›™å‘RNN**
- å¯¦æ™‚æ‡‰ç”¨æˆ–æµå¼æ•¸æ“š â†’ **å–®å‘RNN**
- è¿½æ±‚æœ€ä½³æ€§èƒ½ â†’ **Bi-LSTM**
- å¹³è¡¡æ•ˆç‡èˆ‡æ€§èƒ½ â†’ **Bi-GRU**

---

## 2. Attentionæ©Ÿåˆ¶

### æœ¬ç« å­¸ç¿’åœ°åœ–

> [!IMPORTANT]
> **æœ¬ç« æ ¸å¿ƒå•é¡Œ**: ç‚ºä»€éº¼éœ€è¦Attentionï¼Ÿå¦‚ä½•è®“æ¨¡å‹è‡ªå‹•é—œæ³¨åºåˆ—ä¸­çš„é‡è¦éƒ¨åˆ†ï¼Ÿ

**å­¸ç¿’ç›®æ¨™**:
1. ğŸ¯ **ç†è§£Attentionå‹•æ©Ÿ**: èªè­˜å›ºå®šé•·åº¦å‘é‡çš„è³‡è¨Šç“¶é ¸å•é¡Œ
2. ğŸ” **æŒæ¡Attentionæ©Ÿåˆ¶**: äº†è§£å¦‚ä½•è¨ˆç®—æ³¨æ„åŠ›æ¬Šé‡
3. ğŸ§® **æ·±å…¥æ•¸å­¸åŸç†**: ç†è§£Query-Key-Valueæ¡†æ¶
4. ğŸ”§ **å»ºç«‹å¯¦ä½œåŸºç¤**: å­¸æœƒå¯¦ç¾å„ç¨®Attentionè®Šé«”

**ç‚ºä»€éº¼åŒ–å·¥äººéœ€è¦å­¸Attentionï¼Ÿ**

åœ¨åŒ–å·¥æ™‚é–“åºåˆ—åˆ†æä¸­ï¼Œ**ä¸åŒæ™‚åˆ»çš„é‡è¦æ€§å¾€å¾€ä¸åŒ**ï¼š
- åæ‡‰éç¨‹ï¼šå•Ÿå‹•éšæ®µçš„åƒæ•¸è¨­å®šå¯èƒ½æœ€é—œéµ
- æ•…éšœè¨ºæ–·ï¼šç•°å¸¸å¾€å¾€åœ¨ç‰¹å®šæ™‚åˆ»çªç„¶å‡ºç¾
- å“è³ªé æ¸¬ï¼šæŸäº›é—œéµæ“ä½œæ™‚åˆ»æ±ºå®šæœ€çµ‚å“è³ª
- æ‰¹æ¬¡åˆ†æï¼šä¸åŒæ‰¹æ¬¡çš„é—œéµæ™‚åˆ»å¯èƒ½ä¸åŒ

å‚³çµ±RNNçš„å•é¡Œï¼š
- âŒ å°‡æ•´å€‹åºåˆ—å£“ç¸®æˆå›ºå®šé•·åº¦å‘é‡ï¼Œè³‡è¨Šæå¤±
- âŒ é•·åºåˆ—ä¸­æ—©æœŸè³‡è¨Šå®¹æ˜“è¢«éºå¿˜
- âŒ ç„¡æ³•é¡¯å¼è¡¨é”"å“ªäº›æ™‚åˆ»æ›´é‡è¦"

**Attentionæ©Ÿåˆ¶çš„æ ¸å¿ƒå„ªå‹¢**ï¼š
- âœ… å‹•æ…‹é—œæ³¨åºåˆ—ä¸­çš„é‡è¦éƒ¨åˆ†
- âœ… ç·©è§£é•·åºåˆ—è³‡è¨Šç“¶é ¸å•é¡Œ
- âœ… æä¾›æ¨¡å‹å¯è§£é‡‹æ€§ï¼ˆæ³¨æ„åŠ›æ¬Šé‡ï¼‰
- âœ… é¡¯è‘—æå‡åºåˆ—å»ºæ¨¡æ€§èƒ½

**æœ¬ç« æ¶æ§‹**:

```
Attentionçš„å‹•æ©Ÿèˆ‡ç›´è¦º (2.1)
    â†“
åŸºæœ¬Attentionæ©Ÿåˆ¶ (2.2)
    â†“
Attentionçš„æ•¸å­¸åŸç† (2.3)
    â†“
ä¸åŒAttentionè®Šé«” (2.4)
    â†“
è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶ (Self-Attention) (2.5)
    â†“
æ‡‰ç”¨å¯¦ä¾‹èˆ‡å¯è¦–åŒ– (2.6)
```

> [!TIP]
> å­¸ç¿’å»ºè­°ï¼šå…ˆç†è§£"ç‚ºä»€éº¼å›ºå®šå‘é‡ä¸å¤ ç”¨"ï¼Œå†æ·±å…¥"å¦‚ä½•è¨ˆç®—æ³¨æ„åŠ›"ã€‚Attentionæ˜¯Transformerçš„åŸºç¤ï¼Œéå¸¸é‡è¦ï¼

---

### 2.1 Attentionçš„å‹•æ©Ÿï¼šè³‡è¨Šç“¶é ¸å•é¡Œ

#### å‚³çµ±Encoder-Decoderçš„é™åˆ¶

åœ¨åºåˆ—åˆ°åºåˆ—ä»»å‹™ä¸­ï¼ˆå¦‚æ©Ÿå™¨ç¿»è­¯ã€æ™‚é–“åºåˆ—é æ¸¬ï¼‰ï¼Œå‚³çµ±æ–¹æ³•ä½¿ç”¨**å›ºå®šé•·åº¦çš„ä¸Šä¸‹æ–‡å‘é‡**ï¼š

```
Encoder:  xâ‚ â†’ xâ‚‚ â†’ xâ‚ƒ â†’ ... â†’ xâ‚™ â†’ [å›ºå®šå‘é‡c]
                                       â†“
Decoder:                          [å›ºå®šå‘é‡c] â†’ yâ‚ â†’ yâ‚‚ â†’ ... â†’ yâ‚˜
```

**å•é¡Œ**:
1. **è³‡è¨Šç“¶é ¸**: ç„¡è«–è¼¸å…¥å¤šé•·ï¼Œéƒ½è¦å£“ç¸®æˆå›ºå®šé•·åº¦ $\mathbf{c}$
2. **è³‡è¨Šæå¤±**: é•·åºåˆ—ä¸­æ—©æœŸè³‡è¨Šé›£ä»¥ä¿ç•™åˆ°æœ€å¾Œ
3. **å‡ç­‰å°å¾…**: æ‰€æœ‰è¼¸å…¥æ™‚åˆ»å°è¼¸å‡ºçš„è²¢ç»è¢«è¦–ç‚ºç›¸åŒ

#### åŒ–å·¥å¯¦ä¾‹ï¼šæ‰¹æ¬¡å“è³ªé æ¸¬

è€ƒæ…®ä¸€å€‹æ‰¹æ¬¡åæ‡‰éç¨‹ï¼š

```
æ™‚é–“è»¸: [é ç†±] [åŠ æ–™] [å‡æº«] [åæ‡‰] [ç©©å®š] [é™æº«] [å¸æ–™]
         5åˆ†   10åˆ†   20åˆ†   120åˆ†  30åˆ†   20åˆ†   5åˆ†
```

**å‚³çµ±æ–¹æ³•**: å°‡æ•´å€‹210åˆ†é˜å£“ç¸®æˆä¸€å€‹å›ºå®šå‘é‡

**å•é¡Œ**:
- [åŠ æ–™]éšæ®µçš„åƒæ•¸å¯èƒ½æœ€é—œéµï¼Œä½†æ¬Šé‡èˆ‡å…¶ä»–éšæ®µç›¸åŒ
- [ç©©å®š]éšæ®µæ™‚é–“å¾ˆé•·ä½†å°å“è³ªå½±éŸ¿è¼ƒå°ï¼Œå»ä½”äº†å¤§é‡"è¨˜æ†¶ç©ºé–“"
- æ¨¡å‹ç„¡æ³•è‡ªå‹•è­˜åˆ¥å“ªäº›æ™‚åˆ»æ›´é‡è¦

**ç†æƒ³æƒ…æ³**: æ¨¡å‹æ‡‰è©²èƒ½**å‹•æ…‹é—œæ³¨**é—œéµæ™‚åˆ»
```
æ™‚é–“è»¸: [é ç†±] [åŠ æ–™] [å‡æº«] [åæ‡‰] [ç©©å®š] [é™æº«] [å¸æ–™]
æ³¨æ„åŠ›:  5%     40%    15%    25%    5%     8%     2%
                â†‘                â†‘
            é—œéµæ™‚åˆ»        æ¬¡é‡è¦æ™‚åˆ»
```

é€™å°±æ˜¯**Attentionæ©Ÿåˆ¶**è¦è§£æ±ºçš„å•é¡Œï¼

### 2.2 åŸºæœ¬Attentionæ©Ÿåˆ¶

**æ ¸å¿ƒæ€æƒ³**: ä¸å†ä½¿ç”¨å–®ä¸€å›ºå®šå‘é‡ï¼Œè€Œæ˜¯è®“æ¨¡å‹åœ¨**ç”Ÿæˆæ¯å€‹è¼¸å‡ºæ™‚ï¼Œå‹•æ…‹åœ°é—œæ³¨è¼¸å…¥åºåˆ—çš„ä¸åŒéƒ¨åˆ†**ã€‚

#### Attentionçš„ç›´è¦ºç†è§£

**é¡æ¯”ï¼šäººé¡é–±è®€**

ç•¶ä½ å›ç­”å•é¡Œ"é€™æ®µæ–‡ç« çš„ä¸»è¦è§€é»æ˜¯ä»€éº¼ï¼Ÿ"æ™‚ï¼š
1. ä½ ä¸æœƒå¹³ç­‰åœ°é—œæ³¨æ¯å€‹å­—
2. ä½ æœƒé‡é»é—œæ³¨é—œéµå¥å­
3. ä¸åŒå•é¡Œæœƒè®“ä½ é—œæ³¨ä¸åŒéƒ¨åˆ†

**Attentionæ©Ÿåˆ¶**æ¨¡æ“¬é€™å€‹éç¨‹ï¼š
- **Query (æŸ¥è©¢)**: ç•¶å‰è¦è™•ç†çš„å•é¡Œ/ä»»å‹™
- **Keys (éµ)**: è¼¸å…¥åºåˆ—ä¸­æ¯å€‹ä½ç½®çš„"ç´¢å¼•"
- **Values (å€¼)**: è¼¸å…¥åºåˆ—ä¸­æ¯å€‹ä½ç½®çš„å…§å®¹
- **Attentionæ¬Šé‡**: æ ¹æ“šQueryèˆ‡Keysçš„åŒ¹é…åº¦è¨ˆç®—

#### åŸºæœ¬æµç¨‹

**1. è¨ˆç®—ç›¸é—œæ€§åˆ†æ•¸**

å°æ–¼è¼¸å…¥åºåˆ—çš„æ¯å€‹ä½ç½® $i$ ï¼Œè¨ˆç®—å®ƒèˆ‡ç•¶å‰Queryçš„ç›¸é—œæ€§ï¼š

$$
\text{score}(Query, Key_i) = \text{similarity}(Query, Key_i)
$$

**2. è½‰æ›ç‚ºæ³¨æ„åŠ›æ¬Šé‡**

ä½¿ç”¨softmaxå°‡åˆ†æ•¸æ­¸ä¸€åŒ–ç‚ºæ¦‚ç‡åˆ†ä½ˆï¼š

$$
\alpha_i = \frac{\exp(\text{score}_i)}{\sum_{j=1}^{n} \exp(\text{score}_j)}
$$

å…¶ä¸­ $\sum_{i=1}^{n} \alpha_i = 1$ ï¼Œä¸” $\alpha_i \geq 0$

**3. è¨ˆç®—åŠ æ¬Šä¸Šä¸‹æ–‡å‘é‡**

æ ¹æ“šæ³¨æ„åŠ›æ¬Šé‡å°ValuesåŠ æ¬Šæ±‚å’Œï¼š

$$
\text{Context} = \sum_{i=1}^{n} \alpha_i \cdot Value_i
$$

#### åœ–ç¤ºç†è§£

```
è¼¸å…¥åºåˆ—: [hâ‚] [hâ‚‚] [hâ‚ƒ] [hâ‚„] [hâ‚…]
           â†“    â†“    â†“    â†“    â†“
Keys:     [kâ‚] [kâ‚‚] [kâ‚ƒ] [kâ‚„] [kâ‚…]
Values:   [vâ‚] [vâ‚‚] [vâ‚ƒ] [vâ‚„] [vâ‚…]

Query: [q]
        â†“
è¨ˆç®—ç›¸é—œæ€§: [0.8, 0.3, 0.9, 0.2, 0.4]
        â†“
Softmaxæ­¸ä¸€åŒ–: [0.35, 0.10, 0.38, 0.06, 0.11]
                â†“    â†“    â†“    â†“    â†“
åŠ æ¬Šæ±‚å’Œ: Context = 0.35Ã—vâ‚ + 0.10Ã—vâ‚‚ + 0.38Ã—vâ‚ƒ + 0.06Ã—vâ‚„ + 0.11Ã—vâ‚…
```

**è§€å¯Ÿ**: 
- $k_3$ èˆ‡Queryæœ€åŒ¹é…ï¼ˆåˆ†æ•¸0.9ï¼‰ï¼Œç²å¾—æœ€é«˜æ¬Šé‡ï¼ˆ0.38ï¼‰
- $k_4$ èˆ‡Queryæœ€ä¸åŒ¹é…ï¼ˆåˆ†æ•¸0.2ï¼‰ï¼Œç²å¾—æœ€ä½æ¬Šé‡ï¼ˆ0.06ï¼‰
- ä¸Šä¸‹æ–‡å‘é‡ä¸»è¦ç”± $v_1$ å’Œ $v_3$ è²¢ç»

### 2.3 Attentionçš„æ•¸å­¸åŸç†

#### Query-Key-Valueæ¡†æ¶

**æ ¸å¿ƒå…¬å¼**:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V
$$

å…¶ä¸­ï¼š
- $Q$ : QueryçŸ©é™£ï¼Œshape `[n_queries, d_k]`
- $K$ : KeyçŸ©é™£ï¼Œshape `[n_keys, d_k]`
- $V$ : ValueçŸ©é™£ï¼Œshape `[n_keys, d_v]`
- $d_k$ : Key/Queryçš„ç¶­åº¦
- $\sqrt{d_k}$ : ç¸®æ”¾å› å­ï¼ˆé¿å…é»ç©éå¤§ï¼‰

**æ­¥é©Ÿæ‹†è§£**:

**1. è¨ˆç®—æ³¨æ„åŠ›åˆ†æ•¸ (Attention Scores)**

$$
S = Q K^T
$$

çŸ©é™£ä¹˜æ³•çµæœ: $S \in \mathbb{R}^{n_q \times n_k}$

æ¯å€‹å…ƒç´  $S_{ij}$ è¡¨ç¤ºç¬¬ $i$ å€‹queryèˆ‡ç¬¬ $j$ å€‹keyçš„ç›¸é—œæ€§ï¼š

$$
S_{ij} = \mathbf{q}_i \cdot \mathbf{k}_j = \sum_{d=1}^{d_k} q_{id} \cdot k_{jd}
$$

**2. ç¸®æ”¾ (Scaling)**

$$
S_{\text{scaled}} = \frac{S}{\sqrt{d_k}}
$$

**ç‚ºä»€éº¼è¦ç¸®æ”¾ï¼Ÿ** ç•¶ $d_k$ å¾ˆå¤§æ™‚ï¼Œé»ç©çµæœæœƒå¾ˆå¤§ï¼Œå°è‡´softmaxé€²å…¥é£½å’Œå€ï¼Œæ¢¯åº¦æ¶ˆå¤±ã€‚é™¤ä»¥ $\sqrt{d_k}$ å¯ä»¥ç©©å®šè¨“ç·´ã€‚

**3. æ­¸ä¸€åŒ–ç‚ºæ¬Šé‡ (Softmax)**

$$
A = \text{softmax}(S_{\text{scaled}}) = \text{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right)
$$

å°æ¯ä¸€è¡Œï¼ˆæ¯å€‹queryï¼‰é€²è¡Œsoftmaxï¼š

$$
A_{ij} = \frac{\exp(S_{ij}/\sqrt{d_k})}{\sum_{k=1}^{n_k} \exp(S_{ik}/\sqrt{d_k})}
$$

çµæœ: $A \in \mathbb{R}^{n_q \times n_k}$ ï¼Œæ¯è¡Œå’Œç‚º1

**4. åŠ æ¬Šæ±‚å’Œ (Weighted Sum)**

$$
\text{Output} = A V
$$

çŸ©é™£ä¹˜æ³•çµæœ: $\text{Output} \in \mathbb{R}^{n_q \times d_v}$

æ¯å€‹è¼¸å‡ºå‘é‡æ˜¯Valuesçš„åŠ æ¬Šçµ„åˆï¼š

$$
\text{Output}_i = \sum_{j=1}^{n_k} A_{ij} \mathbf{v}_j
$$

#### æ•¸å€¼ç¯„ä¾‹

å‡è¨­ï¼š
- Query: $Q = [q_1, q_2]$ï¼Œshape `[2, 3]`
- Key: $K = [k_1, k_2, k_3]$ï¼Œshape `[3, 3]`
- Value: $V = [v_1, v_2, v_3]$ï¼Œshape `[3, 2]`

**å…·é«”æ•¸å€¼**:
```python
Q = [[1.0, 0.5, 0.2],   # qâ‚
     [0.3, 0.8, 0.6]]   # qâ‚‚

K = [[0.9, 0.4, 0.1],   # kâ‚
     [0.2, 0.6, 0.3],   # kâ‚‚
     [1.1, 0.5, 0.2]]   # kâ‚ƒ

V = [[2.0, 1.5],        # vâ‚
     [1.0, 2.0],        # vâ‚‚
     [3.0, 1.0]]        # vâ‚ƒ
```

**æ­¥é©Ÿ1: è¨ˆç®— $S = Q K^T$**
```
S = [[1.0Ã—0.9 + 0.5Ã—0.4 + 0.2Ã—0.1,  1.0Ã—0.2 + 0.5Ã—0.6 + 0.2Ã—0.3,  1.0Ã—1.1 + 0.5Ã—0.5 + 0.2Ã—0.2],
     [0.3Ã—0.9 + 0.8Ã—0.4 + 0.6Ã—0.1,  0.3Ã—0.2 + 0.8Ã—0.6 + 0.6Ã—0.3,  0.3Ã—1.1 + 0.8Ã—0.5 + 0.6Ã—0.2]]

  = [[1.12, 0.56, 1.39],
     [0.65, 0.72, 0.85]]
```

**æ­¥é©Ÿ2: ç¸®æ”¾ (å‡è¨­ $d_k=3$, $\sqrt{d_k}\approx1.73$)**
```
S_scaled = [[0.65, 0.32, 0.80],
            [0.38, 0.42, 0.49]]
```

**æ­¥é©Ÿ3: Softmax**
```
A = softmax(S_scaled)
  = [[0.31, 0.23, 0.46],   # qâ‚çš„æ³¨æ„åŠ›: kâ‚ƒæœ€é«˜
     [0.30, 0.32, 0.38]]   # qâ‚‚çš„æ³¨æ„åŠ›: kâ‚ƒç•¥é«˜
```

**æ­¥é©Ÿ4: åŠ æ¬Šæ±‚å’Œ $Output = A V$**
```
Output = [[0.31Ã—2.0 + 0.23Ã—1.0 + 0.46Ã—3.0,  0.31Ã—1.5 + 0.23Ã—2.0 + 0.46Ã—1.0],
          [0.30Ã—2.0 + 0.32Ã—1.0 + 0.38Ã—3.0,  0.30Ã—1.5 + 0.32Ã—2.0 + 0.38Ã—1.0]]

       = [[2.23, 1.39],   # qâ‚çš„è¼¸å‡ºå‘é‡
          [2.06, 1.47]]   # qâ‚‚çš„è¼¸å‡ºå‘é‡
```

**è§£é‡‹**:
- $q_1$ èˆ‡ $k_3$ æœ€åŒ¹é…ï¼ˆæ¬Šé‡0.46ï¼‰ï¼Œæ‰€ä»¥è¼¸å‡ºä¸»è¦ç”± $v_3$ è²¢ç»
- $q_2$ çš„æ³¨æ„åŠ›è¼ƒåˆ†æ•£ï¼Œè¼¸å‡ºæ˜¯ä¸‰å€‹valuesçš„è¼ƒå‡å‹»æ··åˆ

#### ä¸åŒçš„ç›¸ä¼¼åº¦å‡½æ•¸

é™¤äº†é»ç©ï¼Œé‚„æœ‰å…¶ä»–è¨ˆç®—Query-Keyç›¸é—œæ€§çš„æ–¹æ³•ï¼š

**1. é»ç© (Dot Product)** - æœ€å¸¸ç”¨

$$
\text{score}(\mathbf{q}, \mathbf{k}) = \mathbf{q}^T \mathbf{k}
$$

å„ªé»ï¼šè¨ˆç®—æ•ˆç‡é«˜ï¼Œå¯ç”¨çŸ©é™£é‹ç®—åŠ é€Ÿ

**2. ç¸®æ”¾é»ç© (Scaled Dot Product)**

$$
\text{score}(\mathbf{q}, \mathbf{k}) = \frac{\mathbf{q}^T \mathbf{k}}{\sqrt{d_k}}
$$

å„ªé»ï¼šç©©å®šè¨“ç·´ï¼Œæ˜¯æ¨™æº–Transformerä½¿ç”¨çš„æ–¹æ³•

**3. åŠ æ³•æ³¨æ„åŠ› (Additive Attention / Bahdanau Attention)**

$$
\text{score}(\mathbf{q}, \mathbf{k}) = \mathbf{v}^T \tanh(\mathbf{W}_q \mathbf{q} + \mathbf{W}_k \mathbf{k})
$$

å„ªé»ï¼šæ›´æœ‰è¡¨é”åŠ›ï¼Œä½†è¨ˆç®—æˆæœ¬è¼ƒé«˜

**4. é›™ç·šæ€§ (Bilinear)**

$$
\text{score}(\mathbf{q}, \mathbf{k}) = \mathbf{q}^T \mathbf{W} \mathbf{k}
$$

å„ªé»ï¼šå¼•å…¥å¯å­¸ç¿’åƒæ•¸ $\mathbf{W}$ ï¼Œå¢åŠ éˆæ´»æ€§

**5. é¤˜å¼¦ç›¸ä¼¼åº¦ (Cosine Similarity)**

$$
\text{score}(\mathbf{q}, \mathbf{k}) = \frac{\mathbf{q}^T \mathbf{k}}{\|\mathbf{q}\| \|\mathbf{k}\|}
$$

å„ªé»ï¼šå°å‘é‡é•·åº¦ä¸æ•æ„Ÿ

### 2.4 ä¸åŒé¡å‹çš„Attention

#### 1. Encoder-Decoder Attention (äº¤å‰æ³¨æ„åŠ›)

ç”¨æ–¼Seq2Seqæ¨¡å‹ï¼ŒDecoderé—œæ³¨Encoderçš„è¼¸å‡ºã€‚

**çµæ§‹**:
```
Encoderè¼¸å‡º: [hâ‚, hâ‚‚, ..., hâ‚™] â†’ Keys & Values
Decoderç‹€æ…‹: [s_t]             â†’ Query

åœ¨æ™‚é–“æ­¥tï¼ŒDecoderè¨ˆç®—å°Encoderæ¯å€‹ä½ç½®çš„æ³¨æ„åŠ›
```

**å…¬å¼**:
- Query: ä¾†è‡ªDecoderç•¶å‰ç‹€æ…‹ $\mathbf{q} = \mathbf{s}_t$
- Keys: ä¾†è‡ªEncoderæ‰€æœ‰éš±è—ç‹€æ…‹ $\mathbf{K} = [\mathbf{h}_1, \mathbf{h}_2, ..., \mathbf{h}_n]$
- Values: åŒKeysï¼Œ$\mathbf{V} = \mathbf{K}$

$$
\mathbf{c}_t = \sum_{i=1}^{n} \alpha_{ti} \mathbf{h}_i
$$

å…¶ä¸­ï¼š

$$
\alpha_{ti} = \frac{\exp(\text{score}(\mathbf{s}_t, \mathbf{h}_i))}{\sum_{j=1}^{n} \exp(\text{score}(\mathbf{s}_t, \mathbf{h}_j))}
$$

**æ‡‰ç”¨**: æ©Ÿå™¨ç¿»è­¯ã€æ–‡æœ¬æ‘˜è¦ã€åºåˆ—è½‰æ›

#### 2. Self-Attention (è‡ªæ³¨æ„åŠ›)

åºåˆ—å…§éƒ¨çš„æ¯å€‹ä½ç½®é—œæ³¨åºåˆ—ä¸­çš„æ‰€æœ‰ä½ç½®ã€‚

**çµæ§‹**:
```
è¼¸å…¥åºåˆ—: [xâ‚, xâ‚‚, ..., xâ‚™]
æ¯å€‹xáµ¢åŒæ™‚ä½œç‚ºQueryã€Keyã€Value
```

**å…¬å¼**:

$$
\text{Self-Attention}(X) = \text{softmax}\left(\frac{X W_Q (X W_K)^T}{\sqrt{d_k}}\right) X W_V
$$

å…¶ä¸­ï¼š
- $X$ : è¼¸å…¥åºåˆ—çŸ©é™£
- $W_Q, W_K, W_V$ : å¯å­¸ç¿’çš„æŠ•å½±çŸ©é™£

**æ‡‰ç”¨**: Transformerã€BERTã€GPTç­‰æ¨¡å‹çš„æ ¸å¿ƒçµ„ä»¶

#### 3. Multi-Head Attention (å¤šé ­æ³¨æ„åŠ›)

ä¸¦è¡Œé‹è¡Œå¤šå€‹ç¨ç«‹çš„Attentionï¼Œæ•æ‰ä¸åŒå­ç©ºé–“çš„è³‡è¨Šã€‚

**çµæ§‹**:
```
è¼¸å…¥X â†’ [Headâ‚] [Headâ‚‚] ... [Headâ‚•] â†’ Concat â†’ Linear â†’ è¼¸å‡º
```

**å…¬å¼**:

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h) W^O
$$

å…¶ä¸­æ¯å€‹headï¼š

$$
\text{head}_i = \text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)
$$

**å„ªå‹¢**:
- ä¸åŒheadé—œæ³¨ä¸åŒæ¨¡å¼ï¼ˆå¦‚ï¼šä¸€å€‹é—œæ³¨å±€éƒ¨ï¼Œä¸€å€‹é—œæ³¨å…¨å±€ï¼‰
- å¢åŠ æ¨¡å‹è¡¨é”èƒ½åŠ›
- Transformerçš„æ¨™æº–é…ç½®

**ç¯„ä¾‹**: 8å€‹headsï¼Œæ¯å€‹headç¶­åº¦64ï¼Œç¸½ç¶­åº¦ $8 \times 64 = 512$

### 2.5 Self-Attentionè©³è§£

Self-Attentionæ˜¯ç¾ä»£NLPå’Œåºåˆ—å»ºæ¨¡çš„åŸºçŸ³ï¼Œå€¼å¾—æ·±å…¥ç†è§£ã€‚

#### å‹•æ©Ÿ

**å•é¡Œ**: å¦‚ä½•è®“åºåˆ—ä¸­çš„æ¯å€‹ä½ç½®"çœ‹åˆ°"æ•´å€‹åºåˆ—çš„è³‡è¨Šï¼Ÿ

**å‚³çµ±RNN**:
- è³‡è¨Šé€šééš±è—ç‹€æ…‹é€æ­¥å‚³é
- é è·é›¢ä¾è³´é›£ä»¥æ•æ‰
- ç„¡æ³•ä¸¦è¡Œè¨ˆç®—

**Self-Attention**:
- æ¯å€‹ä½ç½®ç›´æ¥èˆ‡æ‰€æœ‰ä½ç½®äº¤äº’
- ä¸€æ­¥åˆ°ä½æ•æ‰é•·è·é›¢ä¾è³´
- å®Œå…¨å¯ä¸¦è¡ŒåŒ–

#### è¨ˆç®—æµç¨‹

**è¼¸å…¥**: åºåˆ— $X = [\mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_n]$ï¼Œshape `[n, d_model]`

**1. ç·šæ€§è®Šæ›ç”ŸæˆQ, K, V**

$$
\begin{aligned}
Q &= X W^Q, \quad \text{shape: } [n, d_k] \\
K &= X W^K, \quad \text{shape: } [n, d_k] \\
V &= X W^V, \quad \text{shape: } [n, d_v]
\end{aligned}
$$

**2. è¨ˆç®—Self-Attention**

$$
\text{Output} = \text{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V
$$

**3. è¼¸å‡ºå½¢ç‹€**: `[n, d_v]`

#### ç›´è¦ºç†è§£

è€ƒæ…®ä¸€å€‹åŒ–å·¥éç¨‹åºåˆ—ï¼š
```
æ™‚é–“æ­¥:  tâ‚      tâ‚‚      tâ‚ƒ      tâ‚„      tâ‚…
ç‹€æ…‹:   [å•Ÿå‹•]  [å‡æº«]  [åæ‡‰]  [ç©©å®š]  [é™æº«]
```

**Self-Attentionè¨ˆç®—**:

å°æ–¼tâ‚ƒ (åæ‡‰éšæ®µ)ï¼š
- Query: "åæ‡‰éšæ®µéœ€è¦ä»€éº¼è³‡è¨Šï¼Ÿ"
- Keys: æ‰€æœ‰æ™‚é–“æ­¥çš„"ç´¢å¼•" [tâ‚, tâ‚‚, tâ‚ƒ, tâ‚„, tâ‚…]
- Values: æ‰€æœ‰æ™‚é–“æ­¥çš„"å…§å®¹" [å•Ÿå‹•ç‹€æ…‹, å‡æº«ç‹€æ…‹, ...]

**æ³¨æ„åŠ›æ¬Šé‡** (å‡è¨­):
```
tâ‚ƒå°å„æ™‚é–“æ­¥çš„æ³¨æ„åŠ›:
tâ‚(å•Ÿå‹•): 0.15  â† åˆå§‹æ¢ä»¶æœ‰ä¸€å®šå½±éŸ¿
tâ‚‚(å‡æº«): 0.35  â† å‡æº«æ–¹å¼å¾ˆé‡è¦
tâ‚ƒ(åæ‡‰): 0.30  â† ç•¶å‰ç‹€æ…‹æœ€é‡è¦
tâ‚„(ç©©å®š): 0.15  â† æœªä¾†è¶¨å‹¢æœ‰åƒè€ƒåƒ¹å€¼
tâ‚…(é™æº«): 0.05  â† é™æº«éšæ®µç›¸é—œæ€§è¼ƒä½
```

**çµæœ**: tâ‚ƒçš„è¼¸å‡ºæ˜¯æ‰€æœ‰æ™‚é–“æ­¥è³‡è¨Šçš„åŠ æ¬Šçµ„åˆï¼Œé‡é»é—œæ³¨tâ‚‚å’Œtâ‚ƒã€‚

#### Self-AttentionçŸ©é™£è¦–è§’

å‡è¨­åºåˆ—é•·åº¦ $n=5$ï¼ŒAttentionæ¬Šé‡çŸ©é™£ $A \in \mathbb{R}^{5 \times 5}$ï¼š

```
        æ³¨æ„å°è±¡
       tâ‚   tâ‚‚   tâ‚ƒ   tâ‚„   tâ‚…
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
tâ‚ â”‚ 0.4  0.3  0.2  0.1  0.0 â”‚ â† tâ‚ä¸»è¦é—œæ³¨è‡ªå·±å’Œtâ‚‚
tâ‚‚ â”‚ 0.2  0.4  0.3  0.1  0.0 â”‚ â† tâ‚‚é—œæ³¨è‡ªå·±å’Œé„°è¿‘
tâ‚ƒ â”‚ 0.15 0.35 0.30 0.15 0.05â”‚ â† tâ‚ƒï¼ˆå¦‚ä¸Šä¾‹ï¼‰
tâ‚„ â”‚ 0.1  0.2  0.3  0.3  0.1 â”‚
tâ‚… â”‚ 0.05 0.1  0.2  0.3  0.35â”‚ â† tâ‚…ä¸»è¦é—œæ³¨å¾ŒæœŸ
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**è§€å¯Ÿ**:
- å°è§’ç·šå€¼è¼ƒé«˜ï¼šæ¯å€‹ä½ç½®å°è‡ªå·±çš„æ³¨æ„åŠ›è¼ƒå¤§
- å¸¶ç‹€çµæ§‹ï¼šç›¸é„°ä½ç½®é€šå¸¸ç›¸é—œæ€§æ›´é«˜
- éå°ç¨±ï¼š $A_{ij} \neq A_{ji}$ ï¼ˆtâ‚é—œæ³¨tâ‚‚ vs tâ‚‚é—œæ³¨tâ‚çš„ç¨‹åº¦ä¸åŒï¼‰

#### ä½ç½®ç·¨ç¢¼ (Positional Encoding)

**å•é¡Œ**: Self-Attentionæ²’æœ‰æ™‚é–“é †åºæ¦‚å¿µï¼

$$
\text{Attention}([x_1, x_2, x_3]) = \text{Attention}([x_2, x_1, x_3])
$$

**è§£æ±º**: åŠ å…¥ä½ç½®ç·¨ç¢¼

$$
\tilde{X} = X + PE
$$

å…¶ä¸­ $PE$ æ˜¯ä½ç½®ç·¨ç¢¼çŸ©é™£ã€‚

**å¸¸è¦‹ä½ç½®ç·¨ç¢¼**:

1. **æ­£å¼¦ä½ç½®ç·¨ç¢¼** (TransformeråŸå§‹æ–¹æ³•):

$$
\begin{aligned}
PE_{(pos, 2i)} &= \sin\left(\frac{pos}{10000^{2i/d}}\right) \\
PE_{(pos, 2i+1)} &= \cos\left(\frac{pos}{10000^{2i/d}}\right)
\end{aligned}
$$

2. **å¯å­¸ç¿’ä½ç½®ç·¨ç¢¼**:
```python
pos_embedding = Embedding(max_length, d_model)
```

3. **ç›¸å°ä½ç½®ç·¨ç¢¼**:
```
ä¸ç·¨ç¢¼çµ•å°ä½ç½®ï¼Œè€Œæ˜¯ç·¨ç¢¼å…©å€‹ä½ç½®ä¹‹é–“çš„ç›¸å°è·é›¢
```

### 2.6 æ‡‰ç”¨å¯¦ä¾‹èˆ‡å¯è¦–åŒ–

#### åŒ–å·¥æ™‚é–“åºåˆ—ä¸­çš„Attentionæ‡‰ç”¨

**æ¡ˆä¾‹1: æ‰¹æ¬¡çµæŸæ™‚é–“é æ¸¬**

**ä»»å‹™**: æ ¹æ“šåæ‡‰éç¨‹å‰50åˆ†é˜çš„æ•¸æ“šï¼Œé æ¸¬æ‰¹æ¬¡çµæŸæ™‚é–“

**æ¨¡å‹æ¶æ§‹**:
```python
inputs = Input(shape=(50, 10))  # 50å€‹æ™‚é–“æ­¥ï¼Œ10å€‹ç‰¹å¾µ

# LSTMç·¨ç¢¼
lstm_out = LSTM(128, return_sequences=True)(inputs)

# Self-Attentionå±¤
attention_weights = Dense(1, activation='tanh')(lstm_out)
attention_weights = Flatten()(attention_weights)
attention_weights = Activation('softmax')(attention_weights)
attention_weights = RepeatVector(128)(attention_weights)
attention_weights = Permute([2, 1])(attention_weights)

# åŠ æ¬Šæ±‚å’Œ
context = Multiply()([lstm_out, attention_weights])
context = Lambda(lambda x: K.sum(x, axis=1))(context)

# é æ¸¬
output = Dense(64, activation='relu')(context)
output = Dense(1)(output)  # é æ¸¬æ™‚é–“

model = Model(inputs=inputs, outputs=output)
```

**Attentionæ¬Šé‡å¯è¦–åŒ–**:

å‡è¨­è¨“ç·´å¾Œç™¼ç¾ï¼š
```
æ™‚é–“æ­¥:  0-10  11-20  21-30  31-40  41-50
æ¬Šé‡:   0.08   0.15   0.45   0.22   0.10

         â†‘ é—œéµæ™‚åˆ»ï¼ˆåæ‡‰å•Ÿå‹•éšæ®µï¼‰
```

**è§£é‡‹**: æ¨¡å‹å­¸åˆ°äº†åæ‡‰å•Ÿå‹•éšæ®µï¼ˆ21-30åˆ†é˜ï¼‰çš„åƒæ•¸æœ€èƒ½é æ¸¬æ‰¹æ¬¡çµæŸæ™‚é–“ã€‚

**æ¡ˆä¾‹2: ç•°å¸¸æª¢æ¸¬èˆ‡å®šä½**

**ä»»å‹™**: æª¢æ¸¬æ™‚é–“åºåˆ—ä¸­çš„ç•°å¸¸ï¼Œä¸¦è­˜åˆ¥å°è‡´ç•°å¸¸çš„é—œéµæ™‚åˆ»

**æ¶æ§‹**: Bidirectional LSTM + Self-Attention

**å¯è¦–åŒ–ç¯„ä¾‹**:
```
æ™‚é–“åºåˆ—: â”â”â”â”â”â”â”â”â–²â”â”â”â”â”â”â”â”
                 â†‘ æª¢æ¸¬åˆ°ç•°å¸¸

Attentionç†±åŠ›åœ–:
        é—œæ³¨æ™‚åˆ»
ç•°å¸¸    â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘  â† æ¨¡å‹é«˜åº¦é—œæ³¨ç•°å¸¸ç™¼ç”Ÿå‰çš„5å€‹æ™‚é–“æ­¥
æ™‚åˆ»    â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
```

**çµè«–**: Attentionæ¬Šé‡æ­ç¤ºäº†ç•°å¸¸çš„"æ ¹æº"æ™‚åˆ»ã€‚

#### å¯¦ä½œæŠ€å·§

**1. ç°¡å–®Attentionå±¤å¯¦ç¾** (Keras)

```python
class AttentionLayer(Layer):
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)
    
    def build(self, input_shape):
        # å¯å­¸ç¿’çš„æ¬Šé‡çŸ©é™£
        self.W = self.add_weight(name='att_weight',
                                 shape=(input_shape[-1], 1),
                                 initializer='random_normal',
                                 trainable=True)
        self.b = self.add_weight(name='att_bias',
                                 shape=(input_shape[1], 1),
                                 initializer='zeros',
                                 trainable=True)
        super(AttentionLayer, self).build(input_shape)
    
    def call(self, x):
        # x shape: (batch, timesteps, features)
        
        # è¨ˆç®—æ³¨æ„åŠ›åˆ†æ•¸
        e = K.tanh(K.dot(x, self.W) + self.b)  # (batch, timesteps, 1)
        
        # Softmaxæ­¸ä¸€åŒ–
        a = K.softmax(e, axis=1)  # (batch, timesteps, 1)
        
        # åŠ æ¬Šæ±‚å’Œ
        output = x * a  # (batch, timesteps, features)
        output = K.sum(output, axis=1)  # (batch, features)
        
        return output
    
    def compute_output_shape(self, input_shape):
        return (input_shape[0], input_shape[-1])

# ä½¿ç”¨
inputs = Input(shape=(timesteps, features))
lstm_out = LSTM(64, return_sequences=True)(inputs)
attention_out = AttentionLayer()(lstm_out)
output = Dense(1)(attention_out)
model = Model(inputs=inputs, outputs=output)
```

**2. ä½¿ç”¨TensorFlowå…§å»ºAttention**

```python
from tensorflow.keras.layers import MultiHeadAttention

# Self-Attention
inputs = Input(shape=(seq_len, d_model))
attn_output = MultiHeadAttention(
    num_heads=8,
    key_dim=64
)(inputs, inputs)  # Qå’ŒKéƒ½ä¾†è‡ªinputs (self-attention)

# Encoder-Decoder Attention
encoder_output = ...  # shape: (batch, enc_len, d_model)
decoder_input = ...   # shape: (batch, dec_len, d_model)

attn_output = MultiHeadAttention(
    num_heads=8,
    key_dim=64
)(decoder_input, encoder_output)  # Queryä¾†è‡ªdecoderï¼ŒK/Vä¾†è‡ªencoder
```

**3. æå–Attentionæ¬Šé‡ç”¨æ–¼å¯è¦–åŒ–**

```python
# å»ºç«‹è¿”å›attentionæ¬Šé‡çš„æ¨¡å‹
attention_model = Model(
    inputs=model.input,
    outputs=model.get_layer('attention_layer').output
)

# ç²å–æ¬Šé‡
attention_weights = attention_model.predict(test_data)

# å¯è¦–åŒ–
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12, 4))
sns.heatmap(attention_weights[0].reshape(1, -1), 
            cmap='YlOrRd', 
            xticklabels=range(timesteps),
            yticklabels=['Attention'])
plt.xlabel('Time Step')
plt.title('Attention Weights Visualization')
plt.show()
```

### 2.7 å°çµï¼šAttentionæ©Ÿåˆ¶

**æ ¸å¿ƒè¦é»**:
1. âœ… Attentionè§£æ±ºå›ºå®šé•·åº¦å‘é‡çš„è³‡è¨Šç“¶é ¸å•é¡Œ
2. âœ… é€éQuery-Key-Valueæ¡†æ¶å‹•æ…‹é—œæ³¨é‡è¦è³‡è¨Š
3. âœ… Self-Attentionå¯¦ç¾åºåˆ—å…§éƒ¨çš„å…¨å±€äº¤äº’
4. âœ… Multi-Head Attentionå¢å¼·æ¨¡å‹è¡¨é”èƒ½åŠ›
5. âœ… Attentionæ¬Šé‡æä¾›æ¨¡å‹å¯è§£é‡‹æ€§

**ä¸åŒAttentioné¡å‹å°æ¯”**:

| é¡å‹ | Queryä¾†æº | Key/Valueä¾†æº | æ‡‰ç”¨å ´æ™¯ |
|-----|----------|--------------|---------|
| **Encoder-Decoder Attention** | Decoderç‹€æ…‹ | Encoderè¼¸å‡º | Seq2Seq, ç¿»è­¯ |
| **Self-Attention** | åºåˆ—è‡ªèº« | åºåˆ—è‡ªèº« | Transformer, BERT |
| **Multi-Head Attention** | å¤šå€‹æŠ•å½± | å¤šå€‹æŠ•å½± | å¢å¼·è¡¨é”èƒ½åŠ› |

**é¸æ“‡æŒ‡å—**:
- éœ€è¦å¯è§£é‡‹æ€§ â†’ **å–®é ­Attention + æ¬Šé‡å¯è¦–åŒ–**
- é•·åºåˆ—å»ºæ¨¡ â†’ **Self-Attention**
- åºåˆ—è½‰æ›ä»»å‹™ â†’ **Encoder-Decoder Attention**
- è¿½æ±‚æ€§èƒ½ â†’ **Multi-Head Attention**

---

## 3. Seq2Seqèˆ‡Encoder-Decoderæ¶æ§‹ + Attentionæ©Ÿåˆ¶

### æœ¬ç« å­¸ç¿’åœ°åœ–

> [!IMPORTANT]
> **æœ¬ç« æ ¸å¿ƒå•é¡Œ**: å¦‚ä½•è™•ç†è¼¸å…¥åºåˆ—å’Œè¼¸å‡ºåºåˆ—é•·åº¦ä¸åŒçš„å•é¡Œï¼ŸAttentionå¦‚ä½•æå‡Seq2Seqæ€§èƒ½ï¼Ÿ

**å­¸ç¿’ç›®æ¨™**:
1. ğŸ¯ **ç†è§£Seq2Seqæ¶æ§‹**: èªè­˜Encoder-Decoderæ¡†æ¶çš„è¨­è¨ˆç†å¿µ
2. ğŸ”„ **æŒæ¡åºåˆ—è½‰æ›**: äº†è§£è®Šé•·åºåˆ—åˆ°è®Šé•·åºåˆ—çš„æ˜ å°„
3. ğŸ§  **æ•´åˆAttentionæ©Ÿåˆ¶**: ç†è§£å¦‚ä½•å°‡Attentionæ‡‰ç”¨æ–¼Seq2Seq
4. ğŸ”§ **å¯¦ä½œå®Œæ•´æ¨¡å‹**: å­¸æœƒå»ºç«‹å¸¶Attentionçš„Seq2Seqæ¨¡å‹

**ç‚ºä»€éº¼åŒ–å·¥äººéœ€è¦å­¸Seq2Seqï¼Ÿ**

åœ¨åŒ–å·¥é ˜åŸŸï¼Œè¨±å¤šä»»å‹™æ¶‰åŠ**åºåˆ—åˆ°åºåˆ—çš„è½‰æ›**ï¼š
- éç¨‹è»Œè·¡é æ¸¬ï¼šè¼¸å…¥æ­·å²è»Œè·¡ â†’ è¼¸å‡ºæœªä¾†è»Œè·¡
- æ‰¹æ¬¡toæ‰¹æ¬¡å»ºæ¨¡ï¼šè¼¸å…¥æ‰¹æ¬¡Aè»Œè·¡ â†’ é æ¸¬æ‰¹æ¬¡Bè»Œè·¡
- æ§åˆ¶ç­–ç•¥ç”Ÿæˆï¼šè¼¸å…¥ç›®æ¨™è»Œè·¡ â†’ è¼¸å‡ºæ§åˆ¶åºåˆ—
- æ•¸æ“šå¡«è£œï¼šè¼¸å…¥ä¸å®Œæ•´åºåˆ— â†’ è¼¸å‡ºå®Œæ•´åºåˆ—

å‚³çµ±æ–¹æ³•çš„å•é¡Œï¼š
- âŒ ç„¡æ³•è™•ç†è¼¸å…¥è¼¸å‡ºé•·åº¦ä¸åŒçš„æƒ…æ³
- âŒ é›£ä»¥å­¸ç¿’è¤‡é›œçš„åºåˆ—è½‰æ›è¦å‰‡
- âŒ ç„¡æ³•è¨˜æ†¶é•·åºåˆ—çš„å…¨éƒ¨è³‡è¨Š

**Seq2Seq + Attentionçš„å„ªå‹¢**ï¼š
- âœ… éˆæ´»è™•ç†è®Šé•·åºåˆ—
- âœ… ç«¯åˆ°ç«¯å­¸ç¿’åºåˆ—æ˜ å°„
- âœ… é€šéAttentioné—œæ³¨è¼¸å…¥é—œéµéƒ¨åˆ†
- âœ… å¤§å¹…æå‡é•·åºåˆ—å»ºæ¨¡æ•ˆæœ

**æœ¬ç« æ¶æ§‹**:

```
Seq2SeqåŸºæœ¬æ¦‚å¿µ (3.1)
    â†“
Encoder-Decoderæ¶æ§‹ (3.2)
    â†“
å‚³çµ±Seq2Seqçš„é™åˆ¶ (3.3)
    â†“
åŠ å…¥Attentionæ©Ÿåˆ¶ (3.4)
    â†“
è¨“ç·´èˆ‡æ¨ç†ç­–ç•¥ (3.5)
    â†“
åŒ–å·¥æ‡‰ç”¨å¯¦ä¾‹ (3.6)
```

> [!TIP]
> å­¸ç¿’å»ºè­°ï¼šå…ˆç†è§£åŸºæœ¬Encoder-Decoderï¼Œå†å¼•å…¥Attentionã€‚æ³¨æ„è¨“ç·´å’Œæ¨ç†éšæ®µçš„å·®ç•°ï¼ˆTeacher Forcingï¼‰ã€‚

---

### 3.1 Seq2SeqåŸºæœ¬æ¦‚å¿µ

**Sequence-to-Sequence (Seq2Seq)** æ¨¡å‹æ—¨åœ¨å°‡ä¸€å€‹åºåˆ—è½‰æ›ç‚ºå¦ä¸€å€‹åºåˆ—ï¼Œå…©è€…é•·åº¦å¯ä»¥ä¸åŒã€‚

#### ä»€éº¼æ˜¯Seq2Seqä»»å‹™ï¼Ÿ

**å®šç¾©**: çµ¦å®šè¼¸å…¥åºåˆ— $X = [x_1, x_2, ..., x_n]$ ï¼Œç”Ÿæˆè¼¸å‡ºåºåˆ— $Y = [y_1, y_2, ..., y_m]$ ï¼Œå…¶ä¸­ $n \neq m$ ï¼ˆå¯ä»¥ç›¸ç­‰ï¼Œä½†ä¸å¿…é ˆï¼‰ã€‚

**ç¶“å…¸æ‡‰ç”¨**:

| é ˜åŸŸ | è¼¸å…¥åºåˆ— | è¼¸å‡ºåºåˆ— | ç¯„ä¾‹ |
|------|---------|---------|------|
| **æ©Ÿå™¨ç¿»è­¯** | æºèªè¨€å¥å­ | ç›®æ¨™èªè¨€å¥å­ | "I love AI" â†’ "æˆ‘æ„›AI" |
| **æ–‡æœ¬æ‘˜è¦** | é•·æ–‡æœ¬ | æ‘˜è¦ | æ–°èæ–‡ç«  â†’ æ¨™é¡Œ |
| **å°è©±ç³»çµ±** | ç”¨æˆ¶å•é¡Œ | æ©Ÿå™¨å›ç­” | "å¤©æ°£å¦‚ä½•ï¼Ÿ" â†’ "ä»Šå¤©æ™´å¤©" |
| **èªéŸ³è­˜åˆ¥** | è²éŸ³æ³¢å½¢ | æ–‡å­— | èªéŸ³ â†’ "ä½ å¥½" |

#### åŒ–å·¥é ˜åŸŸçš„Seq2Seqä»»å‹™

**1. å¤šæ­¥é æ¸¬**
```
è¼¸å…¥: éå»24å°æ™‚çš„æº«åº¦åºåˆ— (24å€‹é»)
è¼¸å‡º: æœªä¾†12å°æ™‚çš„æº«åº¦åºåˆ— (12å€‹é»)

[t-24, ..., t-1] â†’ [t, t+1, ..., t+11]
```

**2. è»Œè·¡åˆ°è»Œè·¡è½‰æ›**
```
è¼¸å…¥: è¨­å®šå€¼è»Œè·¡ (ç›®æ¨™æº«åº¦æ›²ç·š)
è¼¸å‡º: å¯¦éš›æ§åˆ¶è»Œè·¡ (åŠ ç†±åŠŸç‡æ›²ç·š)

[T_è¨­å®š(t1), ..., T_è¨­å®š(tn)] â†’ [P_åŠ ç†±(t1), ..., P_åŠ ç†±(tm)]
```

**3. æ‰¹æ¬¡ç›¸ä¼¼æ€§é æ¸¬**
```
è¼¸å…¥: åƒè€ƒæ‰¹æ¬¡çš„å®Œæ•´è»Œè·¡ (100å€‹æ™‚é–“æ­¥)
è¼¸å‡º: æ–°æ‰¹æ¬¡çš„é æœŸè»Œè·¡ (100å€‹æ™‚é–“æ­¥)

æ‰¹æ¬¡Aè»Œè·¡ â†’ æ‰¹æ¬¡Bè»Œè·¡
```

**4. ç•°å¸¸ä¿®å¾©**
```
è¼¸å…¥: å«æœ‰ç•°å¸¸/ç¼ºå¤±çš„æ„Ÿæ¸¬å™¨æ•¸æ“š
è¼¸å‡º: ä¿®å¾©å¾Œçš„å®Œæ•´æ•¸æ“š

[x1, x2, NaN, NaN, x5, ...] â†’ [x1, x2, xÌ‚3, xÌ‚4, x5, ...]
```

#### Seq2Seq vs å‚³çµ±æ™‚é–“åºåˆ—é æ¸¬

| ç‰¹æ€§ | å‚³çµ±é æ¸¬ | Seq2Seq |
|-----|---------|---------|
| **è¼¸å‡º** | å–®ä¸€å€¼æˆ–å›ºå®šé•·åº¦å‘é‡ | è®Šé•·åºåˆ— |
| **çµæ§‹** | ç°¡å–®RNN/LSTM | Encoder-Decoder |
| **éˆæ´»æ€§** | ä½ï¼ˆè¼¸å‡ºæ ¼å¼å›ºå®šï¼‰ | é«˜ï¼ˆå¯è®Šé•·è¼¸å‡ºï¼‰ |
| **æ‡‰ç”¨** | å–®æ­¥/å¤šæ­¥é æ¸¬ | åºåˆ—è½‰æ›ã€ç”Ÿæˆ |
| **è¤‡é›œåº¦** | è¼ƒä½ | è¼ƒé«˜ |

> [!NOTE]
> Seq2Seqæ˜¯ä¸€å€‹æ›´é€šç”¨çš„æ¡†æ¶ï¼Œå‚³çµ±çš„å¤šæ­¥é æ¸¬å¯ä»¥è¦–ç‚ºSeq2Seqçš„ç‰¹ä¾‹ï¼ˆè¼¸å…¥å’Œè¼¸å‡ºéƒ½æ˜¯æ™‚é–“åºåˆ—ï¼‰ã€‚

### 3.2 Encoder-Decoderæ¶æ§‹

**æ ¸å¿ƒæ€æƒ³**: å°‡Seq2Seqä»»å‹™åˆ†ç‚ºå…©å€‹éšæ®µï¼š
1. **Encoder**: å°‡è¼¸å…¥åºåˆ—ç·¨ç¢¼æˆå›ºå®šé•·åº¦çš„ä¸Šä¸‹æ–‡å‘é‡
2. **Decoder**: æ ¹æ“šä¸Šä¸‹æ–‡å‘é‡ç”Ÿæˆè¼¸å‡ºåºåˆ—

#### åŸºæœ¬æ¶æ§‹

```
è¼¸å…¥åºåˆ—:  xâ‚    xâ‚‚    xâ‚ƒ    xâ‚„
           â†“     â†“     â†“     â†“
Encoder:  [RNN][RNN][RNN][RNN]
                          â†“
                    ä¸Šä¸‹æ–‡å‘é‡ c
                          â†“
Decoder:            [RNN][RNN][RNN]
                     â†“     â†“     â†“
è¼¸å‡ºåºåˆ—:            yâ‚    yâ‚‚    yâ‚ƒ
```

**å·¥ä½œæµç¨‹**:

**éšæ®µ1: Encoding (ç·¨ç¢¼)**
- è¼¸å…¥: $X = [x_1, x_2, ..., x_n]$
- Encoderé€å€‹è™•ç†è¼¸å…¥æ™‚é–“æ­¥
- æœ€å¾Œçš„éš±è—ç‹€æ…‹ä½œç‚ºä¸Šä¸‹æ–‡å‘é‡: $\mathbf{c} = \mathbf{h}_n^{enc}$

**éšæ®µ2: Decoding (è§£ç¢¼)**
- åˆå§‹åŒ–: Decoderçš„åˆå§‹éš±è—ç‹€æ…‹ $\mathbf{h}_0^{dec} = \mathbf{c}$
- Decoderè‡ªå›æ­¸ç”Ÿæˆè¼¸å‡º:
  - è¼¸å…¥å‰ä¸€æ­¥çš„è¼¸å‡ºï¼ˆæˆ–ç‰¹æ®Šèµ·å§‹ç¬¦è™Ÿï¼‰
  - ç”Ÿæˆç•¶å‰æ­¥çš„è¼¸å‡º
  - æ›´æ–°éš±è—ç‹€æ…‹
  - é‡è¤‡ç›´åˆ°ç”ŸæˆçµæŸç¬¦è™Ÿæˆ–é”åˆ°æœ€å¤§é•·åº¦

#### æ•¸å­¸å…¬å¼

**Encoder**:

$$
\mathbf{h}_t^{enc} = f_{enc}(\mathbf{x}_t, \mathbf{h}_{t-1}^{enc})
$$

ä¸Šä¸‹æ–‡å‘é‡:

$$
\mathbf{c} = q(\mathbf{h}_1^{enc}, \mathbf{h}_2^{enc}, ..., \mathbf{h}_n^{enc})
$$

é€šå¸¸ $q$ å‡½æ•¸é¸æ“‡æœ€å¾Œçš„éš±è—ç‹€æ…‹: $\mathbf{c} = \mathbf{h}_n^{enc}$

**Decoder**:

$$
\begin{aligned}
\mathbf{h}_t^{dec} &= f_{dec}(\mathbf{y}_{t-1}, \mathbf{h}_{t-1}^{dec}, \mathbf{c}) \\
\mathbf{y}_t &= g(\mathbf{h}_t^{dec}, \mathbf{c})
\end{aligned}
$$

å…¶ä¸­ï¼š
- $f_{enc}, f_{dec}$ : RNNå–®å…ƒï¼ˆLSTMæˆ–GRUï¼‰
- $g$ : è¼¸å‡ºå‡½æ•¸ï¼ˆé€šå¸¸æ˜¯ç·šæ€§å±¤ + softmaxæˆ–ç·šæ€§è¼¸å‡ºï¼‰

#### Keraså¯¦ç¾ç¯„ä¾‹

**åŸºæœ¬Seq2Seqï¼ˆç„¡Attentionï¼‰**:

```python
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense

# åƒæ•¸è¨­å®š
latent_dim = 256
encoder_input_dim = 10  # è¼¸å…¥ç‰¹å¾µæ•¸
decoder_output_dim = 5   # è¼¸å‡ºç‰¹å¾µæ•¸

# ===== Encoder =====
encoder_inputs = Input(shape=(None, encoder_input_dim))
encoder_lstm = LSTM(latent_dim, return_state=True)
encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)
# åªä¿ç•™ç‹€æ…‹ï¼Œä¸Ÿæ£„è¼¸å‡º
encoder_states = [state_h, state_c]

# ===== Decoder =====
decoder_inputs = Input(shape=(None, decoder_output_dim))
decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(decoder_inputs, 
                                     initial_state=encoder_states)
decoder_dense = Dense(decoder_output_dim, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

# ===== è¨“ç·´æ¨¡å‹ =====
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
model.compile(optimizer='adam', loss='categorical_crossentropy')
```

**æ¨ç†æ¨¡å‹** (é æ¸¬æ™‚ä½¿ç”¨):

```python
# Encoderæ¨¡å‹ï¼ˆèˆ‡è¨“ç·´æ™‚å…±äº«æ¬Šé‡ï¼‰
encoder_model = Model(encoder_inputs, encoder_states)

# Decoderæ¨¡å‹
decoder_state_input_h = Input(shape=(latent_dim,))
decoder_state_input_c = Input(shape=(latent_dim,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]

decoder_outputs, state_h, state_c = decoder_lstm(
    decoder_inputs, initial_state=decoder_states_inputs)
decoder_states = [state_h, state_c]
decoder_outputs = decoder_dense(decoder_outputs)

decoder_model = Model(
    [decoder_inputs] + decoder_states_inputs,
    [decoder_outputs] + decoder_states)

# æ¨ç†éç¨‹ï¼ˆè‡ªå›æ­¸ç”Ÿæˆï¼‰
def decode_sequence(input_seq):
    # ç·¨ç¢¼è¼¸å…¥åºåˆ—
    states_value = encoder_model.predict(input_seq)
    
    # ç”Ÿæˆç¬¬ä¸€å€‹å­—ç¬¦ï¼ˆèµ·å§‹ç¬¦è™Ÿï¼‰
    target_seq = np.zeros((1, 1, decoder_output_dim))
    target_seq[0, 0, start_token_idx] = 1.
    
    # è‡ªå›æ­¸ç”Ÿæˆ
    decoded_sequence = []
    for _ in range(max_decoder_length):
        output, h, c = decoder_model.predict(
            [target_seq] + states_value)
        
        # é¸æ“‡æœ€å¯èƒ½çš„è¼¸å‡º
        sampled_token_index = np.argmax(output[0, -1, :])
        decoded_sequence.append(sampled_token_index)
        
        # åœæ­¢æ¢ä»¶
        if sampled_token_index == stop_token_idx:
            break
        
        # æ›´æ–°ç‹€æ…‹å’Œè¼¸å…¥
        target_seq = np.zeros((1, 1, decoder_output_dim))
        target_seq[0, 0, sampled_token_index] = 1.
        states_value = [h, c]
    
    return decoded_sequence
```

#### é—œéµæ¦‚å¿µ

**1. Teacher Forcing**

**è¨“ç·´æ™‚**: ä½¿ç”¨çœŸå¯¦çš„ç›®æ¨™åºåˆ—ä½œç‚ºDecoderçš„è¼¸å…¥
```
çœŸå¯¦è¼¸å‡º: [yâ‚, yâ‚‚, yâ‚ƒ, yâ‚„]
Decoderè¼¸å…¥: [<START>, yâ‚, yâ‚‚, yâ‚ƒ]  â† ä½¿ç”¨çœŸå¯¦å€¼
```

**å„ªé»**: åŠ é€Ÿè¨“ç·´ï¼Œç©©å®šæ”¶æ–‚
**ç¼ºé»**: è¨“ç·´èˆ‡æ¨ç†ä¸ä¸€è‡´ï¼ˆexposure biasï¼‰

**2. è‡ªå›æ­¸ç”Ÿæˆ**

**æ¨ç†æ™‚**: ä½¿ç”¨æ¨¡å‹è‡ªå·±çš„é æ¸¬ä½œç‚ºä¸‹ä¸€æ­¥çš„è¼¸å…¥
```
Decoderè¼¸å…¥: [<START>, Å·â‚, Å·â‚‚, Å·â‚ƒ]  â† ä½¿ç”¨é æ¸¬å€¼
```

**å•é¡Œ**: æ—©æœŸéŒ¯èª¤æœƒç´¯ç©æ”¾å¤§

**3. èµ·å§‹å’ŒçµæŸç¬¦è™Ÿ**

```
<START>: ç‰¹æ®Šèµ·å§‹ç¬¦è™Ÿï¼Œæ¨™è¨˜è§£ç¢¼é–‹å§‹
<END>:   ç‰¹æ®ŠçµæŸç¬¦è™Ÿï¼Œæ¨™è¨˜åºåˆ—çµæŸ

å®Œæ•´åºåˆ—: [<START>, yâ‚, yâ‚‚, ..., yâ‚˜, <END>]
```

### 3.3 å‚³çµ±Seq2Seqçš„é™åˆ¶

**å•é¡Œ1: è³‡è¨Šç“¶é ¸**

æ•´å€‹è¼¸å…¥åºåˆ—å¿…é ˆå£“ç¸®æˆå–®ä¸€å›ºå®šé•·åº¦å‘é‡ $\mathbf{c}$ï¼š

```
é•·è¼¸å…¥åºåˆ— [xâ‚, xâ‚‚, ..., xâ‚â‚€â‚€] â†’ å›ºå®šå‘é‡c (ç¶­åº¦256)
                                      â†“
                              è³‡è¨Šæå¤±ï¼
```

**å½±éŸ¿**:
- è¼¸å…¥è¶Šé•·ï¼Œè³‡è¨Šæå¤±è¶Šåš´é‡
- æ—©æœŸæ™‚é–“æ­¥çš„è³‡è¨Šé›£ä»¥ä¿ç•™
- æ¨¡å‹æ€§èƒ½éš¨åºåˆ—é•·åº¦å¢åŠ è€Œä¸‹é™

**å•é¡Œ2: æ¢¯åº¦æ¶ˆå¤±**

é•·åºåˆ—å°è‡´åå‘å‚³æ’­å›°é›£ï¼š

```
è¼¸å…¥æ™‚åˆ»t=1çš„æ¢¯åº¦è¦ç¶“é:
Encoder (100æ­¥) â†’ ä¸Šä¸‹æ–‡å‘é‡ â†’ Decoder (50æ­¥)

ç¸½å…±150æ­¥ï¼æ¢¯åº¦å¹¾ä¹æ¶ˆå¤±
```

**å•é¡Œ3: ç„¡å·®åˆ¥å°å¾…**

æ‰€æœ‰è¼¸å…¥æ™‚åˆ»å°æ‰€æœ‰è¼¸å‡ºæ™‚åˆ»çš„è²¢ç»ç›¸åŒï¼š

```
è¼¸å‡ºyâ‚: ä¾è³´æ•´å€‹è¼¸å…¥åºåˆ—çš„"å¹³å‡"è³‡è¨Š
è¼¸å‡ºyâ‚‚: ä¾è³´æ•´å€‹è¼¸å…¥åºåˆ—çš„"å¹³å‡"è³‡è¨Š
...

ä½†å¯¦éš›ä¸Š: yâ‚å¯èƒ½ä¸»è¦ä¾è³´xâ‚-xâ‚ƒï¼Œyâ‚‚å¯èƒ½ä¸»è¦ä¾è³´xâ‚…-xâ‚‡
```

#### æ€§èƒ½ä¸‹é™å¯¦é©—

åŸºæ–¼æ©Ÿå™¨ç¿»è­¯ä»»å‹™çš„ç¶“å…¸å¯¦é©—ï¼ˆCho et al., 2014ï¼‰ï¼š

| è¼¸å…¥é•·åº¦ | ä¸å¸¶Attention BLEU | å¸¶Attention BLEU | æå‡ |
|---------|-------------------|-----------------|------|
| 10-20è© | 28.5 | 30.2 | +1.7 |
| 20-30è© | 24.3 | 29.8 | +5.5 |
| 30-40è© | 18.7 | 28.5 | +9.8 |
| 40-50è© | 12.4 | 26.3 | +13.9 |

**è§€å¯Ÿ**: è¼¸å…¥è¶Šé•·ï¼ŒAttentionçš„æ”¹é€²è¶Šé¡¯è‘—ï¼

> [!IMPORTANT]
> Attentionæ©Ÿåˆ¶æ­£æ˜¯ç‚ºäº†è§£æ±ºé€™äº›å•é¡Œè€Œæå‡ºçš„ã€‚å®ƒå…è¨±Decoderåœ¨ç”Ÿæˆæ¯å€‹è¼¸å‡ºæ™‚ï¼Œå‹•æ…‹é—œæ³¨è¼¸å…¥åºåˆ—çš„ä¸åŒéƒ¨åˆ†ã€‚

### 3.4 Seq2Seq + Attentionæ©Ÿåˆ¶

**æ ¸å¿ƒæ”¹é€²**: ä¸å†ä½¿ç”¨å–®ä¸€ä¸Šä¸‹æ–‡å‘é‡ï¼Œè€Œæ˜¯ç‚ºDecoderçš„æ¯å€‹æ™‚é–“æ­¥è¨ˆç®—å‹•æ…‹ä¸Šä¸‹æ–‡å‘é‡ã€‚

#### Attentionå¢å¼·çš„Seq2Seqæ¶æ§‹

```
Encoder:  [hâ‚][hâ‚‚][hâ‚ƒ][hâ‚„][hâ‚…]  â† Encoderæ‰€æœ‰éš±è—ç‹€æ…‹
           â†“   â†“   â†“   â†“   â†“
         [Attentionå±¤]  â† åœ¨æ¯å€‹Decoderæ­¥è¨ˆç®—
           â†“   â†“   â†“   â†“   â†“
         å‹•æ…‹ä¸Šä¸‹æ–‡å‘é‡câ‚œ
              â†“
Decoder:   [sâ‚][sâ‚‚][sâ‚ƒ]
            â†“   â†“   â†“
è¼¸å‡º:      yâ‚  yâ‚‚  yâ‚ƒ
```

**é—œéµå·®ç•°**:

| ç‰¹æ€§ | å‚³çµ±Seq2Seq | Attention Seq2Seq |
|-----|-------------|-------------------|
| **ä¸Šä¸‹æ–‡å‘é‡** | å›ºå®šï¼Œåªç”¨æœ€å¾Œéš±è—ç‹€æ…‹ | å‹•æ…‹ï¼Œæ¯æ­¥ä¸åŒ |
| **Encoderè¼¸å‡º** | åªä¿ç•™æœ€å¾Œç‹€æ…‹ | ä¿ç•™æ‰€æœ‰éš±è—ç‹€æ…‹ |
| **è³‡è¨Šç“¶é ¸** | åš´é‡ | å¤§å¹…ç·©è§£ |
| **é•·åºåˆ—æ€§èƒ½** | é¡¯è‘—ä¸‹é™ | ç©©å®š |
| **å¯è§£é‡‹æ€§** | ç„¡ | æœ‰ï¼ˆæ³¨æ„åŠ›æ¬Šé‡ï¼‰ |

#### æ•¸å­¸å…¬å¼

**Encoder** (ä¿æŒä¸è®Š):

$$
\mathbf{h}_i^{enc} = f_{enc}(\mathbf{x}_i, \mathbf{h}_{i-1}^{enc}), \quad i=1,2,...,n
$$

ä¿ç•™æ‰€æœ‰éš±è—ç‹€æ…‹: $\mathbf{H}^{enc} = [\mathbf{h}_1^{enc}, \mathbf{h}_2^{enc}, ..., \mathbf{h}_n^{enc}]$

**Decoder with Attention**:

åœ¨æ™‚é–“æ­¥ $t$ï¼š

**æ­¥é©Ÿ1**: è¨ˆç®—æ³¨æ„åŠ›åˆ†æ•¸

$$
e_{ti} = a(\mathbf{s}_{t-1}, \mathbf{h}_i^{enc})
$$

å…¶ä¸­ $a$ æ˜¯æ³¨æ„åŠ›è©•åˆ†å‡½æ•¸ï¼ˆå¦‚é»ç©æˆ–åŠ æ³•æ³¨æ„åŠ›ï¼‰

**æ­¥é©Ÿ2**: æ­¸ä¸€åŒ–ç‚ºæ³¨æ„åŠ›æ¬Šé‡

$$
\alpha_{ti} = \frac{\exp(e_{ti})}{\sum_{j=1}^{n} \exp(e_{tj})}
$$

**æ­¥é©Ÿ3**: è¨ˆç®—ä¸Šä¸‹æ–‡å‘é‡

$$
\mathbf{c}_t = \sum_{i=1}^{n} \alpha_{ti} \mathbf{h}_i^{enc}
$$

**æ­¥é©Ÿ4**: æ›´æ–°Decoderç‹€æ…‹

$$
\mathbf{s}_t = f_{dec}(\mathbf{y}_{t-1}, \mathbf{s}_{t-1}, \mathbf{c}_t)
$$

**æ­¥é©Ÿ5**: ç”Ÿæˆè¼¸å‡º

$$
\mathbf{y}_t = g(\mathbf{s}_t, \mathbf{c}_t, \mathbf{y}_{t-1})
$$

#### Bahdanau Attention (åŠ æ³•æ³¨æ„åŠ›)

**è©•åˆ†å‡½æ•¸**:

$$
e_{ti} = \mathbf{v}^T \tanh(\mathbf{W}_s \mathbf{s}_{t-1} + \mathbf{W}_h \mathbf{h}_i^{enc})
$$

å…¶ä¸­ï¼š
- $\mathbf{W}_s, \mathbf{W}_h$ : å¯å­¸ç¿’çš„æ¬Šé‡çŸ©é™£
- $\mathbf{v}$ : å¯å­¸ç¿’çš„æ¬Šé‡å‘é‡

**å®Œæ•´æµç¨‹**:

```python
# å‡è¨­:
# encoder_outputs shape: (batch, enc_len, enc_hidden)
# decoder_hidden shape: (batch, dec_hidden)

# 1. è¨ˆç®—åˆ†æ•¸
score = v^T @ tanh(W_s @ decoder_hidden + W_h @ encoder_outputs)
# score shape: (batch, enc_len)

# 2. Softmaxæ­¸ä¸€åŒ–
attention_weights = softmax(score)
# attention_weights shape: (batch, enc_len)

# 3. åŠ æ¬Šæ±‚å’Œ
context = sum(attention_weights * encoder_outputs, axis=1)
# context shape: (batch, enc_hidden)

# 4. çµåˆä¸Šä¸‹æ–‡å’ŒDecoderè¼¸å…¥
decoder_input_combined = concat([decoder_input, context], axis=-1)

# 5. Decoderå‰å‘å‚³æ’­
decoder_output, decoder_hidden = decoder_rnn(decoder_input_combined, decoder_hidden)
```

#### Luong Attention (é»ç©æ³¨æ„åŠ›)

**è©•åˆ†å‡½æ•¸**:

$$
e_{ti} = \mathbf{s}_t^T \mathbf{h}_i^{enc}
$$

æ›´ç°¡å–®ï¼Œè¨ˆç®—æ•ˆç‡æ›´é«˜ã€‚

**è®Šé«”**:

1. **Dot**: $\text{score}(\mathbf{s}_t, \mathbf{h}_i) = \mathbf{s}_t^T \mathbf{h}_i$
2. **General**: $\text{score}(\mathbf{s}_t, \mathbf{h}_i) = \mathbf{s}_t^T \mathbf{W} \mathbf{h}_i$
3. **Concat**: $\text{score}(\mathbf{s}_t, \mathbf{h}_i) = \mathbf{v}^T \tanh(\mathbf{W}[\mathbf{s}_t; \mathbf{h}_i])$

#### Keraså¯¦ç¾ï¼šSeq2Seq + Attention

```python
import tensorflow as tf
from tensorflow.keras import layers, Model

class BahdanauAttention(layers.Layer):
    def __init__(self, units):
        super().__init__()
        self.W_s = layers.Dense(units)
        self.W_h = layers.Dense(units)
        self.V = layers.Dense(1)
    
    def call(self, decoder_hidden, encoder_outputs):
        # decoder_hidden shape: (batch, dec_hidden)
        # encoder_outputs shape: (batch, enc_len, enc_hidden)
        
        # æ“´å±•decoder_hiddenä»¥åŒ¹é…encoder_outputs
        decoder_hidden_expanded = tf.expand_dims(decoder_hidden, 1)
        # shape: (batch, 1, dec_hidden)
        
        # è¨ˆç®—åˆ†æ•¸
        score = self.V(tf.nn.tanh(
            self.W_s(decoder_hidden_expanded) + self.W_h(encoder_outputs)
        ))
        # score shape: (batch, enc_len, 1)
        
        # Softmaxæ­¸ä¸€åŒ–
        attention_weights = tf.nn.softmax(score, axis=1)
        # shape: (batch, enc_len, 1)
        
        # åŠ æ¬Šæ±‚å’Œ
        context = attention_weights * encoder_outputs
        context = tf.reduce_sum(context, axis=1)
        # context shape: (batch, enc_hidden)
        
        return context, attention_weights

# å®Œæ•´æ¨¡å‹
class Seq2SeqWithAttention(Model):
    def __init__(self, enc_units, dec_units, attention_units):
        super().__init__()
        # Encoder
        self.encoder = layers.LSTM(enc_units, return_sequences=True, 
                                    return_state=True)
        
        # Attention
        self.attention = BahdanauAttention(attention_units)
        
        # Decoder
        self.decoder_lstm = layers.LSTM(dec_units, return_sequences=True,
                                         return_state=True)
        self.decoder_dense = layers.Dense(output_dim)
    
    def call(self, encoder_input, decoder_input):
        # Encoding
        encoder_outputs, enc_state_h, enc_state_c = self.encoder(encoder_input)
        
        # Decoding with Attention
        decoder_state = [enc_state_h, enc_state_c]
        outputs = []
        
        for t in range(decoder_input.shape[1]):
            # Attention
            context, attention_weights = self.attention(
                decoder_state[0], encoder_outputs)
            
            # çµåˆcontextå’Œdecoderè¼¸å…¥
            decoder_input_t = decoder_input[:, t:t+1, :]
            decoder_input_combined = tf.concat(
                [decoder_input_t, tf.expand_dims(context, 1)], axis=-1)
            
            # Decoder step
            decoder_output, state_h, state_c = self.decoder_lstm(
                decoder_input_combined, initial_state=decoder_state)
            decoder_state = [state_h, state_c]
            
            # è¼¸å‡º
            output = self.decoder_dense(decoder_output)
            outputs.append(output)
        
        return tf.concat(outputs, axis=1)
```

### 3.5 è¨“ç·´èˆ‡æ¨ç†ç­–ç•¥

#### Teacher Forcing

**è¨“ç·´éšæ®µ** - ä½¿ç”¨Teacher Forcing:

```python
# çœŸå¯¦ç›®æ¨™åºåˆ—: [yâ‚, yâ‚‚, yâ‚ƒ, yâ‚„]
# Decoderè¼¸å…¥: [<START>, yâ‚, yâ‚‚, yâ‚ƒ]
# Decoderè¼¸å‡º: [Å·â‚, Å·â‚‚, Å·â‚ƒ, Å·â‚„]
# æå¤±: Compare(Å·, y)

for epoch in range(num_epochs):
    for batch in dataset:
        encoder_input = batch['source']
        decoder_input = batch['target_input']  # [<START>, yâ‚, yâ‚‚, yâ‚ƒ]
        decoder_target = batch['target']       # [yâ‚, yâ‚‚, yâ‚ƒ, yâ‚„]
        
        with tf.GradientTape() as tape:
            predictions = model(encoder_input, decoder_input)
            loss = loss_fn(decoder_target, predictions)
        
        gradients = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(gradients, model.trainable_variables))
```

**å•é¡Œ**: è¨“ç·´æ™‚ç”¨çœŸå¯¦å€¼ï¼Œæ¨ç†æ™‚ç”¨é æ¸¬å€¼ â†’ **exposure bias**

**æ”¹é€²: Scheduled Sampling**

é€æ¼¸å¢åŠ ä½¿ç”¨é æ¸¬å€¼çš„æ¯”ä¾‹ï¼š

```python
# éš¨æ©Ÿæ±ºå®šä½¿ç”¨çœŸå¯¦å€¼é‚„æ˜¯é æ¸¬å€¼
use_teacher_forcing = random.random() < teacher_forcing_ratio

if use_teacher_forcing:
    decoder_input_t = target[:, t]  # çœŸå¯¦å€¼
else:
    decoder_input_t = predicted_output  # é æ¸¬å€¼

# é€æ¼¸é™ä½teacher_forcing_ratio
teacher_forcing_ratio = max(0.5, teacher_forcing_ratio * 0.99)
```

#### æ¨ç†ç­–ç•¥

**1. Greedy Decoding (è²ªå¿ƒè§£ç¢¼)**

æ¯æ­¥é¸æ“‡æ©Ÿç‡æœ€é«˜çš„è¼¸å‡ºï¼š

```python
def greedy_decode(encoder_input):
    # Encoding
    encoder_outputs, encoder_state = encoder(encoder_input)
    
    # åˆå§‹åŒ–
    decoder_input = start_token
    decoder_state = encoder_state
    decoded_sequence = []
    
    for t in range(max_length):
        # Attention + Decoder step
        context, _ = attention(decoder_state, encoder_outputs)
        decoder_output, decoder_state = decoder(decoder_input, decoder_state, context)
        
        # é¸æ“‡æ©Ÿç‡æœ€é«˜çš„token
        predicted_id = tf.argmax(decoder_output, axis=-1)
        decoded_sequence.append(predicted_id)
        
        # åœæ­¢æ¢ä»¶
        if predicted_id == end_token:
            break
        
        # æ›´æ–°è¼¸å…¥
        decoder_input = predicted_id
    
    return decoded_sequence
```

**å„ªé»**: å¿«é€Ÿï¼Œç°¡å–®
**ç¼ºé»**: å¯èƒ½éŒ¯éå…¨å±€æœ€å„ªè§£

**2. Beam Search**

ç¶­è­· $k$ å€‹æœ€æœ‰å¯èƒ½çš„å€™é¸åºåˆ—ï¼š

```python
def beam_search(encoder_input, beam_width=3):
    # Encoding
    encoder_outputs, encoder_state = encoder(encoder_input)
    
    # åˆå§‹åŒ–beam
    beams = [([], 0.0, encoder_state, start_token)]  # (sequence, score, state, last_token)
    
    for t in range(max_length):
        candidates = []
        
        for seq, score, state, last_token in beams:
            # Decoder step
            context, _ = attention(state, encoder_outputs)
            decoder_output, new_state = decoder(last_token, state, context)
            
            # å–top-kå€‹å€™é¸
            top_k_probs, top_k_ids = tf.nn.top_k(decoder_output, k=beam_width)
            
            for prob, token_id in zip(top_k_probs, top_k_ids):
                new_seq = seq + [token_id]
                new_score = score + tf.math.log(prob)
                candidates.append((new_seq, new_score, new_state, token_id))
        
        # å¾æ‰€æœ‰å€™é¸ä¸­é¸æ“‡top-k
        beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]
        
        # æª¢æŸ¥æ˜¯å¦æ‰€æœ‰beaméƒ½çµæŸ
        if all(seq[-1] == end_token for seq, _, _, _ in beams):
            break
    
    # è¿”å›æœ€ä½³åºåˆ—
    best_sequence = beams[0][0]
    return best_sequence
```

**å„ªé»**: æ›´å¯èƒ½æ‰¾åˆ°å…¨å±€æœ€å„ª
**ç¼ºé»**: è¨ˆç®—æˆæœ¬é«˜ï¼ˆ $k$ å€ï¼‰

**3. Sampling (æ¡æ¨£)**

æ ¹æ“šæ©Ÿç‡åˆ†ä½ˆéš¨æ©Ÿæ¡æ¨£ï¼Œå¢åŠ å¤šæ¨£æ€§ï¼š

```python
def sample_decode(encoder_input, temperature=1.0):
    # ... (encodingéƒ¨åˆ†ç›¸åŒ)
    
    for t in range(max_length):
        # ... (decoder step)
        
        # æº«åº¦ç¸®æ”¾ + æ¡æ¨£
        logits = decoder_output / temperature
        predicted_id = tf.random.categorical(logits, num_samples=1)
        
        decoded_sequence.append(predicted_id)
        # ...
```

**æº«åº¦åƒæ•¸**:
- `temperature = 1.0`: æ¨™æº–æ¡æ¨£
- `temperature < 1.0`: æ›´ç¢ºå®šæ€§ï¼ˆæ¥è¿‘greedyï¼‰
- `temperature > 1.0`: æ›´éš¨æ©Ÿï¼ˆå¢åŠ å¤šæ¨£æ€§ï¼‰

### 3.6 åŒ–å·¥æ‡‰ç”¨å¯¦ä¾‹

#### æ¡ˆä¾‹1: å¤šè®Šæ•¸æ™‚é–“åºåˆ—é æ¸¬

**ä»»å‹™**: åŸºæ–¼éå»24å°æ™‚çš„10å€‹éç¨‹è®Šæ•¸ï¼Œé æ¸¬æœªä¾†12å°æ™‚çš„5å€‹é—œéµè®Šæ•¸

**æ•¸æ“š**:
- è¼¸å…¥: `(batch, 24, 10)` - 24å€‹æ™‚é–“æ­¥ï¼Œ10å€‹ç‰¹å¾µ
- è¼¸å‡º: `(batch, 12, 5)` - 12å€‹æ™‚é–“æ­¥ï¼Œ5å€‹ç‰¹å¾µ

**æ¨¡å‹æ¶æ§‹**:

```python
# è¶…åƒæ•¸
encoder_input_dim = 10
decoder_output_dim = 5
encoder_units = 128
decoder_units = 128
attention_units = 64

# Encoder
encoder_inputs = Input(shape=(24, encoder_input_dim))
encoder_lstm = LSTM(encoder_units, return_sequences=True, return_state=True)
encoder_outputs, enc_h, enc_c = encoder_lstm(encoder_inputs)
encoder_states = [enc_h, enc_c]

# Attentionå±¤
attention = BahdanauAttention(attention_units)

# Decoder
decoder_inputs = Input(shape=(12, decoder_output_dim))
decoder_lstm = LSTM(decoder_units, return_sequences=True, return_state=True)

# è¨“ç·´æ¨¡å‹
decoder_outputs_list = []
decoder_states = encoder_states

for t in range(12):
    # Attention
    context, attn_weights = attention(decoder_states[0], encoder_outputs)
    context = tf.expand_dims(context, 1)
    
    # çµåˆè¼¸å…¥å’Œä¸Šä¸‹æ–‡
    decoder_input_t = decoder_inputs[:, t:t+1, :]
    decoder_input_combined = tf.concat([decoder_input_t, context], axis=-1)
    
    # Decoder step
    decoder_output, h, c = decoder_lstm(decoder_input_combined, 
                                        initial_state=decoder_states)
    decoder_states = [h, c]
    
    # è¼¸å‡ºå±¤
    output_t = Dense(decoder_output_dim)(decoder_output)
    decoder_outputs_list.append(output_t)

decoder_outputs = tf.concat(decoder_outputs_list, axis=1)

model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# è¨“ç·´
history = model.fit(
    [X_train_enc, X_train_dec],  # X_train_decæ˜¯ç›®æ¨™åºåˆ—çš„shiftedç‰ˆæœ¬
    y_train,
    validation_data=([X_val_enc, X_val_dec], y_val),
    epochs=100,
    batch_size=32,
    callbacks=[EarlyStopping(patience=10)]
)
```

**Attentionæ¬Šé‡å¯è¦–åŒ–**:

```python
# æå–attentionæ¬Šé‡
attention_model = Model([encoder_inputs, decoder_inputs], 
                        attention_weights_all)
attn_weights = attention_model.predict([X_test_enc, X_test_dec])

# ç¹ªè£½ç†±åŠ›åœ–
plt.figure(figsize=(10, 8))
sns.heatmap(attn_weights[0], cmap='YlOrRd', 
            xticklabels=range(24), yticklabels=range(12))
plt.xlabel('Encoderæ™‚é–“æ­¥ (éå»24å°æ™‚)')
plt.ylabel('Decoderæ™‚é–“æ­¥ (æœªä¾†12å°æ™‚)')
plt.title('Attention Weights Heatmap')
plt.show()
```

**é æœŸæ¨¡å¼**:
```
Attentionç†±åŠ›åœ–é¡¯ç¤º:
- é æ¸¬æœªä¾†1å°æ™‚: é«˜åº¦é—œæ³¨éå»1-2å°æ™‚
- é æ¸¬æœªä¾†6å°æ™‚: é—œæ³¨éå»6-12å°æ™‚
- é æ¸¬æœªä¾†12å°æ™‚: è¼ƒå‡å‹»é—œæ³¨æ•´å€‹24å°æ™‚
```

#### æ¡ˆä¾‹2: æ‰¹æ¬¡è»Œè·¡è½‰æ›

**ä»»å‹™**: çµ¦å®šåƒè€ƒæ‰¹æ¬¡çš„æº«åº¦è»Œè·¡ï¼Œç”Ÿæˆæ–°æ‰¹æ¬¡çš„é æœŸå£“åŠ›è»Œè·¡

**æ‡‰ç”¨å ´æ™¯**: å·¥è—é·ç§»ã€æ‰¹æ¬¡ç›¸ä¼¼æ€§åˆ†æ

**ç‰¹é»**:
- è¼¸å…¥è¼¸å‡ºé•·åº¦å¯ä»¥ä¸åŒ
- éœ€è¦æ•æ‰è¤‡é›œçš„éç·šæ€§æ˜ å°„é—œä¿‚
- Attentionå¹«åŠ©è­˜åˆ¥é—œéµéšæ®µçš„å°æ‡‰é—œä¿‚

**æ•¸æ“šæº–å‚™**:

```python
# æ‰¹æ¬¡Aï¼ˆåƒè€ƒæ‰¹æ¬¡ï¼‰
temperature_trajectory_A = ...  # shape: (100,)
# æ‰¹æ¬¡Bï¼ˆç›®æ¨™æ‰¹æ¬¡ï¼‰
pressure_trajectory_B = ...     # shape: (80,)

# æ§‹å»ºè¨“ç·´é›†
X_encoder = []  # æº«åº¦è»Œè·¡
y_decoder = []  # å£“åŠ›è»Œè·¡

for batch_pair in dataset:
    X_encoder.append(batch_pair['ref_temperature'])
    y_decoder.append(batch_pair['target_pressure'])

X_encoder = np.array(X_encoder)  # (n_samples, 100, 1)
y_decoder = np.array(y_decoder)  # (n_samples, 80, 1)
```

**è¨“ç·´èˆ‡è©•ä¼°**:

```python
# è¨“ç·´
model.fit([X_encoder, y_decoder_shifted], y_decoder,
          epochs=50, validation_split=0.2)

# æ¨ç†ï¼ˆä½¿ç”¨è²ªå¿ƒè§£ç¢¼ï¼‰
def predict_trajectory(input_trajectory):
    encoder_output = encoder_model.predict(input_trajectory)
    
    predicted_trajectory = []
    decoder_input = start_value
    state = initial_state
    
    for t in range(max_output_length):
        context = attention(state, encoder_output)
        output, state = decoder_step(decoder_input, state, context)
        predicted_trajectory.append(output)
        decoder_input = output
    
    return np.array(predicted_trajectory)

# å¯è¦–åŒ–
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(input_temperature, label='è¼¸å…¥æº«åº¦è»Œè·¡')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(true_pressure, label='çœŸå¯¦å£“åŠ›è»Œè·¡', alpha=0.7)
plt.plot(predicted_pressure, label='é æ¸¬å£“åŠ›è»Œè·¡', linestyle='--')
plt.legend()
plt.tight_layout()
plt.show()
```

#### æ¡ˆä¾‹3: ç•°å¸¸æª¢æ¸¬èˆ‡ä¿®å¾©

**ä»»å‹™**: æª¢æ¸¬ä¸¦ä¿®å¾©æ„Ÿæ¸¬å™¨æ•¸æ“šä¸­çš„ç•°å¸¸/ç¼ºå¤±å€¼

**Seq2Seqæ–¹æ³•**:
```
è¼¸å…¥: å«ç•°å¸¸çš„åºåˆ— [xâ‚, xâ‚‚, NaN, xâ‚„_abnormal, xâ‚…, ...]
è¼¸å‡º: ä¿®å¾©å¾Œçš„åºåˆ— [xâ‚, xâ‚‚, xÌ‚â‚ƒ, xÌ‚â‚„, xâ‚…, ...]
```

**æ¨¡å‹è¨­è¨ˆ**:
- Encoder: å­¸ç¿’å«ç•°å¸¸åºåˆ—çš„è¡¨ç¤º
- Attention: è­˜åˆ¥ç•°å¸¸é™„è¿‘çš„æ­£å¸¸æ¨¡å¼
- Decoder: é‡å»ºå®Œæ•´åºåˆ—

**æ•ˆæœè©•ä¼°**:

```python
# åœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°ä¿®å¾©æ•ˆæœ
mae_before = np.mean(np.abs(corrupted_data - true_data))
mae_after = np.mean(np.abs(repaired_data - true_data))

print(f"ä¿®å¾©å‰MAE: {mae_before:.4f}")
print(f"ä¿®å¾©å¾ŒMAE: {mae_after:.4f}")
print(f"æ”¹å–„ç‡: {(1 - mae_after/mae_before)*100:.2f}%")
```

### 3.7 å°çµï¼šSeq2Seq + Attention

**æ ¸å¿ƒè¦é»**:
1. âœ… Seq2Seqé©ç”¨æ–¼è¼¸å…¥è¼¸å‡ºé•·åº¦ä¸åŒçš„åºåˆ—è½‰æ›ä»»å‹™
2. âœ… Encoder-Decoderæ¶æ§‹å°‡ä»»å‹™åˆ†ç‚ºç·¨ç¢¼å’Œè§£ç¢¼å…©éšæ®µ
3. âœ… Attentionæ©Ÿåˆ¶è§£æ±ºå›ºå®šå‘é‡çš„è³‡è¨Šç“¶é ¸å•é¡Œ
4. âœ… Teacher ForcingåŠ é€Ÿè¨“ç·´ä½†å¯èƒ½å°è‡´exposure bias
5. âœ… Beam Searchæé«˜è§£ç¢¼å“è³ªä½†å¢åŠ è¨ˆç®—æˆæœ¬

**æ¶æ§‹å°æ¯”**:

| æ¶æ§‹ | ä¸Šä¸‹æ–‡ | é•·åºåˆ—æ€§èƒ½ | å¯è§£é‡‹æ€§ | è¨ˆç®—æˆæœ¬ |
|------|-------|-----------|---------|---------|
| **å‚³çµ±Seq2Seq** | å›ºå®šå–®ä¸€å‘é‡ | è¼ƒå·® | ç„¡ | ä½ |
| **Seq2Seq + Attention** | å‹•æ…‹å¤šå‘é‡ | è‰¯å¥½ | æœ‰ | ä¸­ |

**é¸æ“‡æŒ‡å—**:
- çŸ­åºåˆ—è½‰æ› â†’ **å‚³çµ±Seq2Seq**
- é•·åºåˆ—è½‰æ› â†’ **Seq2Seq + Attention**
- éœ€è¦å¯è§£é‡‹æ€§ â†’ **åŠ å…¥Attentionä¸¦å¯è¦–åŒ–**
- è®Šé•·è¼¸å‡ºç”Ÿæˆ â†’ **Seq2Seq + Beam Search**

**åŒ–å·¥æ‡‰ç”¨ç¸½çµ**:
- âœ… å¤šæ­¥æ™‚é–“åºåˆ—é æ¸¬
- âœ… æ‰¹æ¬¡åˆ°æ‰¹æ¬¡çš„è»Œè·¡è½‰æ›
- âœ… æ§åˆ¶ç­–ç•¥ç”Ÿæˆ
- âœ… æ•¸æ“šä¿®å¾©èˆ‡å¡«è£œ
- âœ… ç•°å¸¸åºåˆ—é‡æ§‹

---

## ç¸½çµèˆ‡å±•æœ›

### æœ¬å–®å…ƒæ ¸å¿ƒå…§å®¹å›é¡§

**1. é›™å‘RNN (Bidirectional RNN)**
- åŒæ™‚åˆ©ç”¨éå»å’Œæœªä¾†çš„ä¸Šä¸‹æ–‡è³‡è¨Š
- é©ç”¨æ–¼é›¢ç·šåˆ†æä»»å‹™ï¼ˆåºåˆ—åˆ†é¡ã€æ¨™è¨»ï¼‰
- Bi-LSTMå’ŒBi-GRUæ˜¯å¸¸ç”¨è®Šé«”
- æ€§èƒ½æå‡ä½†è¨ˆç®—æˆæœ¬å¢åŠ 

**2. Attentionæ©Ÿåˆ¶**
- è§£æ±ºå›ºå®šé•·åº¦å‘é‡çš„è³‡è¨Šç“¶é ¸
- Query-Key-Valueæ¡†æ¶å¯¦ç¾å‹•æ…‹é—œæ³¨
- Self-Attentionå¯¦ç¾åºåˆ—å…§å…¨å±€äº¤äº’
- æä¾›æ¨¡å‹å¯è§£é‡‹æ€§ï¼ˆæ³¨æ„åŠ›æ¬Šé‡ï¼‰

**3. Seq2Seq + Attention**
- Encoder-Decoderæ¶æ§‹è™•ç†åºåˆ—è½‰æ›
- Attentionç·©è§£é•·åºåˆ—è³‡è¨Šæå¤±
- Teacher Forcingèˆ‡è‡ªå›æ­¸ç”Ÿæˆ
- Beam Searchæå‡è§£ç¢¼å“è³ª

### æŠ€è¡“æ¼”é€²è„ˆçµ¡

```
ç°¡å–®RNN/LSTM (Unit 17åŸºç¤)
    â†“
é›™å‘RNN (åˆ©ç”¨é›™å‘ä¸Šä¸‹æ–‡)
    â†“
Attentionæ©Ÿåˆ¶ (å‹•æ…‹é—œæ³¨é‡è¦éƒ¨åˆ†)
    â†“
Seq2Seq + Attention (éˆæ´»åºåˆ—è½‰æ›)
    â†“
Transformer (å®Œå…¨åŸºæ–¼Attention) â† ä¸‹ä¸€æ­¥å­¸ç¿’æ–¹å‘
```

### åŒ–å·¥é ˜åŸŸæ‡‰ç”¨ç¸½çµ

| æŠ€è¡“ | é©ç”¨ä»»å‹™ | å…¸å‹æ‡‰ç”¨ |
|------|---------|---------|
| **é›™å‘RNN** | æ‰¹æ¬¡åˆ†æã€åºåˆ—åˆ†é¡ | æ‰¹æ¬¡å“è³ªåˆ†é¡ã€éç¨‹éšæ®µè­˜åˆ¥ |
| **Attention** | é—œéµæ™‚åˆ»è­˜åˆ¥ | ç•°å¸¸æª¢æ¸¬ã€å“è³ªé æ¸¬ |
| **Seq2Seq** | åºåˆ—è½‰æ›é æ¸¬ | å¤šæ­¥é æ¸¬ã€è»Œè·¡ç”Ÿæˆ |

### é€²éšå­¸ç¿’æ–¹å‘

**1. Transformeræ¶æ§‹**
- å®Œå…¨åŸºæ–¼Self-Attention
- ä¸¦è¡ŒåŒ–ç¨‹åº¦æ›´é«˜
- BERTã€GPTç­‰æ¨¡å‹çš„åŸºç¤

**2. æ™‚é–“åºåˆ—å°ˆç”¨æ¨¡å‹**
- Temporal Convolutional Networks (TCN)
- WaveNet
- N-BEATS

**3. åŒ–å·¥ç‰¹å®šå„ªåŒ–**
- ç‰©ç†çŸ¥è­˜åµŒå…¥
- å¤šå°ºåº¦æ™‚é–“å»ºæ¨¡
- ä¸ç¢ºå®šæ€§é‡åŒ–

### å¯¦è¸å»ºè­°

**1. æ¨¡å‹é¸æ“‡æ±ºç­–æ¨¹**:
```
éœ€è¦å…¨åºåˆ—è³‡è¨Šï¼Ÿ
â”œâ”€ æ˜¯ â†’ ä½¿ç”¨é›™å‘RNN
â””â”€ å¦ â†’ ä½¿ç”¨å–®å‘RNN

åºåˆ—æ˜¯å¦å¾ˆé•·ï¼ˆ>100æ­¥ï¼‰ï¼Ÿ
â”œâ”€ æ˜¯ â†’ åŠ å…¥Attentionæ©Ÿåˆ¶
â””â”€ å¦ â†’ è€ƒæ…®æ˜¯å¦éœ€è¦Attention

è¼¸å…¥è¼¸å‡ºé•·åº¦ä¸åŒï¼Ÿ
â”œâ”€ æ˜¯ â†’ ä½¿ç”¨Seq2Seqæ¶æ§‹
â””â”€ å¦ â†’ ä½¿ç”¨æ¨™æº–RNNæ¶æ§‹
```

**2. æ€§èƒ½å„ªåŒ–æŠ€å·§**:
- ä½¿ç”¨é›™å‘GRUå¹³è¡¡æ•ˆç‡èˆ‡æ€§èƒ½
- åˆç†è¨­ç½®Attentioné ­æ•¸ï¼ˆ4-8å€‹ï¼‰
- æ‡‰ç”¨Dropouté˜²æ­¢éæ“¬åˆï¼ˆ0.2-0.3ï¼‰
- ä½¿ç”¨Teacher ForcingåŠ é€Ÿè¨“ç·´
- Beam Searchæå‡æ¨ç†å“è³ªï¼ˆbeam_width=3-5ï¼‰

**3. å¯è§£é‡‹æ€§åˆ†æ**:
- å¯è¦–åŒ–Attentionæ¬Šé‡è­˜åˆ¥é—œéµæ™‚åˆ»
- åˆ†æé›™å‘RNNå°ä¸åŒæ–¹å‘çš„ä¾è³´ç¨‹åº¦
- é€šéAttentionæ¬Šé‡é©—è­‰æ¨¡å‹çš„åˆç†æ€§

### åƒè€ƒè³‡æº

**ç¶“å…¸è«–æ–‡**:
1. Schuster & Paliwal (1997) - Bidirectional RNN
2. Bahdanau et al. (2015) - Neural Machine Translation by Jointly Learning to Align and Translate
3. Luong et al. (2015) - Effective Approaches to Attention-based NMT
4. Vaswani et al. (2017) - Attention Is All You Need (Transformer)

**å¯¦ä½œè³‡æº**:
- TensorFlowå®˜æ–¹æ•™ç¨‹: Seq2Seq with Attention
- Kerasç¯„ä¾‹: Neural Machine Translation
- PyTorchæ•™ç¨‹: Seq2Seqæ¨¡å‹å¯¦ä½œ

---

## ç·´ç¿’é¡Œ

### æ¦‚å¿µç†è§£é¡Œ

**1. é¸æ“‡é¡Œ**: ä»¥ä¸‹å“ªç¨®æƒ…æ³**ä¸é©åˆ**ä½¿ç”¨é›™å‘RNNï¼Ÿ
   - A. æ‰¹æ¬¡å“è³ªåˆ†é¡ï¼ˆæ‰¹æ¬¡å·²çµæŸï¼‰
   - B. å¯¦æ™‚æ•…éšœé è­¦ï¼ˆéœ€è¦å³æ™‚æ±ºç­–ï¼‰
   - C. éç¨‹éšæ®µè‡ªå‹•æ¨™è¨»ï¼ˆå®Œæ•´è»Œè·¡å¯ç”¨ï¼‰
   - D. æ‰¹æ¬¡ç›¸ä¼¼æ€§åˆ†æï¼ˆé›¢ç·šåˆ†æï¼‰

**2. ç°¡ç­”é¡Œ**: è§£é‡‹Attentionæ©Ÿåˆ¶å¦‚ä½•è§£æ±ºå‚³çµ±Seq2Seqçš„è³‡è¨Šç“¶é ¸å•é¡Œã€‚

**3. åˆ¤æ–·é¡Œ**: Self-Attentionèƒ½å¤ ç›´æ¥æ•æ‰åºåˆ—ä¸­ä»»æ„å…©å€‹ä½ç½®ä¹‹é–“çš„ä¾è³´é—œä¿‚ï¼Œç„¡éœ€åƒRNNé‚£æ¨£é€æ­¥å‚³éè³‡è¨Šã€‚ï¼ˆå°/éŒ¯ï¼Œä¸¦èªªæ˜ç†ç”±ï¼‰

### ç¨‹å¼å¯¦ä½œé¡Œ

**4. å¯¦ä½œAttentionå±¤**: 
```python
# å®Œæˆä»¥ä¸‹Attentionå±¤çš„å¯¦ç¾
class SimpleAttention(Layer):
    def __init__(self, units):
        super().__init__()
        # TODO: å®šç¾©éœ€è¦çš„æ¬Šé‡å±¤
    
    def call(self, decoder_hidden, encoder_outputs):
        # TODO: å¯¦ç¾Attentionè¨ˆç®—
        # 1. è¨ˆç®—åˆ†æ•¸
        # 2. Softmaxæ­¸ä¸€åŒ–
        # 3. åŠ æ¬Šæ±‚å’Œ
        pass
```

**5. Seq2Seqé æ¸¬**: ä½¿ç”¨æä¾›çš„åŒ–å·¥æ™‚é–“åºåˆ—æ•¸æ“šï¼Œå»ºç«‹ä¸€å€‹Seq2Seq + Attentionæ¨¡å‹ï¼Œé æ¸¬æœªä¾†10å€‹æ™‚é–“æ­¥çš„æº«åº¦ã€‚

### åˆ†ææ‡‰ç”¨é¡Œ

**6. Attentionæ¬Šé‡åˆ†æ**: çµ¦å®šè¨“ç·´å¥½çš„æ¨¡å‹å’Œæ¸¬è©¦æ¨£æœ¬ï¼Œæå–ä¸¦å¯è¦–åŒ–Attentionæ¬Šé‡ï¼Œåˆ†ææ¨¡å‹é—œæ³¨äº†å“ªäº›é—œéµæ™‚åˆ»ã€‚

**7. æ¨¡å‹æ¯”è¼ƒ**: æ¯”è¼ƒä»¥ä¸‹æ¨¡å‹åœ¨åŒ–å·¥æ™‚é–“åºåˆ—é æ¸¬ä»»å‹™ä¸Šçš„æ€§èƒ½ï¼š
   - å–®å‘LSTM
   - é›™å‘LSTM
   - LSTM + Attention
   - Seq2Seq + Attention
   
   æ’°å¯«ç°¡çŸ­å ±å‘Šèªªæ˜å„æ¨¡å‹çš„å„ªåŠ£ã€‚

---

> [!NOTE]
> **æœ¬å–®å…ƒå®Œ**  
> ä½ å·²ç¶“æŒæ¡äº†RNNçš„é€²éšæŠ€è¡“ï¼ä¸‹ä¸€æ­¥å¯ä»¥å­¸ç¿’Transformeræ¶æ§‹ï¼Œæ¢ç´¢å®Œå…¨åŸºæ–¼Attentionçš„åºåˆ—å»ºæ¨¡æ–¹æ³•ã€‚

---

**èª²ç¨‹è³‡è¨Š**
- èª²ç¨‹åç¨±ï¼šAIåœ¨åŒ–å·¥ä¸Šä¹‹æ‡‰ç”¨
- èª²ç¨‹å–®å…ƒï¼šUnit17 - RNNé€²éšæŠ€è¡“ï¼šé›™å‘RNNèˆ‡Attentionæ©Ÿåˆ¶
- èª²ç¨‹è£½ä½œï¼šé€¢ç”²å¤§å­¸ åŒ–å·¥ç³» æ™ºæ…§ç¨‹åºç³»çµ±å·¥ç¨‹å¯¦é©—å®¤
- æˆèª²æ•™å¸«ï¼šèŠæ›œç¦ åŠ©ç†æ•™æˆ
- æ›´æ–°æ—¥æœŸï¼š2026-01-28

**èª²ç¨‹æˆæ¬Š [CC BY-NC-SA 4.0]**
 - æœ¬æ•™æéµå¾ª [å‰µç”¨CC å§“åæ¨™ç¤º-éå•†æ¥­æ€§-ç›¸åŒæ–¹å¼åˆ†äº« 4.0 åœ‹éš› (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh) æˆæ¬Šã€‚

---