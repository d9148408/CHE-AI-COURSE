# Unit17_Example_debutanizer_column | ä½¿ç”¨ LSTM å’Œ GRU é æ¸¬å»ä¸çƒ·å¡” C4 å«é‡

> **èª²ç¨‹å–®å…ƒ**ï¼šPart 4 - æ·±åº¦å­¸ç¿’æ‡‰ç”¨  
> **ä¸»é¡Œ**ï¼šæ™‚åºé æ¸¬ - å»ä¸çƒ·å¡”è»Ÿæ¸¬é‡  
> **æŠ€è¡“**ï¼šLSTMã€GRUã€æ™‚åºç‰¹å¾µå·¥ç¨‹  
> **é›£åº¦**ï¼šâ­â­â­â­  
> **é è¨ˆæ™‚é–“**ï¼š120 åˆ†é˜

---

## ğŸ“š ç›®éŒ„

1. [å­¸ç¿’ç›®æ¨™](#å­¸ç¿’ç›®æ¨™)
2. [èƒŒæ™¯èªªæ˜](#èƒŒæ™¯èªªæ˜)
3. [æ•¸æ“šé›†ä»‹ç´¹](#æ•¸æ“šé›†ä»‹ç´¹)
4. [ç’°å¢ƒè¨­å®šèˆ‡æ•¸æ“šä¸‹è¼‰](#ç’°å¢ƒè¨­å®šèˆ‡æ•¸æ“šä¸‹è¼‰)
5. [æ•¸æ“šæ¢ç´¢èˆ‡åˆ†æ](#æ•¸æ“šæ¢ç´¢èˆ‡åˆ†æ)
6. [æ•¸æ“šé è™•ç†](#æ•¸æ“šé è™•ç†)
7. [LSTM æ¨¡å‹å»ºç«‹èˆ‡è¨“ç·´](#lstm-æ¨¡å‹å»ºç«‹èˆ‡è¨“ç·´)
8. [GRU æ¨¡å‹å»ºç«‹èˆ‡è¨“ç·´](#gru-æ¨¡å‹å»ºç«‹èˆ‡è¨“ç·´)
9. [æ¨¡å‹æ¯”è¼ƒèˆ‡è©•ä¼°](#æ¨¡å‹æ¯”è¼ƒèˆ‡è©•ä¼°)
10. [éæ“¬åˆè¨ºæ–·èˆ‡æ”¹é€²](#éæ“¬åˆè¨ºæ–·èˆ‡æ”¹é€²)
11. [å‚™é¸æ–¹æ¡ˆèˆ‡å»ºè­°](#å‚™é¸æ–¹æ¡ˆèˆ‡å»ºè­°)
12. [çµè«–èˆ‡è¨è«–](#çµè«–èˆ‡è¨è«–)
13. [åƒè€ƒè³‡æº](#åƒè€ƒè³‡æº)

---

## ğŸ¯ å­¸ç¿’ç›®æ¨™

å®Œæˆæœ¬å–®å…ƒå¾Œï¼Œæ‚¨å°‡èƒ½å¤ ï¼š

1. âœ… **ç†è§£åŒ–å·¥è£½ç¨‹è»Ÿæ¸¬é‡çš„æ¦‚å¿µèˆ‡æ‡‰ç”¨**
   - æŒæ¡è»Ÿæ¸¬é‡ï¼ˆSoft Sensorï¼‰åœ¨åŒ–å·¥è£½ç¨‹ä¸­çš„é‡è¦æ€§
   - äº†è§£å»ä¸çƒ·å¡”è£½ç¨‹èˆ‡ C4 å«é‡é æ¸¬çš„å¯¦éš›æ„ç¾©

2. âœ… **æŒæ¡æ™‚åºæ•¸æ“šçš„é è™•ç†æŠ€è¡“**
   - æ™‚åºæ•¸æ“šçš„æ¨™æº–åŒ–èˆ‡åºåˆ—åŒ–
   - æ»‘å‹•çª—å£ï¼ˆSliding Windowï¼‰æ–¹æ³•
   - å·®åˆ†ç‰¹å¾µå·¥ç¨‹

3. âœ… **å»ºç«‹ä¸¦è¨“ç·´ LSTM å’Œ GRU æ¨¡å‹**
   - ç†è§£ LSTM å’Œ GRU çš„æ¶æ§‹èˆ‡åŸç†
   - è¨­è¨ˆé©åˆåŒ–å·¥è£½ç¨‹çš„æ¨¡å‹çµæ§‹
   - ä½¿ç”¨ Keras/TensorFlow å¯¦ç¾æ™‚åºé æ¸¬æ¨¡å‹

4. âœ… **è¨ºæ–·èˆ‡è§£æ±ºéæ“¬åˆå•é¡Œ**
   - è­˜åˆ¥éæ“¬åˆçš„ç—‡ç‹€
   - æ‡‰ç”¨æ­£å‰‡åŒ–æŠ€è¡“ï¼ˆDropoutã€L2ï¼‰
   - èª¿æ•´æ¨¡å‹è¤‡é›œåº¦èˆ‡è¶…åƒæ•¸

5. âœ… **è©•ä¼°æ¨¡å‹æ€§èƒ½èˆ‡æ³›åŒ–èƒ½åŠ›**
   - ä½¿ç”¨å¤šç¨®è©•ä¼°æŒ‡æ¨™ï¼ˆRÂ², RMSE, MAEï¼‰
   - åˆ†ææ®˜å·®åˆ†å¸ƒ
   - æ¯”è¼ƒä¸åŒæ¨¡å‹çš„å„ªåŠ£

---

## ğŸ“– èƒŒæ™¯èªªæ˜

### ä»€éº¼æ˜¯å»ä¸çƒ·å¡”ï¼Ÿ

**å»ä¸çƒ·å¡”ï¼ˆDebutanizer Columnï¼‰** æ˜¯ç…‰æ²¹å’ŒçŸ³åŒ–å·¥æ¥­ä¸­çš„é—œéµè¨­å‚™ï¼Œå±¬æ–¼è„«ç¡«å’ŒçŸ³è…¦æ²¹åˆ†é›¢è£ç½®çš„ä¸€éƒ¨åˆ†ã€‚å…¶ä¸»è¦åŠŸèƒ½æ˜¯å¾çŸ³è…¦æ²¹æµä¸­åˆ†é›¢å‡ºè¼•è³ªçƒ´é¡æˆåˆ†ã€‚

**è£½ç¨‹åŠŸèƒ½ï¼š**
- ğŸ”¹ **å¡”é ‚ç”¢ç‰©**ï¼šç§»é™¤ C3ï¼ˆä¸™çƒ·ï¼‰å’Œ C4ï¼ˆä¸çƒ·ï¼‰ä½œç‚º LP æ°£é«”
- ğŸ”¹ **å¡”åº•ç”¢ç‰©**ï¼šç©©å®šæ±½æ²¹ï¼ˆStabilized Gasolineï¼‰é€å¾€ä¸‹æ¸¸è£½ç¨‹
- ğŸ”¹ **æ§åˆ¶ç›®æ¨™**ï¼š
  - ç¢ºä¿å……åˆ†çš„åˆ†é¤¾æ•ˆæœ
  - æœ€å¤§åŒ–å¡”é ‚ç”¢ç‰©ä¸­çš„ C5 å«é‡ï¼ˆç¬¦åˆæ³•è¦é™åˆ¶ï¼‰
  - æœ€å°åŒ–å¡”åº•ç”¢ç‰©ä¸­çš„ C4 å«é‡ï¼ˆæé«˜ç”¢å“è³ªé‡ï¼‰

### ç‚ºä»€éº¼éœ€è¦è»Ÿæ¸¬é‡ï¼Ÿ

**è»Ÿæ¸¬é‡ï¼ˆSoft Sensorï¼‰** æ˜¯ä¸€ç¨®ä½¿ç”¨æ•¸å­¸æ¨¡å‹å’Œæ˜“æ¸¬è®Šæ•¸ä¾†æ¨ç®—é›£æ¸¬æˆ–ç„¡æ³•ç·šä¸Šæ¸¬é‡è®Šæ•¸çš„æŠ€è¡“ã€‚

**å‚³çµ±æ¸¬é‡çš„é™åˆ¶ï¼š**
- ğŸš« **æ°£ç›¸è‰²è­œå„€ï¼ˆGCï¼‰**ï¼šæ¸¬é‡å»¶é²é•·ï¼ˆ5-30 åˆ†é˜ï¼‰ã€ç¶­è­·æˆæœ¬é«˜
- ğŸš« **å–æ¨£åˆ†æ**ï¼šç„¡æ³•æä¾›å³æ™‚æ•¸æ“šã€äººå·¥æˆæœ¬é«˜
- ğŸš« **å®‰è£å›°é›£**ï¼šæŸäº›ä½ç½®é›£ä»¥å®‰è£æ„Ÿæ¸¬å™¨

**è»Ÿæ¸¬é‡çš„å„ªå‹¢ï¼š**
- âœ… **å³æ™‚é æ¸¬**ï¼šä½¿ç”¨ç¾æœ‰æ„Ÿæ¸¬å™¨æ•¸æ“šå³æ™‚æ¨ç®—ç›®æ¨™è®Šæ•¸
- âœ… **ä½æˆæœ¬**ï¼šç„¡éœ€é¡å¤–ç¡¬é«”æŠ•è³‡
- âœ… **é«˜é »ç‡**ï¼šå¯æä¾›é€£çºŒçš„é æ¸¬å€¼
- âœ… **å½ˆæ€§é«˜**ï¼šå¯æ ¹æ“šéœ€æ±‚èª¿æ•´å’Œæ›´æ–°æ¨¡å‹

### LSTM å’Œ GRU åœ¨åŒ–å·¥è£½ç¨‹ä¸­çš„æ‡‰ç”¨

**ç‚ºä»€éº¼é¸æ“‡å¾ªç’°ç¥ç¶“ç¶²è·¯ï¼ˆRNNï¼‰ï¼Ÿ**

åŒ–å·¥è£½ç¨‹å…·æœ‰ä»¥ä¸‹ç‰¹æ€§ï¼Œä½¿å¾— RNN ç³»åˆ—æ¨¡å‹ç‰¹åˆ¥é©åˆï¼š

1. **æ™‚åºä¾è³´æ€§**ï¼šç•¶å‰ç‹€æ…‹å—éå»ç‹€æ…‹å½±éŸ¿
2. **å‹•æ…‹ç‰¹æ€§**ï¼šç³»çµ±å­˜åœ¨æ…£æ€§å’Œå»¶é²
3. **è¤‡é›œéç·šæ€§**ï¼šè®Šæ•¸é–“é—œä¿‚è¤‡é›œä¸”éç·šæ€§

**LSTMï¼ˆLong Short-Term Memoryï¼‰**
- ğŸ”¹ æ“…é•·æ•æ‰é•·æœŸä¾è³´é—œä¿‚
- ğŸ”¹ é€šéé–€æ§æ©Ÿåˆ¶é¿å…æ¢¯åº¦æ¶ˆå¤±
- ğŸ”¹ é©åˆéœ€è¦è¨˜æ†¶é•·æœŸä¿¡æ¯çš„å ´æ™¯

**GRUï¼ˆGated Recurrent Unitï¼‰**
- ğŸ”¹ LSTM çš„ç°¡åŒ–ç‰ˆæœ¬ï¼Œåƒæ•¸æ›´å°‘
- ğŸ”¹ è¨“ç·´é€Ÿåº¦æ›´å¿«
- ğŸ”¹ åœ¨å°æ•¸æ“šé›†ä¸Šæœ‰æ™‚è¡¨ç¾æ›´å¥½

---

## ğŸ“Š æ•¸æ“šé›†ä»‹ç´¹

### æ•¸æ“šä¾†æº

æœ¬æ¡ˆä¾‹ä½¿ç”¨çš„æ•¸æ“šä¾†è‡ªçœŸå¯¦çš„å·¥æ¥­å»ä¸çƒ·å¡”æ“ä½œè¨˜éŒ„ï¼Œè¨˜éŒ„äº†è£½ç¨‹é‹è¡Œéç¨‹ä¸­çš„å¤šå€‹é—œéµè®Šæ•¸ã€‚

**æ•¸æ“šé›†è³‡è¨Šï¼š**
- ğŸ“ **æª”æ¡ˆåç¨±**ï¼š`debutanizer_data.txt`
- ğŸ“ **æ•¸æ“šé»æ•¸**ï¼š2,394 ç­†
- ğŸ“ **æ¡æ¨£é »ç‡**ï¼šæ¯åˆ†é˜
- ğŸ“ **è®Šæ•¸æ•¸é‡**ï¼š8 å€‹ï¼ˆ7 å€‹è¼¸å…¥ + 1 å€‹è¼¸å‡ºï¼‰
- ğŸ“ **æ•¸æ“šæœŸé–“**ï¼šé€£çºŒé‹è¡Œè¨˜éŒ„

**åƒè€ƒæ–‡ç»ï¼š**
> Fortuna, L., Graziani, S., Rizzo, A., & Xibilia, M. G. (2007). *Soft Sensors for Monitoring and Control of Industrial Processes*. Springer.

### è®Šæ•¸èªªæ˜

**è¼¸å…¥è®Šæ•¸ï¼ˆu1-u7ï¼‰ï¼š**

| è®Šæ•¸ | æè¿° | å–®ä½ | ç‰©ç†æ„ç¾© |
|------|------|------|----------|
| **u1** | Top Temperature<br>å¡”é ‚æº«åº¦ | Â°C | åæ˜ å¡”é ‚æ°£ç›¸çµ„æˆï¼Œå½±éŸ¿è¼•çµ„åˆ†å›æ”¶ |
| **u2** | Top Pressure<br>å¡”é ‚å£“åŠ› | kPa | å½±éŸ¿æ°£æ¶²å¹³è¡¡ï¼Œæ§åˆ¶åˆ†é¤¾æ•ˆæœ |
| **u3** | Reflux Flow<br>å›æµæµé‡ | kg/h | æ§åˆ¶ç²¾é¤¾æ•ˆæœçš„é—œéµè®Šæ•¸ |
| **u4** | Flow to Next<br>æµå‘ä¸‹ä¸€è£½ç¨‹ | kg/h | å¡”é ‚ç”¢ç‰©æµé‡ï¼Œå½±éŸ¿ç‰©æ–™å¹³è¡¡ |
| **u5** | 6th Tray Temperature<br>ç¬¬ 6 å±¤æ¿æº«åº¦ | Â°C | å¡”å…§æº«åº¦åˆ†å¸ƒæŒ‡æ¨™ |
| **u6** | Bottom Temperature 1<br>å¡”åº•æº«åº¦ 1 | Â°C | å¡”åº•é‡çµ„åˆ†æº«åº¦ |
| **u7** | Bottom Temperature 2<br>å¡”åº•æº«åº¦ 2 | Â°C | å¡”åº•æº«åº¦å†—é¤˜æ¸¬é‡ |

**è¼¸å‡ºè®Šæ•¸ï¼ˆyï¼‰ï¼š**

| è®Šæ•¸ | æè¿° | å–®ä½ | é‡è¦æ€§ |
|------|------|------|--------|
| **y** | C4 Content in Bottom Flow<br>å¡”åº•æµä¸­çš„ C4 å«é‡ | mol% | **é—œéµå“è³ªæŒ‡æ¨™**<br>æ±ºå®šç”¢å“æ˜¯å¦ç¬¦åˆè¦æ ¼ |

### æ•¸æ“šç‰¹æ€§åˆ†æ

æ ¹æ“šåŸ·è¡Œçµæœï¼Œæ•¸æ“šé›†å…·æœ‰ä»¥ä¸‹ç‰¹æ€§ï¼š

```
Dataset shape: (2394, 8)
Number of samples: 2394
Number of features: 7
```

**æ•¸æ“šè¦æ¨¡è©•ä¼°ï¼š**
- âœ… æ¨£æœ¬æ•¸é©ä¸­ï¼ˆç´„ 2400 ç­†ï¼‰
- âš ï¸ å°æ–¼æ·±åº¦å­¸ç¿’è€Œè¨€ï¼Œæ•¸æ“šé‡åå°ï¼ˆç†æƒ³ > 5000ï¼‰
- âœ… ç‰¹å¾µæ•¸é‡åˆç†ï¼ˆ7 å€‹è¼¸å…¥è®Šæ•¸ï¼‰
- âœ… ç„¡ç¼ºå¤±å€¼ï¼Œæ•¸æ“šè³ªé‡è‰¯å¥½

---

## ğŸ”§ ç’°å¢ƒè¨­å®šèˆ‡æ•¸æ“šä¸‹è¼‰

### ç’°å¢ƒè¨­å®š

æœ¬å–®å…ƒä½¿ç”¨ä»¥ä¸‹ Python å¥—ä»¶ï¼š

**æ ¸å¿ƒå¥—ä»¶ï¼š**
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
```

**æ©Ÿå™¨å­¸ç¿’å¥—ä»¶ï¼š**
```python
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
```

**æ·±åº¦å­¸ç¿’å¥—ä»¶ï¼š**
```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
```

### æ•¸æ“šä¸‹è¼‰

æ•¸æ“šæª”æ¡ˆæœƒè‡ªå‹•æª¢æŸ¥æ˜¯å¦å­˜åœ¨ï¼Œè‹¥ä¸å­˜åœ¨å‰‡å¾ GitHub ä¸‹è¼‰ï¼š

```python
import requests
import os

url = "https://raw.githubusercontent.com/sj823774188/Debutanizer-Column-Data/main/debutanizer_data.txt"
data_file = os.path.join(DATA_DIR, "debutanizer_data.txt")

# æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
if not os.path.exists(data_file):
    print(f"âœ— æ•¸æ“šæª”æ¡ˆä¸å­˜åœ¨ï¼Œæ­£åœ¨ä¸‹è¼‰...")
    response = requests.get(url)
    with open(data_file, 'wb') as f:
        f.write(response.content)
    print(f"âœ“ ä¸‹è¼‰æˆåŠŸ")
else:
    print(f"âœ“ æ•¸æ“šæª”æ¡ˆå·²å­˜åœ¨")
```

**åŸ·è¡Œçµæœï¼š**
```
âœ“ æ•¸æ“šæª”æ¡ˆå·²å­˜åœ¨æ–¼: d:\MyGit\CHE-AI-COURSE\Part_4\Unit17\data\debutanizer_column\debutanizer_data.txt
```

---

## ğŸ“ˆ æ•¸æ“šæ¢ç´¢èˆ‡åˆ†æ

### è¼‰å…¥æ•¸æ“š

æ•¸æ“šæª”æ¡ˆæ ¼å¼ç‚ºç´”æ–‡å­—æª”ï¼Œä»¥ç©ºæ ¼åˆ†éš”ï¼Œå‰ 4 è¡Œç‚ºæª”é ­èªªæ˜ã€‚

```python
# è¼‰å…¥æ•¸æ“šï¼ˆè·³éå‰ 4 è¡Œæª”é ­ï¼‰
data = np.loadtxt(data_path, skiprows=4)

# å»ºç«‹ DataFrame ä¸¦è³¦äºˆæœ‰æ„ç¾©çš„æ¬„ä½åç¨±
columns = ['u1_TopTemp', 'u2_TopPressure', 'u3_RefluxFlow', 
           'u4_FlowToNext', 'u5_TrayTemp', 'u6_BottomTemp1', 
           'u7_BottomTemp2', 'y_C4Content']
df = pd.DataFrame(data, columns=columns)
```

**åŸ·è¡Œçµæœï¼š**
```
Dataset shape: (2394, 8)
Number of samples: 2394
Number of features: 7

First few rows:
   u1_TopTemp  u2_TopPressure  u3_RefluxFlow  u4_FlowToNext  u5_TrayTemp  u6_BottomTemp1  u7_BottomTemp2  y_C4Content
0      463.48          202.82        3829.29        1645.65       158.91          389.54          399.36         0.66
1      463.14          202.67        3831.33        1618.90       159.21          389.71          399.48         0.66
2      463.28          202.78        3828.48        1607.06       159.66          390.07          399.65         0.66
3      463.44          202.82        3826.51        1626.75       159.99          390.11          399.74         0.67
4      463.48          202.82        3829.29        1645.65       160.20          390.16          399.82         0.67
```

### åŸºæœ¬çµ±è¨ˆä¿¡æ¯

**çµ±è¨ˆæ‘˜è¦è§€å¯Ÿï¼š**

- **ç›®æ¨™è®Šæ•¸ï¼ˆy_C4Contentï¼‰**ï¼š
  - å¹³å‡å€¼ï¼š0.69 mol%ï¼Œæ¨™æº–å·®ï¼š0.04 mol%
  - ç¯„åœï¼š0.59 ~ 0.79 mol%
  - è®Šç•°è¼ƒå°ï¼Œè£½ç¨‹æ§åˆ¶ç©©å®š

- **è¼¸å…¥è®Šæ•¸ç‰¹æ€§ï¼š**
  - å¡”é ‚æº«åº¦ï¼ˆu1ï¼‰ï¼šè®Šç•°ä¿‚æ•¸å°ï¼ˆCV = 0.24%ï¼‰
  - å›æµæµé‡ï¼ˆu3ï¼‰ï¼šç›¸å°ç©©å®šï¼ˆCV = 0.32%ï¼‰
  - æµå‘ä¸‹è£½ç¨‹ï¼ˆu4ï¼‰ï¼šè®Šç•°æœ€å¤§ï¼ˆCV = 4.37%ï¼‰

- **æ•¸æ“šè³ªé‡**ï¼š
  - âœ… ç„¡ç¼ºå¤±å€¼
  - âœ… ç„¡ç•°å¸¸æ¥µç«¯å€¼
  - âœ… æ•¸å€¼ç¯„åœåˆç†

### ç›¸é—œæ€§åˆ†æ

**èˆ‡ç›®æ¨™è®Šæ•¸ï¼ˆy_C4Contentï¼‰çš„ç›¸é—œæ€§ï¼š**
- `u6_BottomTemp1`ï¼šr = 0.78ï¼ˆå¼·æ­£ç›¸é—œï¼‰â­
- `u7_BottomTemp2`ï¼šr = 0.76ï¼ˆå¼·æ­£ç›¸é—œï¼‰â­
- `u5_TrayTemp`ï¼šr = 0.65ï¼ˆä¸­ç­‰æ­£ç›¸é—œï¼‰
- `u1_TopTemp`ï¼šr = 0.42ï¼ˆå¼±æ­£ç›¸é—œï¼‰
- `u3_RefluxFlow`ï¼šr = -0.35ï¼ˆå¼±è² ç›¸é—œï¼‰

**è®Šæ•¸é–“ç›¸é—œæ€§ï¼š**
- `u6` å’Œ `u7`ï¼ˆå…©å€‹å¡”åº•æº«åº¦ï¼‰ï¼šr = 0.99ï¼ˆæ¥µé«˜ç›¸é—œï¼‰
- `u1` å’Œ `u5`ï¼ˆå¡”é ‚èˆ‡å±¤æ¿æº«åº¦ï¼‰ï¼šr = 0.68

ğŸ“Œ **å»ºæ¨¡å•Ÿç¤ºï¼š**
- å¡”åº•æº«åº¦æ˜¯æœ€é‡è¦çš„é æ¸¬å› å­
- å¯èƒ½å­˜åœ¨å¤šé‡å…±ç·šæ€§ï¼ˆu6 å’Œ u7ï¼‰
- æº«åº¦è®Šæ•¸å° C4 å«é‡å½±éŸ¿æœ€å¤§

---

## ğŸ”„ æ•¸æ“šé è™•ç†

### 3.1 æ•¸æ“šåˆ†å‰²

å°‡æ•¸æ“šåˆ†å‰²ç‚ºè¨“ç·´é›†ã€é©—è­‰é›†å’Œæ¸¬è©¦é›†ï¼Œä½¿ç”¨æ™‚åºåˆ†å‰²ä»¥ä¿æŒæ™‚é–“é †åºã€‚

```python
# å®šç¾©åˆ†å‰²æ¯”ä¾‹
train_ratio = 0.7  # 70% è¨“ç·´é›†
val_ratio = 0.15   # 15% é©—è­‰é›†
test_ratio = 0.15  # 15% æ¸¬è©¦é›†

# åˆ†é›¢ç‰¹å¾µå’Œç›®æ¨™
feature_cols = ['u1_TopTemp', 'u2_TopPressure', 'u3_RefluxFlow', 
                'u4_FlowToNext', 'u5_TrayTemp', 'u6_BottomTemp1', 
                'u7_BottomTemp2']
target_col = 'y_C4Content'

X = df[feature_cols].values
y = df[target_col].values

# æ™‚åºåˆ†å‰²ï¼ˆä¸æ‰“äº‚é †åºï¼‰
n_samples = len(X)
train_size = int(n_samples * train_ratio)
val_size = int(n_samples * val_ratio)

X_train = X[:train_size]
X_val = X[train_size:train_size+val_size]
X_test = X[train_size+val_size:]

y_train = y[:train_size]
y_val = y[train_size:train_size+val_size]
y_test = y[train_size+val_size:]
```

**åŸ·è¡Œçµæœï¼š**
```
âœ“ Data split completed
Train set: 1675 samples (70.0%)
Validation set: 359 samples (15.0%)
Test set: 360 samples (15.0%)
```

âš ï¸ **é‡è¦èªªæ˜**ï¼š
- ä½¿ç”¨**æ™‚åºåˆ†å‰²**è€Œééš¨æ©Ÿåˆ†å‰²ï¼Œä»¥æ¨¡æ“¬å¯¦éš›é æ¸¬æƒ…å¢ƒ
- æ¸¬è©¦é›†ä½¿ç”¨æœ€æ–°çš„æ•¸æ“šï¼Œè©•ä¼°æ¨¡å‹å°æœªä¾†æ•¸æ“šçš„æ³›åŒ–èƒ½åŠ›

### 3.2 ç‰¹å¾µå·¥ç¨‹ - æ·»åŠ å·®åˆ†ç‰¹å¾µ

åŒ–å·¥è£½ç¨‹æ•¸æ“šé€šå¸¸åŒ…å«è¶¨å‹¢å’Œé€±æœŸæ€§ï¼Œæ·»åŠ å·®åˆ†ç‰¹å¾µå¯ä»¥å¹«åŠ©æ¨¡å‹æ•æ‰è®ŠåŒ–ç‡ã€‚

```python
# è¨ˆç®—å·®åˆ†ç‰¹å¾µï¼ˆç•¶å‰å€¼ - å‰ä¸€æ™‚åˆ»å€¼ï¼‰
X_diff = np.diff(X, axis=0, prepend=X[0:1])

# çµ„åˆåŸå§‹ç‰¹å¾µå’Œå·®åˆ†ç‰¹å¾µ
X_combined = np.concatenate([X, X_diff], axis=1)

print(f"Original features: {X.shape[1]}")
print(f"Combined features (original + diff): {X_combined.shape[1]}")
```

**åŸ·è¡Œçµæœï¼š**
```
Original features: 7
Combined features (original + diff): 14
```

ğŸ“Œ **ç‚ºä»€éº¼æ·»åŠ å·®åˆ†ç‰¹å¾µï¼Ÿ**
- âœ… æ•æ‰è®ŠåŒ–è¶¨å‹¢ï¼šå·®åˆ†åæ˜ è®Šæ•¸çš„è®ŠåŒ–é€Ÿç‡
- âœ… å¢å¼·æ™‚åºä¿¡æ¯ï¼šå¹«åŠ©æ¨¡å‹ç†è§£å‹•æ…‹ç‰¹æ€§
- âœ… æ”¹å–„é æ¸¬ï¼šå°æ–¼å…·æœ‰æ…£æ€§çš„è£½ç¨‹ç‰¹åˆ¥æœ‰æ•ˆ

### 3.3 æ•¸æ“šæ¨™æº–åŒ–

RNN æ¨¡å‹å°è¼¸å…¥æ•¸æ“šçš„å°ºåº¦æ•æ„Ÿï¼Œå› æ­¤éœ€è¦é€²è¡Œæ¨™æº–åŒ–è™•ç†ã€‚

```python
from sklearn.preprocessing import StandardScaler

# åˆ†åˆ¥æ¨™æº–åŒ–ç‰¹å¾µå’Œç›®æ¨™
scaler_X = StandardScaler()
scaler_y = StandardScaler()

X_scaled = scaler_X.fit_transform(X_combined)
y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()

print("âœ“ Data standardization completed")
print(f"Features - Mean: ~0, Std: ~1")
print(f"Target - Mean: {y_scaled.mean():.6f}, Std: {y_scaled.std():.6f}")
```

âš ï¸ **æ¨™æº–åŒ–æ³¨æ„äº‹é …ï¼š**
- åªåœ¨è¨“ç·´é›†ä¸Š `fit`ï¼Œç„¶å¾Œ `transform` é©—è­‰é›†å’Œæ¸¬è©¦é›†
- ä¿å­˜ scaler ä»¥ä¾¿å¾ŒçºŒåæ¨™æº–åŒ–é æ¸¬çµæœ
- ç‰¹å¾µå’Œç›®æ¨™åˆ†åˆ¥æ¨™æº–åŒ–

### 3.4 å‰µå»ºæ™‚åºåºåˆ—æ•¸æ“š

LSTM å’Œ GRU éœ€è¦ 3D è¼¸å…¥ï¼š`(samples, timesteps, features)`

```python
TIME_STEPS = 20  # å›çœ‹çª—å£é•·åº¦

def create_sequences(X, y, time_steps):
    """
    å°‡æ•¸æ“šè½‰æ›ç‚ºæ™‚åºåºåˆ—æ ¼å¼
    
    Parameters:
    -----------
    X : array, shape (n_samples, n_features)
    y : array, shape (n_samples,)
    time_steps : int, å›çœ‹çª—å£é•·åº¦
    
    Returns:
    --------
    X_seq : array, shape (n_seq, time_steps, n_features)
    y_seq : array, shape (n_seq,)
    """
    X_seq, y_seq = [], []
    
    for i in range(time_steps, len(X)):
        X_seq.append(X[i-time_steps:i])  # å–å‰ time_steps å€‹æ™‚é–“é»
        y_seq.append(y[i])                # ç›®æ¨™ç‚ºç•¶å‰æ™‚åˆ»
    
    return np.array(X_seq), np.array(y_seq)

# ç‚ºè¨“ç·´ã€é©—è­‰ã€æ¸¬è©¦é›†å‰µå»ºåºåˆ—
X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, TIME_STEPS)
X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, TIME_STEPS)
X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, TIME_STEPS)
```

**åŸ·è¡Œçµæœï¼š**
```
âœ“ Sequence data created
Train sequences: (1655, 20, 14) â†’ 1655 samples, 20 timesteps, 14 features
Val sequences: (339, 20, 14)
Test sequences: (340, 20, 14)
```

**3D å¼µé‡çµæ§‹èªªæ˜ï¼š**
```
Shape: (1655, 20, 14)
       â†“     â†“   â†“
    æ¨£æœ¬æ•¸  æ™‚é–“æ­¥  ç‰¹å¾µæ•¸
```

- **æ¨£æœ¬æ•¸ï¼ˆ1655ï¼‰**ï¼šå¯ç”¨æ–¼è¨“ç·´çš„åºåˆ—ç¸½æ•¸
- **æ™‚é–“æ­¥ï¼ˆ20ï¼‰**ï¼šæ¯å€‹åºåˆ—åŒ…å«éå» 20 å€‹æ™‚é–“é»
- **ç‰¹å¾µæ•¸ï¼ˆ14ï¼‰**ï¼š7 å€‹åŸå§‹ç‰¹å¾µ + 7 å€‹å·®åˆ†ç‰¹å¾µ

ğŸ“Œ **TIME_STEPS åƒæ•¸é¸æ“‡ï¼š**
- å¤ªå°ï¼ˆ< 10ï¼‰ï¼šç„¡æ³•æ•æ‰è¶³å¤ çš„æ™‚åºä¿¡æ¯
- å¤ªå¤§ï¼ˆ> 50ï¼‰ï¼šè¨“ç·´æ¨£æœ¬æ¸›å°‘ï¼Œè¨ˆç®—æˆæœ¬å¢åŠ 
- **å»ºè­°**ï¼šåŒ–å·¥è£½ç¨‹é€šå¸¸é¸æ“‡ 20-30

### æ•¸æ“šæº–å‚™ç¸½çµ

ç¶“éé è™•ç†å¾Œçš„æ•¸æ“šç‰¹æ€§ï¼š

| éšæ®µ | è¨“ç·´é›† | é©—è­‰é›† | æ¸¬è©¦é›† | ç¸½è¨ˆ |
|------|--------|--------|--------|------|
| **åŸå§‹æ•¸æ“š** | 1675 | 359 | 360 | 2394 |
| **åºåˆ—æ•¸æ“š** | 1655 | 339 | 340 | 2334 |
| **æå¤±æ¨£æœ¬** | 20 | 20 | 20 | 60 |

**æå¤±æ¨£æœ¬èªªæ˜**ï¼š
- æ¯å€‹æ•¸æ“šé›†é–‹é ­çš„ `TIME_STEPS` å€‹æ¨£æœ¬ç„¡æ³•å½¢æˆå®Œæ•´åºåˆ—
- é€™æ˜¯æ»‘å‹•çª—å£æ–¹æ³•çš„æ­£å¸¸ç¾è±¡
- æå¤±æ¯”ä¾‹ï¼š60/2394 = 2.5%ï¼ˆå¯æ¥å—ï¼‰

---

## ğŸ§  LSTM æ¨¡å‹å»ºç«‹èˆ‡è¨“ç·´

### 4.1 LSTM åŸç†ç°¡ä»‹

**LSTMï¼ˆLong Short-Term Memoryï¼‰** æ˜¯ä¸€ç¨®ç‰¹æ®Šçš„ RNNï¼Œå°ˆé–€è¨­è¨ˆç”¨æ–¼è§£æ±ºé•·æœŸä¾è³´å•é¡Œã€‚

**LSTM çš„æ ¸å¿ƒçµ„ä»¶ï¼š**

1. **éºå¿˜é–€ï¼ˆForget Gateï¼‰**ï¼šæ±ºå®šä¸Ÿæ£„å“ªäº›èˆŠä¿¡æ¯
   
   $$
   f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
   $$

2. **è¼¸å…¥é–€ï¼ˆInput Gateï¼‰**ï¼šæ±ºå®šæ¥æ”¶å“ªäº›æ–°ä¿¡æ¯
   
   $$
   i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
   $$

3. **è¼¸å‡ºé–€ï¼ˆOutput Gateï¼‰**ï¼šæ±ºå®šè¼¸å‡ºä»€éº¼
   
   $$
   o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
   $$

4. **ç´°èƒç‹€æ…‹ï¼ˆCell Stateï¼‰**ï¼šæ”œå¸¶é•·æœŸè¨˜æ†¶

**ç‚ºä»€éº¼ LSTM é©åˆåŒ–å·¥è£½ç¨‹ï¼Ÿ**
- âœ… å¯ä»¥è¨˜æ†¶é•·æœŸçš„è£½ç¨‹ç‹€æ…‹
- âœ… èƒ½å¤ è™•ç†æ™‚é–“å»¶é²æ•ˆæ‡‰
- âœ… å°æ–¼è¤‡é›œéç·šæ€§é—œä¿‚å»ºæ¨¡èƒ½åŠ›å¼·

### 4.2 LSTM æ¨¡å‹æ¶æ§‹è¨­è¨ˆ

æœ¬æ¡ˆä¾‹æ¡ç”¨**é›™å±¤ LSTM + æ­£å‰‡åŒ–**æ¶æ§‹ï¼š

```python
def build_lstm_model(input_shape, units=[32, 16], dropout_rate=0.35):
    """
    å»ºç«‹é›™å±¤ LSTM æ¨¡å‹ï¼ˆv3 å„ªåŒ–ç‰ˆï¼‰
    
    æ¶æ§‹ï¼š
    - ç¬¬ä¸€å±¤ LSTMï¼ˆ32 unitsï¼‰+ BatchNorm + Dropout
    - ç¬¬äºŒå±¤ LSTMï¼ˆ16 unitsï¼‰+ BatchNorm + Dropout
    - Dense ç·©è¡å±¤ï¼ˆ8 unitsï¼‰+ Dropout
    - è¼¸å‡ºå±¤ï¼ˆ1 unitï¼‰
    
    æ­£å‰‡åŒ–æŠ€è¡“ï¼š
    - Dropoutï¼šé˜²æ­¢éæ“¬åˆ
    - L2 Regularizationï¼šé™åˆ¶æ¬Šé‡å¤§å°
    - Batch Normalizationï¼šç©©å®šè¨“ç·´
    """
    from tensorflow.keras.layers import BatchNormalization
    
    model = Sequential(name='LSTM_Model')
    
    # ç¬¬ä¸€å±¤ LSTM
    model.add(LSTM(
        units=units[0],
        return_sequences=True,  # è¼¸å‡ºå®Œæ•´åºåˆ—çµ¦ä¸‹ä¸€å±¤
        input_shape=input_shape,
        kernel_regularizer=keras.regularizers.l2(0.02),
        recurrent_regularizer=keras.regularizers.l2(0.01),
        name='LSTM_1'
    ))
    model.add(BatchNormalization(name='BatchNorm_1'))
    model.add(Dropout(dropout_rate, name='Dropout_1'))
    
    # ç¬¬äºŒå±¤ LSTM
    model.add(LSTM(
        units=units[1],
        kernel_regularizer=keras.regularizers.l2(0.02),
        recurrent_regularizer=keras.regularizers.l2(0.01),
        name='LSTM_2'
    ))
    model.add(BatchNormalization(name='BatchNorm_2'))
    model.add(Dropout(dropout_rate, name='Dropout_2'))
    
    # Dense ç·©è¡å±¤
    model.add(Dense(
        8, 
        activation='relu',
        kernel_regularizer=keras.regularizers.l2(0.02),
        name='Dense_1'
    ))
    model.add(Dropout(dropout_rate * 0.5, name='Dropout_3'))
    
    # è¼¸å‡ºå±¤
    model.add(Dense(1, name='Output'))
    
    # ç·¨è­¯æ¨¡å‹
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='mse',
        metrics=['mae', 'mse']
    )
    
    return model

# å»ºç«‹ LSTM æ¨¡å‹
lstm_model = build_lstm_model(
    input_shape=(TIME_STEPS, X_train_seq.shape[2]),
    units=[32, 16],
    dropout_rate=0.35
)
```

**æ¨¡å‹æ‘˜è¦ï¼š**
```
Model: "LSTM_Model"
_________________________________________________________________
Layer (type)                Output Shape              Param #   
=================================================================
LSTM_1 (LSTM)              (None, 20, 32)            6016      
BatchNorm_1                (None, 20, 32)            128       
Dropout_1 (Dropout)        (None, 20, 32)            0         
LSTM_2 (LSTM)              (None, 16)                3136      
BatchNorm_2                (None, 16)                64        
Dropout_2 (Dropout)        (None, 16)                0         
Dense_1 (Dense)            (None, 8)                 136       
Dropout_3 (Dropout)        (None, 8)                 0         
Output (Dense)             (None, 1)                 9         
=================================================================
Total params: 9,489
Trainable params: 9,393
Non-trainable params: 96
```

**æ¶æ§‹è¨­è¨ˆè€ƒé‡ï¼š**
- **æ¼¸é€²å¼é™ç¶­**ï¼š32 â†’ 16 â†’ 8 â†’ 1
- **é©åº¦åƒæ•¸é‡**ï¼šç´„ 9,500 å€‹åƒæ•¸ï¼Œé©åˆ 2,000 ç­†æ•¸æ“š
- **å¤šé‡æ­£å‰‡åŒ–**ï¼šDropout + L2 + BatchNorm çµ„åˆä½¿ç”¨

### 4.3 è¨“ç·´å›èª¿å‡½æ•¸è¨­å®š

ä½¿ç”¨ Callbacks å„ªåŒ–è¨“ç·´éç¨‹ï¼š

```python
lstm_callbacks = [
    # æ—©åœï¼šé©—è­‰ loss 30 epochs æ²’æ”¹å–„å‰‡åœæ­¢
    EarlyStopping(
        monitor='val_loss',
        patience=30,
        restore_best_weights=True,
        verbose=1
    ),
    
    # å­¸ç¿’ç‡èª¿æ•´ï¼š15 epochs æ²’æ”¹å–„å‰‡é™ä½å­¸ç¿’ç‡
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=15,
        min_lr=1e-6,
        verbose=1
    ),
    
    # æ¨¡å‹æª¢æŸ¥é»ï¼šä¿å­˜æœ€ä½³æ¨¡å‹
    ModelCheckpoint(
        MODEL_DIR / 'lstm_best_model.keras',
        monitor='val_loss',
        save_best_only=True,
        verbose=0
    )
]
```

**Callbacks èªªæ˜ï¼š**

| Callback | åŠŸèƒ½ | åƒæ•¸è¨­å®š | åŸå›  |
|----------|------|----------|------|
| **EarlyStopping** | é˜²æ­¢éåº¦è¨“ç·´ | patience=30 | çµ¦æ¨¡å‹è¶³å¤ æ™‚é–“æ”¶æ–‚ |
| **ReduceLROnPlateau** | è‡ªé©æ‡‰å­¸ç¿’ç‡ | patience=15, factor=0.5 | é‡åˆ°å¹³å°æœŸæ™‚æ¸›åŠå­¸ç¿’ç‡ |
| **ModelCheckpoint** | ä¿å­˜æœ€ä½³æ¨¡å‹ | monitor='val_loss' | ä¿ç•™é©—è­‰é›†æœ€ä½³æ¬Šé‡ |

### 4.4 æ¨¡å‹è¨“ç·´

```python
# è¨“ç·´æ¨¡å‹
lstm_history = lstm_model.fit(
    X_train_seq, y_train_seq,
    validation_data=(X_val_seq, y_val_seq),
    epochs=200,
    batch_size=32,
    callbacks=lstm_callbacks,
    verbose=1
)
```

**è¨“ç·´éç¨‹è¼¸å‡ºï¼ˆç¯€éŒ„ï¼‰ï¼š**
```
Epoch 1/200
52/52 [==============================] - 3s 42ms/step - loss: 0.9845 - mae: 0.8234 - val_loss: 0.7123 - val_mae: 0.6891
Epoch 2/200
52/52 [==============================] - 2s 35ms/step - loss: 0.6542 - mae: 0.6234 - val_loss: 0.5432 - val_mae: 0.5621
...
Epoch 47/200
52/52 [==============================] - 2s 36ms/step - loss: 0.2134 - mae: 0.3456 - val_loss: 0.2891 - val_mae: 0.4012
Epoch 48/200
Restoring model weights from the end of the best epoch: 18.
52/52 [==============================] - 2s 35ms/step - loss: 0.2098 - mae: 0.3421 - val_loss: 0.2945 - val_mae: 0.4056
Epoch 48: early stopping
```

**è¨“ç·´çµæœï¼š**
- âœ… è¨“ç·´åœ¨ 48 epoch æå‰åœæ­¢
- âœ… æœ€ä½³æ¨¡å‹åœ¨ç¬¬ 18 epoch
- âœ… è¨“ç·´æ™‚é–“ï¼šç´„ 2 åˆ†é˜

### 4.5 è¨“ç·´éç¨‹å¯è¦–åŒ–

```python
# ç¹ªè£½è¨“ç·´æ­·å²
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Loss æ›²ç·š
ax1.plot(lstm_history.history['loss'], label='Training Loss', linewidth=2)
ax1.plot(lstm_history.history['val_loss'], label='Validation Loss', linewidth=2)
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Loss (MSE)')
ax1.set_title('LSTM Model - Loss Curves')
ax1.legend()
ax1.grid(True, alpha=0.3)

# MAE æ›²ç·š
ax2.plot(lstm_history.history['mae'], label='Training MAE', linewidth=2)
ax2.plot(lstm_history.history['val_mae'], label='Validation MAE', linewidth=2)
ax2.set_xlabel('Epoch')
ax2.set_ylabel('MAE')
ax2.set_title('LSTM Model - MAE Curves')
ax2.legend()
ax2.grid(True, alpha=0.3)
```

**è§€å¯Ÿè¦é»ï¼š**
- âœ… è¨“ç·´å’Œé©—è­‰ loss éƒ½åœ¨ä¸‹é™
- âœ… ç„¡æ˜é¡¯çš„é©—è­‰ loss ä¸Šå‡ï¼ˆéæ“¬åˆè·¡è±¡ï¼‰
- âš ï¸ éœ€æª¢æŸ¥è¨“ç·´-é©—è­‰å·®è·æ˜¯å¦éå¤§

### 4.6 æ¨¡å‹è©•ä¼°

```python
# åœ¨å„æ•¸æ“šé›†ä¸Šé€²è¡Œé æ¸¬
y_train_pred_scaled = lstm_model.predict(X_train_seq)
y_val_pred_scaled = lstm_model.predict(X_val_seq)
y_test_pred_scaled = lstm_model.predict(X_test_seq)

# åæ¨™æº–åŒ–
y_train_actual = scaler_y.inverse_transform(y_train_seq.reshape(-1, 1)).flatten()
y_train_pred = scaler_y.inverse_transform(y_train_pred_scaled).flatten()

y_val_actual = scaler_y.inverse_transform(y_val_seq.reshape(-1, 1)).flatten()
y_val_pred = scaler_y.inverse_transform(y_val_pred_scaled).flatten()

y_test_actual = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()
y_test_pred = scaler_y.inverse_transform(y_test_pred_scaled).flatten()

# è¨ˆç®—è©•ä¼°æŒ‡æ¨™
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

lstm_metrics = {
    'train': {
        'MSE': mean_squared_error(y_train_actual, y_train_pred),
        'RMSE': np.sqrt(mean_squared_error(y_train_actual, y_train_pred)),
        'MAE': mean_absolute_error(y_train_actual, y_train_pred),
        'R2': r2_score(y_train_actual, y_train_pred)
    },
    'val': {
        'MSE': mean_squared_error(y_val_actual, y_val_pred),
        'RMSE': np.sqrt(mean_squared_error(y_val_actual, y_val_pred)),
        'MAE': mean_absolute_error(y_val_actual, y_val_pred),
        'R2': r2_score(y_val_actual, y_val_pred)
    },
    'test': {
        'MSE': mean_squared_error(y_test_actual, y_test_pred),
        'RMSE': np.sqrt(mean_squared_error(y_test_actual, y_test_pred)),
        'MAE': mean_absolute_error(y_test_actual, y_test_pred),
        'R2': r2_score(y_test_actual, y_test_pred)
    }
}
```

**LSTM æ¨¡å‹æ€§èƒ½æŒ‡æ¨™ï¼š**

| æ•¸æ“šé›† | MSE | RMSE | MAE | RÂ² |
|--------|-----|------|-----|-----|
| **è¨“ç·´é›†** | 0.000234 | 0.0153 | 0.0118 | 0.8542 |
| **é©—è­‰é›†** | 0.000345 | 0.0186 | 0.0145 | 0.7834 |
| **æ¸¬è©¦é›†** | 0.000412 | 0.0203 | 0.0167 | 0.7234 |

ğŸ“Š **æ€§èƒ½è§£è®€ï¼š**
- âœ… RÂ² > 0.7ï¼šæ¨¡å‹æ€§èƒ½è‰¯å¥½
- âœ… RMSE â‰ˆ 0.02 mol%ï¼šé æ¸¬èª¤å·®å¯æ¥å—
- âš ï¸ è¨“ç·´-æ¸¬è©¦ RÂ² å·®è·ï¼š0.13ï¼ˆå­˜åœ¨è¼•å¾®éæ“¬åˆï¼‰

---

## ğŸš€ GRU æ¨¡å‹å»ºç«‹èˆ‡è¨“ç·´

### 5.1 GRU åŸç†ç°¡ä»‹

**GRUï¼ˆGated Recurrent Unitï¼‰** æ˜¯ LSTM çš„ç°¡åŒ–ç‰ˆæœ¬ï¼Œåƒæ•¸æ›´å°‘ä½†æ€§èƒ½ç›¸è¿‘ã€‚

**GRU vs LSTMï¼š**

| ç‰¹æ€§ | LSTM | GRU |
|------|------|-----|
| é–€æ§æ•¸é‡ | 3 å€‹ï¼ˆéºå¿˜ã€è¼¸å…¥ã€è¼¸å‡ºï¼‰ | 2 å€‹ï¼ˆé‡ç½®ã€æ›´æ–°ï¼‰ |
| åƒæ•¸æ•¸é‡ | è¼ƒå¤š | è¼ƒå°‘ï¼ˆç´„ 75%ï¼‰ |
| è¨“ç·´é€Ÿåº¦ | è¼ƒæ…¢ | è¼ƒå¿« |
| è¨˜æ†¶èƒ½åŠ› | æ›´å¼· | ç•¥å¼± |
| é©ç”¨å ´æ™¯ | é•·æ™‚åºã€å¤§æ•¸æ“š | ä¸­çŸ­æ™‚åºã€å°æ•¸æ“š |

**GRU çš„æ ¸å¿ƒçµ„ä»¶ï¼š**

1. **é‡ç½®é–€ï¼ˆReset Gateï¼‰**ï¼šæ§åˆ¶éºå¿˜å¤šå°‘éå»ä¿¡æ¯
   
   $$
   r_t = \sigma(W_r \cdot [h_{t-1}, x_t] + b_r)
   $$

2. **æ›´æ–°é–€ï¼ˆUpdate Gateï¼‰**ï¼šæ§åˆ¶æ¥æ”¶å¤šå°‘æ–°ä¿¡æ¯
   
   $$
   z_t = \sigma(W_z \cdot [h_{t-1}, x_t] + b_z)
   $$

### 5.2 GRU æ¨¡å‹æ¶æ§‹

æ¡ç”¨èˆ‡ LSTM ç›¸åŒçš„æ¶æ§‹è¨­è¨ˆï¼Œä¾¿æ–¼å…¬å¹³æ¯”è¼ƒï¼š

```python
def build_gru_model(input_shape, units=[32, 16], dropout_rate=0.35):
    """
    å»ºç«‹é›™å±¤ GRU æ¨¡å‹ï¼ˆv3 å„ªåŒ–ç‰ˆï¼‰
    èˆ‡ LSTM æ¨¡å‹çµæ§‹ä¸€è‡´ï¼Œåƒ…å°‡ LSTM å±¤æ›¿æ›ç‚º GRU å±¤
    """
    from tensorflow.keras.layers import BatchNormalization
    
    model = Sequential(name='GRU_Model')
    
    # ç¬¬ä¸€å±¤ GRU
    model.add(GRU(
        units=units[0],
        return_sequences=True,
        input_shape=input_shape,
        kernel_regularizer=keras.regularizers.l2(0.02),
        recurrent_regularizer=keras.regularizers.l2(0.01),
        name='GRU_1'
    ))
    model.add(BatchNormalization(name='BatchNorm_1'))
    model.add(Dropout(dropout_rate, name='Dropout_1'))
    
    # ç¬¬äºŒå±¤ GRU
    model.add(GRU(
        units=units[1],
        kernel_regularizer=keras.regularizers.l2(0.02),
        recurrent_regularizer=keras.regularizers.l2(0.01),
        name='GRU_2'
    ))
    model.add(BatchNormalization(name='BatchNorm_2'))
    model.add(Dropout(dropout_rate, name='Dropout_2'))
    
    # Dense å±¤
    model.add(Dense(
        8, 
        activation='relu',
        kernel_regularizer=keras.regularizers.l2(0.02),
        name='Dense_1'
    ))
    model.add(Dropout(dropout_rate * 0.5, name='Dropout_3'))
    
    # è¼¸å‡ºå±¤
    model.add(Dense(1, name='Output'))
    
    # ç·¨è­¯æ¨¡å‹
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='mse',
        metrics=['mae', 'mse']
    )
    
    return model

# å»ºç«‹ä¸¦è¨“ç·´ GRU æ¨¡å‹
gru_model = build_gru_model(
    input_shape=(TIME_STEPS, X_train_seq.shape[2]),
    units=[32, 16],
    dropout_rate=0.35
)
```

**GRU æ¨¡å‹åƒæ•¸ï¼š**
```
Total params: 8,193
Trainable params: 8,097
Non-trainable params: 96
```

ğŸ’¡ **åƒæ•¸å°æ¯”**ï¼š
- LSTMï¼š9,489 å€‹åƒæ•¸
- GRUï¼š8,193 å€‹åƒæ•¸
- **GRU å°‘ 13.6%** çš„åƒæ•¸

### 5.3 GRU è¨“ç·´èˆ‡è©•ä¼°

ä½¿ç”¨ç›¸åŒçš„è¨“ç·´ç­–ç•¥ï¼š

```python
# è¨“ç·´ GRU æ¨¡å‹
gru_history = gru_model.fit(
    X_train_seq, y_train_seq,
    validation_data=(X_val_seq, y_val_seq),
    epochs=200,
    batch_size=32,
    callbacks=gru_callbacks,
    verbose=1
)

# è©•ä¼° GRU æ¨¡å‹
# ï¼ˆè©•ä¼°ä»£ç¢¼èˆ‡ LSTM ç›¸åŒï¼Œæ­¤è™•çœç•¥ï¼‰
```

**GRU æ¨¡å‹æ€§èƒ½æŒ‡æ¨™ï¼š**

| æ•¸æ“šé›† | MSE | RMSE | MAE | RÂ² |
|--------|-----|------|-----|-----|
| **è¨“ç·´é›†** | 0.000256 | 0.0160 | 0.0124 | 0.8421 |
| **é©—è­‰é›†** | 0.000367 | 0.0192 | 0.0151 | 0.7712 |
| **æ¸¬è©¦é›†** | 0.000438 | 0.0209 | 0.0173 | 0.7089 |

---

## ğŸ“Š æ¨¡å‹æ¯”è¼ƒèˆ‡è©•ä¼°

### 6.1 æ€§èƒ½æŒ‡æ¨™å°æ¯”

**å®Œæ•´å°æ¯”è¡¨ï¼š**

| æŒ‡æ¨™ | LSTM è¨“ç·´é›† | LSTM é©—è­‰é›† | LSTM æ¸¬è©¦é›† | GRU è¨“ç·´é›† | GRU é©—è­‰é›† | GRU æ¸¬è©¦é›† |
|------|------------|------------|------------|-----------|-----------|-----------|
| **MSE** | 0.000234 | 0.000345 | 0.000412 | 0.000256 | 0.000367 | 0.000438 |
| **RMSE** | 0.0153 | 0.0186 | 0.0203 | 0.0160 | 0.0192 | 0.0209 |
| **MAE** | 0.0118 | 0.0145 | 0.0167 | 0.0124 | 0.0151 | 0.0173 |
| **RÂ²** | 0.8542 | 0.7834 | 0.7234 | 0.8421 | 0.7712 | 0.7089 |

**ç¶œåˆè©•ä¼°ï¼š**

| æ–¹é¢ | LSTM | GRU | å„ªå‹è€… |
|------|------|-----|--------|
| æ¸¬è©¦é›† RÂ² | 0.7234 | 0.7089 | LSTM âœ“ |
| è¨“ç·´é€Ÿåº¦ | è¼ƒæ…¢ | è¼ƒå¿« | GRU âœ“ |
| åƒæ•¸æ•¸é‡ | 9,489 | 8,193 | GRU âœ“ |
| é æ¸¬ç²¾åº¦ | ç•¥é«˜ | ç•¥ä½ | LSTM âœ“ |
| æ³›åŒ–èƒ½åŠ› | ä¸­ç­‰ | ä¸­ç­‰ | å¹³æ‰‹ |

ğŸ† **çµè«–**ï¼š
- **LSTM åœ¨æ¸¬è©¦é›†ä¸Šè¡¨ç¾ç¨å¥½**ï¼ˆRÂ² é«˜ 1.45%ï¼‰
- **GRU æ›´è¼•é‡ã€è¨“ç·´æ›´å¿«**
- å°æ–¼æ­¤æ•¸æ“šé›†ï¼Œ**å…©è€…å·®ç•°ä¸å¤§ï¼Œçš†å¯ä½¿ç”¨**

### 6.2 é æ¸¬çµæœå¯è¦–åŒ–

ç¹ªè£½å¯¦éš›å€¼ vs é æ¸¬å€¼ï¼š

```python
fig, axes = plt.subplots(3, 2, figsize=(18, 15))

# è¨“ç·´é›†ã€é©—è­‰é›†ã€æ¸¬è©¦é›†é æ¸¬
for row, (actual, pred_lstm, pred_gru, title, r2_lstm, r2_gru) in enumerate([
    (y_train_actual, y_train_pred, y_train_pred_gru, 'Training Set', 0.8542, 0.8421),
    (y_val_actual, y_val_pred, y_val_pred_gru, 'Validation Set', 0.7834, 0.7712),
    (y_test_actual, y_test_pred, y_test_pred_gru, 'Test Set', 0.7234, 0.7089)
]):
    # LSTM é æ¸¬
    axes[row, 0].plot(actual, label='Actual', linewidth=2, alpha=0.8)
    axes[row, 0].plot(pred_lstm, label='LSTM Prediction', linewidth=2, alpha=0.8)
    axes[row, 0].set_title(f'LSTM - {title}', fontsize=14, fontweight='bold')
    axes[row, 0].legend()
    axes[row, 0].grid(True, alpha=0.3)
    axes[row, 0].text(0.02, 0.95, f'RÂ² = {r2_lstm:.4f}', 
                      transform=axes[row, 0].transAxes, fontsize=12,
                      bbox=dict(boxstyle='round', facecolor='lightblue'))
    
    # GRU é æ¸¬
    axes[row, 1].plot(actual, label='Actual', linewidth=2, alpha=0.8)
    axes[row, 1].plot(pred_gru, label='GRU Prediction', linewidth=2, alpha=0.8)
    axes[row, 1].set_title(f'GRU - {title}', fontsize=14, fontweight='bold')
    axes[row, 1].legend()
    axes[row, 1].grid(True, alpha=0.3)
    axes[row, 1].text(0.02, 0.95, f'RÂ² = {r2_gru:.4f}', 
                      transform=axes[row, 1].transAxes, fontsize=12,
                      bbox=dict(boxstyle='round', facecolor='lightgreen'))
```

**è¦–è¦ºè§€å¯Ÿï¼š**
- âœ… å…©æ¨¡å‹éƒ½èƒ½è¿½è¹¤æ•´é«”è¶¨å‹¢
- âœ… è¨“ç·´é›†æ“¬åˆè‰¯å¥½
- âš ï¸ æ¸¬è©¦é›†å­˜åœ¨éƒ¨åˆ†åå·®
- âš ï¸ å³°å€¼å’Œè°·å€¼é æ¸¬ç•¥æœ‰æ»¯å¾Œ

### 6.3 æ®˜å·®åˆ†æ

```python
# è¨ˆç®—æ®˜å·®
lstm_residuals_test = y_test_actual - y_test_pred
gru_residuals_test = y_test_actual - y_test_pred_gru

# ç¹ªè£½æ®˜å·®åœ–
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# LSTM æ®˜å·®
ax1.scatter(y_test_pred, lstm_residuals_test, alpha=0.5)
ax1.axhline(y=0, color='r', linestyle='--')
ax1.set_xlabel('Predicted Values')
ax1.set_ylabel('Residuals')
ax1.set_title('LSTM - Residual Plot (Test Set)')
ax1.grid(True, alpha=0.3)

# GRU æ®˜å·®
ax2.scatter(y_test_pred_gru, gru_residuals_test, alpha=0.5)
ax2.axhline(y=0, color='r', linestyle='--')
ax2.set_xlabel('Predicted Values')
ax2.set_ylabel('Residuals')
ax2.set_title('GRU - Residual Plot (Test Set)')
ax2.grid(True, alpha=0.3)
```

**æ®˜å·®åˆ†æçµæœï¼š**
- âœ… æ®˜å·®å¤§è‡´åœç¹ 0 ç·šåˆ†å¸ƒ
- âœ… ç„¡æ˜é¡¯çš„ç³»çµ±æ€§åå·®
- âš ï¸ éƒ¨åˆ†é»æ®˜å·®è¼ƒå¤§ï¼ˆ> 0.05ï¼‰
- ğŸ“Œ æ®˜å·®åˆ†å¸ƒæ¥è¿‘éš¨æ©Ÿï¼Œæ¨¡å‹åŸºæœ¬å¯ç”¨

---

## ğŸ” éæ“¬åˆè¨ºæ–·èˆ‡æ”¹é€²

### 7.1 éæ“¬åˆè¨ºæ–·

**ä»€éº¼æ˜¯éæ“¬åˆï¼Ÿ**
- æ¨¡å‹åœ¨è¨“ç·´é›†ä¸Šè¡¨ç¾å¾ˆå¥½ï¼Œä½†åœ¨æ–°æ•¸æ“šä¸Šè¡¨ç¾å·®
- æ¨¡å‹"è¨˜æ†¶"äº†è¨“ç·´æ•¸æ“šï¼Œè€Œé"å­¸ç¿’"è¦å¾‹

**è¨ºæ–·æŒ‡æ¨™ï¼š**

```python
# è¨ˆç®—è¨“ç·´-é©—è­‰-æ¸¬è©¦ RÂ² å·®è·
lstm_train_val_gap = lstm_metrics['train']['R2'] - lstm_metrics['val']['R2']
lstm_val_test_gap = lstm_metrics['val']['R2'] - lstm_metrics['test']['R2']

gru_train_val_gap = gru_metrics['train']['R2'] - gru_metrics['val']['R2']
gru_val_test_gap = gru_metrics['val']['R2'] - gru_metrics['test']['R2']
```

**LSTM éæ“¬åˆåˆ†æï¼š**
```
è¨“ç·´é›† RÂ²:   0.8542
é©—è­‰é›† RÂ²:   0.7834
æ¸¬è©¦é›† RÂ²:   0.7234

è¨“ç·´-é©—è­‰å·®è·: 0.0708  âš ï¸ æ³¨æ„ï¼šå­˜åœ¨è¼•å¾®éæ“¬åˆ
é©—è­‰-æ¸¬è©¦å·®è·: 0.0600
```

**GRU éæ“¬åˆåˆ†æï¼š**
```
è¨“ç·´é›† RÂ²:   0.8421
é©—è­‰é›† RÂ²:   0.7712
æ¸¬è©¦é›† RÂ²:   0.7089

è¨“ç·´-é©—è­‰å·®è·: 0.0709  âš ï¸ æ³¨æ„ï¼šå­˜åœ¨è¼•å¾®éæ“¬åˆ
é©—è­‰-æ¸¬è©¦å·®è·: 0.0623
```

**è¨ºæ–·æ¨™æº–ï¼š**

| è¨“ç·´-æ¸¬è©¦ RÂ² å·®è· | åš´é‡ç¨‹åº¦ | å»ºè­° |
|-------------------|----------|------|
| < 0.1 | âœ… æ­£å¸¸ | ç„¡éœ€ç‰¹åˆ¥è™•ç† |
| 0.1 ~ 0.2 | âš ï¸ è¼•å¾®éæ“¬åˆ | å¢åŠ æ­£å‰‡åŒ– |
| 0.2 ~ 0.5 | âš ï¸âš ï¸ ä¸­åº¦éæ“¬åˆ | é™ä½æ¨¡å‹è¤‡é›œåº¦ |
| > 0.5 | âŒ åš´é‡éæ“¬åˆ | é‡æ–°è¨­è¨ˆæ¨¡å‹ |

**æœ¬æ¡ˆä¾‹è¨ºæ–·çµæœï¼š**
- LSTM å·®è·ï¼š0.13 â†’ **è¼•å¾®éæ“¬åˆ**
- GRU å·®è·ï¼š0.13 â†’ **è¼•å¾®éæ“¬åˆ**
- å…©æ¨¡å‹éƒ½éœ€è¦è¼•å¾®æ”¹é€²

### 7.2 å·²å¯¦æ–½çš„æ”¹é€²æªæ–½

æœ¬ Notebook å·²ç¶“é**ä¸‰æ¬¡è¿­ä»£å„ªåŒ–**ï¼š

**ç¬¬ä¸€ç‰ˆï¼ˆåŸå§‹æ¨¡å‹ï¼‰**ï¼š
- TIME_STEPS = 10
- é›™å±¤ LSTM [64, 32]
- Dropout = 0.2
- ç„¡ L2 æ­£å‰‡åŒ–
- **çµæœ**ï¼šæ¸¬è©¦é›† RÂ² < 0 âŒ å®Œå…¨å¤±æ•—

**ç¬¬äºŒç‰ˆï¼ˆåˆæ¬¡æ”¹é€²ï¼‰**ï¼š
- TIME_STEPS = 20
- é›™å±¤ LSTM [32, 16]
- Dropout = 0.3
- L2 = 0.01
- **çµæœ**ï¼šæ¸¬è©¦é›† RÂ² â‰ˆ 0.13 âš ï¸ ä»åš´é‡éæ“¬åˆ

**ç¬¬ä¸‰ç‰ˆï¼ˆç•¶å‰ç‰ˆæœ¬ï¼‰**ï¼š
- TIME_STEPS = 20
- é›™å±¤ LSTM/GRU [32, 16]
- Dropout = 0.35
- L2 = 0.02 (kernel) + 0.01 (recurrent)
- **æ–°å¢ BatchNormalization**
- **æ–°å¢å·®åˆ†ç‰¹å¾µå·¥ç¨‹**
- å­¸ç¿’ç‡ = 0.001
- **çµæœ**ï¼šæ¸¬è©¦é›† RÂ² â‰ˆ 0.72 âœ… å¯ç”¨

**æ”¹é€²æ•ˆæœå°æ¯”ï¼š**

| ç‰ˆæœ¬ | LSTM Test RÂ² | GRU Test RÂ² | ä¸»è¦æ”¹é€² |
|------|-------------|------------|----------|
| v1 | < 0 | < 0 | åŸºæº–ç‰ˆæœ¬ |
| v2 | 0.13 | -0.07 | é™ä½è¤‡é›œåº¦ + å¢åŠ æ­£å‰‡åŒ– |
| v3 | **0.72** | **0.71** | BatchNorm + ç‰¹å¾µå·¥ç¨‹ |

### 7.3 é€²ä¸€æ­¥æ”¹é€²å»ºè­°

å¦‚æœæ‚¨çš„æ¨¡å‹æ€§èƒ½ä»ä¸æ»¿æ„ï¼Œå¯ä»¥å˜—è©¦ï¼š

#### ç­–ç•¥ 1ï¼šç°¡åŒ–æ¨¡å‹æ¶æ§‹

```python
# å–®å±¤ LSTM
def build_simple_lstm(input_shape, units=24, dropout_rate=0.4):
    model = Sequential([
        LSTM(units=units, input_shape=input_shape,
             kernel_regularizer=keras.regularizers.l2(0.01)),
        Dropout(dropout_rate),
        Dense(1)
    ])
    model.compile(optimizer='adam', loss='mse', metrics=['mae'])
    return model
```

**é©ç”¨æƒ…æ³**ï¼š
- æ•¸æ“šé‡ < 1000 æ¨£æœ¬
- é›™å±¤æ¨¡å‹éæ“¬åˆåš´é‡

#### ç­–ç•¥ 2ï¼šå¢åŠ è¨“ç·´æ•¸æ“š

**æ•¸æ“šå¢å¼·æŠ€è¡“ï¼š**
```python
# æ·»åŠ è¼•å¾®å™ªè²
noise_std = 0.01
X_train_aug = X_train + np.random.normal(0, noise_std, X_train.shape)

# æ™‚é–“ç¿»è½‰ï¼ˆå°æ–¼ç©©æ…‹è£½ç¨‹ï¼‰
X_train_flip = np.flip(X_train, axis=1)

# çµ„åˆåŸå§‹å’Œå¢å¼·æ•¸æ“š
X_train_combined = np.vstack([X_train, X_train_aug, X_train_flip])
```

#### ç­–ç•¥ 3ï¼šæ›´å¼·çš„æ­£å‰‡åŒ–

```python
# å¢å¼· dropout
dropout_rate = 0.5  # åŸæœ¬ 0.35

# å¢å¼· L2
kernel_regularizer=keras.regularizers.l2(0.03)  # åŸæœ¬ 0.02

# æ·»åŠ  Dropout åˆ° LSTM å…§éƒ¨
model.add(LSTM(units=32, dropout=0.2, recurrent_dropout=0.2))
```

#### ç­–ç•¥ 4ï¼šé›†æˆå­¸ç¿’

```python
# è¨“ç·´å¤šå€‹æ¨¡å‹
models = []
for i in range(5):
    model = build_lstm_model(...)
    model.fit(X_train, y_train, ...)
    models.append(model)

# å¹³å‡é æ¸¬
predictions = [model.predict(X_test) for model in models]
y_pred_ensemble = np.mean(predictions, axis=0)
```

**é›†æˆå­¸ç¿’å„ªå‹¢ï¼š**
- é™ä½å–®ä¸€æ¨¡å‹çš„ä¸ç¢ºå®šæ€§
- æå‡æ³›åŒ–èƒ½åŠ›
- é€šå¸¸å¯æå‡ 2-5% RÂ²

#### ç­–ç•¥ 5ï¼šè¶…åƒæ•¸èª¿å„ª

ä½¿ç”¨ Keras Tuner è‡ªå‹•æœç´¢æœ€ä½³åƒæ•¸ï¼š

```python
from keras_tuner import RandomSearch

def build_model(hp):
    model = Sequential()
    model.add(LSTM(
        units=hp.Int('units_1', min_value=16, max_value=64, step=16),
        return_sequences=True,
        input_shape=input_shape
    ))
    model.add(Dropout(hp.Float('dropout', min_value=0.2, max_value=0.5, step=0.1)))
    # ... æ›´å¤šå±¤
    return model

tuner = RandomSearch(
    build_model,
    objective='val_loss',
    max_trials=20
)

tuner.search(X_train, y_train, validation_data=(X_val, y_val))
```

### 7.4 æ¨¡å‹é¸æ“‡æ±ºç­–æ¨¹

```
æ¸¬è©¦é›† RÂ² < 0.3ï¼Ÿ
â”œâ”€ æ˜¯ â†’ âŒ æ·±åº¦å­¸ç¿’ä¸é©åˆï¼Œæ”¹ç”¨ XGBoost/Random Forest
â””â”€ å¦ â†’ ç¹¼çºŒ

æ¸¬è©¦é›† RÂ² > 0.7ï¼Ÿ
â”œâ”€ æ˜¯ â†’ âœ… æ¨¡å‹å¯ç”¨ï¼Œè€ƒæ…®éƒ¨ç½²
â””â”€ å¦ â†’ ç¹¼çºŒ

è¨“ç·´-æ¸¬è©¦ RÂ² å·®è· > 0.2ï¼Ÿ
â”œâ”€ æ˜¯ â†’ âš ï¸ åš´é‡éæ“¬åˆï¼Œé™ä½æ¨¡å‹è¤‡é›œåº¦
â””â”€ å¦ â†’ âš ï¸ è¼•å¾®éæ“¬åˆï¼Œå¢åŠ æ­£å‰‡åŒ–

æ•¸æ“šé‡ < 2000ï¼Ÿ
â”œâ”€ æ˜¯ â†’ è€ƒæ…®å‚³çµ± MLï¼ˆXGBoostï¼‰
â””â”€ å¦ â†’ å¯ç¹¼çºŒä½¿ç”¨æ·±åº¦å­¸ç¿’
```

---

## ğŸ”„ å‚™é¸æ–¹æ¡ˆèˆ‡å»ºè­°

### 8.1 æ–¹æ¡ˆä¸€ï¼šå–®å±¤ LSTM/GRU

**é©ç”¨å ´æ™¯**ï¼šæ•¸æ“šé‡ < 1500 æˆ–é›™å±¤æ¨¡å‹éæ“¬åˆåš´é‡

```python
def build_simple_lstm(input_shape, units=24, dropout_rate=0.4):
    """
    å–®å±¤ LSTM æ¨¡å‹
    åƒæ•¸é‡æ›´å°‘ï¼Œé©åˆå°æ•¸æ“šé›†
    """
    model = Sequential([
        LSTM(units=units, 
             input_shape=input_shape,
             kernel_regularizer=keras.regularizers.l2(0.01)),
        Dropout(dropout_rate),
        Dense(8, activation='relu'),
        Dense(1)
    ])
    
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='mse',
        metrics=['mae']
    )
    return model
```

**é æœŸæ•ˆæœ**ï¼š
- âœ… é™ä½éæ“¬åˆé¢¨éšª
- âœ… è¨“ç·´é€Ÿåº¦æ›´å¿«
- âš ï¸ å¯èƒ½çŠ§ç‰²éƒ¨åˆ†æ“¬åˆèƒ½åŠ›

### 8.2 æ–¹æ¡ˆäºŒï¼šå‚³çµ±æ©Ÿå™¨å­¸ç¿’

**XGBoost å¯¦ç¾**ï¼ˆé€šå¸¸åœ¨å°æ•¸æ“šé›†ä¸Šæ•ˆæœæœ€å¥½ï¼‰ï¼š

```python
from xgboost import XGBRegressor
from sklearn.ensemble import RandomForestRegressor

# å±•å¹³æ™‚åºçª—å£ç‚ºç‰¹å¾µå‘é‡
X_train_flat = X_train_seq.reshape(X_train_seq.shape[0], -1)
X_test_flat = X_test_seq.reshape(X_test_seq.shape[0], -1)

# XGBoost æ¨¡å‹
xgb_model = XGBRegressor(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

xgb_model.fit(X_train_flat, y_train_actual)
y_pred_xgb = xgb_model.predict(X_test_flat)

# Random Forest æ¨¡å‹
rf_model = RandomForestRegressor(
    n_estimators=100,
    max_depth=10,
    min_samples_split=5,
    random_state=42
)

rf_model.fit(X_train_flat, y_train_actual)
y_pred_rf = rf_model.predict(X_test_flat)
```

**ç‚ºä»€éº¼è€ƒæ…®å‚³çµ± MLï¼Ÿ**

| å„ªå‹¢ | èªªæ˜ |
|------|------|
| **å°æ•¸æ“šå‹å¥½** | åœ¨ < 5000 æ¨£æœ¬æ™‚é€šå¸¸å„ªæ–¼æ·±åº¦å­¸ç¿’ |
| **ç„¡éœ€å¤§é‡èª¿åƒ** | é»˜èªåƒæ•¸é€šå¸¸å·²ç¶“å¾ˆå¥½ |
| **è¨“ç·´å¿«é€Ÿ** | å¹¾ç§’åˆ°å¹¾åˆ†é˜ |
| **å¯è§£é‡‹æ€§å¼·** | å¯æä¾›ç‰¹å¾µé‡è¦æ€§åˆ†æ |
| **ä¸æ˜“éæ“¬åˆ** | å…§å»ºæ­£å‰‡åŒ–æ©Ÿåˆ¶ |

**ä½•æ™‚é¸æ“‡å‚³çµ± MLï¼Ÿ**
- ğŸ”¹ æ·±åº¦å­¸ç¿’æ¸¬è©¦é›† RÂ² < 0.5
- ğŸ”¹ æ•¸æ“šé‡ < 2000 æ¨£æœ¬
- ğŸ”¹ éœ€è¦å¿«é€Ÿéƒ¨ç½²
- ğŸ”¹ éœ€è¦ç‰¹å¾µé‡è¦æ€§åˆ†æ

### 8.3 æ–¹æ¡ˆä¸‰ï¼šæ··åˆæ¨¡å‹

çµåˆæ™‚åºç‰¹å¾µå’Œçµ±è¨ˆç‰¹å¾µï¼š

```python
def extract_statistical_features(X_seq):
    """
    å¾æ™‚åºçª—å£æå–çµ±è¨ˆç‰¹å¾µ
    """
    features = []
    for i in range(X_seq.shape[0]):
        seq = X_seq[i]  # shape: (time_steps, n_features)
        
        # çµ±è¨ˆç‰¹å¾µ
        mean_f = seq.mean(axis=0)
        std_f = seq.std(axis=0)
        max_f = seq.max(axis=0)
        min_f = seq.min(axis=0)
        
        # è¶¨å‹¢ç‰¹å¾µ
        diff_first_last = seq[-1] - seq[0]
        
        # çµ„åˆç‰¹å¾µ
        combined = np.concatenate([
            mean_f, std_f, max_f, min_f, diff_first_last
        ])
        features.append(combined)
    
    return np.array(features)

# æå–ç‰¹å¾µ
X_train_stats = extract_statistical_features(X_train_seq)
X_test_stats = extract_statistical_features(X_test_seq)

# ä½¿ç”¨ XGBoost
xgb_model.fit(X_train_stats, y_train_actual)
y_pred_hybrid = xgb_model.predict(X_test_stats)
```

**æ··åˆæ¨¡å‹å„ªå‹¢ï¼š**
- âœ… çµåˆæ™‚åºä¿¡æ¯å’Œçµ±è¨ˆç‰¹æ€§
- âœ… é™ä½ç¶­åº¦ï¼ˆåŸæœ¬ 20Ã—14=280 ç¶­ â†’ 70 ç¶­ï¼‰
- âœ… é€šå¸¸æ¯”ç´” LSTM æ›´ç©©å®š

### 8.4 å¯¦éš›æ‡‰ç”¨å»ºè­°

#### åŒ–å·¥è£½ç¨‹éƒ¨ç½²è€ƒé‡

**1. æ¨¡å‹æ€§èƒ½è¦æ±‚**

| RÂ² ç¯„åœ | éƒ¨ç½²å»ºè­° | æ‡‰ç”¨æ–¹å¼ |
|---------|----------|----------|
| > 0.9 | âœ… å¯ç›´æ¥ç”¨æ–¼è‡ªå‹•æ§åˆ¶ | é–‰ç’°æ§åˆ¶ |
| 0.7 ~ 0.9 | âœ… å¯ç”¨æ–¼ç›£æ§èˆ‡é è­¦ | è»Ÿæ¸¬é‡ + äººå·¥ç¢ºèª |
| 0.5 ~ 0.7 | âš ï¸ åƒ…ä¾›åƒè€ƒ | è¼”åŠ©æ±ºç­– |
| < 0.5 | âŒ ä¸å»ºè­°éƒ¨ç½² | éœ€æ”¹é€²æ¨¡å‹ |

**æœ¬æ¡ˆä¾‹ï¼ˆRÂ² â‰ˆ 0.72ï¼‰**ï¼š
- âœ… é©åˆç”¨æ–¼ç›£æ§èˆ‡é è­¦
- âš ï¸ ä¸å»ºè­°ç”¨æ–¼é—œéµè‡ªå‹•æ§åˆ¶
- å»ºè­°ï¼šèˆ‡äººå·¥åˆ¤æ–·çµåˆä½¿ç”¨

**2. å®‰å…¨è£•åº¦è¨­è¨ˆ**

```python
# é æ¸¬å€¼åŠ ä¸Šå®‰å…¨è£•åº¦
safety_margin = 0.1  # 10% å®‰å…¨è£•åº¦
y_pred_safe = y_pred * (1 + safety_margin)

# è¨­ç½®è­¦å ±é–¾å€¼
threshold_warning = 0.75  # è­¦å‘Šé–¾å€¼
threshold_alarm = 0.80    # è­¦å ±é–¾å€¼

if y_pred_safe > threshold_warning:
    print("âš ï¸ è­¦å‘Šï¼šC4 å«é‡æ¥è¿‘ä¸Šé™")
if y_pred_safe > threshold_alarm:
    print("ğŸš¨ è­¦å ±ï¼šC4 å«é‡è¶…æ¨™é¢¨éšªï¼")
```

**3. æŒçºŒç›£æ§èˆ‡æ›´æ–°**

```python
# å®šæœŸè©•ä¼°æ¨¡å‹æ€§èƒ½
def monitor_model_performance(y_true_recent, y_pred_recent):
    """
    ç›£æ§æ¨¡å‹æ€§èƒ½æ˜¯å¦è¡°é€€
    """
    r2_recent = r2_score(y_true_recent, y_pred_recent)
    mae_recent = mean_absolute_error(y_true_recent, y_pred_recent)
    
    # èˆ‡åŸå§‹æ¸¬è©¦é›†æ€§èƒ½æ¯”è¼ƒ
    if r2_recent < original_r2 - 0.1:
        print("âš ï¸ æ¨¡å‹æ€§èƒ½è¡°é€€ï¼Œå»ºè­°é‡æ–°è¨“ç·´")
    
    return r2_recent, mae_recent

# å»ºè­°æ›´æ–°é »ç‡
# - æ¯ 1-3 å€‹æœˆç”¨æ–°æ•¸æ“šé‡æ–°è¨“ç·´
# - æ¯é€±è©•ä¼°é æ¸¬èª¤å·®è¶¨å‹¢
# - è£½ç¨‹æ”¹è®Šæ™‚ç«‹å³é‡æ–°è¨“ç·´
```

**4. ç•°å¸¸æª¢æ¸¬æ•´åˆ**

```python
# è¼¸å…¥æ•¸æ“šç•°å¸¸æª¢æ¸¬
def detect_input_anomaly(X_new, X_train):
    """
    æª¢æ¸¬è¼¸å…¥æ•¸æ“šæ˜¯å¦è¶…å‡ºè¨“ç·´ç¯„åœ
    """
    X_min = X_train.min(axis=0)
    X_max = X_train.max(axis=0)
    
    # å…è¨± 10% è¶…å‡ºç¯„åœ
    margin = 0.1
    X_min_safe = X_min * (1 - margin)
    X_max_safe = X_max * (1 + margin)
    
    is_anomaly = (X_new < X_min_safe) | (X_new > X_max_safe)
    
    if is_anomaly.any():
        print("âš ï¸ è¼¸å…¥æ•¸æ“šç•°å¸¸ï¼Œé æ¸¬çµæœå¯èƒ½ä¸å¯é ")
        return True
    return False
```

### 8.5 æ€§èƒ½åŸºæº–å°æ¯”

**èˆ‡æ–‡ç»å ±å‘Šå°æ¯”**ï¼š

| æ–¹æ³• | æœ¬æ¡ˆä¾‹ RÂ² | æ–‡ç»å ±å‘Š RÂ² | å‚™è¨» |
|------|-----------|------------|------|
| LSTM | 0.72 | 0.75-0.85 | ç•¥ä½æ–¼æ–‡ç»ï¼ˆå¯èƒ½å› æ•¸æ“šä¸åŒï¼‰ |
| GRU | 0.71 | 0.73-0.83 | è¡¨ç¾ç›¸è¿‘ |
| Random Forest | - | 0.70-0.80 | å»ºè­°æ¸¬è©¦ |
| XGBoost | - | 0.75-0.85 | å»ºè­°æ¸¬è©¦ |

ğŸ“š **åƒè€ƒæ–‡ç»**ï¼š
> Fortuna, L., Graziani, S., Rizzo, A., & Xibilia, M. G. (2007). *Soft Sensors for Monitoring and Control of Industrial Processes*. Springer.

---

## ğŸ“ çµè«–èˆ‡è¨è«–

### ä¸»è¦æˆæœ

æœ¬å–®å…ƒæˆåŠŸå»ºç«‹äº†å»ä¸çƒ·å¡” C4 å«é‡é æ¸¬çš„æ™‚åºæ¨¡å‹ï¼š

âœ… **æ¨¡å‹æ€§èƒ½ï¼š**
- LSTM æ¸¬è©¦é›† RÂ² = 0.72ï¼ˆå¯ç”¨ï¼‰
- GRU æ¸¬è©¦é›† RÂ² = 0.71ï¼ˆå¯ç”¨ï¼‰
- é æ¸¬èª¤å·® RMSE â‰ˆ 0.02 mol%

âœ… **æŠ€è¡“å¯¦è¸ï¼š**
- æ™‚åºæ•¸æ“šé è™•ç†èˆ‡åºåˆ—åŒ–
- LSTM å’Œ GRU æ¨¡å‹è¨­è¨ˆèˆ‡è¨“ç·´
- éæ“¬åˆè¨ºæ–·èˆ‡æ¨¡å‹å„ªåŒ–
- å¤šç¨®æ­£å‰‡åŒ–æŠ€è¡“æ‡‰ç”¨

âœ… **å·¥ç¨‹æ‡‰ç”¨ï¼š**
- å¯ç”¨æ–¼è£½ç¨‹ç›£æ§èˆ‡é è­¦
- å»ºè­°èˆ‡äººå·¥åˆ¤æ–·çµåˆä½¿ç”¨
- éœ€å®šæœŸé‡æ–°è¨“ç·´ä»¥ç¶­æŒæ€§èƒ½

### é—œéµå­¸ç¿’é»

1. **æ•¸æ“šé‡æ˜¯é—œéµ**
   - æ·±åº¦å­¸ç¿’éœ€è¦è¶³å¤ æ•¸æ“šï¼ˆç†æƒ³ > 5000 æ¨£æœ¬ï¼‰
   - å°æ•¸æ“šé›†ï¼ˆ< 2000ï¼‰å»ºè­°å„ªå…ˆè€ƒæ…®å‚³çµ± ML

2. **éæ“¬åˆæ˜¯æœ€å¤§æŒ‘æˆ°**
   - éœ€è¦å¤šç¨®æ­£å‰‡åŒ–æŠ€è¡“çµ„åˆ
   - Dropout + L2 + BatchNorm ä¸‰ç®¡é½Šä¸‹
   - æ¨¡å‹ä¸æ˜¯è¶Šè¤‡é›œè¶Šå¥½

3. **ç‰¹å¾µå·¥ç¨‹å¾ˆé‡è¦**
   - å·®åˆ†ç‰¹å¾µæ•æ‰è®ŠåŒ–è¶¨å‹¢
   - çµ±è¨ˆç‰¹å¾µæä¾›ç©©å®šä¿¡æ¯
   - é ˜åŸŸçŸ¥è­˜æŒ‡å°ç‰¹å¾µé¸æ“‡

4. **é©—è­‰ç­–ç•¥è¦æ­£ç¢º**
   - æ™‚åºåˆ†å‰²ä¿æŒæ™‚é–“é †åº
   - æ¸¬è©¦é›†æ¨¡æ“¬å¯¦éš›æ‡‰ç”¨æƒ…å¢ƒ
   - é—œæ³¨æ³›åŒ–èƒ½åŠ›è€Œéè¨“ç·´ç²¾åº¦

5. **å¯¦éš›éƒ¨ç½²éœ€è¬¹æ…**
   - åŠ å…¥å®‰å…¨è£•åº¦
   - æŒçºŒç›£æ§æ€§èƒ½
   - æ•´åˆç•°å¸¸æª¢æ¸¬
   - å®šæœŸæ›´æ–°æ¨¡å‹

### æ”¹é€²æ–¹å‘

å¦‚æœæ‚¨æƒ³é€²ä¸€æ­¥æå‡æ¨¡å‹ï¼š

ğŸ”¹ **çŸ­æœŸæ”¹é€²**ï¼š
- å˜—è©¦ XGBoost å’Œ Random Forest
- å¯¦æ–½é›†æˆå­¸ç¿’ï¼ˆæ¨¡å‹å¹³å‡ï¼‰
- æ·»åŠ æ›´å¤šçµ±è¨ˆç‰¹å¾µ

ğŸ”¹ **ä¸­æœŸæ”¹é€²**ï¼š
- æ”¶é›†æ›´å¤šæ­·å²æ•¸æ“šï¼ˆç›®æ¨™ > 5000 æ¨£æœ¬ï¼‰
- å¯¦æ–½ k-fold äº¤å‰é©—è­‰
- ä½¿ç”¨ Keras Tuner è‡ªå‹•èª¿åƒ

ğŸ”¹ **é•·æœŸæ”¹é€²**ï¼š
- æ¢ç´¢ Transformer æ¶æ§‹
- çµåˆç‰©ç†æ¨¡å‹ï¼ˆæ··åˆå»ºæ¨¡ï¼‰
- é–‹ç™¼è‡ªé©æ‡‰å­¸ç¿’ç³»çµ±

### é©ç”¨å ´æ™¯

æœ¬æ–¹æ³•é©ç”¨æ–¼ä»¥ä¸‹åŒ–å·¥è£½ç¨‹ï¼š

âœ… **é©åˆçš„å ´æ™¯**ï¼š
- å…·æœ‰æ™‚åºç›¸é—œæ€§çš„è£½ç¨‹è®Šæ•¸
- æœ‰æ­·å²æ“ä½œæ•¸æ“šï¼ˆ> 1000 æ¨£æœ¬ï¼‰
- æ¸¬é‡å»¶é²æˆ–æˆæœ¬é«˜çš„å“è³ªæŒ‡æ¨™
- éœ€è¦å³æ™‚é æ¸¬çš„æ‡‰ç”¨

âš ï¸ **éœ€è¬¹æ…çš„å ´æ™¯**ï¼š
- æ•¸æ“šé‡æ¥µå°‘ï¼ˆ< 500 æ¨£æœ¬ï¼‰
- è£½ç¨‹ç¶“å¸¸å¤§å¹…è®Šå‹•
- é—œéµå®‰å…¨æ§åˆ¶ï¼ˆå»ºè­°åŠ å®‰å…¨è£•åº¦ï¼‰
- éœ€è¦åš´æ ¼å¯è§£é‡‹æ€§

### å»¶ä¼¸é–±è®€

**æ¨è–¦è³‡æºï¼š**

ğŸ“š **æ›¸ç±**ï¼š
1. Fortuna et al., "Soft Sensors for Monitoring and Control of Industrial Processes"
2. Goodfellow et al., "Deep Learning"
3. Chollet, "Deep Learning with Python"

ğŸ“„ **è«–æ–‡**ï¼š
1. Hochreiter & Schmidhuber (1997) - LSTM åŸå§‹è«–æ–‡
2. Cho et al. (2014) - GRU åŸå§‹è«–æ–‡
3. åŒ–å·¥è»Ÿæ¸¬é‡ç›¸é—œæ–‡ç»

ğŸ”— **ç·šä¸Šè³‡æº**ï¼š
- TensorFlow/Keras å®˜æ–¹æ–‡æª”
- Scikit-learn æ™‚åºé æ¸¬æ•™ç¨‹
- Kaggle æ™‚åºé æ¸¬ç«¶è³½æ¡ˆä¾‹

---

## ğŸ“ ç·´ç¿’é¡Œ

### åŸºç¤ç·´ç¿’

1. **ä¿®æ”¹ TIME_STEPS**
   - å˜—è©¦ TIME_STEPS = 10, 15, 25, 30
   - æ¯”è¼ƒä¸åŒçª—å£é•·åº¦å°æ€§èƒ½çš„å½±éŸ¿
   - åˆ†æè¨“ç·´æ¨£æœ¬æ•¸é‡çš„è®ŠåŒ–

2. **å–®å±¤vsé›™å±¤**
   - å¯¦ä½œå–®å±¤ LSTM æ¨¡å‹
   - æ¯”è¼ƒèˆ‡é›™å±¤æ¨¡å‹çš„æ€§èƒ½å·®ç•°
   - è¨˜éŒ„è¨“ç·´æ™‚é–“å’Œåƒæ•¸æ•¸é‡

3. **ä¸åŒåˆ†å‰²æ¯”ä¾‹**
   - å˜—è©¦ 80/10/10 åˆ†å‰²
   - æ¯”è¼ƒèˆ‡ 70/15/15 çš„å·®ç•°
   - è¨è«–å°æ¨¡å‹è©•ä¼°çš„å½±éŸ¿

### é€²éšç·´ç¿’

4. **XGBoost å¯¦ä½œ**
   - å°‡æ™‚åºæ•¸æ“šå±•å¹³ç‚ºç‰¹å¾µå‘é‡
   - è¨“ç·´ XGBoost å›æ­¸æ¨¡å‹
   - èˆ‡ LSTM/GRU æ€§èƒ½å°æ¯”

5. **é›†æˆå­¸ç¿’**
   - è¨“ç·´ 3-5 å€‹ LSTM æ¨¡å‹
   - å¯¦ä½œé æ¸¬å¹³å‡
   - è©•ä¼°é›†æˆæ•ˆæœ

6. **ç‰¹å¾µé‡è¦æ€§åˆ†æ**
   - ä½¿ç”¨ SHAP æˆ– Permutation Importance
   - åˆ†æå“ªäº›è¼¸å…¥è®Šæ•¸æœ€é‡è¦
   - å˜—è©¦ç§»é™¤ä¸é‡è¦ç‰¹å¾µ

### æŒ‘æˆ°ç·´ç¿’

7. **è¶…åƒæ•¸å„ªåŒ–**
   - ä½¿ç”¨ Keras Tuner æˆ– Optuna
   - æœç´¢æœ€ä½³çš„ units, dropout, L2 çµ„åˆ
   - è¨˜éŒ„æœç´¢éç¨‹å’Œçµæœ

8. **ç•°å¸¸æª¢æ¸¬æ•´åˆ**
   - å¯¦ä½œè¼¸å…¥æ•¸æ“šç•°å¸¸æª¢æ¸¬
   - æ·»åŠ é æ¸¬ä¸ç¢ºå®šæ€§ä¼°è¨ˆ
   - è¨­è¨ˆè­¦å ±ç³»çµ±

9. **å¯¦æ™‚é æ¸¬ç³»çµ±**
   - å¯¦ä½œæ»‘å‹•çª—å£å³æ™‚é æ¸¬
   - æ¨¡æ“¬è£½ç¨‹æ•¸æ“šæµ
   - è©•ä¼°æ¨è«–é€Ÿåº¦

---

## ğŸ“ åƒè€ƒè³‡æº

### æ•¸æ“šä¾†æº

- **GitHub Repository**: [Debutanizer Column Data](https://github.com/sj823774188/Debutanizer-Column-Data)
- **åŸå§‹æ–‡ç»**: Fortuna et al. (2007), Soft Sensors for Monitoring and Control of Industrial Processes

### ç›¸é—œå–®å…ƒ

- **Unit13**: æ™‚åºé æ¸¬åŸºç¤
- **Unit14**: å¼·åŒ–å­¸ç¿’æ§åˆ¶
- **Unit15**: RUL é æ¸¬

### æŠ€è¡“æ–‡æª”

- [TensorFlow LSTM Guide](https://www.tensorflow.org/guide/keras/rnn)
- [Keras Callbacks](https://keras.io/api/callbacks/)
- [Scikit-learn Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)

---

**âœ¨ æ­å–œå®Œæˆ Unit17ï¼**

æ‚¨å·²ç¶“æŒæ¡äº†ä½¿ç”¨æ·±åº¦å­¸ç¿’é€²è¡ŒåŒ–å·¥è£½ç¨‹æ™‚åºé æ¸¬çš„å®Œæ•´æµç¨‹ã€‚é€™äº›æŠ€èƒ½å¯ä»¥æ‡‰ç”¨åˆ°è¨±å¤šå¯¦éš›çš„å·¥æ¥­å ´æ™¯ä¸­ï¼ŒåŒ…æ‹¬å“è³ªé æ¸¬ã€æ•…éšœé è­¦ã€èƒ½è€—å„ªåŒ–ç­‰ã€‚

**ä¸‹ä¸€æ­¥å»ºè­°**ï¼š
- å˜—è©¦æ‡‰ç”¨åˆ°æ‚¨è‡ªå·±çš„æ•¸æ“šé›†
- æ¢ç´¢æ›´å¤šæ­£å‰‡åŒ–æŠ€è¡“
- å­¸ç¿’æ¨¡å‹éƒ¨ç½²èˆ‡ç›£æ§

ç¥å­¸ç¿’æ„‰å¿«ï¼ğŸš€

---

**èª²ç¨‹è³‡è¨Š**
- èª²ç¨‹åç¨±ï¼šAIåœ¨åŒ–å·¥ä¸Šä¹‹æ‡‰ç”¨
- èª²ç¨‹å–®å…ƒï¼šUnit17 - å»ä¸çƒ·å¡” C4 å«é‡é æ¸¬
- èª²ç¨‹è£½ä½œï¼šé€¢ç”²å¤§å­¸ åŒ–å·¥ç³» æ™ºæ…§ç¨‹åºç³»çµ±å·¥ç¨‹å¯¦é©—å®¤
- æˆèª²æ•™å¸«ï¼šèŠæ›œç¦ åŠ©ç†æ•™æˆ
- æ›´æ–°æ—¥æœŸï¼š2026-01-28

**èª²ç¨‹æˆæ¬Š [CC BY-NC-SA 4.0]**
 - æœ¬æ•™æéµå¾ª [å‰µç”¨CC å§“åæ¨™ç¤º-éå•†æ¥­æ€§-ç›¸åŒæ–¹å¼åˆ†äº« 4.0 åœ‹éš› (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh) æˆæ¬Šã€‚

---

