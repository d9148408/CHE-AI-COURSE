# Unit 13: XGBoost æ¨¡å‹ (Extreme Gradient Boosting)

**èª²ç¨‹åç¨±**ï¼šAI åœ¨åŒ–å·¥ä¸Šä¹‹æ‡‰ç”¨  
**èª²ç¨‹ä»£ç¢¼**ï¼šCHE-AI-114  
**æˆèª²æ•™å¸«**ï¼šèŠæ›œç¦ åŠ©ç†æ•™æˆ  
**å–®å…ƒä¸»é¡Œ**ï¼šXGBoost æ¨¡å‹  
**é©ç”¨å°è±¡**ï¼šåŒ–å­¸å·¥ç¨‹å­¸ç³»å­¸ç”Ÿ  

---

## å­¸ç¿’ç›®æ¨™

å®Œæˆæœ¬å–®å…ƒå¾Œï¼Œå­¸ç”Ÿå°‡èƒ½å¤ ï¼š

1. ç†è§£ XGBoost çš„æ ¸å¿ƒåŸç†èˆ‡å„ªåŒ–æ©Ÿåˆ¶
2. æŒæ¡ XGBoost ç›¸è¼ƒæ–¼å‚³çµ± GBDT çš„æ”¹é€²ä¹‹è™•
3. å®‰è£ä¸¦ä½¿ç”¨ XGBoost å¥—ä»¶é€²è¡Œå›æ­¸èˆ‡åˆ†é¡ä»»å‹™
4. ç†è§£ XGBoost çš„é—œéµè¶…åƒæ•¸åŠå…¶èª¿æ•´ç­–ç•¥
5. æ‡‰ç”¨æ­£å‰‡åŒ–æŠ€è¡“é¿å…éæ“¬åˆ
6. åˆ†æç‰¹å¾µé‡è¦æ€§èˆ‡æ¨¡å‹å¯è§£é‡‹æ€§
7. ä½¿ç”¨æ—©åœæ©Ÿåˆ¶å„ªåŒ–è¨“ç·´æ•ˆç‡
8. æ‡‰ç”¨ XGBoost æ–¼åŒ–å·¥è£½ç¨‹é æ¸¬å•é¡Œ
9. æ¯”è¼ƒ XGBoost èˆ‡ sklearn GradientBoosting çš„æ€§èƒ½å·®ç•°

---

## 1. XGBoost ç°¡ä»‹

### 1.1 ä»€éº¼æ˜¯ XGBoostï¼Ÿ

**XGBoost (Extreme Gradient Boosting)** æ˜¯ç”±é™³å¤©å¥‡åšå£«æ–¼ 2014 å¹´é–‹ç™¼çš„æ¢¯åº¦æå‡æ¨¹æ¼”ç®—æ³•å„ªåŒ–ç‰ˆæœ¬ï¼Œæ˜¯ç›®å‰æ©Ÿå™¨å­¸ç¿’ç«¶è³½èˆ‡å·¥æ¥­æ‡‰ç”¨ä¸­æœ€å—æ­¡è¿çš„æ¨¡å‹ä¹‹ä¸€ã€‚

**æ ¸å¿ƒç‰¹è‰²**ï¼š
- **æ¥µè‡´å„ªåŒ–çš„ GBDT**ï¼šåœ¨å‚³çµ± Gradient Boosting åŸºç¤ä¸Šå¤§å¹…æ”¹é€²
- **é«˜æ•ˆèƒ½**ï¼šä½¿ç”¨ä¸¦è¡ŒåŒ–ã€å¿«å–å„ªåŒ–ã€åˆ†ä½ˆå¼è¨ˆç®—
- **é«˜æº–ç¢ºåº¦**ï¼šåŠ å…¥äºŒéšå°æ•¸è³‡è¨Šèˆ‡æ­£å‰‡åŒ–
- **éˆæ´»æ€§**ï¼šæ”¯æ´è‡ªè¨‚æå¤±å‡½æ•¸ã€ç¼ºå¤±å€¼è™•ç†ã€ä¸å¹³è¡¡è³‡æ–™

**XGBoost çš„æ­·å²èˆ‡å½±éŸ¿**ï¼š
- 2014 å¹´ç™¼è¡¨è«–æ–‡ä¸¦é–‹æº
- 2015-2016 å¹´åœ¨ Kaggle ç«¶è³½ä¸­å¤§æ”¾ç•°å½©
- æˆç‚ºæ©Ÿå™¨å­¸ç¿’ç«¶è³½çš„æ¨™é…å·¥å…·
- å»£æ³›æ‡‰ç”¨æ–¼å·¥æ¥­ç•Œï¼ˆæ¨è–¦ç³»çµ±ã€å»£å‘Šé»æ“Šé æ¸¬ã€é¢¨éšªè©•ä¼°ç­‰ï¼‰
- å•Ÿç™¼äº†å¾ŒçºŒçš„ LightGBMã€CatBoost ç­‰æ¨¡å‹

### 1.2 ç‚ºä»€éº¼éœ€è¦ XGBoostï¼Ÿ

**å‚³çµ± GBDT (sklearn GradientBoosting) çš„ä¾·é™**ï¼š
- **è¨“ç·´é€Ÿåº¦æ…¢**ï¼šå¿…é ˆåºåˆ—è¨“ç·´ï¼Œç„¡æ³•å……åˆ†åˆ©ç”¨å¤šæ ¸ CPU
- **è¨˜æ†¶é«”æ¶ˆè€—å¤§**ï¼šæ²’æœ‰æœ€ä½³åŒ–è³‡æ–™çµæ§‹
- **ç¼ºä¹æ­£å‰‡åŒ–**ï¼šå®¹æ˜“éæ“¬åˆ
- **åŠŸèƒ½æœ‰é™**ï¼šä¸æ”¯æ´ç¼ºå¤±å€¼ã€é¡åˆ¥ç‰¹å¾µè™•ç†è¼ƒå¼±

**XGBoost çš„æ”¹é€²**ï¼š
- **ç³»çµ±å±¤é¢å„ªåŒ–**ï¼š
  - ä¸¦è¡ŒåŒ–ç‰¹å¾µæœå°‹ï¼ˆColumn Blockã€Cache-aware Accessï¼‰
  - åˆ†ä½ˆå¼è¨ˆç®—æ”¯æ´
  - Out-of-core è¨ˆç®—ï¼ˆè™•ç†è¶…å¤§è³‡æ–™é›†ï¼‰
  
- **æ¼”ç®—æ³•å±¤é¢å„ªåŒ–**ï¼š
  - äºŒéšæ³°å‹’å±•é–‹ï¼ˆä½¿ç”¨äºŒéšå°æ•¸ï¼‰
  - æ­£å‰‡åŒ–é …ï¼ˆL1ã€L2ï¼‰
  - Shrinkage èˆ‡ Column Subsampling
  
- **å¯¦ç”¨åŠŸèƒ½**ï¼š
  - å…§å»ºç¼ºå¤±å€¼è™•ç†
  - æ”¯æ´å¤šç¨®æå¤±å‡½æ•¸
  - æ—©åœæ©Ÿåˆ¶
  - äº¤å‰é©—è­‰

### 1.3 XGBoost vs å‚³çµ± GBDT

| ç‰¹æ€§ | sklearn GBDT | XGBoost |
|------|-------------|---------|
| **è¨“ç·´é€Ÿåº¦** | æ…¢ | å¿«ï¼ˆä¸¦è¡ŒåŒ–ï¼‰ |
| **è¨˜æ†¶é«”ä½¿ç”¨** | é«˜ | å„ªåŒ–ï¼ˆColumn Blockï¼‰ |
| **æ­£å‰‡åŒ–** | ç„¡ | L1 + L2 |
| **äºŒéšå°æ•¸** | å¦ | æ˜¯ |
| **ç¼ºå¤±å€¼è™•ç†** | éœ€é è™•ç† | å…§å»ºæ”¯æ´ |
| **åˆ†ä½ˆå¼è¨ˆç®—** | å¦ | æ”¯æ´ |
| **æ—©åœæ©Ÿåˆ¶** | åŸºæœ¬ | å®Œæ•´ |
| **äº¤å‰é©—è­‰** | æ‰‹å‹• | å…§å»º cv() |
| **ç‰¹å¾µé‡è¦æ€§** | åŸºæœ¬ | å¤šç¨®æ–¹æ³• |
| **æº–ç¢ºåº¦** | é«˜ | éå¸¸é«˜ |

### 1.4 XGBoost çš„æ‡‰ç”¨å ´æ™¯

**æœ€é©åˆçš„æƒ…å¢ƒ**ï¼š
- çµæ§‹åŒ–è¡¨æ ¼è³‡æ–™ï¼ˆéåœ–åƒã€éæ–‡å­—ï¼‰
- ç‰¹å¾µæ•¸é‡é©ä¸­ï¼ˆ10-1000 å€‹ï¼‰
- éœ€è¦é«˜æº–ç¢ºåº¦èˆ‡å¯è§£é‡‹æ€§
- è³‡æ–™é‡ä¸­åˆ°å¤§ï¼ˆ1000-10M ç­†ï¼‰

**åŒ–å·¥é ˜åŸŸæ‡‰ç”¨æ¡ˆä¾‹**ï¼š

| æ‡‰ç”¨é ˜åŸŸ | é æ¸¬ç›®æ¨™ | å„ªå‹¢ |
|---------|---------|------|
| **è£½ç¨‹å„ªåŒ–** | ç”¢ç‡ã€å“è³ª | ç²¾æº–æ•æ‰éç·šæ€§é—œä¿‚ |
| **æ•…éšœè¨ºæ–·** | è¨­å‚™ç‹€æ…‹åˆ†é¡ | è™•ç†ä¸å¹³è¡¡è³‡æ–™èƒ½åŠ›å¼· |
| **å“è³ªé æ¸¬** | ç”¢å“è¦æ ¼ | ç‰¹å¾µé‡è¦æ€§åˆ†ææ¸…æ™° |
| **èƒ½è€—é æ¸¬** | èƒ½æºæ¶ˆè€— | è™•ç†å¤§é‡æ„Ÿæ¸¬å™¨è³‡æ–™ |
| **å‰©é¤˜å£½å‘½** | RUL é æ¸¬ | æ™‚é–“åºåˆ—ç‰¹å¾µå·¥ç¨‹ä½³ |
| **åæ‡‰å‹•åŠ›å­¸** | åæ‡‰é€Ÿç‡ | è‡ªå‹•å­¸ç¿’äº¤äº’ä½œç”¨ |

---

## 2. XGBoost çš„æ•¸å­¸åŸç†

### 2.1 ç›®æ¨™å‡½æ•¸

XGBoost çš„ç›®æ¨™æ˜¯æœ€å°åŒ–ä»¥ä¸‹ç›®æ¨™å‡½æ•¸ï¼š

$$
\mathcal{L}(\phi) = \sum_{i=1}^{n} l(y_i, \hat{y}_i) + \sum_{k=1}^{K} \Omega(f_k)
$$

å…¶ä¸­ï¼š
- $l(y_i, \hat{y}_i)$ ï¼šæå¤±å‡½æ•¸ï¼ˆå¦‚ MSEã€Log Lossï¼‰
- $\Omega(f_k)$ ï¼šæ­£å‰‡åŒ–é …ï¼ˆæ§åˆ¶æ¨¡å‹è¤‡é›œåº¦ï¼‰
- $K$ ï¼šæ¨¹çš„æ•¸é‡
- $f_k$ ï¼šç¬¬ $k$ æ£µæ¨¹

**é æ¸¬å€¼**ç‚ºæ‰€æœ‰æ¨¹çš„åŠ ç¸½ï¼š

$$
\hat{y}_i = \sum_{k=1}^{K} f_k(x_i) = \hat{y}_i^{(0)} + f_1(x_i) + f_2(x_i) + \ldots + f_K(x_i)
$$

### 2.2 åŠ æ³•è¨“ç·´ç­–ç•¥

XGBoost æ¡ç”¨ **åŠ æ³•è¨“ç·´ (Additive Training)**ï¼Œæ¯æ¬¡æ·»åŠ ä¸€æ£µæ–°æ¨¹ï¼š

åœ¨ç¬¬ $t$ è¼ªè¿­ä»£ï¼Œç›®æ¨™å‡½æ•¸ç‚ºï¼š

$$
\mathcal{L}^{(t)} = \sum_{i=1}^{n} l(y_i, \hat{y}_i^{(t-1)} + f_t(x_i)) + \Omega(f_t)
$$

å…¶ä¸­ $\hat{y}_i^{(t-1)}$ æ˜¯å‰ $t-1$ æ£µæ¨¹çš„é æ¸¬å€¼ã€‚

### 2.3 äºŒéšæ³°å‹’å±•é–‹ï¼ˆXGBoost æ ¸å¿ƒå‰µæ–°ï¼‰

**å‚³çµ± GBDT** ä½¿ç”¨ä¸€éšå°æ•¸ï¼ˆæ¢¯åº¦ï¼‰ï¼š

$$
l(y_i, \hat{y}_i^{(t-1)} + f_t(x_i)) \approx l(y_i, \hat{y}_i^{(t-1)}) + g_i \cdot f_t(x_i)
$$

å…¶ä¸­ $g_i = \frac{\partial l(y_i, \hat{y})}{\partial \hat{y}} \bigg|_{\hat{y}=\hat{y}^{(t-1)}}$

**XGBoost** ä½¿ç”¨ **äºŒéšæ³°å‹’å±•é–‹**ï¼ŒåŠ å…¥äºŒéšå°æ•¸ï¼ˆHessianï¼‰ï¼š

$$
l(y_i, \hat{y}_i^{(t-1)} + f_t(x_i)) \approx l(y_i, \hat{y}_i^{(t-1)}) + g_i \cdot f_t(x_i) + \frac{1}{2} h_i \cdot f_t^2(x_i)
$$

å…¶ä¸­ï¼š
- $g_i = \frac{\partial l}{\partial \hat{y}} \bigg|_{\hat{y}=\hat{y}^{(t-1)}}$ ï¼šä¸€éšå°æ•¸ï¼ˆæ¢¯åº¦ï¼‰
- $h_i = \frac{\partial^2 l}{\partial \hat{y}^2} \bigg|_{\hat{y}=\hat{y}^{(t-1)}}$ ï¼šäºŒéšå°æ•¸ï¼ˆHessianï¼‰

**ç‚ºä»€éº¼ä½¿ç”¨äºŒéšå°æ•¸ï¼Ÿ**
- æä¾›æ›´å¤šæ›²ç‡è³‡è¨Šï¼Œæ›´ç²¾ç¢ºçš„è¿‘ä¼¼
- åŠ å¿«æ”¶æ–‚é€Ÿåº¦
- æ”¯æ´æ›´å»£æ³›çš„æå¤±å‡½æ•¸

### 2.4 ç°¡åŒ–ç›®æ¨™å‡½æ•¸

ç§»é™¤å¸¸æ•¸é …å¾Œï¼Œç¬¬ $t$ è¼ªçš„ç›®æ¨™å‡½æ•¸ç°¡åŒ–ç‚ºï¼š

$$
\tilde{\mathcal{L}}^{(t)} = \sum_{i=1}^{n} \left[ g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i) \right] + \Omega(f_t)
$$

### 2.5 æ­£å‰‡åŒ–é …

XGBoost å®šç¾©æ¨¹çš„è¤‡é›œåº¦ç‚ºï¼š

$$
\Omega(f_t) = \gamma T + \frac{1}{2} \lambda \sum_{j=1}^{T} w_j^2
$$

å…¶ä¸­ï¼š
- $T$ ï¼šæ¨¹çš„è‘‰ç¯€é»æ•¸é‡
- $w_j$ ï¼šç¬¬ $j$ å€‹è‘‰ç¯€é»çš„æ¬Šé‡ï¼ˆé æ¸¬å€¼ï¼‰
- $\gamma$ ï¼šè‘‰ç¯€é»æ‡²ç½°ä¿‚æ•¸ï¼ˆæ§åˆ¶æ¨¹çš„å¤§å°ï¼‰
- $\lambda$ ï¼šL2 æ­£å‰‡åŒ–ä¿‚æ•¸ï¼ˆæ§åˆ¶è‘‰ç¯€é»æ¬Šé‡ï¼‰

**æ­£å‰‡åŒ–çš„ä½œç”¨**ï¼š
- $\gamma$ ï¼šé¿å…æ¨¹éæ·±ï¼ˆé¡ä¼¼ `min_child_weight` ï¼‰
- $\lambda$ ï¼šé¿å…è‘‰ç¯€é»æ¬Šé‡éå¤§ï¼ˆå¹³æ»‘é æ¸¬å€¼ï¼‰

### 2.6 æ¨¹çµæ§‹å­¸ç¿’

å°æ–¼ä¸€æ£µæ¨¹ï¼Œå°‡æ¨£æœ¬åˆ†é…åˆ°å„è‘‰ç¯€é»å¾Œï¼Œå®šç¾©ï¼š
- $I_j = \{i | q(x_i) = j\}$ ï¼šåˆ†é…åˆ°è‘‰ç¯€é» $j$ çš„æ¨£æœ¬é›†åˆ
- $G_j = \sum_{i \in I_j} g_i$ ï¼šè©²è‘‰ç¯€é»çš„ä¸€éšæ¢¯åº¦å’Œ
- $H_j = \sum_{i \in I_j} h_i$ ï¼šè©²è‘‰ç¯€é»çš„äºŒéšæ¢¯åº¦å’Œ

ç›®æ¨™å‡½æ•¸å¯é‡å¯«ç‚ºï¼š

$$
\tilde{\mathcal{L}}^{(t)} = \sum_{j=1}^{T} \left[ G_j w_j + \frac{1}{2} (H_j + \lambda) w_j^2 \right] + \gamma T
$$

å° $w_j$ æ±‚åå°ä¸¦ä»¤å…¶ç‚º 0ï¼Œå¾—åˆ° **æœ€å„ªè‘‰ç¯€é»æ¬Šé‡**ï¼š

$$
w_j^* = -\frac{G_j}{H_j + \lambda}
$$

ä»£å›ç›®æ¨™å‡½æ•¸ï¼Œå¾—åˆ° **æœ€å„ªç›®æ¨™å‡½æ•¸å€¼**ï¼š

$$
\tilde{\mathcal{L}}^{(t)}(q) = -\frac{1}{2} \sum_{j=1}^{T} \frac{G_j^2}{H_j + \lambda} + \gamma T
$$

### 2.7 åˆ†è£‚å¢ç›Š

è©•ä¼°æ˜¯å¦åˆ†è£‚æŸå€‹ç¯€é»çš„ **å¢ç›Š (Gain)**ï¼š

$$
\text{Gain} = \frac{1}{2} \left[ \frac{G_L^2}{H_L + \lambda} + \frac{G_R^2}{H_R + \lambda} - \frac{(G_L + G_R)^2}{H_L + H_R + \lambda} \right] - \gamma
$$

å…¶ä¸­ï¼š
- $G_L, H_L$ ï¼šå·¦å­æ¨¹çš„æ¢¯åº¦å’Œã€Hessian å’Œ
- $G_R, H_R$ ï¼šå³å­æ¨¹çš„æ¢¯åº¦å’Œã€Hessian å’Œ
- $\gamma$ ï¼šè‘‰ç¯€é»æ‡²ç½°

**åˆ†è£‚æ±ºç­–**ï¼š
- è‹¥ $\text{Gain} > 0$ ï¼Œå‰‡åˆ†è£‚
- å¦å‰‡ï¼Œåœæ­¢åˆ†è£‚

---

## 3. XGBoost å¥—ä»¶å®‰è£

### 3.1 å®‰è£æŒ‡ä»¤

```bash
# ä½¿ç”¨ pip å®‰è£
pip install xgboost

# ä½¿ç”¨ conda å®‰è£
conda install -c conda-forge xgboost

# å®‰è£ GPU ç‰ˆæœ¬ï¼ˆéœ€è¦ CUDAï¼‰
pip install xgboost[gpu]
```

### 3.2 é©—è­‰å®‰è£

```python
import xgboost as xgb
print(f"XGBoost version: {xgb.__version__}")

# æª¢æŸ¥ GPU æ”¯æ´
print(f"GPU available: {xgb.get_config()['use_rmm']}")
```

### 3.3 ä¸»è¦ API ä»‹é¢

XGBoost æä¾›ä¸‰ç¨® APIï¼š

1. **åŸç”Ÿ API**ï¼ˆLearning APIï¼‰ï¼š
   ```python
   dtrain = xgb.DMatrix(X_train, label=y_train)
   params = {'max_depth': 3, 'eta': 0.1}
   model = xgb.train(params, dtrain, num_boost_round=100)
   ```

2. **Scikit-learn API**ï¼ˆæ¨è–¦åˆå­¸è€…ï¼‰ï¼š
   ```python
   from xgboost import XGBRegressor, XGBClassifier
   model = XGBRegressor(max_depth=3, learning_rate=0.1)
   model.fit(X_train, y_train)
   ```

3. **Dask API**ï¼ˆåˆ†ä½ˆå¼ï¼‰ï¼š
   ```python
   import xgboost as xgb
   dtrain = xgb.dask.DaskDMatrix(client, X_train, y_train)
   ```

**æœ¬èª²ç¨‹ä½¿ç”¨ Scikit-learn API**ï¼Œå› å…¶èˆ‡ sklearn ä¸€è‡´ï¼Œæ˜“æ–¼å­¸ç¿’ã€‚

---

## 4. XGBoost çš„è¶…åƒæ•¸

### 4.1 æ¨¹çµæ§‹åƒæ•¸

| åƒæ•¸ | èªªæ˜ | é è¨­å€¼ | æ¨è–¦ç¯„åœ |
|------|------|-------|---------|
| **max_depth** | æ¨¹çš„æœ€å¤§æ·±åº¦ | 6 | 3-10 |
| **min_child_weight** | å­ç¯€é»æœ€å°æ¬Šé‡å’Œ | 1 | 1-10 |
| **gamma** | åˆ†è£‚æœ€å°æå¤±æ¸›å°‘ | 0 | 0-5 |
| **max_leaves** | æœ€å¤§è‘‰ç¯€é»æ•¸ | 0ï¼ˆç„¡é™åˆ¶ï¼‰ | 2^depth |

**åƒæ•¸èªªæ˜**ï¼š

- **max_depth**ï¼š
  - æ§åˆ¶æ¨¹çš„æ·±åº¦ï¼Œé˜²æ­¢éæ“¬åˆ
  - åŒ–å·¥æ‡‰ç”¨å»ºè­°ï¼š3-6ï¼ˆæ·ºæ¨¹é€šå¸¸å·²è¶³å¤ ï¼‰
  
- **min_child_weight**ï¼š
  - å­ç¯€é»æ¨£æœ¬çš„ Hessian å’Œçš„æœ€å°å€¼
  - è¶Šå¤§è¶Šä¿å®ˆï¼ˆé˜²æ­¢éæ“¬åˆï¼‰
  - ä¸å¹³è¡¡è³‡æ–™å»ºè­°èª¿é«˜ï¼ˆ3-10ï¼‰

- **gamma**ï¼ˆåˆç¨± min_split_lossï¼‰ï¼š
  - åˆ†è£‚ç¯€é»çš„æœ€å°æå¤±æ¸›å°‘
  - è¶Šå¤§è¶Šä¿å®ˆ
  - ç¯„åœï¼š0-5

### 4.2 Boosting åƒæ•¸

| åƒæ•¸ | èªªæ˜ | é è¨­å€¼ | æ¨è–¦ç¯„åœ |
|------|------|-------|---------|
| **n_estimators** | æ¨¹çš„æ•¸é‡ | 100 | 50-1000 |
| **learning_rate** (eta) | å­¸ç¿’ç‡ | 0.3 | 0.01-0.3 |
| **subsample** | æ¨£æœ¬æ¡æ¨£æ¯”ä¾‹ | 1.0 | 0.5-1.0 |
| **colsample_bytree** | ç‰¹å¾µæ¡æ¨£æ¯”ä¾‹ï¼ˆæ¯æ£µæ¨¹ï¼‰ | 1.0 | 0.5-1.0 |
| **colsample_bylevel** | ç‰¹å¾µæ¡æ¨£æ¯”ä¾‹ï¼ˆæ¯å±¤ï¼‰ | 1.0 | 0.5-1.0 |
| **colsample_bynode** | ç‰¹å¾µæ¡æ¨£æ¯”ä¾‹ï¼ˆæ¯ç¯€é»ï¼‰ | 1.0 | 0.5-1.0 |

**åƒæ•¸èª¿æ•´ç­–ç•¥**ï¼š

```python
# ç­–ç•¥ 1ï¼šå¿«é€Ÿæ¸¬è©¦
params = {
    'n_estimators': 100,
    'learning_rate': 0.1,
    'max_depth': 5
}

# ç­–ç•¥ 2ï¼šé«˜æº–ç¢ºåº¦ï¼ˆæ¨è–¦ï¼‰
params = {
    'n_estimators': 300,
    'learning_rate': 0.05,
    'max_depth': 6,
    'subsample': 0.8,
    'colsample_bytree': 0.8
}

# ç­–ç•¥ 3ï¼šæ¥µè‡´æ€§èƒ½ï¼ˆæ…¢ï¼‰
params = {
    'n_estimators': 1000,
    'learning_rate': 0.01,
    'max_depth': 7,
    'subsample': 0.8,
    'colsample_bytree': 0.8
}
```

### 4.3 æ­£å‰‡åŒ–åƒæ•¸

| åƒæ•¸ | èªªæ˜ | é è¨­å€¼ | æ¨è–¦ç¯„åœ |
|------|------|-------|---------|
| **reg_alpha** | L1 æ­£å‰‡åŒ–ä¿‚æ•¸ | 0 | 0-1 |
| **reg_lambda** | L2 æ­£å‰‡åŒ–ä¿‚æ•¸ | 1 | 0-10 |

**æ­£å‰‡åŒ–ä½œç”¨**ï¼š
- **reg_alpha (L1)**ï¼šé¼“å‹µç¨€ç–è§£ï¼Œç‰¹å¾µé¸æ“‡
- **reg_lambda (L2)**ï¼šå¹³æ»‘æ¬Šé‡ï¼Œé˜²æ­¢éæ“¬åˆ

```python
# éæ“¬åˆåš´é‡æ™‚åŠ å¼·æ­£å‰‡åŒ–
params = {
    'reg_alpha': 0.5,   # L1
    'reg_lambda': 2.0   # L2
}
```

### 4.4 å­¸ç¿’ç›®æ¨™åƒæ•¸

| åƒæ•¸ | èªªæ˜ | å¯é¸å€¼ |
|------|------|-------|
| **objective** | æå¤±å‡½æ•¸ | 'reg:squarederror', 'reg:logistic', 'binary:logistic', 'multi:softmax' ç­‰ |
| **eval_metric** | è©•ä¼°æŒ‡æ¨™ | 'rmse', 'mae', 'logloss', 'auc', 'error' ç­‰ |

**å›æ­¸å¸¸ç”¨**ï¼š
```python
objective='reg:squarederror'  # MSE
eval_metric='rmse'
```

**äºŒå…ƒåˆ†é¡å¸¸ç”¨**ï¼š
```python
objective='binary:logistic'  # Log Loss
eval_metric='auc'
```

**å¤šå…ƒåˆ†é¡å¸¸ç”¨**ï¼š
```python
objective='multi:softmax'    # è¼¸å‡ºé¡åˆ¥
objective='multi:softprob'   # è¼¸å‡ºæ©Ÿç‡
eval_metric='mlogloss'
```

### 4.5 å…¶ä»–é‡è¦åƒæ•¸

| åƒæ•¸ | èªªæ˜ | é è¨­å€¼ | å»ºè­° |
|------|------|-------|------|
| **random_state** | éš¨æ©Ÿç¨®å­ | 0 | è¨­å®šå›ºå®šå€¼ç¢ºä¿å¯é‡ç¾ |
| **n_jobs** | ä¸¦è¡ŒåŸ·è¡Œç·’æ•¸ | 1 | -1ï¼ˆä½¿ç”¨æ‰€æœ‰æ ¸å¿ƒï¼‰ |
| **early_stopping_rounds** | æ—©åœè¼ªæ•¸ | None | 10-50 |
| **verbosity** | æ—¥èªŒç´šåˆ¥ | 1 | 0ï¼ˆéœé»˜ï¼‰ã€1ï¼ˆè­¦å‘Šï¼‰ã€2ï¼ˆè³‡è¨Šï¼‰ã€3ï¼ˆèª¿è©¦ï¼‰ |
| **tree_method** | æ¨¹æ§‹å»ºæ¼”ç®—æ³• | 'auto' | 'hist'ï¼ˆå¿«ï¼‰ã€'exact'ï¼ˆæº–ç¢ºï¼‰ |

---

## 5. XGBoost å›æ­¸æ¨¡å‹å¯¦ä½œ

### 5.1 åŸºæœ¬å›æ­¸ç¯„ä¾‹

```python
import xgboost as xgb
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# è¼‰å…¥è³‡æ–™ï¼ˆå‡è¨­å·²æº–å‚™å¥½ï¼‰
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# å»ºç«‹ XGBoost å›æ­¸æ¨¡å‹
model = XGBRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5,
    random_state=42,
    n_jobs=-1
)

# è¨“ç·´æ¨¡å‹
model.fit(X_train, y_train)

# é æ¸¬
y_pred = model.predict(X_test)

# è©•ä¼°
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)
print(f"RMSE: {rmse:.4f}")
print(f"RÂ²: {r2:.4f}")
```

### 5.2 ä½¿ç”¨é©—è­‰é›†èˆ‡æ—©åœ

```python
# å»ºç«‹é©—è­‰é›†
X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train, test_size=0.2, random_state=42
)

# è¨“ç·´æ™‚ç›£æ§é©—è­‰é›†
model = XGBRegressor(
    n_estimators=1000,
    learning_rate=0.05,
    max_depth=5,
    random_state=42,
    early_stopping_rounds=50  # 50 è¼ªç„¡æ”¹å–„å‰‡åœæ­¢
)

model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    verbose=10  # æ¯ 10 è¼ªé¡¯ç¤ºä¸€æ¬¡
)

print(f"æœ€ä½³è¿­ä»£æ¬¡æ•¸: {model.best_iteration}")
print(f"æœ€ä½³åˆ†æ•¸: {model.best_score:.4f}")
```

### 5.3 ç‰¹å¾µé‡è¦æ€§åˆ†æ

```python
import matplotlib.pyplot as plt

# æ–¹æ³• 1ï¼šä½¿ç”¨ feature_importances_
importances = model.feature_importances_
feature_names = X.columns

# æ’åºä¸¦ç¹ªåœ–
indices = np.argsort(importances)[::-1]
plt.figure(figsize=(10, 6))
plt.bar(range(len(importances)), importances[indices])
plt.xticks(range(len(importances)), feature_names[indices], rotation=45)
plt.title("Feature Importances (Gain)")
plt.tight_layout()
plt.show()

# æ–¹æ³• 2ï¼šä½¿ç”¨ plot_importance
from xgboost import plot_importance
fig, ax = plt.subplots(figsize=(10, 8))
plot_importance(model, ax=ax, max_num_features=20, importance_type='gain')
plt.tight_layout()
plt.show()
```

**importance_type é¸é …**ï¼š
- **'weight'**ï¼šç‰¹å¾µåœ¨æ‰€æœ‰æ¨¹ä¸­è¢«ä½¿ç”¨çš„æ¬¡æ•¸
- **'gain'**ï¼šç‰¹å¾µå¸¶ä¾†çš„å¹³å‡å¢ç›Š
- **'cover'**ï¼šç‰¹å¾µå½±éŸ¿çš„æ¨£æœ¬å¹³å‡æ•¸é‡

### 5.4 è¶…åƒæ•¸èª¿æ•´

```python
from sklearn.model_selection import GridSearchCV

# å®šç¾©åƒæ•¸ç¶²æ ¼
param_grid = {
    'n_estimators': [100, 300, 500],
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 5, 7],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

# å»ºç«‹åŸºç¤æ¨¡å‹
base_model = XGBRegressor(random_state=42, n_jobs=-1)

# ç¶²æ ¼æœå°‹
grid_search = GridSearchCV(
    estimator=base_model,
    param_grid=param_grid,
    cv=5,
    scoring='neg_mean_squared_error',
    verbose=1,
    n_jobs=-1
)

grid_search.fit(X_train, y_train)

print(f"æœ€ä½³åƒæ•¸: {grid_search.best_params_}")
print(f"æœ€ä½³åˆ†æ•¸: {-grid_search.best_score_:.4f}")

# ä½¿ç”¨æœ€ä½³æ¨¡å‹
best_model = grid_search.best_estimator_
```

### 5.5 æ¨¡å‹ä¿å­˜èˆ‡è¼‰å…¥

```python
import pickle
import joblib

# æ–¹æ³• 1ï¼šä½¿ç”¨ pickle
with open('xgb_model.pkl', 'wb') as f:
    pickle.dump(model, f)

with open('xgb_model.pkl', 'rb') as f:
    loaded_model = pickle.load(f)

# æ–¹æ³• 2ï¼šä½¿ç”¨ joblibï¼ˆæ¨è–¦ï¼Œæ›´å¿«ï¼‰
joblib.dump(model, 'xgb_model.joblib')
loaded_model = joblib.load('xgb_model.joblib')

# æ–¹æ³• 3ï¼šXGBoost åŸç”Ÿæ ¼å¼
model.save_model('xgb_model.json')
loaded_model = XGBRegressor()
loaded_model.load_model('xgb_model.json')
```

---

## 6. XGBoost åˆ†é¡æ¨¡å‹å¯¦ä½œ

### 6.1 äºŒå…ƒåˆ†é¡ç¯„ä¾‹

```python
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report

# å»ºç«‹åˆ†é¡æ¨¡å‹
model = XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5,
    random_state=42,
    eval_metric='auc',
    use_label_encoder=False
)

# è¨“ç·´æ¨¡å‹
model.fit(X_train, y_train)

# é æ¸¬é¡åˆ¥
y_pred = model.predict(X_test)

# é æ¸¬æ©Ÿç‡
y_pred_proba = model.predict_proba(X_test)[:, 1]

# è©•ä¼°
accuracy = accuracy_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred_proba)

print(f"Accuracy: {accuracy:.4f}")
print(f"AUC: {auc:.4f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
```

### 6.2 è™•ç†ä¸å¹³è¡¡è³‡æ–™

```python
# æ–¹æ³• 1ï¼šä½¿ç”¨ scale_pos_weight
pos_ratio = len(y_train[y_train==0]) / len(y_train[y_train==1])

model = XGBClassifier(
    scale_pos_weight=pos_ratio,  # è‡ªå‹•å¹³è¡¡
    max_depth=5,
    n_estimators=200
)

# æ–¹æ³• 2ï¼šèª¿æ•´åˆ†é¡é–¾å€¼
threshold = 0.3  # é™ä½é–¾å€¼æé«˜å¬å›ç‡
y_pred_adjusted = (y_pred_proba > threshold).astype(int)
```

### 6.3 å¤šå…ƒåˆ†é¡

```python
# å¤šå…ƒåˆ†é¡ï¼ˆ3 é¡ä»¥ä¸Šï¼‰
model = XGBClassifier(
    objective='multi:softprob',  # è¼¸å‡ºæ©Ÿç‡
    num_class=3,  # é¡åˆ¥æ•¸é‡
    n_estimators=200,
    learning_rate=0.05
)

model.fit(X_train, y_train)

# é æ¸¬
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)

# è©•ä¼°
from sklearn.metrics import accuracy_score
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
```

---

## 7. XGBoost é€²éšæŠ€å·§

### 7.1 è™•ç†ç¼ºå¤±å€¼

XGBoost å¯ä»¥è‡ªå‹•è™•ç†ç¼ºå¤±å€¼ï¼ˆNaNï¼‰ï¼Œæœƒå­¸ç¿’æœ€ä½³çš„ç¼ºå¤±å€¼åˆ†é…æ–¹å‘ã€‚

```python
# XGBoost è‡ªå‹•è™•ç† NaN
# ä¸éœ€è¦é å…ˆå¡«è£œç¼ºå¤±å€¼
model = XGBRegressor()
model.fit(X_train, y_train)  # X_train å¯åŒ…å« NaN
```

### 7.2 è‡ªè¨‚æå¤±å‡½æ•¸

```python
import numpy as np

def custom_objective(y_true, y_pred):
    """
    è‡ªè¨‚æå¤±å‡½æ•¸ç¯„ä¾‹ï¼šHuber Loss
    è¿”å›ä¸€éšå’ŒäºŒéšå°æ•¸
    """
    delta = 1.0
    residual = y_true - y_pred
    
    # ä¸€éšå°æ•¸
    grad = np.where(
        np.abs(residual) <= delta,
        -residual,
        -delta * np.sign(residual)
    )
    
    # äºŒéšå°æ•¸
    hess = np.where(
        np.abs(residual) <= delta,
        np.ones_like(residual),
        np.zeros_like(residual)
    )
    
    return grad, hess

# ä½¿ç”¨è‡ªè¨‚æå¤±å‡½æ•¸ï¼ˆåŸç”Ÿ APIï¼‰
dtrain = xgb.DMatrix(X_train, label=y_train)
params = {'max_depth': 5, 'eta': 0.1}
model = xgb.train(
    params,
    dtrain,
    num_boost_round=100,
    obj=custom_objective
)
```

### 7.3 äº¤å‰é©—è­‰

```python
# ä½¿ç”¨åŸç”Ÿ API çš„ cv å‡½æ•¸
dtrain = xgb.DMatrix(X_train, label=y_train)

params = {
    'max_depth': 5,
    'eta': 0.1,
    'objective': 'reg:squarederror',
    'eval_metric': 'rmse'
}

cv_results = xgb.cv(
    params,
    dtrain,
    num_boost_round=100,
    nfold=5,
    metrics='rmse',
    early_stopping_rounds=10,
    verbose_eval=10
)

print(f"æœ€ä½³è¼ªæ•¸: {cv_results.shape[0]}")
print(f"æœ€ä½³ RMSE: {cv_results['test-rmse-mean'].iloc[-1]:.4f}")
```

### 7.4 å­¸ç¿’æ›²ç·šåˆ†æ

```python
# è¨“ç·´æ™‚è¨˜éŒ„æ¯è¼ªçš„åˆ†æ•¸
eval_set = [(X_train, y_train), (X_test, y_test)]
eval_result = {}

model = XGBRegressor(n_estimators=200, learning_rate=0.05)
model.fit(
    X_train, y_train,
    eval_set=eval_set,
    eval_metric='rmse',
    verbose=False
)

# ç¹ªè£½å­¸ç¿’æ›²ç·š
results = model.evals_result()
epochs = len(results['validation_0']['rmse'])
x_axis = range(0, epochs)

plt.figure(figsize=(10, 6))
plt.plot(x_axis, results['validation_0']['rmse'], label='Train')
plt.plot(x_axis, results['validation_1']['rmse'], label='Test')
plt.xlabel('Boosting Round')
plt.ylabel('RMSE')
plt.title('XGBoost Learning Curve')
plt.legend()
plt.grid(True)
plt.show()
```

### 7.5 SHAP å€¼è§£é‡‹æ€§åˆ†æ

```python
import shap

# å»ºç«‹ SHAP explainer
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# ç¹ªè£½ SHAP summary plot
shap.summary_plot(shap_values, X_test, plot_type="bar")

# ç¹ªè£½å–®ä¸€æ¨£æœ¬çš„ SHAP waterfall
shap.waterfall_plot(shap.Explanation(
    values=shap_values[0], 
    base_values=explainer.expected_value, 
    data=X_test.iloc[0]
))
```

---

## 8. XGBoost vs sklearn GradientBoosting æ¯”è¼ƒ

### 8.1 æ€§èƒ½æ¯”è¼ƒ

```python
import time
from sklearn.ensemble import GradientBoostingRegressor

# sklearn GBDT
start = time.time()
sklearn_model = GradientBoostingRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5,
    random_state=42
)
sklearn_model.fit(X_train, y_train)
sklearn_time = time.time() - start
sklearn_rmse = np.sqrt(mean_squared_error(y_test, sklearn_model.predict(X_test)))

# XGBoost
start = time.time()
xgb_model = XGBRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5,
    random_state=42,
    n_jobs=-1
)
xgb_model.fit(X_train, y_train)
xgb_time = time.time() - start
xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_model.predict(X_test)))

print("æ€§èƒ½æ¯”è¼ƒï¼š")
print(f"sklearn GBDT - è¨“ç·´æ™‚é–“: {sklearn_time:.2f}s, RMSE: {sklearn_rmse:.4f}")
print(f"XGBoost      - è¨“ç·´æ™‚é–“: {xgb_time:.2f}s, RMSE: {xgb_rmse:.4f}")
print(f"é€Ÿåº¦æå‡: {sklearn_time/xgb_time:.2f}x")
```

### 8.2 ç‰¹æ€§å°æ¯”è¡¨

| ç‰¹æ€§ | sklearn GBDT | XGBoost |
|------|-------------|---------|
| **è¨“ç·´é€Ÿåº¦** | æ…¢ | å¿«ï¼ˆ2-10xï¼‰ |
| **è¨˜æ†¶é«”ä½¿ç”¨** | é«˜ | ä½ï¼ˆå„ªåŒ–ï¼‰ |
| **æº–ç¢ºåº¦** | é«˜ | æ›´é«˜ |
| **æ­£å‰‡åŒ–** | ç„¡ | L1 + L2 |
| **ç¼ºå¤±å€¼è™•ç†** | éœ€é è™•ç† | è‡ªå‹•è™•ç† |
| **ä¸¦è¡ŒåŒ–** | ç„¡ | å®Œæ•´æ”¯æ´ |
| **æ—©åœ** | åŸºæœ¬ | å®Œæ•´ |
| **API éˆæ´»æ€§** | ä¸­ | é«˜ |
| **GPU æ”¯æ´** | ç„¡ | æœ‰ |
| **åˆ†ä½ˆå¼è¨ˆç®—** | ç„¡ | æœ‰ |

### 8.3 ä½¿ç”¨å»ºè­°

**é¸æ“‡ sklearn GradientBoosting**ï¼š
- è³‡æ–™é‡å°ï¼ˆ< 10K ç­†ï¼‰
- ä¸åœ¨æ„è¨“ç·´é€Ÿåº¦
- ä¸éœ€è¦ç‰¹æ®ŠåŠŸèƒ½
- å¸Œæœ›ç¨‹å¼ç¢¼ç°¡å–®

**é¸æ“‡ XGBoost**ï¼š
- è¿½æ±‚æœ€é«˜æº–ç¢ºåº¦
- è³‡æ–™é‡å¤§ï¼ˆ> 10K ç­†ï¼‰
- æœ‰ç¼ºå¤±å€¼éœ€è™•ç†
- éœ€è¦å¿«é€Ÿè¨“ç·´
- éœ€è¦é€²éšåŠŸèƒ½ï¼ˆè‡ªè¨‚æå¤±ã€GPU åŠ é€Ÿç­‰ï¼‰

---

## 9. åŒ–å·¥é ˜åŸŸæ‡‰ç”¨æ¡ˆä¾‹

### 9.1 æ¡ˆä¾‹ä¸€ï¼šåæ‡‰å™¨ç”¢ç‡é æ¸¬

**èƒŒæ™¯**ï¼šé æ¸¬åŒ–å­¸åæ‡‰å™¨çš„ç”¢ç‰©ç”¢ç‡

**ç‰¹å¾µ**ï¼š
- æº«åº¦ (Temperature)
- å£“åŠ› (Pressure)
- å‚¬åŒ–åŠ‘ç”¨é‡ (Catalyst Amount)
- åæ‡‰æ™‚é–“ (Reaction Time)
- åŸæ–™é…æ¯” (Feed Ratio)

```python
# å»ºç«‹æ¨¡å‹
model = XGBRegressor(
    n_estimators=300,
    learning_rate=0.05,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

# è¨“ç·´
model.fit(X_train, y_train)

# ç‰¹å¾µé‡è¦æ€§
plot_importance(model, importance_type='gain', max_num_features=10)

# çµæœï¼šç™¼ç¾æº«åº¦å’Œå‚¬åŒ–åŠ‘ç”¨é‡æœ€é‡è¦
```

### 9.2 æ¡ˆä¾‹äºŒï¼šè¨­å‚™æ•…éšœåˆ†é¡

**èƒŒæ™¯**ï¼šé æ¸¬è¨­å‚™æ˜¯å¦æœƒåœ¨æœªä¾† 7 å¤©å…§æ•…éšœ

**é¡åˆ¥**ï¼š
- 0: æ­£å¸¸
- 1: æ•…éšœ

```python
# è™•ç†ä¸å¹³è¡¡è³‡æ–™
pos_ratio = (y_train == 0).sum() / (y_train == 1).sum()

model = XGBClassifier(
    scale_pos_weight=pos_ratio,
    n_estimators=500,
    learning_rate=0.03,
    max_depth=5,
    subsample=0.8,
    eval_metric='auc'
)

# ä½¿ç”¨æ—©åœ
model.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    early_stopping_rounds=50,
    verbose=10
)

# è©•ä¼°
y_pred_proba = model.predict_proba(X_test)[:, 1]
auc = roc_auc_score(y_test, y_pred_proba)
print(f"AUC: {auc:.4f}")
```

### 9.3 æ¡ˆä¾‹ä¸‰ï¼šè’¸é¤¾å¡”æº«åº¦æ§åˆ¶

**èƒŒæ™¯**ï¼šé æ¸¬è’¸é¤¾å¡”é ‚æº«åº¦

**æŒ‘æˆ°**ï¼š
- æ™‚é–“åºåˆ—è³‡æ–™
- éœ€è¦æ»¯å¾Œç‰¹å¾µ

```python
# å»ºç«‹æ»¯å¾Œç‰¹å¾µ
def create_lag_features(df, target_col, lags=[1, 2, 3, 5, 10]):
    for lag in lags:
        df[f'{target_col}_lag_{lag}'] = df[target_col].shift(lag)
    return df.dropna()

# ç‰¹å¾µå·¥ç¨‹
df = create_lag_features(df, 'temperature')

# è¨“ç·´æ¨¡å‹
model = XGBRegressor(
    n_estimators=200,
    learning_rate=0.05,
    max_depth=4,
    subsample=0.9
)

model.fit(X_train, y_train)

# æ™‚é–“åºåˆ—é æ¸¬
y_pred = model.predict(X_test)
```

---

## 10. å¯¦æˆ°æ¡ˆä¾‹åˆ†æ

æœ¬ç« ç¯€åŸºæ–¼å…©å€‹å®Œæ•´çš„ Notebook å¯¦æˆ°æ¼”ç·´ï¼Œå±•ç¤º XGBoost åœ¨å›æ­¸èˆ‡åˆ†é¡ä»»å‹™ä¸­çš„æ‡‰ç”¨ã€‚

---

### 10.1 æ¡ˆä¾‹ä¸€ï¼šåŒ–å·¥è¨­å‚™æ•…éšœè¨ºæ–·ï¼ˆåˆ†é¡ä»»å‹™ï¼‰

#### 10.1.1 æ¡ˆä¾‹èƒŒæ™¯

**å•é¡Œæè¿°**ï¼šåŒ–å·¥è¨­å‚™æ•…éšœå¤šåˆ†é¡è¨ºæ–·ç³»çµ±  
**æ•¸æ“šè¦æ¨¡**ï¼š150,000 å€‹æ™‚é–“åºåˆ—æ•¸æ“šé»  
**ç‰¹å¾µæ•¸é‡**ï¼š30 å€‹ï¼ˆ15 å‚³æ„Ÿå™¨ + 8 è¨­å‚™åƒæ•¸ + 7 è¡ç”Ÿç‰¹å¾µï¼‰  
**é æ¸¬ç›®æ¨™**ï¼š7 ç¨®è¨­å‚™ç‹€æ…‹ï¼ˆæ¥µåº¦ä¸å¹³è¡¡ï¼‰

**é¡åˆ¥åˆ†å¸ƒï¼ˆæ¨¡æ“¬çœŸå¯¦å ´æ™¯ï¼‰**ï¼š
- 0: æ­£å¸¸é‹è¡Œ (70.0% - 105,000)
- 1: è¼•å¾®ç£¨æ (15.0% - 22,500)
- 2: æº«åº¦ç•°å¸¸ (5.0% - 7,500)
- 3: å£“åŠ›æ³¢å‹• (4.0% - 6,000)
- 4: æ³„æ¼è­¦å‘Š (3.0% - 4,500)
- 5: åš´é‡æ•…éšœ (2.0% - 3,000)
- 6: ç·Šæ€¥åœæ©Ÿ (1.0% - 1,500) âš ï¸

**æ•¸æ“šç‰¹å¾µé¡å‹**ï¼š

1. **å‚³æ„Ÿå™¨æ•¸æ“šï¼ˆ15å€‹ï¼‰**ï¼š
   - æº«åº¦ï¼šå…¥å£/å‡ºå£/å£é¢/ç’°å¢ƒ
   - å£“åŠ›ï¼šé€²æ–™/å‡ºæ–™/å·®å£“
   - æµé‡ï¼šé€²æ–™/ç”¢å“/å¾ªç’°
   - æŒ¯å‹•ï¼šX/Y/Zè»¸
   - è²éŸ³ã€é›»æµæ¶ˆè€—

2. **è¨­å‚™åƒæ•¸ï¼ˆ8å€‹ï¼‰**ï¼š
   - é‹è¡Œæ™‚é–“ã€å•Ÿåœæ¬¡æ•¸ã€ç¶­è­·é–“éš”
   - è¨­å‚™å¹´é½¡ã€å‹è™Ÿã€æ“ä½œå“¡ã€ç­æ¬¡
   - è² è¼‰ç‡

3. **è¡ç”Ÿç‰¹å¾µï¼ˆ7å€‹ï¼‰**ï¼š
   - æº«å·®ã€å£“é™æ¯”ã€æŒ¯å‹•å¹…åº¦
   - æ»¾å‹•æ¨™æº–å·®ã€æ™‚é–“ç‰¹å¾µ
   - å¥åº·æŒ‡æ•¸ã€ç•°å¸¸è¨ˆæ•¸

#### 10.1.2 æ•¸æ“šé è™•ç†ç­–ç•¥

**è™•ç†é¡åˆ¥ä¸å¹³è¡¡çš„æ–¹æ³•**ï¼š
```python
# 1. è¨ˆç®—æ¨£æœ¬æ¬Šé‡
sample_weights = compute_sample_weight('balanced', y_train)

# 2. åœ¨ XGBoost ä¸­ä½¿ç”¨
xgb_model = XGBClassifier(
    n_estimators=300,
    max_depth=8,
    learning_rate=0.05,
    tree_method='gpu_hist'  # GPU åŠ é€Ÿ
)

xgb_model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    sample_weight=sample_weights,  # é—œéµï¼šè™•ç†ä¸å¹³è¡¡
    early_stopping_rounds=30
)
```

**æ•¸æ“šåˆ†å‰²ç­–ç•¥**ï¼š
- è¨“ç·´é›†ï¼š60% (90,000 ç­†)
- é©—è­‰é›†ï¼š20% (30,000 ç­†)
- æ¸¬è©¦é›†ï¼š20% (30,000 ç­†)
- ä½¿ç”¨åˆ†å±¤æŠ½æ¨£ï¼ˆstratifyï¼‰ä¿æŒé¡åˆ¥æ¯”ä¾‹

#### 10.1.3 æ¨¡å‹æ€§èƒ½æ¯”è¼ƒ

è¨“ç·´äº† 6 å€‹ä¸åŒæ¨¡å‹é€²è¡Œå°æ¯”ï¼š

| æ¨¡å‹ | Accuracy | F1 (Macro) | F1 (Weighted) | è¨“ç·´æ™‚é–“ (ç§’) |
|------|----------|------------|---------------|--------------|
| **XGBoost (GPU)** | **0.8412** | **0.7577** | **0.8469** | **9.50** âš¡ |
| XGBoost (CPU) | 0.8400 | 0.7528 | 0.8457 | 13.21 |
| Random Forest | 0.8375 | 0.7409 | 0.8423 | 3.64 |
| sklearn GBDT | 0.8255 | 0.7515 | 0.8348 | 578.25 ğŸ¢ |
| Logistic Regression | 0.8051 | 0.7196 | 0.8159 | 13.54 |
| SVM | 0.7938 | 0.7295 | 0.8090 | 123.24 |

![æ¨¡å‹æ€§èƒ½æ¯”è¼ƒ](../Unit13/outputs/P3_Unit13_XGBoost_Classification_Advanced/figs/model_comparison.png)

#### 10.1.4 é—œéµç™¼ç¾èˆ‡åˆ†æ

**1ï¸âƒ£ XGBoost å„ªå‹¢æ˜é¡¯ä½†ä¸å£“å€’æ€§**

**æ€§èƒ½åˆ†æ**ï¼š
- XGBoost (GPU) æœ€ä½³ï¼Œä½†åƒ…æ¯” Random Forest é«˜ **0.46%**
- åŸå› åˆ†æï¼š
  - âœ… **æ•¸æ“šè¦æ¨¡é©ä¸­**ï¼ˆ150Kï¼‰ï¼ŒRF å·²èƒ½å……åˆ†å­¸ç¿’
  - âœ… **ç‰¹å¾µå·¥ç¨‹å……åˆ†**ï¼šè¡ç”Ÿç‰¹å¾µå·²æ•æ‰é—œéµæ¨¡å¼
  - âœ… **é¡åˆ¥åˆ†é›¢åº¦é«˜**ï¼šä¸åŒæ•…éšœé¡å‹ç‰¹å¾µå·®ç•°æ˜é¡¯
  - âœ… **RF è¶…åƒæ•¸å„ªåŒ–è‰¯å¥½**ï¼šmax_depth=15, n_estimators=100

**ç‚ºä»€éº¼ XGBoost æ²’æœ‰å£“å€’æ€§å„ªå‹¢ï¼Ÿ**

| åŸå›  | èªªæ˜ |
|------|------|
| **å•é¡Œä¸å¤ å›°é›£** | é¡åˆ¥é‚Šç•Œç›¸å°æ¸…æ™°ï¼ŒRF å·²è¶³å¤  |
| **æ•¸æ“šé‡ç´šé©ä¸­** | 150K æ¨£æœ¬ï¼ŒXGBoost å„ªå‹¢æœªå……åˆ†ç™¼æ® |
| **ç‰¹å¾µå·¥ç¨‹æˆåŠŸ** | Health_Index, Vibration_Magnitude ç­‰è¡ç”Ÿç‰¹å¾µå·²éå¸¸æœ‰æ•ˆ |
| **RF é…ç½®å„ªè‰¯** | æ·±åº¦èˆ‡æ¨¹æ•¸é‡è¨­ç½®åˆç† |
| **XGBoost æœªæ¥µè‡´èª¿åƒ** | ä»æœ‰è¶…åƒæ•¸å„ªåŒ–ç©ºé–“ |

**2ï¸âƒ£ è¨“ç·´é€Ÿåº¦å°æ¯”**

```
sklearn GBDT:  578.25 ç§’ï¼ˆæ¥µæ…¢ï¼‰
XGBoost (CPU): 13.21 ç§’ (43.8x åŠ é€Ÿ)
XGBoost (GPU): 9.50 ç§’ (60.9x åŠ é€Ÿ)
Random Forest: 3.64 ç§’ï¼ˆæœ€å¿«ï¼Œä½†æ€§èƒ½ç¨å·®ï¼‰
```

**GPU åŠ é€Ÿæ•ˆæœ**ï¼š1.39xï¼ˆCPU â†’ GPUï¼‰
- åœ¨ 150K æ¨£æœ¬è¦æ¨¡ä¸‹ï¼ŒGPU å„ªå‹¢ä¸æ˜é¡¯
- å»ºè­° > 500K æ¨£æœ¬æ™‚ä½¿ç”¨ GPU

**3ï¸âƒ£ é¡åˆ¥ä¸å¹³è¡¡è™•ç†æ•ˆæœ**

**F1 Score åˆ†æ**ï¼š
- **F1 (Macro)**: 0.7577ï¼ˆæœªåŠ æ¬Šå¹³å‡ï¼Œåæ˜ å°‘æ•¸é¡åˆ¥æ€§èƒ½ï¼‰
- **F1 (Weighted)**: 0.8469ï¼ˆåŠ æ¬Šå¹³å‡ï¼Œåæ˜ æ•´é«”æ€§èƒ½ï¼‰
- **å·®è·**: 8.92%

èªªæ˜æ¨¡å‹å°å°‘æ•¸é¡åˆ¥ï¼ˆç·Šæ€¥åœæ©Ÿ 1%ï¼‰ä»æœ‰æŒ‘æˆ°ï¼Œä½† `sample_weight` æœ‰æ•ˆæ”¹å–„ã€‚

**æ”¹é€²å»ºè­°**ï¼š
```python
# é‡å°æ¥µå°‘æ•¸é¡åˆ¥é€²ä¸€æ­¥å„ªåŒ–
xgb_model = XGBClassifier(
    n_estimators=500,  # å¢åŠ æ¨¹æ•¸é‡
    max_depth=10,      # å¢åŠ æ·±åº¦
    learning_rate=0.03, # é™ä½å­¸ç¿’ç‡
    subsample=0.7,     # å¢åŠ éš¨æ©Ÿæ€§
    colsample_bytree=0.7,
    reg_alpha=0.5,     # å¢åŠ  L1 æ­£å‰‡åŒ–ï¼ˆç‰¹å¾µé¸æ“‡ï¼‰
    reg_lambda=2.0,    # å¢åŠ  L2 æ­£å‰‡åŒ–ï¼ˆå¹³æ»‘ï¼‰
    tree_method='gpu_hist'
)
```

#### 10.1.5 æ··æ·†çŸ©é™£åˆ†æ

**è§€å¯Ÿé‡é»**ï¼š
- æ­£å¸¸é‹è¡Œï¼ˆClass 0ï¼‰ï¼šæº–ç¢ºç‡æœ€é«˜
- ç·Šæ€¥åœæ©Ÿï¼ˆClass 6ï¼‰ï¼šå¬å›ç‡è¼ƒä½ï¼ˆæ¨£æœ¬æ¥µå°‘ï¼‰
- ä¸»è¦æ··æ·†ï¼šç›¸é„°åš´é‡ç¨‹åº¦çš„é¡åˆ¥ï¼ˆå¦‚ Class 5 vs 6ï¼‰

#### 10.1.6 ç‰¹å¾µé‡è¦æ€§åˆ†æ

**Top 10 æœ€é‡è¦ç‰¹å¾µ**ï¼š

| æ’å | ç‰¹å¾µåç¨± | é‡è¦æ€§åˆ†æ•¸ | é¡å‹ |
|------|---------|-----------|------|
| 1 | Health_Index | 0.185 | è¡ç”Ÿç‰¹å¾µ |
| 2 | Vibration_Magnitude | 0.142 | è¡ç”Ÿç‰¹å¾µ |
| 3 | Temp_Diff_IO | 0.098 | è¡ç”Ÿç‰¹å¾µ |
| 4 | Days_Since_Maintenance | 0.087 | è¨­å‚™åƒæ•¸ |
| 5 | Operating_Hours | 0.076 | è¨­å‚™åƒæ•¸ |
| 6 | Abnormal_Count | 0.065 | è¡ç”Ÿç‰¹å¾µ |
| 7 | Temp_Inlet | 0.054 | å‚³æ„Ÿå™¨ |
| 8 | Noise_Level | 0.048 | å‚³æ„Ÿå™¨ |
| 9 | Current | 0.041 | å‚³æ„Ÿå™¨ |
| 10 | Pressure_Drop_Ratio | 0.039 | è¡ç”Ÿç‰¹å¾µ |

**é—œéµæ´å¯Ÿ**ï¼š
- âœ… **è¡ç”Ÿç‰¹å¾µæœ€é‡è¦**ï¼šå‰ 3 åéƒ½æ˜¯è¡ç”Ÿç‰¹å¾µ
- âœ… **ç¶œåˆæŒ‡æ¨™æœ€æœ‰æ•ˆ**ï¼šHealth_Index æ•´åˆå¤šå€‹åŸå§‹ç‰¹å¾µ
- âœ… **ç¶­è­·æ­·å²é—œéµ**ï¼šDays_Since_Maintenance æ’åç¬¬ 4
- âœ… **åŸå§‹å‚³æ„Ÿå™¨æ¬¡è¦**ï¼šéœ€é€éè¡ç”Ÿç‰¹å¾µæ‰èƒ½ç™¼æ®åƒ¹å€¼

**å¯¦å‹™å»ºè­°**ï¼š
1. **æŒçºŒç›£æ§ Health_Index**ï¼šä½œç‚ºé è­¦æ ¸å¿ƒæŒ‡æ¨™
2. **å¼·åŒ–ç¶­è­·è¨ˆåŠƒ**ï¼šä¾ Days_Since_Maintenance é é˜²æ€§ç¶­è­·
3. **æŒ¯å‹•ç›£æ¸¬ç³»çµ±**ï¼šVibration_Magnitude æ˜¯é—œéµæ•…éšœå‰å…†
4. **æº«åº¦å·®ç•°ç›£æ§**ï¼šTemp_Diff_IO ç•°å¸¸è¡¨ç¤ºç†±äº¤æ›å•é¡Œ

#### 10.1.7 æ•¸æ“šè¦æ¨¡å½±éŸ¿åˆ†æ

è¨“ç·´ä¸åŒæ¨£æœ¬æ•¸é‡çš„æ¨¡å‹ï¼š

| æ¨£æœ¬æ•¸ | Accuracy | F1 (Weighted) | è¨“ç·´æ™‚é–“ (ç§’) |
|--------|----------|---------------|--------------|
| 5,000 | 0.7823 | 0.7892 | 0.68 |
| 10,000 | 0.8102 | 0.8156 | 1.25 |
| 30,000 | 0.8289 | 0.8378 | 3.45 |
| 50,000 | 0.8356 | 0.8421 | 5.82 |
| 85,000 | 0.8398 | 0.8448 | 9.17 |

**è§€å¯Ÿ**ï¼š
- 5K â†’ 85Kï¼šF1 Score æå‡ **5.56%**
- è¨“ç·´æ™‚é–“èˆ‡æ¨£æœ¬æ•¸**ç·šæ€§å¢é•·**
- 50K å¾Œæ€§èƒ½æå‡è¶¨ç·©ï¼ˆé‚Šéš›æ•ˆç›Šéæ¸›ï¼‰

#### 10.1.8 å¯¦å‹™éƒ¨ç½²å»ºè­°

**1. æ¨¡å‹é¸æ“‡**ï¼š
- è‹¥è¿½æ±‚æ¥µè‡´æ€§èƒ½ï¼šXGBoost
- è‹¥è¿½æ±‚æ€§åƒ¹æ¯”ï¼šRandom Forestï¼ˆæ€§èƒ½æ¥è¿‘ï¼Œè¨“ç·´è¶…å¿«ï¼‰
- è‹¥è³‡æºå—é™ï¼šLogistic Regressionï¼ˆ13ç§’è¨“ç·´ï¼Œ81.6% F1ï¼‰

**2. ç›£æ§ç­–ç•¥**ï¼š
```python
# å³æ™‚ç›£æ§ Top 5 ç‰¹å¾µ
monitor_features = [
    'Health_Index',
    'Vibration_Magnitude',
    'Temp_Diff_IO',
    'Days_Since_Maintenance',
    'Abnormal_Count'
]

# è¨­å®šè­¦å ±é–¾å€¼
threshold = {
    'Health_Index': 60,  # < 60 ç™¼å‡ºè­¦å‘Š
    'Vibration_Magnitude': 8,  # > 8 ç•°å¸¸
    'Days_Since_Maintenance': 150  # > 150 å¤©éœ€ç¶­è­·
}
```

**3. æŒçºŒæ”¹é€²**ï¼š
- æ”¶é›†æ›´å¤šç·Šæ€¥åœæ©Ÿæ¡ˆä¾‹ï¼ˆç•¶å‰åƒ… 1500 ç­†ï¼‰
- åŠ å…¥æ™‚åºç‰¹å¾µï¼ˆæ»¾å‹•çª—å£çµ±è¨ˆï¼‰
- ä½¿ç”¨ SMOTE æˆ– ADASYN åˆæˆå°‘æ•¸é¡åˆ¥æ¨£æœ¬

---

### 10.2 æ¡ˆä¾‹äºŒï¼šåŒ–å·¥åæ‡‰å™¨ç”¢ç‡é æ¸¬ï¼ˆå›æ­¸ä»»å‹™ï¼‰

#### 10.2.1 æ¡ˆä¾‹èƒŒæ™¯

**å•é¡Œæè¿°**ï¼šé æ¸¬åŒ–å­¸åæ‡‰å™¨çš„ç”¢ç‰©ç”¢ç‡ï¼ˆåŒ–å·¥è£½ç¨‹å„ªåŒ–æ ¸å¿ƒä»»å‹™ï¼‰  
**æ•¸æ“šè¦æ¨¡**ï¼š100,000 ç­†æ­·å²ç”Ÿç”¢æ•¸æ“š  
**ç‰¹å¾µæ•¸é‡**ï¼š20 å€‹ï¼ˆ9 æ“ä½œåƒæ•¸ + 4 æ™‚é–“ç‰¹å¾µ + 7 è¡ç”Ÿç‰¹å¾µï¼‰  
**é æ¸¬ç›®æ¨™**ï¼šç”¢ç‡ (Yield, %) - é€£çºŒå€¼å›æ­¸  
**æŒ‘æˆ°**ï¼šå« 5% ç¼ºå¤±å€¼ã€10% ç•°å¸¸å€¼ã€å­£ç¯€æ€§æ³¢å‹•

**ç‰¹å¾µåˆ†é¡**ï¼š

1. **é€£çºŒæ“ä½œåƒæ•¸ï¼ˆ6å€‹ï¼‰**ï¼š
   - Feed_Flow (åŸæ–™æµé‡, kg/h)
   - Feed_Temp (é€²æ–™æº«åº¦, Â°C)
   - Pressure (åæ‡‰å£“åŠ›, bar)
   - Steam_Flow (è’¸æ±½æµé‡, kg/h)
   - Cooling_Water (å†·å»æ°´æµé‡, L/min)
   - Load_Factor (è² è¼‰å› å­, 0-1)

2. **é¡åˆ¥æ“ä½œåƒæ•¸ï¼ˆ3å€‹ï¼‰**ï¼š
   - Equipment_Status (è¨­å‚™ç‹€æ…‹ï¼šGood/Warn/Critical)
   - Operation_Mode (æ“ä½œæ¨¡å¼ï¼šNormal/Intensive/Maintenance)
   - Composition (åŸæ–™çµ„æˆï¼šA/B/C/D)

3. **æ™‚é–“ç‰¹å¾µï¼ˆ4å€‹ï¼‰**ï¼š
   - Operating_Hours (ç´¯è¨ˆé‹è¡Œæ™‚æ•¸)
   - Season (å­£ç¯€ï¼šSpring/Summer/Fall/Winter)
   - Month (æœˆä»½ï¼š1-12)
   - Ambient_Temp (ç’°å¢ƒæº«åº¦, Â°C)

4. **è¡ç”Ÿç‰¹å¾µï¼ˆ7å€‹ï¼‰**ï¼š
   - Temp_Squared (æº«åº¦å¹³æ–¹)
   - Temp_Flow_Interaction (æº«åº¦Ã—æµé‡äº¤äº’ä½œç”¨)
   - Pressure_Composition_Interaction (å£“åŠ›Ã—çµ„æˆäº¤äº’ä½œç”¨)
   - Flow_Cubed (æµé‡ä¸‰æ¬¡æ–¹ï¼Œæ•æ‰éç·šæ€§)
   - Equipment_Status_Encoded (è¨­å‚™ç‹€æ…‹ç·¨ç¢¼)
   - Operation_Mode_Encoded (æ“ä½œæ¨¡å¼ç·¨ç¢¼)
   - Composition_Encoded (çµ„æˆç·¨ç¢¼)

#### 10.2.2 æ•¸æ“šæ¢ç´¢æ€§åˆ†æ

**æ•¸æ“šåˆ†å¸ƒç‰¹å¾µ**ï¼š
- ç”¢ç‡ç¯„åœï¼š60-95%ï¼ˆå‡å€¼ï¼š78.2%ï¼Œæ¨™æº–å·®ï¼š6.8%ï¼‰
- å­˜åœ¨æ˜é¡¯çš„å­£ç¯€æ€§æ•ˆæ‡‰ï¼ˆå¤å­£ç”¢ç‡åä½ï¼‰
- è¨­å‚™ç‹€æ…‹ Good (70%) vs Warn (20%) vs Critical (10%)
- 5% ç¼ºå¤±å€¼é›†ä¸­åœ¨ Feed_Flowã€Pressureã€Cooling_Water

**ç›¸é—œæ€§åˆ†æï¼ˆTop 5ï¼‰**ï¼š
1. Pressure - Yield: **r = 0.68** â­ æœ€å¼·æ­£ç›¸é—œ
2. Feed_Temp - Yield: **r = 0.54**
3. Ambient_Temp - Yield: **r = -0.42** ï¼ˆè² ç›¸é—œï¼Œå¤å­£ç”¢ç‡ä½ï¼‰
4. Equipment_Status - Yield: **r = 0.51**
5. Load_Factor - Yield: **r = 0.39**

#### 10.2.3 XGBoost æ¨¡å‹è¨“ç·´ç­–ç•¥

**æ•¸æ“šåˆ‡åˆ†**ï¼š
- è¨“ç·´é›†ï¼š60,000 ç­† (60%)
- é©—è­‰é›†ï¼š20,000 ç­† (20%) - ç”¨æ–¼æ—©åœèˆ‡è¶…åƒæ•¸èª¿æ•´
- æ¸¬è©¦é›†ï¼š20,000 ç­† (20%) - æœ€çµ‚æ€§èƒ½è©•ä¼°

**è™•ç†ç¼ºå¤±å€¼**ï¼š
```python
# XGBoost è‡ªå‹•è™•ç†ç¼ºå¤±å€¼ï¼ˆç„¡éœ€æ‰‹å‹•å¡«è£œï¼‰
# æ¨¡å‹æœƒå­¸ç¿’å°‡ç¼ºå¤±å€¼åˆ†é…åˆ°å·¦æˆ–å³å­æ¨¹
xgb_model = XGBRegressor()
xgb_model.fit(X_train, y_train)  # X_train å¯åŒ…å« NaN
```

**è™•ç†é¡åˆ¥è®Šæ•¸**ï¼š
```python
from sklearn.preprocessing import LabelEncoder

# å° 3 å€‹é¡åˆ¥ç‰¹å¾µé€²è¡Œç·¨ç¢¼
label_encoders = {}
categorical_cols = ['Equipment_Status', 'Operation_Mode', 'Composition']

for col in categorical_cols:
    le = LabelEncoder()
    X_train[col] = le.fit_transform(X_train[col])
    X_test[col] = le.transform(X_test[col])
    label_encoders[col] = le
```

#### 10.2.4 æ¨¡å‹æ€§èƒ½æ¯”è¼ƒ

è¨“ç·´äº† 5 å€‹ä¸åŒæ¨¡å‹é€²è¡Œå°æ¯”ï¼ˆ100,000 ç­†è¨“ç·´æ•¸æ“šï¼‰ï¼š

| æ¨¡å‹ | è¨“ç·´æ™‚é–“ | Test RMSE | Test MAE | Test RÂ² | é€Ÿåº¦æ¯”è¼ƒ | RMSE æ”¹å–„ |
|------|---------|-----------|----------|---------|---------|----------|
| **Linear Regression** | 0.09s | 11.44 | 9.06 | 0.9819 | 1.0x (åŸºæº–) | 0.0% |
| **Random Forest** | 8.49s | 16.46 | 13.00 | 0.9625 | 98.4x âš ï¸ | **-43.9%** â¬‡ï¸ |
| **sklearn GBDT** | 95.16s | 10.84 | 8.56 | 0.9837 | 1102x âš ï¸ | +5.2% |
| **XGBoost (CPU)** | **5.17s** | **9.69** | **7.65** | **0.9870** | 59.9x | **+15.3%** â­ |
| **XGBoost (GPU)** | **3.34s** | **9.70** | **7.64** | **0.9870** | 38.7x ğŸš€ | **+15.2%** â­ |

**é—œéµç™¼ç¾**ï¼š

1. **XGBoost é€Ÿåº¦å„ªå‹¢æ˜é¡¯**ï¼š
   - æ¯” sklearn GBDT å¿« **18.4 å€**ï¼ˆCPUï¼‰å’Œ **28.5 å€**ï¼ˆGPUï¼‰
   - æ¯” Random Forest å¿« 1.64 å€ä¸”æº–ç¢ºåº¦é«˜ 2.45%

2. **GPU åŠ é€Ÿæ•ˆæœ**ï¼š
   - GPU vs CPU: **1.55x** åŠ é€Ÿï¼ˆ100K æ•¸æ“šï¼‰
   - å»ºè­°ï¼šæ•¸æ“šé‡ > 500K æ™‚ GPU å„ªå‹¢æ›´æ˜é¡¯

3. **é æ¸¬æº–ç¢ºåº¦**ï¼š
   - XGBoost ç›¸æ¯” Linear Regression RMSE æ”¹å–„ **15.3%**
   - XGBoost ç›¸æ¯” sklearn GBDT RMSE æ”¹å–„ **10.6%**
   - RÂ² é”åˆ° **0.9870**ï¼ˆè§£é‡‹ 98.7% è®Šç•°ï¼‰

4. **Random Forest è¡¨ç¾ç•°å¸¸**ï¼š
   - è¨“ç·´æ™‚é–“é•·ï¼ˆ8.49sï¼‰ä½†æ€§èƒ½æœ€å·®ï¼ˆRMSE 16.46ï¼‰
   - åŸå› ï¼šé«˜ç¶­æ•¸æ“šï¼ˆ20 ç‰¹å¾µï¼‰+ è¤‡é›œéç·šæ€§é—œä¿‚ï¼ŒRF ä¸è¶³ä»¥æ•æ‰

#### 10.2.5 ç‰¹å¾µé‡è¦æ€§åˆ†æ

**XGBoost (CPU) ç‰¹å¾µé‡è¦æ€§ (Gain) - Top 10**ï¼š

| æ’å | ç‰¹å¾µåç¨± | é‡è¦æ€§ | è§£é‡‹ |
|------|---------|--------|------|
| 1 | **Flow_Cubed** | 0.6019 | æµé‡ä¸‰æ¬¡æ–¹ï¼ˆéç·šæ€§æ•ˆæ‡‰ï¼‰â­ |
| 2 | Operating_Hours | 0.1158 | è¨­å‚™ç´¯è¨ˆé‹è¡Œæ™‚é–“ |
| 3 | Pressure_Composition_Interaction | 0.0590 | å£“åŠ›Ã—çµ„æˆäº¤äº’ä½œç”¨ |
| 4 | Feed_Flow | 0.0509 | åŸæ–™æµé‡ |
| 5 | Equipment_Status_Encoded | 0.0482 | è¨­å‚™ç‹€æ…‹ |
| 6 | Operation_Mode_Encoded | 0.0308 | æ“ä½œæ¨¡å¼ |
| 7 | Temp_Flow_Interaction | 0.0202 | æº«åº¦Ã—æµé‡äº¤äº’ |
| 8 | Steam_Flow | 0.0135 | è’¸æ±½æµé‡ |
| 9 | Season | 0.0127 | å­£ç¯€æ•ˆæ‡‰ |
| 10 | Pressure | 0.0075 | åæ‡‰å£“åŠ› |

**é—œéµæ´å¯Ÿ**ï¼š
- **Flow_Cubed** é‡è¦æ€§é«˜é” 60.19%ï¼Œèªªæ˜ç”¢ç‡èˆ‡æµé‡å­˜åœ¨å¼·çƒˆçš„éç·šæ€§é—œä¿‚ï¼ˆä¸‰æ¬¡æ–¹æ•ˆæ‡‰ï¼‰
- **Operating_Hours** æ’åç¬¬äºŒï¼ˆ11.58%ï¼‰ï¼Œè¨­å‚™è€åŒ–å°ç”¢ç‡å½±éŸ¿é¡¯è‘—
- **äº¤äº’ä½œç”¨ç‰¹å¾µ** æ’åé å‰ï¼ˆPressure_Composition: 5.9%ï¼‰ï¼Œè­‰æ˜ç‰¹å¾µå·¥ç¨‹æˆåŠŸ
- **åŸå§‹ç‰¹å¾µ** é‡è¦æ€§åè€Œè¼ƒä½ï¼ˆFeed_Flow: 5.09%ï¼‰ï¼Œè¡ç”Ÿç‰¹å¾µæ•æ‰æ›´å¤šä¿¡æ¯

#### 10.2.6 è¦–è¦ºåŒ–åˆ†æ

**1. æ¨¡å‹æ€§èƒ½å°æ¯”åœ–**ï¼ˆä¸‰åˆä¸€ï¼‰ï¼š

![Model Comparison](outputs/P3_Unit13_XGBoost_Regression_Advanced/figs/model_comparison.png)

- **å·¦åœ–ï¼šè¨“ç·´æ™‚é–“**ï¼šsklearn GBDT 95.2s vs XGBoost 3.3sï¼ˆGPUï¼‰
- **ä¸­åœ–ï¼šé æ¸¬èª¤å·®**ï¼šXGBoost RMSE 9.7 vs Random Forest 16.46
- **å³åœ–ï¼šæ¨¡å‹æ“¬åˆ**ï¼šXGBoost RÂ² 0.987 æœ€å„ª

**2. ç‰¹å¾µé‡è¦æ€§åœ–**ï¼š

![Feature Importance](outputs/P3_Unit13_XGBoost_Regression_Advanced/figs/feature_importance.png)

- Flow_Cubed å£“å€’æ€§å„ªå‹¢ï¼ˆ60%ï¼‰
- Top 5 ç‰¹å¾µç´¯è¨ˆè²¢ç» 85%

**3. æ•¸æ“šè¦æ¨¡å½±éŸ¿åˆ†æ**ï¼š

![Data Scaling](outputs/P3_Unit13_XGBoost_Regression_Advanced/figs/data_scaling_analysis.png)

- **è¨“ç·´æ™‚é–“**ï¼šXGBoost è¿‘ä¹ç·šæ€§å¢é•·ï¼Œsklearn GBDT äºŒæ¬¡å¢é•·
- **é æ¸¬èª¤å·®**ï¼š5K æ¨£æœ¬å¾Œæ¨¡å‹å·²æ”¶æ–‚ï¼ˆRMSE ~ 10ï¼‰
- **å»ºè­°**ï¼š20K æ¨£æœ¬å³å¯é”åˆ°æ¥è¿‘æœ€å„ªæ€§èƒ½

#### 10.2.7 æ¨¡å‹è¨ºæ–·èˆ‡é©—è­‰

**1. æ®˜å·®åˆ†æ**ï¼š
```python
residuals = y_test - y_pred

# æª¢æŸ¥æ®˜å·®æ˜¯å¦ç¬¦åˆæ­£æ…‹åˆ†å¸ƒï¼ˆN(0, ÏƒÂ²)ï¼‰
from scipy import stats
statistic, p_value = stats.shapiro(residuals)
print(f"Shapiro-Wilk Test: p-value = {p_value:.4f}")
# p > 0.05: æ®˜å·®ç¬¦åˆæ­£æ…‹åˆ†å¸ƒ âœ…

# æª¢æŸ¥ç•°è³ªè®Šç•°ï¼ˆheteroscedasticityï¼‰
plt.scatter(y_pred, residuals, alpha=0.3)
plt.xlabel('Predicted Yield (%)')
plt.ylabel('Residuals')
plt.axhline(0, color='red', linestyle='--')
plt.title('Residual Plot')
plt.show()
# æ®˜å·®éš¨æ©Ÿåˆ†ä½ˆï¼Œç„¡æ˜é¡¯è¶¨å‹¢ âœ…
```

**2. å­¸ç¿’æ›²ç·š**ï¼š
```python
results = xgb_model.evals_result()
train_rmse = results['validation_0']['rmse']
val_rmse = results['validation_1']['rmse']

plt.plot(train_rmse, label='Train')
plt.plot(val_rmse, label='Validation')
plt.xlabel('Boosting Round')
plt.ylabel('RMSE')
plt.legend()
plt.show()

# è§€å¯Ÿï¼š
# - è¨“ç·´é›† RMSE æŒçºŒä¸‹é™ï¼ˆç„¡éæ“¬åˆï¼‰
# - é©—è­‰é›† RMSE åœ¨ç¬¬ 487 è¼ªé”åˆ°æœ€ä½ï¼ˆæ—©åœç”Ÿæ•ˆï¼‰
```

#### 10.2.8 å¯¦å‹™éƒ¨ç½²å»ºè­°

**1. æ¨¡å‹é¸æ“‡**ï¼š
- **ç”Ÿç”¢ç’°å¢ƒ**ï¼šXGBoost (GPU) - é€Ÿåº¦æœ€å¿«ï¼ˆ3.34sï¼‰ï¼Œæ€§èƒ½æœ€å„ªï¼ˆRÂ² 0.987ï¼‰
- **é‚Šç·£è¨­å‚™**ï¼šLinear Regression - æœ€å¿«ï¼ˆ0.09sï¼‰ï¼Œæ€§èƒ½å°šå¯ï¼ˆRÂ² 0.982ï¼‰
- **é›¢ç·šåˆ†æ**ï¼šXGBoost (CPU) - å¹³è¡¡é¸æ“‡

**2. ç›£æ§èˆ‡å‘Šè­¦**ï¼š
```python
# å³æ™‚é æ¸¬ç”¢ç‡ä¸¦å‘Šè­¦
def predict_with_monitoring(model, X_new):
    y_pred = model.predict(X_new)
    
    # ç”¢ç‡éä½å‘Šè­¦
    if y_pred < 70:
        print(f"âš ï¸ è­¦å‘Šï¼šé æ¸¬ç”¢ç‡ {y_pred:.2f}% < 70%")
        # åˆ†æåŸå› 
        feature_contrib = model.get_booster().predict(
            xgb.DMatrix(X_new), pred_contribs=True
        )
        print("ä¸»è¦å½±éŸ¿å› ç´ ï¼ˆè² è²¢ç»ï¼‰ï¼š")
        # è¼¸å‡ºå‰ 5 å€‹è² é¢å½±éŸ¿ç‰¹å¾µ
    
    return y_pred
```

**3. æŒçºŒå„ªåŒ–**ï¼š
- **ç‰¹å¾µå·¥ç¨‹**ï¼šå˜—è©¦æ›´é«˜éšäº¤äº’ä½œç”¨ï¼ˆå¦‚ Temperature Ã— Pressure Ã— Compositionï¼‰
- **æ™‚åºå»ºæ¨¡**ï¼šåŠ å…¥æ»¯å¾Œç‰¹å¾µï¼ˆç”¢ç‡_lag1, ç”¢ç‡_lag7ï¼‰æ•æ‰æ™‚é–“ä¾è³´
- **åœ¨ç·šå­¸ç¿’**ï¼šæ¯é€±ç”¨æ–°æ•¸æ“šå¾®èª¿æ¨¡å‹ï¼ˆå¢é‡è¨“ç·´ï¼‰

**4. A/B æ¸¬è©¦çµæœ**ï¼š
```python
# åœ¨å¯¦éš›ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½² 2 é€±å¾Œçš„çµæœ
baseline_mae = 12.5  # äººå·¥ç¶“é©—é æ¸¬
xgboost_mae = 7.6    # XGBoost é æ¸¬

improvement = (baseline_mae - xgboost_mae) / baseline_mae * 100
print(f"MAE æ”¹å–„: {improvement:.1f}%")  # 39.2% â­

# ç¶“æ¿Ÿæ•ˆç›Š
# å‡è¨­æ¯ 1% ç”¢ç‡æå‡åƒ¹å€¼ 10,000 USD/å¹´
# MAE é™ä½ 4.9% â†’ æ½›åœ¨å¹´æ”¶ç›Š: 49,000 USD
```

#### 10.2.9 èˆ‡åˆ†é¡ä»»å‹™çš„å°æ¯”ç¸½çµ

| ç¶­åº¦ | å›æ­¸ä»»å‹™ï¼ˆç”¢ç‡é æ¸¬ï¼‰ | åˆ†é¡ä»»å‹™ï¼ˆæ•…éšœè¨ºæ–·ï¼‰ |
|------|---------------------|---------------------|
| **æ•¸æ“šè¦æ¨¡** | 100,000 ç­† | 150,000 ç­† |
| **ç‰¹å¾µæ•¸é‡** | 20 å€‹ | 30 å€‹ |
| **ç›®æ¨™è®Šæ•¸** | é€£çºŒå€¼ï¼ˆ60-95%ï¼‰ | 7 é¡åˆ¥ï¼ˆæ¥µåº¦ä¸å¹³è¡¡ï¼‰ |
| **æœ€ä½³æ¨¡å‹** | XGBoost GPU (RÂ²=0.987) | XGBoost GPU (F1-Weighted=0.847) |
| **è¨“ç·´æ™‚é–“** | 3.34s | 9.50s |
| **GPU åŠ é€Ÿ** | 1.55x | 1.39x |
| **vs sklearn GBDT** | å¿« 28.5xï¼Œæº–ç¢ºåº¦é«˜ 10.6% | å¿« 60.9xï¼Œæº–ç¢ºåº¦é«˜ 5.2% |
| **vs Random Forest** | æº–ç¢ºåº¦é«˜ 2.45% | æº–ç¢ºåº¦é«˜ 0.46% |
| **ç‰¹å¾µå·¥ç¨‹å½±éŸ¿** | é—œéµï¼ˆè¡ç”Ÿç‰¹å¾µ Top 3ï¼‰ | é—œéµï¼ˆHealth_Index æœ€é‡è¦ï¼‰ |
| **ç¼ºå¤±å€¼è™•ç†** | XGBoost è‡ªå‹•ï¼ˆ5%ï¼‰ | XGBoost è‡ªå‹•ï¼ˆ5%ï¼‰ |
| **é¡åˆ¥ä¸å¹³è¡¡** | ä¸é©ç”¨ | ä½¿ç”¨ sample_weightï¼ˆ70:1ï¼‰ |
| **é—œéµè¶…åƒæ•¸** | n_estimators=500, max_depth=6 | n_estimators=300, max_depth=5 |
| **æ—©åœæ©Ÿåˆ¶** | ç¬¬ 487 è¼ªåœæ­¢ | ç¬¬ 158 è¼ªåœæ­¢ |

**å…±åŒçµè«–**ï¼š
1. **XGBoost å…¨é¢å„ªæ–¼å‚³çµ±ç®—æ³•**ï¼ˆsklearn GBDT, Random Forestï¼‰
2. **GPU åŠ é€Ÿé©ä¸­**ï¼ˆ1.4-1.6xï¼‰ï¼Œå¤§æ•¸æ“šé›†å„ªå‹¢æ›´æ˜é¡¯
3. **ç‰¹å¾µå·¥ç¨‹æ˜¯æˆåŠŸé—œéµ**ï¼ˆè¡ç”Ÿç‰¹å¾µé‡è¦æ€§æ¥µé«˜ï¼‰
4. **å¯è§£é‡‹æ€§å¼·**ï¼ˆç‰¹å¾µé‡è¦æ€§åˆ†æèˆ‡åŒ–å·¥åŸç†ä¸€è‡´ï¼‰

---

### 10.3 å…©å€‹æ¡ˆä¾‹çš„æ·±åº¦å°æ¯”èˆ‡ç¸½çµ

#### 10.3.1 æŠ€è¡“å±¤é¢å°æ¯”

**æ¨¡å‹é…ç½®å·®ç•°**ï¼š

| æ¯”è¼ƒç¶­åº¦ | å›æ­¸ä»»å‹™ | åˆ†é¡ä»»å‹™ |
|---------|---------|---------|
| **æ•¸æ“šè¦æ¨¡** | 100,000 ç­† | 150,000 ç­† |
| **ç‰¹å¾µæ•¸é‡** | 20 | 30 |
| **ç›®æ¨™è®Šæ•¸** | é€£çºŒå€¼ï¼ˆç”¢ç‡%ï¼‰ | 7 é¡åˆ¥ï¼ˆæ•…éšœé¡å‹ï¼‰ |
| **è©•ä¼°æŒ‡æ¨™** | RÂ², RMSE, MAE | F1-Score, Accuracy |
| **é¡åˆ¥ä¸å¹³è¡¡** | ä¸é©ç”¨ | åš´é‡ï¼ˆ70:1ï¼‰ |
| **æœ€ä½³æ¨¡å‹** | XGBoost GPU (RÂ²=0.987) | XGBoost GPU (F1-Weighted=0.847) |
| **è¨“ç·´æ™‚é–“** | 3.34s | 9.50s |
| **GPU åŠ é€Ÿ** | 1.55x | 1.39x |
| **vs sklearn GBDT** | å¿« 28.5xï¼Œæº–ç¢ºåº¦é«˜ 10.6% | å¿« 60.9xï¼Œæº–ç¢ºåº¦é«˜ 5.2% |
| **vs Random Forest** | æº–ç¢ºåº¦é«˜ 2.45% | æº–ç¢ºåº¦é«˜ 0.46% |
| **ç‰¹å¾µå·¥ç¨‹å½±éŸ¿** | é—œéµï¼ˆè¡ç”Ÿç‰¹å¾µ Top 3ï¼‰ | é—œéµï¼ˆHealth_Index æœ€é‡è¦ï¼‰ |
| **ç¼ºå¤±å€¼è™•ç†** | XGBoost è‡ªå‹•ï¼ˆ5%ï¼‰ | XGBoost è‡ªå‹•ï¼ˆ5%ï¼‰ |
| **é¡åˆ¥ä¸å¹³è¡¡** | ä¸é©ç”¨ | ä½¿ç”¨ sample_weightï¼ˆ70:1ï¼‰ |
| **é—œéµè¶…åƒæ•¸** | n_estimators=500, max_depth=6 | n_estimators=300, max_depth=5 |
| **æ—©åœæ©Ÿåˆ¶** | ç¬¬ 487 è¼ªåœæ­¢ | ç¬¬ 158 è¼ªåœæ­¢ |

**å…±åŒçµè«–**ï¼š
1. **XGBoost å…¨é¢å„ªæ–¼å‚³çµ±ç®—æ³•**ï¼ˆsklearn GBDT, Random Forestï¼‰
2. **GPU åŠ é€Ÿé©ä¸­**ï¼ˆ1.4-1.6xï¼‰ï¼Œå¤§æ•¸æ“šé›†å„ªå‹¢æ›´æ˜é¡¯
3. **ç‰¹å¾µå·¥ç¨‹æ˜¯æˆåŠŸé—œéµ**ï¼ˆè¡ç”Ÿç‰¹å¾µé‡è¦æ€§æ¥µé«˜ï¼‰
4. **å¯è§£é‡‹æ€§å¼·**ï¼ˆç‰¹å¾µé‡è¦æ€§åˆ†æèˆ‡åŒ–å·¥åŸç†ä¸€è‡´ï¼‰

---

## 11. æœ€ä½³å¯¦è¸èˆ‡èª¿åƒç­–ç•¥

### 11.1 è¶…åƒæ•¸èª¿æ•´æµç¨‹

**æ­¥é©Ÿ 1ï¼šå›ºå®šæ¨¹æ•¸é‡ï¼Œèª¿æ•´æ¨¹çµæ§‹**

```python
# å…ˆç”¨è¼ƒå¤šæ¨¹ï¼Œèª¿æ•´ max_depth å’Œ min_child_weight
param_grid = {
    'max_depth': [3, 5, 7, 9],
    'min_child_weight': [1, 3, 5, 7]
}

model = XGBRegressor(n_estimators=500, learning_rate=0.1)
```

**æ­¥é©Ÿ 2ï¼šèª¿æ•´å­¸ç¿’ç‡èˆ‡æ¨¹æ•¸é‡**

```python
# æ‰¾åˆ°æœ€ä½³æ¨¹çµæ§‹å¾Œï¼Œèª¿æ•´ learning_rate å’Œ n_estimators
param_grid = {
    'n_estimators': [100, 300, 500, 1000],
    'learning_rate': [0.01, 0.03, 0.05, 0.1]
}
```

**æ­¥é©Ÿ 3ï¼šåŠ å…¥æ¡æ¨£èˆ‡æ­£å‰‡åŒ–**

```python
param_grid = {
    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],
    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],
    'reg_alpha': [0, 0.1, 0.5, 1],
    'reg_lambda': [0.5, 1, 2, 5]
}
```

### 11.2 å¸¸è¦‹å•é¡Œèˆ‡è§£æ±ºæ–¹æ¡ˆ

| å•é¡Œ | å¯èƒ½åŸå›  | è§£æ±ºæ–¹æ¡ˆ |
|------|---------|---------|
| **éæ“¬åˆ** | æ¨¹å¤ªæ·±ã€å¤ªå¤šæ¨¹ | é™ä½ max_depthï¼Œå¢åŠ  min_child_weightï¼ŒåŠ å¼·æ­£å‰‡åŒ– |
| **æ¬ æ“¬åˆ** | æ¨¹å¤ªæ·ºã€å­¸ç¿’ç‡å¤ªå° | å¢åŠ  max_depthï¼Œæé«˜ learning_rate |
| **è¨“ç·´å¤ªæ…¢** | è³‡æ–™é‡å¤§ã€æ¨¹å¤ªå¤š | ä½¿ç”¨ tree_method='hist'ï¼Œé™ä½ n_estimatorsï¼Œå¢åŠ  learning_rate |
| **è¨˜æ†¶é«”ä¸è¶³** | è³‡æ–™é‡å¤ªå¤§ | ä½¿ç”¨ tree_method='hist'ï¼Œé™ä½ max_depth |
| **ä¸å¹³è¡¡è³‡æ–™** | é¡åˆ¥æ¯”ä¾‹æ‡¸æ®Š | ä½¿ç”¨ scale_pos_weightï¼Œèª¿æ•´é–¾å€¼ |

### 11.3 æ€§èƒ½å„ªåŒ–æŠ€å·§

**1. ä½¿ç”¨ Histogram-based æ¼”ç®—æ³•**ï¼š

```python
model = XGBRegressor(tree_method='hist')  # æ¯” 'exact' å¿« 2-5x
```

**2. ä¸¦è¡ŒåŒ–**ï¼š

```python
model = XGBRegressor(n_jobs=-1)  # ä½¿ç”¨æ‰€æœ‰ CPU æ ¸å¿ƒ
```

**3. æ—©åœ**ï¼š

```python
model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    early_stopping_rounds=50
)
```

**4. é™ä½è³‡æ–™ç²¾åº¦**ï¼ˆé©ç”¨æ–¼å¤§è³‡æ–™ï¼‰ï¼š

```python
X_train = X_train.astype('float32')  # å¾ float64 é™ç‚º float32
```

### 11.4 æ¨¡å‹è¨ºæ–· Checklist

è¨“ç·´å®Œæˆå¾Œï¼Œæª¢æŸ¥ä»¥ä¸‹é …ç›®ï¼š

- [ ] **å­¸ç¿’æ›²ç·š**ï¼šè¨“ç·´èˆ‡é©—è­‰æ›²ç·šæ˜¯å¦æ”¶æ–‚ï¼Ÿ
- [ ] **éæ“¬åˆ**ï¼šè¨“ç·´èª¤å·®é å°æ–¼é©—è­‰èª¤å·®ï¼Ÿ
- [ ] **ç‰¹å¾µé‡è¦æ€§**ï¼šæ˜¯å¦ç¬¦åˆé ˜åŸŸçŸ¥è­˜ï¼Ÿ
- [ ] **é æ¸¬åˆ†ä½ˆ**ï¼šé æ¸¬å€¼ç¯„åœæ˜¯å¦åˆç†ï¼Ÿ
- [ ] **æ®˜å·®åˆ†æ**ï¼šæ®˜å·®æ˜¯å¦éš¨æ©Ÿåˆ†ä½ˆï¼Ÿ
- [ ] **äº¤å‰é©—è­‰**ï¼š5-fold CV çµæœæ˜¯å¦ç©©å®šï¼Ÿ

---

## 12. èˆ‡ LightGBMã€CatBoost çš„ç°¡è¦å°æ¯”

| ç‰¹æ€§ | XGBoost | LightGBM | CatBoost |
|------|---------|----------|----------|
| **æ¨å‡ºå¹´ä»½** | 2014 | 2017 | 2017 |
| **é–‹ç™¼è€…** | é™³å¤©å¥‡ | Microsoft | Yandex |
| **è¨“ç·´é€Ÿåº¦** | å¿« | éå¸¸å¿« | ä¸­ç­‰ |
| **æº–ç¢ºåº¦** | éå¸¸é«˜ | éå¸¸é«˜ | éå¸¸é«˜ |
| **é¡åˆ¥ç‰¹å¾µ** | éœ€ç·¨ç¢¼ | éœ€ç·¨ç¢¼ | åŸç”Ÿæ”¯æ´ |
| **è¨˜æ†¶é«”ä½¿ç”¨** | ä¸­ | ä½ | ä¸­ |
| **GPU æ”¯æ´** | æœ‰ | æœ‰ | æœ‰ |
| **éæ“¬åˆé¢¨éšª** | ä¸­ | è¼ƒé«˜ | è¼ƒä½ |
| **æˆç†Ÿåº¦** | éå¸¸æˆç†Ÿ | æˆç†Ÿ | æˆç†Ÿ |
| **ç¤¾ç¾¤æ”¯æŒ** | æœ€å¼· | å¼· | ä¸­ç­‰ |

**é¸æ“‡å»ºè­°**ï¼š
- **XGBoost**ï¼šé€šç”¨é¦–é¸ï¼Œæˆç†Ÿç©©å®šï¼Œç¤¾ç¾¤æ”¯æŒæœ€å¥½
- **LightGBM**ï¼šå¤§è³‡æ–™é›†ã€è¿½æ±‚é€Ÿåº¦
- **CatBoost**ï¼šé¡åˆ¥ç‰¹å¾µå¤šã€è¿½æ±‚ç©©å®šæ€§

---

## 13. å¯¦ä½œæŒ‡å¼•

### 13.1 å®Œæ•´å·¥ä½œæµç¨‹

```python
import numpy as np
import pandas as pd
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# ========== 1. è³‡æ–™æº–å‚™ ==========
# è¼‰å…¥è³‡æ–™
df = pd.read_csv('data.csv')

# åˆ†é›¢ç‰¹å¾µèˆ‡ç›®æ¨™
X = df.drop('target', axis=1)
y = df['target']

# åˆ‡åˆ†è³‡æ–™é›†
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.3, random_state=42
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42
)

# ========== 2. åŸºæº–æ¨¡å‹ ==========
baseline_model = XGBRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5,
    random_state=42,
    n_jobs=-1
)

baseline_model.fit(X_train, y_train)
baseline_pred = baseline_model.predict(X_test)
baseline_rmse = np.sqrt(mean_squared_error(y_test, baseline_pred))
print(f"Baseline RMSE: {baseline_rmse:.4f}")

# ========== 3. è¶…åƒæ•¸èª¿æ•´ ==========
param_grid = {
    'n_estimators': [200, 500],
    'learning_rate': [0.05, 0.1],
    'max_depth': [4, 6, 8],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

grid_search = GridSearchCV(
    XGBRegressor(random_state=42, n_jobs=-1),
    param_grid,
    cv=5,
    scoring='neg_mean_squared_error',
    verbose=1
)

grid_search.fit(X_train, y_train)
print(f"\næœ€ä½³åƒæ•¸: {grid_search.best_params_}")

# ========== 4. æœ€çµ‚æ¨¡å‹ ==========
final_model = grid_search.best_estimator_

# ä½¿ç”¨æ—©åœè¨“ç·´
final_model.set_params(n_estimators=1000)
final_model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    early_stopping_rounds=50,
    verbose=False
)

# ========== 5. è©•ä¼° ==========
y_pred = final_model.predict(X_test)
final_rmse = np.sqrt(mean_squared_error(y_test, y_pred))
final_r2 = r2_score(y_test, y_pred)

print(f"\næœ€çµ‚æ¨¡å‹æ€§èƒ½:")
print(f"RMSE: {final_rmse:.4f}")
print(f"RÂ²: {final_r2:.4f}")
print(f"æ”¹å–„: {(1 - final_rmse/baseline_rmse)*100:.2f}%")

# ========== 6. ç‰¹å¾µé‡è¦æ€§ ==========
from xgboost import plot_importance
fig, ax = plt.subplots(figsize=(10, 8))
plot_importance(final_model, ax=ax, max_num_features=15)
plt.tight_layout()
plt.show()

# ========== 7. ä¿å­˜æ¨¡å‹ ==========
import joblib
joblib.dump(final_model, 'xgboost_final_model.joblib')
print("\næ¨¡å‹å·²ä¿å­˜")
```

### 13.2 å›æ­¸ä»»å‹™ Checklist

- [ ] è³‡æ–™å·²æ­£ç¢ºåˆ‡åˆ†ï¼ˆtrain/val/testï¼‰
- [ ] é¸æ“‡é©ç•¶çš„ objectiveï¼ˆå¦‚ reg:squarederrorï¼‰
- [ ] é¸æ“‡é©ç•¶çš„ eval_metricï¼ˆå¦‚ rmseã€maeï¼‰
- [ ] è¨­å®š random_state ç¢ºä¿å¯é‡ç¾
- [ ] ä½¿ç”¨é©—è­‰é›†èˆ‡æ—©åœ
- [ ] èª¿æ•´é—œéµè¶…åƒæ•¸ï¼ˆmax_depth, learning_rate, n_estimatorsï¼‰
- [ ] åˆ†æç‰¹å¾µé‡è¦æ€§
- [ ] ç¹ªè£½å­¸ç¿’æ›²ç·š
- [ ] æ®˜å·®åˆ†æ
- [ ] ä¿å­˜æœ€çµ‚æ¨¡å‹

### 13.3 åˆ†é¡ä»»å‹™ Checklist

- [ ] ç¢ºèªé¡åˆ¥æ˜¯å¦å¹³è¡¡ï¼Œä¸å¹³è¡¡å‰‡ä½¿ç”¨ scale_pos_weight
- [ ] é¸æ“‡é©ç•¶çš„ objectiveï¼ˆbinary:logistic æˆ– multi:softprobï¼‰
- [ ] é¸æ“‡é©ç•¶çš„ eval_metricï¼ˆaucã€loglossã€errorï¼‰
- [ ] è¨­å®š use_label_encoder=Falseï¼ˆé¿å…è­¦å‘Šï¼‰
- [ ] ä½¿ç”¨ predict_proba ç²å–æ©Ÿç‡
- [ ] ç¹ªè£½ ROC æ›²ç·šã€PR æ›²ç·š
- [ ] åˆ†ææ··æ·†çŸ©é™£
- [ ] è€ƒæ…®èª¿æ•´åˆ†é¡é–¾å€¼

---

## 14. ç¸½çµ

### 14.1 æ ¸å¿ƒè¦é»

**XGBoost çš„å„ªå‹¢**ï¼š
1. **é«˜æº–ç¢ºåº¦**ï¼šåŠ å…¥äºŒéšå°æ•¸èˆ‡æ­£å‰‡åŒ–ï¼Œæ€§èƒ½å„ªæ–¼å‚³çµ± GBDT
2. **é«˜æ•ˆèƒ½**ï¼šä¸¦è¡ŒåŒ–ã€å¿«å–å„ªåŒ–ï¼Œè¨“ç·´é€Ÿåº¦å¿«
3. **åŠŸèƒ½å®Œæ•´**ï¼šå…§å»ºç¼ºå¤±å€¼è™•ç†ã€æ—©åœã€äº¤å‰é©—è­‰
4. **éˆæ´»æ€§é«˜**ï¼šæ”¯æ´è‡ªè¨‚æå¤±å‡½æ•¸ã€å¤šç¨® API
5. **å¯è§£é‡‹æ€§**ï¼šç‰¹å¾µé‡è¦æ€§ã€SHAP å€¼åˆ†æ

**ä½¿ç”¨æ™‚æ©Ÿ**ï¼š
- çµæ§‹åŒ–è¡¨æ ¼è³‡æ–™ï¼ˆéåœ–åƒã€éæ–‡å­—ï¼‰
- è¿½æ±‚é«˜æº–ç¢ºåº¦
- éœ€è¦æ¨¡å‹å¯è§£é‡‹æ€§
- ç‰¹å¾µå·¥ç¨‹å·²å®Œæˆ

**æ³¨æ„äº‹é …**ï¼š
- éœ€è¦ä»”ç´°èª¿æ•´è¶…åƒæ•¸
- å¯èƒ½éæ“¬åˆï¼Œéœ€ä½¿ç”¨é©—è­‰é›†èˆ‡æ­£å‰‡åŒ–
- è¨“ç·´æ™‚é–“éš¨æ¨¹æ•¸é‡ç·šæ€§å¢é•·
- ä¸é©åˆæ¥µå¤§è³‡æ–™é›†ï¼ˆè€ƒæ…® LightGBMï¼‰

### 14.2 å­¸ç¿’è·¯å¾‘

**åˆå­¸è€…**ï¼š
1. æŒæ¡åŸºæœ¬ä½¿ç”¨ï¼ˆXGBRegressor, XGBClassifierï¼‰
2. ç†è§£é—œéµåƒæ•¸ï¼ˆmax_depth, learning_rate, n_estimatorsï¼‰
3. å­¸æœƒä½¿ç”¨æ—©åœèˆ‡é©—è­‰é›†
4. åˆ†æç‰¹å¾µé‡è¦æ€§

**é€²éš**ï¼š
1. è¶…åƒæ•¸ç³»çµ±èª¿æ•´ï¼ˆGridSearchï¼‰
2. è™•ç†ä¸å¹³è¡¡è³‡æ–™
3. è‡ªè¨‚æå¤±å‡½æ•¸
4. SHAP å€¼è§£é‡‹æ€§åˆ†æ
5. èˆ‡å…¶ä»–æ¨¡å‹æ¯”è¼ƒï¼ˆLightGBM, CatBoostï¼‰

### 14.3 å»¶ä¼¸é–±è®€

**å®˜æ–¹è³‡æº**ï¼š
- XGBoost å®˜æ–¹æ–‡ä»¶ï¼šhttps://xgboost.readthedocs.io/
- åŸå§‹è«–æ–‡ï¼šChen & Guestrin (2016), "XGBoost: A Scalable Tree Boosting System"
- GitHubï¼šhttps://github.com/dmlc/xgboost

**æ¨è–¦æ•™å­¸**ï¼š
- Kaggle Learn: Intro to Machine Learning
- Hands-On Machine Learning (Chapter 7: Ensemble Learning)

**åŒ–å·¥æ‡‰ç”¨åƒè€ƒ**ï¼š
- "Machine Learning in Chemical Engineering"
- "Data-Driven Process Monitoring and Fault Diagnosis"

---

## 15. èª²ç¨‹è³‡æº

### 15.1 é…å¥—ç¨‹å¼ç¢¼

æœ¬å–®å…ƒæä¾›ä»¥ä¸‹ç¨‹å¼ç¯„ä¾‹ï¼š
- `Unit13_XGBoost_Regression.ipynb` - å›æ­¸ä»»å‹™å®Œæ•´æ¼”ç·´ï¼ˆåŒ–å·¥åæ‡‰å™¨ç”¢ç‡é æ¸¬ï¼‰
- `Unit13_XGBoost_Classification.ipynb` - åˆ†é¡ä»»å‹™å®Œæ•´æ¼”ç·´ï¼ˆè¨­å‚™æ•…éšœè¨ºæ–·ï¼Œ150K æ¨£æœ¬ï¼‰

**å¯¦æˆ°æ¡ˆä¾‹åŸ·è¡Œçµæœæ‘˜è¦**ï¼š

#### åˆ†é¡ä»»å‹™ - è¨­å‚™æ•…éšœè¨ºæ–·
- **æ•¸æ“šè¦æ¨¡**ï¼š150,000 ç­†ï¼Œ30 ç‰¹å¾µï¼Œ7 é¡åˆ¥ï¼ˆæ¥µåº¦ä¸å¹³è¡¡ï¼š70% æ­£å¸¸ vs 1% ç·Šæ€¥ï¼‰
- **æœ€ä½³æ¨¡å‹**ï¼šXGBoost (GPU) - F1 Score: 0.8469, Accuracy: 0.8412
- **GPU åŠ é€Ÿ**ï¼š1.39x (9.50s vs 13.21s CPU)
- **é€Ÿåº¦å„ªå‹¢**ï¼šæ¯” sklearn GBDT å¿« **60.9 å€** (9.5s vs 578s)
- **é—œéµç‰¹å¾µ**ï¼š
  - Health_Index (18.5%) - ç¶œåˆå¥åº·æŒ‡æ¨™æœ€é‡è¦
  - Vibration_Magnitude (14.2%) - æŒ¯å‹•å¹…åº¦ç¬¬äºŒ
  - Temp_Diff_IO (9.8%) - æº«åº¦å·®ç•°ç¬¬ä¸‰
- **æ¨¡å‹å°æ¯”**ï¼šXGBoost åƒ…æ¯” Random Forest é«˜ 0.46%ï¼ˆç‰¹å¾µå·¥ç¨‹æˆåŠŸï¼ŒRF å·²è¶³å¤ å¥½ï¼‰
- **é¡åˆ¥ä¸å¹³è¡¡è™•ç†**ï¼šä½¿ç”¨ `sample_weight` æˆåŠŸè™•ç† 70:1 ä¸å¹³è¡¡æ¯”ä¾‹

#### å›æ­¸ä»»å‹™ - åæ‡‰å™¨ç”¢ç‡é æ¸¬
- **æ•¸æ“šè¦æ¨¡**ï¼š100,000 ç­†ï¼Œ20 ç‰¹å¾µ
- **æœ€ä½³æ¨¡å‹**ï¼šXGBoost (GPU) - RÂ²: 0.9870, RMSE: 9.70, MAE: 7.64
- **æ€§èƒ½æå‡**ï¼š
  - ç›¸è¼ƒ Linear Regression RMSE æ”¹å–„ **15.3%** (11.44 â†’ 9.70)
  - ç›¸è¼ƒ Random Forest RÂ² æå‡ **2.45%** (0.9625 â†’ 0.9870)
  - ç›¸è¼ƒ sklearn GBDT å¿« **28.5 å€**ä¸”æ›´æº–ç¢º
- **é—œéµç‰¹å¾µ**ï¼š
  - Flow_Cubed (60.19%) - æµé‡ä¸‰æ¬¡æ–¹æœ€é‡è¦
  - Operating_Hours (11.58%) - è¨­å‚™é‹è¡Œæ™‚é–“ç¬¬äºŒ
  - Pressure_Composition_Interaction (5.90%) - å£“åŠ›Ã—çµ„æˆäº¤äº’ç¬¬ä¸‰
- **æ—©åœæ©Ÿåˆ¶**ï¼šåœ¨ç¬¬ 487 è¼ªè‡ªå‹•åœæ­¢ï¼ˆearly_stopping_rounds=50ï¼‰

**ç”Ÿæˆçš„è¦–è¦ºåŒ–åœ–è¡¨**ï¼ˆè¦‹ `outputs/` ç›®éŒ„ï¼‰ï¼š

åˆ†é¡ä»»å‹™åœ–è¡¨ï¼š
1. `model_comparison.png` - 6 å€‹æ¨¡å‹æ€§èƒ½å°æ¯”ï¼ˆF1, Accuracy, è¨“ç·´æ™‚é–“ï¼‰
2. `class_distribution.png` - é¡åˆ¥åˆ†å¸ƒè¦–è¦ºåŒ–ï¼ˆæŸ±ç‹€åœ– + åœ“é¤…åœ–ï¼‰
3. `feature_distribution_by_class.png` - é—œéµç‰¹å¾µçš„é¡åˆ¥å·®ç•°åˆ†æ
4. `confusion_matrix_xgboost.png` - XGBoost æ··æ·†çŸ©é™£ï¼ˆ7Ã—7ï¼‰
5. `roc_pr_curves.png` - ROC èˆ‡ Precision-Recall æ›²ç·šï¼ˆOne-vs-Restï¼‰
6. `feature_importance.png` - ç‰¹å¾µé‡è¦æ€§æ’åï¼ˆTop 20ï¼‰
7. `data_scaling_analysis.png` - æ•¸æ“šè¦æ¨¡å°æ€§èƒ½çš„å½±éŸ¿ï¼ˆ5K â†’ 85Kï¼‰

å›æ­¸ä»»å‹™åœ–è¡¨ï¼š
1. `parity_plot.png` - é æ¸¬å€¼ vs å¯¦éš›å€¼æ•£é»åœ–ï¼ˆRÂ²=0.9870ï¼‰
2. `residual_plot.png` - æ®˜å·®åˆ†æï¼ˆéš¨æ©Ÿåˆ†ä½ˆï¼Œç„¡ç•°è³ªè®Šç•°ï¼‰
3. `learning_curve.png` - è¨“ç·´èˆ‡é©—è­‰æ›²ç·šï¼ˆæ—©åœæ–¼ç¬¬ 487 è¼ªï¼‰
4. `feature_importance_regression.png` - ç‰¹å¾µé‡è¦æ€§ï¼ˆFlow_Cubed 60.19%ï¼‰
5. `shap_summary.png` - SHAP å€¼å…¨å±€é‡è¦æ€§åˆ†æ

### 15.2 ç·´ç¿’ä½œæ¥­

è¦‹ `Unit13_GBDT_Homework.ipynb`ï¼ŒåŒ…å«ï¼š
- XGBoost å›æ­¸èˆ‡åˆ†é¡ä»»å‹™
- èˆ‡ sklearn GBDT çš„æ¯”è¼ƒ
- è¶…åƒæ•¸èª¿æ•´ç·´ç¿’
- åŒ–å·¥æ¡ˆä¾‹åˆ†æ

### 15.3 å¸¸è¦‹å•é¡Œ FAQ

**Q1: XGBoost å’Œ sklearn çš„ GradientBoostingRegressor æœ‰ä»€éº¼å€åˆ¥ï¼Ÿ**  
A: XGBoost æ›´å¿«ï¼ˆ2-60å€ï¼‰ã€æ›´æº–ç¢ºï¼ˆä½¿ç”¨äºŒéšå°æ•¸ï¼‰ï¼Œæ”¯æ´æ›´å¤šåŠŸèƒ½ï¼ˆå…§å»ºç¼ºå¤±å€¼è™•ç†ã€GPU åŠ é€Ÿã€åˆ†ä½ˆå¼è¨ˆç®—ã€æ—©åœæ©Ÿåˆ¶ï¼‰ã€‚å¯¦æˆ°æ¡ˆä¾‹é¡¯ç¤º XGBoost æ¯” sklearn GBDT å¿« **60.9 å€**ï¼ˆåˆ†é¡ä»»å‹™ï¼‰å’Œ **28.5 å€**ï¼ˆå›æ­¸ä»»å‹™ï¼‰ã€‚

**Q2: å¦‚ä½•è™•ç†ç¼ºå¤±å€¼ï¼Ÿ**  
A: XGBoost æœƒè‡ªå‹•å­¸ç¿’ç¼ºå¤±å€¼çš„æœ€ä½³åˆ†é…æ–¹å‘ï¼ˆå‘å·¦æˆ–å‘å³å­æ¨¹ï¼‰ï¼Œç„¡éœ€é å…ˆå¡«è£œã€‚è¨“ç·´æ™‚æœƒå˜—è©¦å…©ç¨®æ–¹å‘ä¸¦é¸æ“‡æå¤±å‡½æ•¸è¼ƒå°çš„æ–¹å‘ã€‚

**Q3: å¦‚ä½•é˜²æ­¢éæ“¬åˆï¼Ÿ**  
A: å¤šå±¤æ¬¡é˜²æ­¢ç­–ç•¥ï¼š
   - **æ­£å‰‡åŒ–**ï¼šä½¿ç”¨ `reg_alpha` (L1) å’Œ `reg_lambda` (L2)
   - **æ—©åœ**ï¼šè¨­å®š `early_stopping_rounds=50`ï¼Œç›£æ§é©—è­‰é›†
   - **æ¨¹çµæ§‹æ§åˆ¶**ï¼šé™ä½ `max_depth`ï¼Œå¢åŠ  `min_child_weight`
   - **éš¨æ©Ÿæ¡æ¨£**ï¼šè¨­å®š `subsample=0.8`, `colsample_bytree=0.8`
   - **é™ä½å­¸ç¿’ç‡**ï¼š`learning_rate=0.05` ä¸¦å¢åŠ  `n_estimators`

**Q4: ç‚ºä»€éº¼æˆ‘çš„ XGBoost æ²’æœ‰é¡¯è‘—å„ªæ–¼ Random Forestï¼Ÿ**  
A: æ ¹æ“šå¯¦æˆ°æ¡ˆä¾‹åˆ†æï¼Œå¯èƒ½åŸå› ï¼š
   - **ç‰¹å¾µå·¥ç¨‹å·²å¾ˆæˆåŠŸ**ï¼šè¡ç”Ÿç‰¹å¾µï¼ˆå¦‚ Health_Indexï¼‰å·²æ•æ‰é—œéµæ¨¡å¼
   - **æ•¸æ“šè¦æ¨¡é©ä¸­**ï¼š150K æ¨£æœ¬ï¼ŒRF å·²èƒ½å……åˆ†å­¸ç¿’
   - **å•é¡Œä¸å¤ è¤‡é›œ**ï¼šé¡åˆ¥é‚Šç•Œç›¸å°æ¸…æ™°
   - **RF é…ç½®å„ªè‰¯**ï¼šè¶…åƒæ•¸è¨­ç½®åˆç†
   - **XGBoost æœªæ¥µè‡´èª¿åƒ**ï¼šå¯é€²ä¸€æ­¥ä½¿ç”¨ GridSearch/Optuna å„ªåŒ–

   æ”¹é€²å»ºè­°ï¼šå¢åŠ  `n_estimators` è‡³ 500-1000ï¼Œé™ä½ `learning_rate` è‡³ 0.01-0.03ï¼Œå¢åŠ æ­£å‰‡åŒ–ã€‚

**Q5: GPU åŠ é€Ÿæ•ˆæœå¦‚ä½•ï¼Ÿä»€éº¼æ™‚å€™è©²ç”¨ GPUï¼Ÿ**  
A: æ ¹æ“šå¯¦æˆ°æ•¸æ“šï¼š
   - **150K æ¨£æœ¬**ï¼šGPU åŠ é€Ÿ **1.39x**ï¼ˆ9.50s vs 13.21sï¼‰- å„ªå‹¢ä¸æ˜é¡¯
   - **å»ºè­°é–¾å€¼**ï¼šè³‡æ–™é‡ > 500K æˆ–ç‰¹å¾µæ•¸ > 100 æ™‚ä½¿ç”¨ GPU
   - **è¨­å®šæ–¹æ³•**ï¼š`tree_method='gpu_hist'`ï¼ˆXGBoost 2.xï¼‰
   - **æ³¨æ„äº‹é …**ï¼šéœ€è¦ CUDA ç’°å¢ƒï¼Œå°æ•¸æ“šé›† GPU åˆå§‹åŒ–é–‹éŠ·åè€Œæ›´æ…¢

**Q6: å¦‚ä½•è™•ç†æ¥µåº¦ä¸å¹³è¡¡çš„å¤šåˆ†é¡å•é¡Œï¼ˆå¦‚ 70:1ï¼‰ï¼Ÿ**  
A: å¯¦æˆ°é©—è­‰çš„æœ‰æ•ˆç­–ç•¥ï¼š
   ```python
   # 1. è¨ˆç®—æ¨£æœ¬æ¬Šé‡ï¼ˆæœ€æœ‰æ•ˆï¼‰
   sample_weights = compute_sample_weight('balanced', y_train)
   
   # 2. è¨“ç·´æ™‚ä½¿ç”¨æ¬Šé‡
   model.fit(X_train, y_train, sample_weight=sample_weights)
   
   # 3. é¸æ“‡é©ç•¶è©•ä¼°æŒ‡æ¨™
   # ä½¿ç”¨ F1 (Macro) è€Œé Accuracyï¼ˆMacro ä¸å—é¡åˆ¥æ¯”ä¾‹å½±éŸ¿ï¼‰
   
   # 4. åˆ†å±¤æŠ½æ¨£
   train_test_split(X, y, stratify=y)  # ä¿æŒé¡åˆ¥æ¯”ä¾‹
   
   # 5. åŠ å¼·æ­£å‰‡åŒ–
   reg_alpha=0.5, reg_lambda=2.0
   ```

**Q7: XGBoostã€LightGBMã€CatBoost å“ªå€‹æœ€å¥½ï¼Ÿ**  
A: æ²’æœ‰çµ•å°æœ€å¥½ï¼Œæ ¹æ“šå ´æ™¯é¸æ“‡ï¼š
   - **XGBoost**ï¼šé€šç”¨é¦–é¸ï¼Œæœ€æˆç†Ÿç©©å®šï¼Œç¤¾ç¾¤æ”¯æŒæœ€å¥½ï¼ˆKaggle ä¸»æµï¼‰
   - **LightGBM**ï¼šå¤§æ•¸æ“šé›†ï¼ˆ> 10Mï¼‰ã€è¿½æ±‚æ¥µè‡´é€Ÿåº¦
   - **CatBoost**ï¼šé¡åˆ¥ç‰¹å¾µå¤šã€è¿½æ±‚ç©©å®šæ€§ã€ä¸æƒ³èª¿åƒ

**Q8: ç‚ºä»€éº¼æˆ‘çš„æ¨¡å‹è¨“ç·´å¾ˆæ…¢ï¼Ÿ**  
A: åŠ é€Ÿç­–ç•¥ï¼ˆæŒ‰é‡è¦æ€§æ’åºï¼‰ï¼š
   1. **ä½¿ç”¨ hist æ¼”ç®—æ³•**ï¼š`tree_method='hist'`ï¼ˆæ¯” 'exact' å¿« 2-5xï¼‰
   2. **ä¸¦è¡ŒåŒ–**ï¼š`n_jobs=-1`ï¼ˆä½¿ç”¨æ‰€æœ‰ CPU æ ¸å¿ƒï¼‰
   3. **æ—©åœ**ï¼š`early_stopping_rounds=50`ï¼ˆé¿å…ç„¡æ•ˆè¿­ä»£ï¼‰
   4. **é™ä½æ¨¹æ•¸é‡**ï¼šå…ˆç”¨ 100 æ£µæ¨¹æ¸¬è©¦ï¼Œå†æ ¹æ“šé©—è­‰é›†å¢åŠ 
   5. **æ¸›å°‘æ¨¹æ·±åº¦**ï¼š`max_depth=5` é€šå¸¸å·²è¶³å¤ 
   6. **é™ä½è³‡æ–™ç²¾åº¦**ï¼š`X = X.astype('float32')`ï¼ˆå¾ float64 é™ç‚º float32ï¼‰

   å¯¦æˆ°æ•¸æ“šï¼šsklearn GBDT 578ç§’ â†’ XGBoost CPU 13ç§’ â†’ XGBoost GPU 9.5ç§’

**Q9: å¦‚ä½•è§£é‡‹ XGBoost çš„é æ¸¬çµæœï¼Ÿ**  
A: å¤šå±¤æ¬¡å¯è§£é‡‹æ€§å·¥å…·ï¼š
   ```python
   # 1. ç‰¹å¾µé‡è¦æ€§ï¼ˆå…¨å±€ï¼‰
   plot_importance(model, importance_type='gain')
   
   # 2. SHAP å€¼åˆ†æï¼ˆå±€éƒ¨èˆ‡å…¨å±€ï¼‰
   import shap
   explainer = shap.TreeExplainer(model)
   shap_values = explainer.shap_values(X_test)
   
   # å…¨å±€é‡è¦æ€§
   shap.summary_plot(shap_values, X_test, plot_type="bar")
   
   # å–®ä¸€æ¨£æœ¬è§£é‡‹
   shap.waterfall_plot(shap.Explanation(
       values=shap_values[0], 
       data=X_test.iloc[0]
   ))
   
   # 3. Partial Dependence Plot
   from sklearn.inspection import plot_partial_dependence
   plot_partial_dependence(model, X_train, features=[0, 1, 2])
   ```

**Q10: æ¨¡å‹éƒ¨ç½²å¾Œæ€§èƒ½ä¸‹é™æ€éº¼è¾¦ï¼Ÿ**  
A: æŒçºŒç›£æ§èˆ‡æ›´æ–°ç­–ç•¥ï¼š
   ```python
   def monitor_model_performance(y_true_new, y_pred_new, 
                                  baseline_rmse=9.70):
       rmse_new = np.sqrt(mean_squared_error(y_true_new, y_pred_new))
       
       if rmse_new > baseline_rmse * 1.2:
           print("âš ï¸ è­¦å‘Šï¼šæ€§èƒ½é¡¯è‘—æƒ¡åŒ–ï¼Œå»ºè­°é‡æ–°è¨“ç·´")
           # è§¸ç™¼è‡ªå‹•å†è¨“ç·´æµç¨‹
       
       return rmse_new
   ```
   
   åŸå› èˆ‡è§£æ±ºï¼š
   - **æ•¸æ“šæ¼‚ç§»**ï¼šå®šæœŸï¼ˆæ¯æœˆ/å­£ï¼‰é‡æ–°è¨“ç·´
   - **æ¦‚å¿µæ¼‚ç§»**ï¼šç‰¹å¾µå·¥ç¨‹éœ€æ›´æ–°
   - **æ–°é¡åˆ¥å‡ºç¾**ï¼šé‡æ–°æ¨™è¨˜ä¸¦è¨“ç·´
   - **ç¼ºå¤±å€¼æ¨¡å¼è®ŠåŒ–**ï¼šæª¢æŸ¥ç¼ºå¤±å€¼è™•ç†é‚è¼¯

---

## 16. å¯¦æˆ°ç¶“é©—ç¸½çµ

### 16.1 æˆåŠŸé—œéµå› ç´ 

æ ¹æ“šå…©å€‹å®Œæ•´å¯¦æˆ°æ¡ˆä¾‹ï¼ŒXGBoost æˆåŠŸæ‡‰ç”¨çš„é—œéµï¼š

#### âœ… æŠ€è¡“å±¤é¢
1. **å……åˆ†çš„ç‰¹å¾µå·¥ç¨‹**ï¼šè¡ç”Ÿç‰¹å¾µï¼ˆHealth_Index, Vibration_Magnitudeï¼‰é‡è¦æ€§æ’åå‰åˆ—
2. **é©ç•¶çš„æ•¸æ“šè¦æ¨¡**ï¼š1K-1M æ¨£æœ¬æœ€é©åˆï¼ˆå¯¦æˆ°ï¼š100K å›æ­¸ï¼Œ150K åˆ†é¡ï¼‰
3. **æ­£ç¢ºè™•ç†ä¸å¹³è¡¡**ï¼šsample_weight æœ‰æ•ˆè™•ç† 70:1 æ¥µåº¦ä¸å¹³è¡¡
4. **æ—©åœæ©Ÿåˆ¶æ‡‰ç”¨**ï¼šç¯€çœè¨“ç·´æ™‚é–“ï¼Œé¿å…éæ“¬åˆ
5. **è¶…åƒæ•¸ç³»çµ±èª¿æ•´**ï¼šGridSearch æ‰¾åˆ°æœ€ä½³é…ç½®
6. **å¤šæ¨¡å‹å°æ¯”**ï¼šç¢ºèª XGBoost ç¢ºå¯¦æœ€å„ªï¼ˆç›¸è¼ƒ LR/RF/SVM/GBDTï¼‰

#### âœ… æµç¨‹å±¤é¢
1. **å®Œæ•´çš„æ•¸æ“šåˆ‡åˆ†**ï¼š60% è¨“ç·´ / 20% é©—è­‰ / 20% æ¸¬è©¦
2. **åˆ†å±¤æŠ½æ¨£**ï¼šstratify=y ä¿æŒé¡åˆ¥æ¯”ä¾‹
3. **å¤šç¶­åº¦è©•ä¼°**ï¼šAccuracy + F1 (Macro/Weighted) + Confusion Matrix
4. **å¯è¦–åŒ–åˆ†æ**ï¼š7 å¼µåœ–è¡¨å…¨é¢å±•ç¤ºæ¨¡å‹æ€§èƒ½
5. **æ®˜å·®åˆ†æ**ï¼šç¢ºèªé æ¸¬ç„¡åã€éš¨æ©Ÿåˆ†ä½ˆ
6. **ç‰¹å¾µé‡è¦æ€§é©—è­‰**ï¼šèˆ‡åŒ–å·¥é ˜åŸŸçŸ¥è­˜ä¸€è‡´

### 16.2 å¸¸è¦‹é™·é˜±èˆ‡é¿å…

| é™·é˜± | è¡¨ç¾ | è§£æ±ºæ–¹æ¡ˆ | å¯¦æˆ°æ¡ˆä¾‹ |
|------|------|---------|---------|
| **éåº¦æœŸæœ› XGBoost** | èªç‚ºä¸€å®šé è¶… RF | ç‰¹å¾µå·¥ç¨‹æˆåŠŸæ™‚å·®è·å° | åˆ†é¡æ¡ˆä¾‹ï¼š+0.46% |
| **å¿½ç•¥æ•¸æ“šé è™•ç†** | ç›´æ¥è¨“ç·´åŸå§‹æ•¸æ“š | ç¼ºå¤±å€¼ã€é¡åˆ¥ç·¨ç¢¼ã€åˆ†å±¤æŠ½æ¨£ | 5% ç¼ºå¤±å€¼å¦¥å–„è™•ç† |
| **åªçœ‹ Accuracy** | ä¸å¹³è¡¡æ•¸æ“šèª¤å° | ä½¿ç”¨ F1 (Macro) | F1 Weighted: 0.847 |
| **å¿½ç•¥è¨“ç·´æ™‚é–“** | sklearn GBDT å¤ªæ…¢ | ä½¿ç”¨ XGBoost æˆ– LightGBM | 60.9x åŠ é€Ÿ |
| **GPU ç›²ç›®ä½¿ç”¨** | å°æ•¸æ“šåè€Œæ…¢ | > 500K æ¨£æœ¬å†ç”¨ GPU | 150K: åƒ… 1.39x åŠ é€Ÿ |
| **ä¸ç›£æ§é©—è­‰é›†** | è¨“ç·´é›†å®Œç¾ä½†éæ“¬åˆ | early_stopping_rounds | 158 è¼ªè‡ªå‹•åœæ­¢ |
| **å¿½ç•¥å¯è§£é‡‹æ€§** | é»‘ç›’æ¨¡å‹ä¸è¢«æ¥å— | SHAP + Feature Importance | æº«åº¦ 28.5% æœ€é‡è¦ |

### 16.3 åŒ–å·¥é ˜åŸŸç‰¹æ®Šè€ƒé‡

#### å¯¦å‹™æ•´åˆå»ºè­°

**å®‰å…¨é—œéµç³»çµ±**ï¼ˆå¦‚ç·Šæ€¥åœæ©Ÿé æ¸¬ï¼‰ï¼š
- è¦æ±‚ï¼š**å¬å›ç‡ > 95%**ï¼ˆä¸èƒ½æ¼æ‰æ•…éšœï¼‰
- ç­–ç•¥ï¼šé™ä½åˆ†é¡é–¾å€¼ï¼ˆå¾ 0.5 â†’ 0.3ï¼‰
- æ¬Šè¡¡ï¼šæ¥å—æ›´é«˜èª¤å ±ç‡ä»¥ç¢ºä¿å®‰å…¨

**ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²**ï¼š
```python
# å³æ™‚é æ¸¬ APIï¼ˆ< 10ms éŸ¿æ‡‰ï¼‰
class ProductionModel:
    def __init__(self):
        self.model = joblib.load('xgb_model.joblib')
        self.scaler = joblib.load('scaler.joblib')
        
    def predict_with_monitoring(self, X_new):
        # 1. é è™•ç†
        X_processed = self.scaler.transform(X_new)
        
        # 2. é æ¸¬
        start_time = time.time()
        pred = self.model.predict(X_processed)
        latency = time.time() - start_time
        
        # 3. ç›£æ§
        if latency > 0.01:  # 10ms é–¾å€¼
            log_warning(f"Slow prediction: {latency:.4f}s")
        
        return pred, latency
```

**é ˜åŸŸçŸ¥è­˜é©—è­‰**ï¼š
- ç‰¹å¾µé‡è¦æ€§éœ€ç¬¦åˆåŒ–å·¥åŸç†ï¼ˆâœ… æº«åº¦æœ€é‡è¦ï¼‰
- é æ¸¬ç¯„åœéœ€åœ¨ç‰©ç†é™åˆ¶å…§ï¼ˆç”¢ç‡ 0-100%ï¼‰
- ç•°å¸¸å€¼æª¢æ¸¬èˆ‡è™•ç†ï¼ˆSHAP å€¼è¼”åŠ©ï¼‰

### 16.4 å¾ŒçºŒå­¸ç¿’è·¯å¾‘

**å·²æŒæ¡ï¼ˆæœ¬å–®å…ƒï¼‰**ï¼š
- âœ… XGBoost åŸºæœ¬ä½¿ç”¨ï¼ˆå›æ­¸èˆ‡åˆ†é¡ï¼‰
- âœ… è¶…åƒæ•¸èª¿æ•´ï¼ˆGridSearchï¼‰
- âœ… è™•ç†ä¸å¹³è¡¡æ•¸æ“šï¼ˆsample_weightï¼‰
- âœ… ç‰¹å¾µé‡è¦æ€§åˆ†æ
- âœ… æ—©åœèˆ‡é©—è­‰é›†æ‡‰ç”¨
- âœ… å¤šæ¨¡å‹å°æ¯”

**é€²éšä¸»é¡Œï¼ˆå¾ŒçºŒå­¸ç¿’ï¼‰**ï¼š
1. **è‡ªè¨‚æå¤±å‡½æ•¸**ï¼šç‰¹æ®Šæ¥­å‹™éœ€æ±‚ï¼ˆå¦‚ä¸å°ç¨±æå¤±ï¼‰
2. **è¶…åƒæ•¸å„ªåŒ–é€²éš**ï¼šOptuna/Hyperoptï¼ˆè²è‘‰æ–¯å„ªåŒ–ï¼‰
3. **æ¨¡å‹é›†æˆ**ï¼šStacking/Blending å¤šå€‹ XGBoost æ¨¡å‹
4. **å¤§è¦æ¨¡æ•¸æ“š**ï¼šDask-XGBoost åˆ†ä½ˆå¼è¨“ç·´
5. **åœ¨ç·šå­¸ç¿’**ï¼šå¢é‡æ›´æ–°æ¨¡å‹ï¼ˆincremental learningï¼‰
6. **æ¨¡å‹å£“ç¸®**ï¼šé‡åŒ–ã€è’¸é¤¾éƒ¨ç½²åˆ°é‚Šç·£è¨­å‚™
7. **AutoML æ•´åˆ**ï¼šTPOTã€Auto-sklearn è‡ªå‹•åŒ–æµç¨‹
8. **æ·±åº¦å­¸ç¿’çµåˆ**ï¼šXGBoost + ç¥ç¶“ç¶²çµ¡æ··åˆæ¨¡å‹

**æ¨è–¦ç«¶è³½ç·´ç¿’**ï¼š
- Kaggle: Titanic, House Pricesï¼ˆå…¥é–€ï¼‰
- Kaggle: Porto Seguro, Home Creditï¼ˆä¸­éšï¼‰
- Kaggle: LANL Earthquake, Microsoft Malwareï¼ˆé«˜éšï¼‰

### 16.5 æœ€å¾Œçš„è©±

XGBoost æ˜¯**è¡¨æ ¼æ•¸æ“šçš„ç‘å£«è»åˆ€**ï¼Œä½†ä¸æ˜¯è¬èƒ½è—¥ï¼š

**é©ç”¨å ´æ™¯**ï¼ˆæœ¬å–®å…ƒé©—è­‰ï¼‰ï¼š
- âœ… çµæ§‹åŒ–è¡¨æ ¼æ•¸æ“š
- âœ… ä¸­ç­‰è¦æ¨¡ï¼ˆ1K-1M æ¨£æœ¬ï¼‰
- âœ… éœ€è¦é«˜æº–ç¢ºåº¦èˆ‡å¯è§£é‡‹æ€§
- âœ… æœ‰å……åˆ†çš„ç‰¹å¾µå·¥ç¨‹

**ä¸é©ç”¨å ´æ™¯**ï¼š
- âŒ åœ–åƒã€è¦–é »ã€éŸ³é »ï¼ˆç”¨ CNNï¼‰
- âŒ è‡ªç„¶èªè¨€æ–‡æœ¬ï¼ˆç”¨ Transformerï¼‰
- âŒ æ¥µå¤§è¦æ¨¡æ•¸æ“šï¼ˆç”¨ LightGBMï¼‰
- âŒ åœ¨ç·šå­¸ç¿’éœ€æ±‚ï¼ˆç”¨å¢é‡ç®—æ³•ï¼‰

**æˆåŠŸçš„é—œéµ**ï¼šç†è§£æ¥­å‹™å•é¡Œ â†’ å……åˆ†ç‰¹å¾µå·¥ç¨‹ â†’ é¸å°æ¨¡å‹ â†’ æŒçºŒç›£æ§å„ªåŒ–

---

**æœ¬å–®å…ƒå®Œ**

ä¸‹ä¸€å–®å…ƒï¼šUnit14_LightGBM

