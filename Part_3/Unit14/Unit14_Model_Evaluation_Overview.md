# Unit14 æ¨¡å‹è©•ä¼°èˆ‡é¸æ“‡ | Model Evaluation and Selection

**é€¢ç”²å¤§å­¸ åŒ–å­¸å·¥ç¨‹å­¸ç³»**  
**èª²ç¨‹åç¨±**: AIåœ¨åŒ–å·¥ä¸Šä¹‹æ‡‰ç”¨  
**èª²ç¨‹ä»£ç¢¼**: CHE-AI-114  
**æˆèª²æ•™å¸«**: èŠæ›œç¦ åŠ©ç†æ•™æˆ

---

## ğŸ“š èª²ç¨‹å¤§ç¶± (Table of Contents)

1. [å–®å…ƒç°¡ä»‹](#1-å–®å…ƒç°¡ä»‹)
2. [æ¨¡å‹è©•ä¼°çš„é‡è¦æ€§](#2-æ¨¡å‹è©•ä¼°çš„é‡è¦æ€§)
3. [å›æ­¸æ¨¡å‹è©•ä¼°æŒ‡æ¨™](#3-å›æ­¸æ¨¡å‹è©•ä¼°æŒ‡æ¨™)
4. [åˆ†é¡æ¨¡å‹è©•ä¼°æŒ‡æ¨™](#4-åˆ†é¡æ¨¡å‹è©•ä¼°æŒ‡æ¨™)
5. [äº¤å‰é©—è­‰é€²éšæŠ€å·§](#5-äº¤å‰é©—è­‰é€²éšæŠ€å·§)
6. [åå·®-æ–¹å·®æ¬Šè¡¡](#6-åå·®-æ–¹å·®æ¬Šè¡¡)
7. [å­¸ç¿’æ›²ç·šèˆ‡é©—è­‰æ›²ç·š](#7-å­¸ç¿’æ›²ç·šèˆ‡é©—è­‰æ›²ç·š)
8. [æ¨¡å‹æ¯”è¼ƒçš„çµ±è¨ˆæª¢å®š](#8-æ¨¡å‹æ¯”è¼ƒçš„çµ±è¨ˆæª¢å®š)
9. [å¤šç›®æ¨™æ¨¡å‹é¸æ“‡](#9-å¤šç›®æ¨™æ¨¡å‹é¸æ“‡)
10. [åŒ–å·¥é ˜åŸŸæ‡‰ç”¨æ¡ˆä¾‹](#10-åŒ–å·¥é ˜åŸŸæ‡‰ç”¨æ¡ˆä¾‹)
11. [ç¸½çµèˆ‡æœ€ä½³å¯¦è¸](#11-ç¸½çµèˆ‡æœ€ä½³å¯¦è¸)

---

## 1. å–®å…ƒç°¡ä»‹

### 1.1 å­¸ç¿’ç›®æ¨™

åœ¨å‰é¢çš„ Unit10-13 ä¸­ï¼Œæˆ‘å€‘å­¸ç¿’äº†å„ç¨®ç›£ç£å¼å­¸ç¿’æ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼š
- **Unit10**: ç·šæ€§æ¨¡å‹ (Linear Regression, Ridge, Lasso, ElasticNet, SGD)
- **Unit11**: éç·šæ€§æ¨¡å‹ (Polynomial, Decision Tree, Gradient Boosting, SVM, GPR)
- **Unit12**: åˆ†é¡æ¨¡å‹ (Logistic Regression, SVC, Decision Tree, Gradient Boosting, Naive Bayes)
- **Unit13**: é›†æˆå­¸ç¿’ (Stacking, Random Forest, XGBoost, LightGBM, CatBoost)

æœ¬å–®å…ƒ (Unit14) å°‡æ•™å°å­¸ç”Ÿå¦‚ä½•**ç³»çµ±æ€§åœ°è©•ä¼°å’Œé¸æ“‡æœ€é©åˆçš„æ¨¡å‹**ï¼Œé€™æ˜¯æ©Ÿå™¨å­¸ç¿’å»ºæ¨¡æµç¨‹ä¸­æœ€é—œéµçš„ä¸€ç’°ã€‚

**æ ¸å¿ƒå­¸ç¿’ç›®æ¨™**ï¼š
1. ç†è§£å„ç¨®æ¨¡å‹è©•ä¼°æŒ‡æ¨™çš„å«ç¾©ã€é©ç”¨å ´æ™¯èˆ‡é™åˆ¶
2. æŒæ¡äº¤å‰é©—è­‰çš„é€²éšæŠ€å·§ï¼Œç¢ºä¿æ¨¡å‹æ³›åŒ–èƒ½åŠ›
3. è¨ºæ–·æ¨¡å‹çš„éæ“¬åˆèˆ‡æ¬ æ“¬åˆå•é¡Œ
4. å­¸æœƒä½¿ç”¨çµ±è¨ˆæ–¹æ³•æ¯”è¼ƒä¸åŒæ¨¡å‹çš„æ€§èƒ½
5. åœ¨å¤šé‡è€ƒé‡ä¸‹ (æº–ç¢ºåº¦ã€æˆæœ¬ã€å¯è§£é‡‹æ€§) åšå‡ºæœ€ä½³æ¨¡å‹é¸æ“‡

### 1.2 ç‚ºä»€éº¼æ¨¡å‹è©•ä¼°å¦‚æ­¤é‡è¦ï¼Ÿ

> **"In God we trust, all others bring data."** â€” W. Edwards Deming

åœ¨å·¥æ¥­å¯¦å‹™ä¸­ï¼Œé¸éŒ¯æ¨¡å‹å¯èƒ½å°è‡´ï¼š
- âŒ **å®‰å…¨é¢¨éšª**: æ•…éšœé æ¸¬æ¨¡å‹å¤±æº–ï¼Œå°è‡´è¨­å‚™äº‹æ•…
- âŒ **ç¶“æ¿Ÿæå¤±**: ç”¢é‡é æ¸¬åå·®ï¼Œé€ æˆåŸæ–™æµªè²»æˆ–ç”¢èƒ½ä¸è¶³
- âŒ **æ³•è¦å•é¡Œ**: æ’æ”¾é æ¸¬ä¸æº–ç¢ºï¼Œé•åç’°ä¿æ³•è¦
- âŒ **è³‡æºæµªè²»**: éåº¦è¤‡é›œçš„æ¨¡å‹å¢åŠ è¨ˆç®—æˆæœ¬å’Œç¶­è­·é›£åº¦

å› æ­¤ï¼Œæˆ‘å€‘éœ€è¦ï¼š
- âœ… **å®¢è§€çš„è©•ä¼°æ¨™æº–**: ä¸èƒ½åƒ…æ†‘ "æ„Ÿè¦º" é¸æ“‡æ¨¡å‹
- âœ… **ç³»çµ±æ€§çš„æ¯”è¼ƒæ–¹æ³•**: å…¬å¹³æ¯”è¼ƒä¸åŒæ¨¡å‹çš„å„ªåŠ£
- âœ… **å…¨é¢çš„æ€§èƒ½è€ƒé‡**: ä¸åªçœ‹æº–ç¢ºåº¦ï¼Œé‚„è¦è€ƒæ…®æˆæœ¬ã€ç©©å¥æ€§ã€å¯è§£é‡‹æ€§

### 1.3 åŒ–å·¥é ˜åŸŸçš„ç‰¹æ®Šè€ƒé‡

åŒ–å·¥ç¨‹åºå»ºæ¨¡æœ‰å…¶ç¨ç‰¹æ€§ï¼š

| è€ƒé‡é¢å‘ | åŒ–å·¥ç‰¹é» | å°æ¨¡å‹é¸æ“‡çš„å½±éŸ¿ |
|---------|---------|-----------------|
| **å®‰å…¨æ€§** | æ¶‰åŠé«˜æº«ã€é«˜å£“ã€æœ‰æ¯’ç‰©è³ª | éœ€è¦é«˜å¯é æ€§æ¨¡å‹ï¼Œèª¤åˆ¤ä»£åƒ¹æ¥µé«˜ |
| **å¯è§£é‡‹æ€§** | å·¥ç¨‹å¸«éœ€ç†è§£æ¨¡å‹é‚è¼¯ | é»‘ç®±æ¨¡å‹éœ€é¡å¤–è§£é‡‹å·¥å…· |
| **å¯¦æ™‚æ€§** | ç·šä¸Šæ§åˆ¶éœ€å³æ™‚å›æ‡‰ | æ¨¡å‹è¨ˆç®—é€Ÿåº¦å¾ˆé‡è¦ |
| **å°æ¨£æœ¬** | å¯¦é©—æ•¸æ“šå–å¾—æˆæœ¬é«˜ | éœ€è¦æ“…é•·è™•ç†å°æ•¸æ“šçš„æ¨¡å‹ |
| **å¤šç›®æ¨™** | ç”¢ç‡ã€èƒ½è€—ã€å“è³ªéœ€å¹³è¡¡ | å–®ä¸€æŒ‡æ¨™ä¸è¶³ä»¥è©•ä¼°æ¨¡å‹ |
| **å‹•æ…‹è®ŠåŒ–** | ç¨‹åºæ¢ä»¶éš¨æ™‚é–“æ”¹è®Š | æ¨¡å‹éœ€å…·å‚™ç©©å¥æ€§ |

### 1.4 æœ¬å–®å…ƒæ¶æ§‹

```
Unit14 æ¨¡å‹è©•ä¼°èˆ‡é¸æ“‡
â”‚
â”œâ”€â”€ ç¬¬ä¸€éƒ¨åˆ†ï¼šè©•ä¼°æŒ‡æ¨™
â”‚   â”œâ”€â”€ å›æ­¸æŒ‡æ¨™ (MAE, MSE, RMSE, RÂ², MAPE)
â”‚   â””â”€â”€ åˆ†é¡æŒ‡æ¨™ (Accuracy, Precision, Recall, F1, ROC-AUC)
â”‚
â”œâ”€â”€ ç¬¬äºŒéƒ¨åˆ†ï¼šé©—è­‰æ–¹æ³•
â”‚   â”œâ”€â”€ äº¤å‰é©—è­‰æŠ€å·§ (K-Fold, Stratified, Time Series, Nested CV)
â”‚   â”œâ”€â”€ åå·®-æ–¹å·®æ¬Šè¡¡
â”‚   â””â”€â”€ å­¸ç¿’æ›²ç·šèˆ‡é©—è­‰æ›²ç·š
â”‚
â”œâ”€â”€ ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ¨¡å‹æ¯”è¼ƒ
â”‚   â”œâ”€â”€ çµ±è¨ˆæª¢å®šæ–¹æ³• (t-test, Wilcoxon test)
â”‚   â””â”€â”€ å¤šç›®æ¨™æ±ºç­–æ¡†æ¶
â”‚
â””â”€â”€ ç¬¬å››éƒ¨åˆ†ï¼šåŒ–å·¥æ‡‰ç”¨
    â”œâ”€â”€ å‚¬åŒ–åŠ‘ç¯©é¸æœ€ä½³åŒ–
    â””â”€â”€ å“è³ªé æ¸¬ç³»çµ±å»ºç½®
```

---

## 2. æ¨¡å‹è©•ä¼°çš„é‡è¦æ€§

### 2.1 æ¨¡å‹è©•ä¼° vs æ¨¡å‹è¨“ç·´

è¨±å¤šåˆå­¸è€…å®¹æ˜“é™·å…¥çš„è¿·æ€ï¼š

| è¿·æ€ | äº‹å¯¦ |
|------|------|
| "æˆ‘çš„æ¨¡å‹åœ¨è¨“ç·´é›†ä¸Š RÂ² = 0.99ï¼Œå¾ˆå²å®³ï¼" | å¯èƒ½éæ“¬åˆï¼Œæ¸¬è©¦é›†è¡¨ç¾å¯èƒ½å¾ˆå·® |
| "æº–ç¢ºåº¦ 95%ï¼Œæ‡‰è©²å¯ä»¥ç”¨äº†å§ï¼Ÿ" | ä¸å¹³è¡¡æ•¸æ“šä¸‹ï¼Œæº–ç¢ºåº¦å¯èƒ½èª¤å° |
| "é€™å€‹æ¨¡å‹è¨“ç·´æœ€å¿«ï¼Œå°±ç”¨å®ƒå§ï¼" | è¨“ç·´å¿«ä¸ä»£è¡¨é æ¸¬æº–ï¼Œéœ€ç¶œåˆè©•ä¼° |
| "è¤‡é›œçš„æ¨¡å‹ï¼ˆå¦‚æ·±åº¦å­¸ç¿’ï¼‰ä¸€å®šæ¯”ç°¡å–®æ¨¡å‹å¥½" | å°æ•¸æ“šä¸‹ï¼Œç°¡å–®æ¨¡å‹å¯èƒ½æ›´ç©©å¥ |

**æ­£ç¢ºçš„å»ºæ¨¡å¿ƒæ…‹**ï¼š
```
è¨“ç·´æ¨¡å‹ â‰  å®Œæˆå»ºæ¨¡
è©•ä¼°æ¨¡å‹ â†’ ç™¼ç¾å•é¡Œ â†’ æ”¹é€²æ¨¡å‹ â†’ å†è©•ä¼° â†’ ... â†’ éƒ¨ç½²æ‡‰ç”¨
```

### 2.2 è©•ä¼°æŒ‡æ¨™çš„é¸æ“‡åŸå‰‡

é¸æ“‡è©•ä¼°æŒ‡æ¨™æ™‚ï¼Œéœ€è€ƒæ…®ï¼š

#### 2.2.1 æ¥­å‹™ç›®æ¨™å°é½Š

ä¸åŒçš„åŒ–å·¥æ‡‰ç”¨ï¼Œå°éŒ¯èª¤çš„å®¹å¿åº¦ä¸åŒï¼š

**æ¡ˆä¾‹ 1: åæ‡‰å™¨æº«åº¦é æ¸¬**
- ç›®æ¨™ï¼šç¶­æŒç©©å®šæ“ä½œï¼Œé¿å…è¶…æº«
- é—œæ³¨ï¼š**æœ€å¤§èª¤å·® (Max Error)**ï¼Œè€Œéå¹³å‡èª¤å·®
- åŸå› ï¼šå³ä½¿ 99 æ¬¡é æ¸¬æº–ç¢ºï¼Œ1 æ¬¡å¤§èª¤å·®å°±å¯èƒ½å°è‡´å®‰å…¨äº‹æ•…

**æ¡ˆä¾‹ 2: ç”¢å“å“è³ªåˆ†ç´š (åˆæ ¼/ä¸åˆæ ¼)**
- ç›®æ¨™ï¼šæ¸›å°‘å®¢è¨´ï¼Œå¯§å¯èª¤åˆ¤åˆæ ¼ç‚ºä¸åˆæ ¼
- é—œæ³¨ï¼š**Recall (å¬å›ç‡)**ï¼Œç¢ºä¿ä¸æ”¾éä¸åˆæ ¼å“
- åŸå› ï¼šä¸åˆæ ¼å“æµå…¥å¸‚å ´çš„ä»£åƒ¹ >> èª¤åˆ¤åˆæ ¼å“çš„ä»£åƒ¹

**æ¡ˆä¾‹ 3: å‚¬åŒ–åŠ‘æ´»æ€§é æ¸¬**
- ç›®æ¨™ï¼šæº–ç¢ºä¼°è¨ˆå‚¬åŒ–åŠ‘æ€§èƒ½ï¼Œå„ªåŒ–é…æ–¹
- é—œæ³¨ï¼š**RÂ² å’Œ MAPE**ï¼Œè©•ä¼°æ•´é«”é æ¸¬èƒ½åŠ›
- åŸå› ï¼šéœ€è¦å…¨é¢è©•ä¼°é æ¸¬çš„æº–ç¢ºåº¦å’Œç›¸å°èª¤å·®

#### 2.2.2 æ•¸æ“šç‰¹æ€§è€ƒé‡

| æ•¸æ“šç‰¹æ€§ | æ¨è–¦æŒ‡æ¨™ | åŸå›  |
|---------|---------|------|
| **ä¸å¹³è¡¡åˆ†é¡** | Precision, Recall, F1, ROC-AUC | Accuracy æœƒè¢«å¤šæ•¸é¡ä¸»å° |
| **ç•°å¸¸å€¼å¤š** | MAE, Median Absolute Error | MSE/RMSE å°ç•°å¸¸å€¼æ•æ„Ÿ |
| **å°ºåº¦å·®ç•°å¤§** | MAPE, RÂ² | çµ•å°èª¤å·® (MAE/RMSE) ç„¡æ³•æ¯”è¼ƒä¸åŒå°ºåº¦å•é¡Œ |
| **æ™‚é–“åºåˆ—** | æ–¹å‘æº–ç¢ºåº¦ (Directional Accuracy) | è¶¨å‹¢é æ¸¬å¯èƒ½æ¯”æ•¸å€¼æº–ç¢ºæ€§æ›´é‡è¦ |
| **å¤šç›®æ¨™å•é¡Œ** | Pareto å‰æ²¿åˆ†æ | å–®ä¸€æŒ‡æ¨™ç„¡æ³•åæ˜ å¤šç›®æ¨™å¹³è¡¡ |

#### 2.2.3 æ¨¡å‹å¯æ¯”æ€§

è©•ä¼°å¤šå€‹æ¨¡å‹æ™‚ï¼Œéœ€ç¢ºä¿ï¼š
- âœ… **ç›¸åŒçš„è¨“ç·´/æ¸¬è©¦é›†åŠƒåˆ†**
- âœ… **ç›¸åŒçš„è©•ä¼°æŒ‡æ¨™**
- âœ… **ç›¸åŒçš„å‰è™•ç†æµç¨‹**
- âœ… **å¤šæ¬¡äº¤å‰é©—è­‰å–å¹³å‡**

### 2.3 å¸¸è¦‹çš„è©•ä¼°é™·é˜±

#### é™·é˜± 1: æ•¸æ“šæ´©æ¼ (Data Leakage)

**éŒ¯èª¤åšæ³•**ï¼š
```python
# âŒ åœ¨åŠƒåˆ†è¨“ç·´/æ¸¬è©¦é›†"ä¹‹å‰"é€²è¡Œæ¨™æº–åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)  # ä½¿ç”¨å…¨éƒ¨æ•¸æ“šè¨ˆç®—å‡å€¼/æ¨™æº–å·®
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)
```

**å•é¡Œ**ï¼šæ¸¬è©¦é›†çš„çµ±è¨ˆè³‡è¨Šæ´©æ¼åˆ°è¨“ç·´éç¨‹ï¼Œé«˜ä¼°æ¨¡å‹æ€§èƒ½ã€‚

**æ­£ç¢ºåšæ³•**ï¼š
```python
# âœ… å…ˆåŠƒåˆ†ï¼Œå†åªç”¨è¨“ç·´é›†é€²è¡Œæ¨™æº–åŒ–
X_train, X_test, y_train, y_test = train_test_split(X, y)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # åªç”¨è¨“ç·´é›†
X_test_scaled = scaler.transform(X_test)  # æ¸¬è©¦é›†ç”¨è¨“ç·´é›†çš„åƒæ•¸
```

#### é™·é˜± 2: éåº¦æ“¬åˆæ¸¬è©¦é›†

**å•é¡Œæƒ…å¢ƒ**ï¼š
1. è¨“ç·´å¤šå€‹æ¨¡å‹ï¼Œåœ¨æ¸¬è©¦é›†ä¸Šæ¯”è¼ƒ
2. é¸å‡ºæ¸¬è©¦é›†è¡¨ç¾æœ€å¥½çš„æ¨¡å‹
3. åŸºæ–¼æ¸¬è©¦é›†çµæœèª¿æ•´è¶…åƒæ•¸
4. é‡è¤‡æ­¥é©Ÿ 1-3 å¤šæ¬¡

**å¾Œæœ**ï¼šæ¸¬è©¦é›†è®Šæˆ "éš±æ€§çš„è¨“ç·´é›†"ï¼Œæ¨¡å‹åœ¨çœŸå¯¦æ–°æ•¸æ“šä¸Šè¡¨ç¾ä¸ä½³ã€‚

**è§£æ±ºæ–¹æ¡ˆ**ï¼šä½¿ç”¨ä¸‰çµ„æ•¸æ“šé›†
```
è¨“ç·´é›† (Training Set) â†’ è¨“ç·´æ¨¡å‹åƒæ•¸
é©—è­‰é›† (Validation Set) â†’ é¸æ“‡æ¨¡å‹ã€èª¿æ•´è¶…åƒæ•¸
æ¸¬è©¦é›† (Test Set) â†’ æœ€çµ‚è©•ä¼°ï¼Œåªç”¨ä¸€æ¬¡ï¼
```

æˆ–ä½¿ç”¨ **Nested Cross-Validation** (åµŒå¥—äº¤å‰é©—è­‰)ã€‚

#### é™·é˜± 3: å¿½ç•¥æ™‚é–“åºåˆ—çš„é †åºæ€§

**åŒ–å·¥å ´æ™¯**ï¼šé æ¸¬åæ‡‰å™¨ä¸‹ä¸€å°æ™‚çš„æº«åº¦

**éŒ¯èª¤åšæ³•**ï¼š
```python
# âŒ éš¨æ©ŸåŠƒåˆ†è¨“ç·´/æ¸¬è©¦é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
```

**å•é¡Œ**ï¼šç”¨ "æœªä¾†" çš„æ•¸æ“šé æ¸¬ "éå»"ï¼Œé«˜ä¼°æ¨¡å‹æ€§èƒ½ã€‚

**æ­£ç¢ºåšæ³•**ï¼š
```python
# âœ… æŒ‰æ™‚é–“é †åºåŠƒåˆ†
split_point = int(0.8 * len(X))
X_train, X_test = X[:split_point], X[split_point:]
y_train, y_test = y[:split_point], y[split_point:]
```

æˆ–ä½¿ç”¨ **Time Series Cross-Validation**ã€‚

### 2.4 è©•ä¼°æµç¨‹çš„æœ€ä½³å¯¦è¸

**æ¨™æº–è©•ä¼°æµç¨‹ (5 æ­¥é©Ÿ)**ï¼š

```python
# æ­¥é©Ÿ 1: åŠƒåˆ†æ•¸æ“šé›† (70% è¨“ç·´, 15% é©—è­‰, 15% æ¸¬è©¦)
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.176, random_state=42)

# æ­¥é©Ÿ 2: åœ¨è¨“ç·´é›†ä¸Šé€²è¡Œäº¤å‰é©—è­‰
cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')
print(f"CV RÂ²: {cv_scores.mean():.3f} Â± {cv_scores.std():.3f}")

# æ­¥é©Ÿ 3: åœ¨é©—è­‰é›†ä¸Šèª¿æ•´è¶…åƒæ•¸
param_grid = {'alpha': [0.1, 1, 10]}
grid_search = GridSearchCV(model, param_grid, cv=5)
grid_search.fit(X_train, y_train)
best_model = grid_search.best_estimator_

# æ­¥é©Ÿ 4: åœ¨é©—è­‰é›†ä¸Šè©•ä¼°å¤šç¨®æŒ‡æ¨™
y_val_pred = best_model.predict(X_val)
print(f"Validation MAE: {mean_absolute_error(y_val, y_val_pred):.3f}")
print(f"Validation RÂ²: {r2_score(y_val, y_val_pred):.3f}")

# æ­¥é©Ÿ 5: æœ€çµ‚åœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°ï¼ˆåªåšä¸€æ¬¡ï¼ï¼‰
y_test_pred = best_model.predict(X_test)
print(f"Test RÂ²: {r2_score(y_test, y_test_pred):.3f}")
```

**é—œéµåŸå‰‡**ï¼š
- ğŸ“Œ **æ¸¬è©¦é›†åªç”¨ä¸€æ¬¡**ï¼šä½œç‚ºæ¨¡å‹æœ€çµ‚çš„ç„¡åä¼°è¨ˆ
- ğŸ“Œ **é©—è­‰é›†ç”¨æ–¼èª¿åƒ**ï¼šå¯ä»¥å¤šæ¬¡ä½¿ç”¨ï¼Œä½†è¦è¨˜éŒ„æ‰€æœ‰å˜—è©¦
- ğŸ“Œ **è¨“ç·´é›†ç”¨æ–¼æ“¬åˆ**ï¼šå¯ä»¥ç”¨äº¤å‰é©—è­‰è©•ä¼°ç©©å®šæ€§
- ğŸ“Œ **è¨˜éŒ„æ‰€æœ‰å¯¦é©—**ï¼šé¿å…é¸æ“‡æ€§å ±å‘Šï¼Œç¢ºä¿çµæœå¯é‡ç¾

---

## 3. å›æ­¸æ¨¡å‹è©•ä¼°æŒ‡æ¨™

å›æ­¸æ¨¡å‹ç”¨æ–¼é æ¸¬é€£çºŒæ•¸å€¼ (å¦‚æº«åº¦ã€å£“åŠ›ã€ç”¢ç‡)ï¼Œå¸¸è¦‹è©•ä¼°æŒ‡æ¨™åŒ…æ‹¬ï¼š

### 3.1 å¹³å‡çµ•å°èª¤å·® (MAE - Mean Absolute Error)

#### å®šç¾©

$$
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
$$

å…¶ä¸­ï¼š
- $y_i$ ï¼šçœŸå¯¦å€¼
- $\hat{y}_i$ ï¼šé æ¸¬å€¼
- $n$ ï¼šæ¨£æœ¬æ•¸

#### ç‰¹æ€§

âœ… **å„ªé»**ï¼š
- ç›´è§€æ˜“æ‡‚ï¼Œèˆ‡ç›®æ¨™è®Šæ•¸åŒå–®ä½
- å°ç•°å¸¸å€¼ä¸æ•æ„Ÿ (robust)
- æ‰€æœ‰èª¤å·®æ¬Šé‡ç›¸åŒ

âŒ **ç¼ºé»**ï¼š
- ç„¡æ³•å€åˆ†é«˜ä¼°å’Œä½ä¼°
- ä¸å¯å¾®åˆ† (åœ¨æŸäº›å„ªåŒ–ç®—æ³•ä¸­ä¸é©ç”¨)

#### é©ç”¨å ´æ™¯

- ç•¶**ç•°å¸¸å€¼å­˜åœ¨**ä½†ä¸æ‡‰ä¸»å°è©•ä¼°
- éœ€è¦**ç›´è§€è§£é‡‹**èª¤å·®å¤§å°
- åŒ–å·¥æ¡ˆä¾‹ï¼šé æ¸¬åæ‡‰ç‰©æ¿ƒåº¦ï¼Œå¶çˆ¾çš„æ¸¬é‡èª¤å·®ä¸æ‡‰éåº¦å½±éŸ¿è©•ä¼°

#### sklearn å¯¦ä½œ

```python
from sklearn.metrics import mean_absolute_error

mae = mean_absolute_error(y_true, y_pred)
print(f"MAE: {mae:.3f}")
```

### 3.2 å‡æ–¹èª¤å·® (MSE - Mean Squared Error)

#### å®šç¾©

$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

#### ç‰¹æ€§

âœ… **å„ªé»**ï¼š
- å¯å¾®åˆ†ï¼Œé©åˆæ¢¯åº¦ä¸‹é™å„ªåŒ–
- æ•¸å­¸æ€§è³ªè‰¯å¥½
- æ”¾å¤§å¤§èª¤å·®çš„å½±éŸ¿

âŒ **ç¼ºé»**ï¼š
- å–®ä½æ˜¯ç›®æ¨™è®Šæ•¸çš„å¹³æ–¹ï¼Œä¸ç›´è§€
- å°ç•°å¸¸å€¼éå¸¸æ•æ„Ÿ
- é›£ä»¥è·¨å•é¡Œæ¯”è¼ƒ

#### é©ç”¨å ´æ™¯

- ç•¶**å¤§èª¤å·®ä»£åƒ¹é é«˜æ–¼å°èª¤å·®**
- éœ€è¦**æ‡²ç½°æ¥µç«¯é æ¸¬åå·®**
- åŒ–å·¥æ¡ˆä¾‹ï¼šåæ‡‰å™¨æº«åº¦æ§åˆ¶ï¼Œå¤§å¹…åé›¢å¯èƒ½å°è‡´å®‰å…¨å•é¡Œ

#### sklearn å¯¦ä½œ

```python
from sklearn.metrics import mean_squared_error

mse = mean_squared_error(y_true, y_pred)
print(f"MSE: {mse:.3f}")
```

### 3.3 å‡æ–¹æ ¹èª¤å·® (RMSE - Root Mean Squared Error)

#### å®šç¾©

$$
\text{RMSE} = \sqrt{\text{MSE}} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
$$

#### ç‰¹æ€§

âœ… **å„ªé»**ï¼š
- èˆ‡ç›®æ¨™è®Šæ•¸åŒå–®ä½ (æ¯” MSE æ›´ç›´è§€)
- ä¿ç•™ MSE å°å¤§èª¤å·®çš„æ•æ„Ÿæ€§
- æœ€å¸¸ç”¨çš„å›æ­¸æŒ‡æ¨™ä¹‹ä¸€

âŒ **ç¼ºé»**ï¼š
- ä»å°ç•°å¸¸å€¼æ•æ„Ÿ
- å—æ•¸æ“šå°ºåº¦å½±éŸ¿

#### é©ç”¨å ´æ™¯

- **ç¶œåˆè©•ä¼°**é æ¸¬æº–ç¢ºåº¦çš„é¦–é¸
- ç•¶å¤§èª¤å·®éœ€è¦è¢«é‡é»é—œæ³¨
- åŒ–å·¥æ¡ˆä¾‹ï¼šç”¢é‡é æ¸¬ï¼Œå¤§å¹…åå·®å½±éŸ¿ç”Ÿç”¢è¨ˆç•«

#### sklearn å¯¦ä½œ

```python
from sklearn.metrics import mean_squared_error
import numpy as np

rmse = np.sqrt(mean_squared_error(y_true, y_pred))
print(f"RMSE: {rmse:.3f}")

# æˆ–ä½¿ç”¨ squared=False åƒæ•¸
rmse = mean_squared_error(y_true, y_pred, squared=False)
```

### 3.4 æ±ºå®šä¿‚æ•¸ (RÂ² - Coefficient of Determination)

#### å®šç¾©

$$
R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2} = 1 - \frac{\text{SS}_{\text{res}}}{\text{SS}_{\text{tot}}}
$$

å…¶ä¸­ï¼š
- $\text{SS}_{\text{res}}$ ï¼šæ®˜å·®å¹³æ–¹å’Œ (Residual Sum of Squares)
- $\text{SS}_{\text{tot}}$ ï¼šç¸½å¹³æ–¹å’Œ (Total Sum of Squares)
- $\bar{y}$ ï¼šçœŸå¯¦å€¼çš„å¹³å‡

#### ç‰¹æ€§

âœ… **å„ªé»**ï¼š
- **ç„¡å–®ä½**ï¼Œç¯„åœ $(-\infty, 1]$ 
- $R^2 = 1$ ï¼šå®Œç¾é æ¸¬
- $R^2 = 0$ ï¼šæ¨¡å‹èˆ‡å¹³å‡å€¼é æ¸¬ç›¸åŒ
- $R^2 < 0$ ï¼šæ¨¡å‹æ¯”å¹³å‡å€¼é‚„å·®
- å¯è·¨å•é¡Œæ¯”è¼ƒ

âŒ **ç¼ºé»**ï¼š
- æ·»åŠ æ›´å¤šç‰¹å¾µæœƒå°è‡´ $R^2$ å–®èª¿å¢åŠ  (éåº¦æ“¬åˆé¢¨éšª)
- ä¸é©ç”¨æ–¼éç·šæ€§é—œä¿‚åˆ¤æ–·
- è¨“ç·´é›†ä¸Šçš„ $R^2$ å¯èƒ½éæ–¼æ¨‚è§€

#### é©ç”¨å ´æ™¯

- **æ¯”è¼ƒä¸åŒæ¨¡å‹**åœ¨ç›¸åŒæ•¸æ“šä¸Šçš„è¡¨ç¾
- è©•ä¼°æ¨¡å‹è§£é‡‹äº†å¤šå°‘è®Šç•°
- åŒ–å·¥æ¡ˆä¾‹ï¼šæ¯”è¼ƒä¸åŒå‚¬åŒ–åŠ‘æ€§èƒ½é æ¸¬æ¨¡å‹

#### sklearn å¯¦ä½œ

```python
from sklearn.metrics import r2_score

r2 = r2_score(y_true, y_pred)
print(f"RÂ²: {r2:.3f}")
```

#### é€²éšï¼šèª¿æ•´å¾Œçš„ RÂ² (Adjusted RÂ²)

$$
R^2_{\text{adj}} = 1 - \frac{(1 - R^2)(n - 1)}{n - p - 1}
$$

å…¶ä¸­ï¼š
- $n$ ï¼šæ¨£æœ¬æ•¸
- $p$ ï¼šç‰¹å¾µæ•¸

**ç›®çš„**ï¼šæ‡²ç½°æ¨¡å‹è¤‡é›œåº¦ï¼Œé˜²æ­¢éåº¦æ·»åŠ ç„¡ç”¨ç‰¹å¾µã€‚

```python
def adjusted_r2(r2, n_samples, n_features):
    return 1 - (1 - r2) * (n_samples - 1) / (n_samples - n_features - 1)

adj_r2 = adjusted_r2(r2, n_samples=100, n_features=5)
```

### 3.5 å¹³å‡çµ•å°ç™¾åˆ†æ¯”èª¤å·® (MAPE - Mean Absolute Percentage Error)

#### å®šç¾©

$$
\text{MAPE} = \frac{100\%}{n} \sum_{i=1}^{n} \left| \frac{y_i - \hat{y}_i}{y_i} \right|
$$

#### ç‰¹æ€§

âœ… **å„ªé»**ï¼š
- **ç›¸å°èª¤å·®**ï¼Œæ˜“æ–¼ç†è§£ (å¦‚ "èª¤å·® 5%")
- ç„¡å–®ä½ï¼Œå¯è·¨å°ºåº¦æ¯”è¼ƒ
- æ¥­å‹™éƒ¨é–€å®¹æ˜“æ¥å—

âŒ **ç¼ºé»**ï¼š
- ç•¶ $y_i \approx 0$ æ™‚æœƒçˆ†ç‚¸
- å°ä½ä¼°çš„æ‡²ç½°æ¯”é«˜ä¼°é‡
- ä¸å°ç¨±æ€§

#### é©ç”¨å ´æ™¯

- éœ€è¦**ç›¸å°èª¤å·®**è©•ä¼°
- ç›®æ¨™è®Šæ•¸ä¸æ¥è¿‘é›¶
- åŒ–å·¥æ¡ˆä¾‹ï¼šé æ¸¬ç”¢å“åƒ¹æ ¼ï¼Œç™¾åˆ†æ¯”èª¤å·®æ›´æœ‰æ„ç¾©

#### sklearn å¯¦ä½œ

```python
from sklearn.metrics import mean_absolute_percentage_error

mape = mean_absolute_percentage_error(y_true, y_pred) * 100
print(f"MAPE: {mape:.2f}%")
```

### 3.6 æŒ‡æ¨™æ¯”è¼ƒèˆ‡é¸æ“‡

| æŒ‡æ¨™ | å–®ä½ | å°ç•°å¸¸å€¼æ•æ„Ÿåº¦ | å¯è§£é‡‹æ€§ | é©ç”¨å ´æ™¯ |
|------|------|--------------|---------|---------|
| **MAE** | èˆ‡ $y$ åŒ | ä½ | â˜…â˜…â˜…â˜…â˜… | å­˜åœ¨ç•°å¸¸å€¼ï¼Œéœ€ç›´è§€è§£é‡‹ |
| **MSE** | $y^2$ | æ¥µé«˜ | â˜…â˜…â˜†â˜†â˜† | å¤§èª¤å·®ä»£åƒ¹é«˜ï¼Œå„ªåŒ–ç›®æ¨™ |
| **RMSE** | èˆ‡ $y$ åŒ | é«˜ | â˜…â˜…â˜…â˜…â˜† | ç¶œåˆè©•ä¼°ï¼Œæœ€å¸¸ç”¨ |
| **RÂ²** | ç„¡ | ä¸­ | â˜…â˜…â˜…â˜…â˜† | æ¯”è¼ƒæ¨¡å‹ï¼Œè§£é‡‹è®Šç•° |
| **MAPE** | % | ä¸­ | â˜…â˜…â˜…â˜…â˜… | ç›¸å°èª¤å·®é‡è¦ï¼Œæ¥­å‹™æºé€š |

### 3.7 åŒ–å·¥æ¡ˆä¾‹ï¼šåæ‡‰å™¨ç”¢ç‡é æ¸¬

**å ´æ™¯**ï¼šé æ¸¬åŒ–å­¸åæ‡‰ç”¢ç‡ (å–®ä½ï¼š%)ï¼Œç¯„åœ 60-95%

**æ•¸æ“šç‰¹é»**ï¼š
- 100 å€‹æ­·å²åæ‡‰æ•¸æ“š
- ç›®æ¨™è®Šæ•¸ï¼šç”¢ç‡ (Yield)
- ç‰¹å¾µï¼šæº«åº¦ã€å£“åŠ›ã€å‚¬åŒ–åŠ‘é‡ã€åæ‡‰æ™‚é–“

**æ¨¡å‹æ¯”è¼ƒçµæœ**ï¼š

| æ¨¡å‹ | MAE | RMSE | RÂ² | MAPE |
|------|-----|------|----|----|
| Linear Regression | 2.3% | 3.1% | 0.85 | 2.8% |
| Random Forest | 1.8% | 2.5% | 0.91 | 2.2% |
| XGBoost | 1.5% | 2.2% | 0.93 | 1.9% |

**æ±ºç­–è€ƒé‡**ï¼š
1. **æº–ç¢ºåº¦**ï¼šXGBoost æœ€ä½³ (MAPE 1.9%)
2. **ç©©å¥æ€§**ï¼šMAE é¡¯ç¤º XGBoost å°å€‹åˆ¥é æ¸¬ä¹Ÿè¼ƒå¯é 
3. **è§£é‡‹æ€§**ï¼šRandom Forest ç‰¹å¾µé‡è¦æ€§æ›´æ˜“è§£è®€
4. **è¨ˆç®—æˆæœ¬**ï¼šLinear Regression æœ€å¿«ï¼Œé©åˆå³æ™‚æ‡‰ç”¨

**æœ€çµ‚é¸æ“‡**ï¼š
- è‹¥é‡è¦–æº–ç¢ºåº¦ â†’ **XGBoost**
- è‹¥éœ€è¦å¿«é€Ÿæ¨ç† â†’ **Linear Regression**
- è‹¥éœ€è¦è§£é‡‹æ€§ â†’ **Random Forest** + SHAP åˆ†æ

---

## 4. åˆ†é¡æ¨¡å‹è©•ä¼°æŒ‡æ¨™

åˆ†é¡æ¨¡å‹ç”¨æ–¼é æ¸¬é›¢æ•£é¡åˆ¥ (å¦‚åˆæ ¼/ä¸åˆæ ¼ã€æ•…éšœé¡å‹A/B/C)ï¼Œè©•ä¼°æ–¹å¼èˆ‡å›æ­¸æ¨¡å‹å®Œå…¨ä¸åŒã€‚

### 4.1 æ··æ·†çŸ©é™£ (Confusion Matrix)

æ··æ·†çŸ©é™£æ˜¯æ‰€æœ‰åˆ†é¡æŒ‡æ¨™çš„åŸºç¤ã€‚

#### äºŒå…ƒåˆ†é¡æ··æ·†çŸ©é™£

|  | é æ¸¬ç‚ºæ­£é¡ (Positive) | é æ¸¬ç‚ºè² é¡ (Negative) |
|--|---------------------|---------------------|
| **å¯¦éš›ç‚ºæ­£é¡** | TP (True Positive) | FN (False Negative) |
| **å¯¦éš›ç‚ºè² é¡** | FP (False Positive) | TN (True Negative) |

**åŒ–å·¥æ¡ˆä¾‹**ï¼šç”¢å“å“è³ªæª¢æ¸¬ (æ­£é¡=ä¸åˆæ ¼ï¼Œè² é¡=åˆæ ¼)

| æƒ…æ³ | å«ç¾© | å¾Œæœ |
|------|------|------|
| **TP** | æ­£ç¢ºè­˜åˆ¥ä¸åˆæ ¼å“ | âœ… é¿å…ä¸è‰¯å“æµå‡º |
| **TN** | æ­£ç¢ºè­˜åˆ¥åˆæ ¼å“ | âœ… æ­£å¸¸å‡ºè²¨ |
| **FP** (Type I Error) | èª¤åˆ¤åˆæ ¼å“ç‚ºä¸åˆæ ¼ | âš ï¸ æµªè²»è‰¯å“ï¼Œå¢åŠ æˆæœ¬ |
| **FN** (Type II Error) | èª¤åˆ¤ä¸åˆæ ¼å“ç‚ºåˆæ ¼ | âŒ å®¢è¨´ã€å¬å›ã€å“ç‰Œæå®³ |

**é‡é»**ï¼šFN å’Œ FP çš„ä»£åƒ¹é€šå¸¸**ä¸å°ç­‰**ï¼Œéœ€æ ¹æ“šæ¥­å‹™å ´æ™¯æ±ºå®šå„ªå…ˆé™ä½å“ªä¸€å€‹ã€‚

#### sklearn å¯¦ä½œ

```python
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_true, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# æå–å„é …æ•¸å€¼
tn, fp, fn, tp = cm.ravel()
```

### 4.2 æº–ç¢ºç‡ (Accuracy)

#### å®šç¾©

$$
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} = \frac{\text{æ­£ç¢ºé æ¸¬æ•¸}}{\text{ç¸½æ¨£æœ¬æ•¸}}
$$

#### ç‰¹æ€§

âœ… **å„ªé»**ï¼š
- æœ€ç›´è§€çš„æŒ‡æ¨™
- è¨ˆç®—ç°¡å–®

âŒ **ç¼ºé»**ï¼š
- **ä¸å¹³è¡¡æ•¸æ“šä¸‹åš´é‡èª¤å°**
- ç„¡æ³•å€åˆ†ä¸åŒé¡å‹çš„éŒ¯èª¤

#### ä¸å¹³è¡¡æ•¸æ“šé™·é˜±

**æ¡ˆä¾‹**ï¼šåæ‡‰å™¨æ•…éšœé æ¸¬
- æ­£å¸¸é‹è¡Œï¼š9900 ç­†
- æ•…éšœï¼š100 ç­†

**æ¨¡å‹ A** (å…¨éƒ¨é æ¸¬ç‚ºæ­£å¸¸)ï¼š
- Accuracy = 9900 / 10000 = **99%** âœ¨
- ä½†å®Œå…¨ç„¡æ³•é æ¸¬æ•…éšœï¼âŒ

**æ¨¡å‹ B** (æ­£ç¢ºé æ¸¬ 80% æ•…éšœ)ï¼š
- TP=80, FN=20, TN=9800, FP=100
- Accuracy = (80+9800) / 10000 = **98.8%**
- çœ‹ä¼¼æ¯”æ¨¡å‹ A å·®ï¼Œä½†å¯¦éš›æœ‰ç”¨ï¼

**çµè«–**ï¼šä¸å¹³è¡¡æ•¸æ“šæ™‚ï¼ŒAccuracy ç„¡æ„ç¾©ï¼Œéœ€è¦å…¶ä»–æŒ‡æ¨™ã€‚

#### sklearn å¯¦ä½œ

```python
from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_true, y_pred)
print(f"Accuracy: {accuracy:.3f}")
```

### 4.3 ç²¾ç¢ºç‡ (Precision)

#### å®šç¾©

$$
\text{Precision} = \frac{TP}{TP + FP} = \frac{\text{æ­£ç¢ºé æ¸¬ç‚ºæ­£é¡çš„æ•¸é‡}}{\text{æ‰€æœ‰é æ¸¬ç‚ºæ­£é¡çš„æ•¸é‡}}
$$

#### å«ç¾©

åœ¨æ‰€æœ‰**æ¨¡å‹é æ¸¬ç‚ºæ­£é¡**çš„æ¨£æœ¬ä¸­ï¼ŒçœŸæ­£æ˜¯æ­£é¡çš„æ¯”ä¾‹ã€‚

**å•é¡Œ**ï¼š"ç•¶æ¨¡å‹èªªé€™æ˜¯ä¸åˆæ ¼å“æ™‚ï¼Œå®ƒæœ‰å¤šå¯ä¿¡ï¼Ÿ"

#### é©ç”¨å ´æ™¯

- ç•¶ **FP ä»£åƒ¹å¾ˆé«˜**
- åŒ–å·¥æ¡ˆä¾‹ï¼š
  - **æ–°ææ–™ç¯©é¸**ï¼šèª¤åˆ¤ç‚ºæœ‰æ½›åŠ›çš„ææ–™æµªè²»å¾ŒçºŒæ˜‚è²´çš„å¯¦é©—
  - **ç”¢å“å¬å›æ±ºç­–**ï¼šèª¤åˆ¤è§¸ç™¼å¬å›é€ æˆå·¨å¤§æˆæœ¬

#### sklearn å¯¦ä½œ

```python
from sklearn.metrics import precision_score

precision = precision_score(y_true, y_pred)
print(f"Precision: {precision:.3f}")
```

### 4.4 å¬å›ç‡ (Recall / Sensitivity / True Positive Rate)

#### å®šç¾©

$$
\text{Recall} = \frac{TP}{TP + FN} = \frac{\text{æ­£ç¢ºé æ¸¬ç‚ºæ­£é¡çš„æ•¸é‡}}{\text{å¯¦éš›ç‚ºæ­£é¡çš„æ•¸é‡}}
$$

#### å«ç¾©

åœ¨æ‰€æœ‰**å¯¦éš›ç‚ºæ­£é¡**çš„æ¨£æœ¬ä¸­ï¼Œæ¨¡å‹æˆåŠŸè­˜åˆ¥çš„æ¯”ä¾‹ã€‚

**å•é¡Œ**ï¼š"å¯¦éš›çš„ä¸åˆæ ¼å“ä¸­ï¼Œæœ‰å¤šå°‘è¢«æ¨¡å‹æŠ“å‡ºä¾†äº†ï¼Ÿ"

#### é©ç”¨å ´æ™¯

- ç•¶ **FN ä»£åƒ¹å¾ˆé«˜**
- åŒ–å·¥æ¡ˆä¾‹ï¼š
  - **è¨­å‚™æ•…éšœé è­¦**ï¼šæ¼æ‰æ•…éšœå¾µå…†å¯èƒ½å°è‡´åœæ©Ÿæˆ–äº‹æ•…
  - **å®‰å…¨äº‹æ•…é æ¸¬**ï¼šå¯§å¯èª¤å ±ä¹Ÿä¸èƒ½æ¼å ±
  - **ç’°ä¿æ’æ”¾ç›£æ¸¬**ï¼šæ¼æª¢è¶…æ¨™æ’æ”¾å¯èƒ½é¢è‡¨æ³•å¾‹è²¬ä»»

#### sklearn å¯¦ä½œ

```python
from sklearn.metrics import recall_score

recall = recall_score(y_true, y_pred)
print(f"Recall: {recall:.3f}")
```

### 4.5 F1 åˆ†æ•¸ (F1-Score)

#### å®šç¾©

$$
F_1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} = \frac{2 \times TP}{2 \times TP + FP + FN}
$$

#### ç‰¹æ€§

- Precision å’Œ Recall çš„**èª¿å’Œå¹³å‡æ•¸**
- å¹³è¡¡å…©è€…çš„æ¬Šé‡
- ç•¶ Precision æˆ– Recall ä»»ä¸€å€‹å¾ˆä½æ™‚ï¼ŒF1 ä¹Ÿæœƒå¾ˆä½

#### é©ç”¨å ´æ™¯

- éœ€è¦**å¹³è¡¡ Precision å’Œ Recall**
- ä¸å¹³è¡¡æ•¸æ“šçš„ç¶œåˆè©•ä¼°
- åŒ–å·¥æ¡ˆä¾‹ï¼šå“è³ªæª¢æ¸¬ç³»çµ±ï¼Œæ—¢è¦æ¸›å°‘èª¤åˆ¤ (Precision)ï¼Œåˆè¦é¿å…æ¼æª¢ (Recall)

#### sklearn å¯¦ä½œ

```python
from sklearn.metrics import f1_score

f1 = f1_score(y_true, y_pred)
print(f"F1-Score: {f1:.3f}")
```

#### é€²éšï¼šF-beta åˆ†æ•¸

$$
F_\beta = (1 + \beta^2) \times \frac{\text{Precision} \times \text{Recall}}{\beta^2 \times \text{Precision} + \text{Recall}}
$$

- $\beta > 1$ ï¼šæ›´é‡è¦– Recall
- $\beta < 1$ ï¼šæ›´é‡è¦– Precision
- $\beta = 1$ ï¼šå³ F1-Score

```python
from sklearn.metrics import fbeta_score

# æ›´é‡è¦– Recall (FN ä»£åƒ¹æ›´é«˜)
f2 = fbeta_score(y_true, y_pred, beta=2)

# æ›´é‡è¦– Precision (FP ä»£åƒ¹æ›´é«˜)
f0_5 = fbeta_score(y_true, y_pred, beta=0.5)
```

### 4.6 ROC æ›²ç·šèˆ‡ AUC

#### ROC æ›²ç·š (Receiver Operating Characteristic Curve)

**å®šç¾©**ï¼š
- X è»¸ï¼šFalse Positive Rate (FPR) = $\frac{FP}{FP + TN}$ 
- Y è»¸ï¼šTrue Positive Rate (TPR) = Recall = $\frac{TP}{TP + FN}$ 

**åŸç†**ï¼š
- åˆ†é¡æ¨¡å‹é€šå¸¸è¼¸å‡ºæ©Ÿç‡ $P(y=1)$ 
- è¨­å®šä¸åŒé–¾å€¼ (threshold) å°‡æ©Ÿç‡è½‰ç‚ºé¡åˆ¥
- é–¾å€¼è¶Šä½ï¼Œè¶Šå®¹æ˜“é æ¸¬ç‚ºæ­£é¡ â†’ TPR â†‘, FPR â†‘
- ç¹ªè£½æ‰€æœ‰é–¾å€¼ä¸‹çš„ (FPR, TPR) é»é€£æˆæ›²ç·š

#### AUC (Area Under the ROC Curve)

**å®šç¾©**ï¼šROC æ›²ç·šä¸‹æ–¹çš„é¢ç©

**ç¯„åœ**ï¼š0 åˆ° 1
- AUC = 0.5ï¼šéš¨æ©ŸçŒœæ¸¬ (å°è§’ç·š)
- AUC = 1.0ï¼šå®Œç¾åˆ†é¡
- AUC < 0.5ï¼šæ¯”éš¨æ©Ÿé‚„å·® (é æ¸¬åäº†)

**å„ªé»**ï¼š
- ä¸å—åˆ†é¡é–¾å€¼å½±éŸ¿
- ä¸å—é¡åˆ¥ä¸å¹³è¡¡å½±éŸ¿ (æ¯” Accuracy ç©©å¥)
- å–®ä¸€æ•¸å€¼ï¼Œæ˜“æ–¼æ¯”è¼ƒæ¨¡å‹

**ç¼ºé»**ï¼š
- ä¸ç›´è§€ï¼Œé›£ä»¥å‘éæŠ€è¡“äººå“¡è§£é‡‹
- å°æ¥µåº¦ä¸å¹³è¡¡æ•¸æ“šï¼ŒPrecision-Recall Curve æ›´å¥½

#### sklearn å¯¦ä½œ

```python
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

# è¨ˆç®— ROC æ›²ç·š
y_proba = model.predict_proba(X_test)[:, 1]  # æ­£é¡çš„æ©Ÿç‡
fpr, tpr, thresholds = roc_curve(y_test, y_proba)

# è¨ˆç®— AUC
auc = roc_auc_score(y_test, y_proba)

# ç¹ªè£½ ROC æ›²ç·š
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.3f})')
plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate (Recall)')
plt.title('ROC Curve')
plt.legend()
plt.grid(True)
plt.show()
```

### 4.7 Precision-Recall æ›²ç·š

#### é©ç”¨å ´æ™¯

ç•¶**æ­£é¡æ¨£æœ¬éå¸¸å°‘**æ™‚ (æ¥µåº¦ä¸å¹³è¡¡)ï¼ŒPR æ›²ç·šæ¯” ROC æ›²ç·šæ›´èƒ½åæ˜ æ¨¡å‹æ€§èƒ½ã€‚

**åŸå› **ï¼šROC æ›²ç·šçš„ FPR åˆ†æ¯åŒ…å«å¤§é‡è² é¡ (TN)ï¼Œå³ä½¿ FP å¾ˆå¤šï¼ŒFPR ä»å¯èƒ½å¾ˆä½ï¼Œç„¡æ³•åæ˜ å•é¡Œã€‚

#### sklearn å¯¦ä½œ

```python
from sklearn.metrics import precision_recall_curve, average_precision_score

precision, recall, thresholds = precision_recall_curve(y_test, y_proba)
ap = average_precision_score(y_test, y_proba)

plt.figure(figsize=(8, 6))
plt.plot(recall, precision, label=f'PR Curve (AP = {ap:.3f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.grid(True)
plt.show()
```

### 4.8 å¤šåˆ†é¡è©•ä¼°æŒ‡æ¨™

#### æ··æ·†çŸ©é™£

å¤šåˆ†é¡å•é¡Œçš„æ··æ·†çŸ©é™£æ˜¯ $n \times n$ çŸ©é™£ (n ç‚ºé¡åˆ¥æ•¸)ã€‚

```python
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm = confusion_matrix(y_true, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
disp.plot(cmap='Blues')
plt.show()
```

#### Macro / Micro / Weighted å¹³å‡

å¤šåˆ†é¡çš„ Precision, Recall, F1 æœ‰ä¸‰ç¨®å¹³å‡æ–¹å¼ï¼š

| å¹³å‡æ–¹å¼ | è¨ˆç®—æ–¹æ³• | é©ç”¨å ´æ™¯ |
|---------|---------|---------|
| **Macro** | å„é¡åˆ¥æŒ‡æ¨™çš„ç®—è¡“å¹³å‡ | å„é¡åˆ¥åŒç­‰é‡è¦ |
| **Micro** | å…¨å±€ TP, FP, FN è¨ˆç®— | é‡è¦–æ¨£æœ¬å¤šçš„é¡åˆ¥ |
| **Weighted** | æŒ‰é¡åˆ¥æ¨£æœ¬æ•¸åŠ æ¬Šå¹³å‡ | å¹³è¡¡é¡åˆ¥é‡è¦æ€§èˆ‡æ¨£æœ¬æ•¸ |

```python
from sklearn.metrics import classification_report

print(classification_report(y_true, y_pred, target_names=class_names))
```

**è¼¸å‡ºç¯„ä¾‹**ï¼š
```
              precision    recall  f1-score   support

     Class A       0.85      0.90      0.87       100
     Class B       0.78      0.72      0.75        50
     Class C       0.92      0.88      0.90       150

    accuracy                           0.87       300
   macro avg       0.85      0.83      0.84       300
weighted avg       0.87      0.87      0.87       300
```

### 4.9 æŒ‡æ¨™é¸æ“‡æ±ºç­–æ¨¹

```
æ˜¯å¦ç‚ºä¸å¹³è¡¡æ•¸æ“šï¼Ÿ
â”œâ”€ å¦ â†’ Accuracy + F1-Score
â””â”€ æ˜¯ â†’ FP å’Œ FN å“ªå€‹ä»£åƒ¹æ›´é«˜ï¼Ÿ
    â”œâ”€ FN ä»£åƒ¹é«˜ (æ¼å ±åš´é‡) â†’ å„ªåŒ– Recall
    â”œâ”€ FP ä»£åƒ¹é«˜ (èª¤å ±åš´é‡) â†’ å„ªåŒ– Precision
    â””â”€ éƒ½é‡è¦ â†’ F1-Score + ROC-AUC / PR-AUC
```

### 4.10 åŒ–å·¥æ¡ˆä¾‹ï¼šåæ‡‰å™¨æ•…éšœåˆ†é¡

**å ´æ™¯**ï¼šé æ¸¬åæ‡‰å™¨æ•…éšœé¡å‹
- é¡åˆ¥ Aï¼šæ­£å¸¸é‹è¡Œ (9000 ç­†)
- é¡åˆ¥ Bï¼šè¼•å¾®ç•°å¸¸ (800 ç­†)
- é¡åˆ¥ Cï¼šåš´é‡æ•…éšœ (200 ç­†)

**æ¨¡å‹æ¯”è¼ƒ**ï¼š

| æ¨¡å‹ | Accuracy | Macro F1 | Weighted F1 | Class C Recall |
|------|----------|----------|-------------|---------------|
| Logistic Regression | 0.92 | 0.65 | 0.89 | 0.45 |
| Random Forest | 0.94 | 0.78 | 0.92 | 0.72 |
| XGBoost | 0.95 | 0.82 | 0.94 | 0.85 |

**æ±ºç­–è€ƒé‡**ï¼š
1. **Accuracy** å—é¡åˆ¥ A ä¸»å°ï¼Œä¸å¯é 
2. **Macro F1** å¹³ç­‰å°å¾…å„é¡åˆ¥ï¼ŒXGBoost æœ€ä½³
3. **Class C Recall** æœ€é—œéµ (åš´é‡æ•…éšœä¸èƒ½æ¼å ±)
4. XGBoost å°åš´é‡æ•…éšœçš„å¬å›ç‡ 85%ï¼Œé¡¯è‘—å„ªæ–¼å…¶ä»–æ¨¡å‹

**æœ€çµ‚é¸æ“‡**ï¼š**XGBoost**ï¼Œä¸¦é‡å°é¡åˆ¥ C èª¿æ•´åˆ†é¡é–¾å€¼é€²ä¸€æ­¥æå‡ Recallã€‚

---

## 5. äº¤å‰é©—è­‰é€²éšæŠ€å·§

äº¤å‰é©—è­‰ (Cross-Validation, CV) æ˜¯è©•ä¼°æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„é»ƒé‡‘æ¨™æº–ï¼Œæ¯”å–®æ¬¡åŠƒåˆ†è¨“ç·´/æ¸¬è©¦é›†æ›´ç©©å¥ã€‚

### 5.1 K-Fold Cross-Validation (K æŠ˜äº¤å‰é©—è­‰)

#### åŸç†

1. å°‡æ•¸æ“šé›†éš¨æ©ŸåŠƒåˆ†ç‚º K å€‹å¤§å°ç›¸ç­‰çš„å­é›† (folds)
2. æ¯æ¬¡ä½¿ç”¨ K-1 å€‹ fold è¨“ç·´ï¼Œ1 å€‹ fold æ¸¬è©¦
3. é‡è¤‡ K æ¬¡ï¼Œæ¯å€‹ fold éƒ½ä½œç‚ºæ¸¬è©¦é›†ä¸€æ¬¡
4. å– K æ¬¡çµæœçš„å¹³å‡ä½œç‚ºæœ€çµ‚è©•ä¼°

#### å„ªé»

- å……åˆ†åˆ©ç”¨æ•¸æ“š (æ¯å€‹æ¨£æœ¬éƒ½è¢«æ¸¬è©¦é)
- è©•ä¼°æ›´ç©©å¥ï¼Œæ¸›å°‘éš¨æ©ŸåŠƒåˆ†çš„å½±éŸ¿
- å¯è¨ˆç®—æ€§èƒ½çš„**æ¨™æº–å·®**ï¼Œè©•ä¼°æ¨¡å‹ç©©å®šæ€§

#### sklearn å¯¦ä½œ

```python
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(random_state=42)

# 5 æŠ˜äº¤å‰é©—è­‰
scores = cross_val_score(model, X, y, cv=5, scoring='r2')

print(f"Cross-validation RÂ² scores: {scores}")
print(f"Mean RÂ²: {scores.mean():.3f} Â± {scores.std():.3f}")
```

#### K å€¼é¸æ“‡

| K å€¼ | è¨“ç·´é›†æ¯”ä¾‹ | è¨ˆç®—æˆæœ¬ | æ–¹å·® | åå·® | é©ç”¨å ´æ™¯ |
|------|----------|---------|------|------|---------|
| **K=5** | 80% | ä½ | è¼ƒé«˜ | è¼ƒä½ | å¤§æ•¸æ“šé›†ï¼Œå¿«é€Ÿè©•ä¼° |
| **K=10** | 90% | ä¸­ | é©ä¸­ | é©ä¸­ | **æœ€å¸¸ç”¨**ï¼Œå¹³è¡¡åå·®èˆ‡æ–¹å·® |
| **K=N** (LOOCV) | $\frac{N-1}{N}$ | æ¥µé«˜ | æ¥µé«˜ | æ¥µä½ | å°æ•¸æ“šé›† |

**å»ºè­°**ï¼šä¸€èˆ¬ä½¿ç”¨ **K=5 æˆ– K=10**ã€‚

### 5.2 Stratified K-Fold Cross-Validation (åˆ†å±¤ K æŠ˜)

#### å•é¡Œ

æ¨™æº– K-Fold å¯èƒ½å°è‡´æŸäº› fold çš„é¡åˆ¥åˆ†ä½ˆä¸å‡ï¼Œå°¤å…¶åœ¨**ä¸å¹³è¡¡æ•¸æ“š**ä¸­ã€‚

**ä¾‹å­**ï¼š
- ç¸½æ•¸æ“šï¼š1000 ç­† (900 æ­£å¸¸, 100 æ•…éšœ)
- 5-Fold åŠƒåˆ†æ™‚ï¼Œå¯èƒ½æŸå€‹ fold åªæœ‰ 5 å€‹æ•…éšœæ¨£æœ¬ï¼Œå¦ä¸€å€‹æœ‰ 30 å€‹

#### è§£æ±ºæ–¹æ¡ˆ

Stratified K-Fold ç¢ºä¿æ¯å€‹ fold çš„**é¡åˆ¥æ¯”ä¾‹**èˆ‡åŸå§‹æ•¸æ“šç›¸åŒã€‚

#### sklearn å¯¦ä½œ

```python
from sklearn.model_selection import StratifiedKFold, cross_val_score

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=cv, scoring='f1')

print(f"Stratified CV F1: {scores.mean():.3f} Â± {scores.std():.3f}")
```

**ä½¿ç”¨æ™‚æ©Ÿ**ï¼š
- âœ… åˆ†é¡å•é¡Œ (ç‰¹åˆ¥æ˜¯ä¸å¹³è¡¡æ•¸æ“š)
- âŒ å›æ­¸å•é¡Œ (ç„¡é¡åˆ¥æ¦‚å¿µ)

### 5.3 Time Series Cross-Validation (æ™‚é–“åºåˆ—äº¤å‰é©—è­‰)

#### å•é¡Œ

åŒ–å·¥ç¨‹åºæ•¸æ“šå¸¸å…·æœ‰**æ™‚é–“åºåˆ—ç‰¹æ€§**ï¼š
- å‰å¾Œæ¨£æœ¬å­˜åœ¨ç›¸é—œæ€§
- éš¨æ©ŸåŠƒåˆ†æœƒå°è‡´ "ç”¨æœªä¾†é æ¸¬éå»"
- æ¨™æº– K-Fold æœƒ**åš´é‡é«˜ä¼°**æ¨¡å‹æ€§èƒ½

#### è§£æ±ºæ–¹æ¡ˆï¼šæ™‚é–“åºåˆ—åˆ†å‰² (Time Series Split)

**åŸç†**ï¼š
- ä¿æŒæ™‚é–“é †åº
- è¨“ç·´é›†å§‹çµ‚åœ¨æ¸¬è©¦é›†ä¹‹å‰
- é€æ­¥æ“´å¤§è¨“ç·´é›†

**åœ–ç¤º**ï¼š
```
Fold 1: [Train] [Test]
Fold 2: [Train---] [Test]
Fold 3: [Train-------] [Test]
Fold 4: [Train-----------] [Test]
Fold 5: [Train---------------] [Test]
```

#### sklearn å¯¦ä½œ

```python
from sklearn.model_selection import TimeSeriesSplit

tscv = TimeSeriesSplit(n_splits=5)

for train_index, test_index in tscv.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    model.fit(X_train, y_train)
    score = model.score(X_test, y_test)
    print(f"Fold RÂ²: {score:.3f}")
```

**åŒ–å·¥æ¡ˆä¾‹**ï¼šåæ‡‰å™¨æº«åº¦é æ¸¬
- æ•¸æ“šï¼šé€£çºŒ 1000 å°æ™‚çš„é‹è¡Œè¨˜éŒ„
- ç›®æ¨™ï¼šé æ¸¬ä¸‹ä¸€å°æ™‚æº«åº¦
- å¿…é ˆä½¿ç”¨ TimeSeriesSplitï¼Œå¦å‰‡æ¨¡å‹æœƒ "ä½œå¼Š"

### 5.4 Leave-One-Out Cross-Validation (ç•™ä¸€æ³•)

#### åŸç†

- ç‰¹æ®Šçš„ K-Foldï¼Œå…¶ä¸­ K = N (æ¨£æœ¬æ•¸)
- æ¯æ¬¡åªç•™ä¸€å€‹æ¨£æœ¬ä½œç‚ºæ¸¬è©¦é›†
- é‡è¤‡ N æ¬¡

#### å„ªé»

- è¨“ç·´é›†æœ€å¤§ (N-1 å€‹æ¨£æœ¬)
- åå·®æœ€ä½

#### ç¼ºé»

- è¨ˆç®—æˆæœ¬æ¥µé«˜ (è¨“ç·´ N æ¬¡)
- æ–¹å·®é«˜ (æ¸¬è©¦é›†åªæœ‰ 1 å€‹æ¨£æœ¬)

#### é©ç”¨å ´æ™¯

- **å°æ•¸æ“šé›†** (N < 100)
- è¨ˆç®—è³‡æºå……è¶³
- åŒ–å·¥æ¡ˆä¾‹ï¼šæ˜‚è²´çš„å‚¬åŒ–åŠ‘å¯¦é©—ï¼Œåªæœ‰ 50 å€‹æ¨£æœ¬

#### sklearn å¯¦ä½œ

```python
from sklearn.model_selection import LeaveOneOut

loo = LeaveOneOut()
scores = cross_val_score(model, X, y, cv=loo, scoring='r2')

print(f"LOOCV RÂ²: {scores.mean():.3f} Â± {scores.std():.3f}")
```

### 5.5 Nested Cross-Validation (åµŒå¥—äº¤å‰é©—è­‰)

#### å•é¡Œ

ä½¿ç”¨äº¤å‰é©—è­‰èª¿æ•´è¶…åƒæ•¸æ™‚ï¼Œå¯èƒ½**éåº¦æ“¬åˆé©—è­‰é›†**ï¼Œå°è‡´é«˜ä¼°æ¨¡å‹æ€§èƒ½ã€‚

#### è§£æ±ºæ–¹æ¡ˆï¼šå…©å±¤ CV

**å¤–å±¤ CV**ï¼šè©•ä¼°æ¨¡å‹æ³›åŒ–èƒ½åŠ› (å…¬æ­£è©•ä¼°)  
**å…§å±¤ CV**ï¼šé¸æ“‡è¶…åƒæ•¸ (èª¿åƒ)

**æµç¨‹**ï¼š
```
å¤–å±¤ Fold 1:
  â”œâ”€ å…§å±¤ CV (5-Fold) â†’ æ‰¾å‡ºæœ€ä½³è¶…åƒæ•¸
  â””â”€ ç”¨æœ€ä½³è¶…åƒæ•¸åœ¨å¤–å±¤æ¸¬è©¦é›†è©•ä¼°
å¤–å±¤ Fold 2:
  â”œâ”€ å…§å±¤ CV (5-Fold) â†’ æ‰¾å‡ºæœ€ä½³è¶…åƒæ•¸
  â””â”€ ç”¨æœ€ä½³è¶…åƒæ•¸åœ¨å¤–å±¤æ¸¬è©¦é›†è©•ä¼°
...
å¤–å±¤ Fold 5:
  â”œâ”€ å…§å±¤ CV (5-Fold) â†’ æ‰¾å‡ºæœ€ä½³è¶…åƒæ•¸
  â””â”€ ç”¨æœ€ä½³è¶…åƒæ•¸åœ¨å¤–å±¤æ¸¬è©¦é›†è©•ä¼°

æœ€çµ‚è©•ä¼° = å¤–å±¤ 5 å€‹ fold çš„å¹³å‡
```

#### sklearn å¯¦ä½œ

```python
from sklearn.model_selection import GridSearchCV, cross_val_score

# å…§å±¤ CVï¼šè¶…åƒæ•¸æœç´¢
param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [5, 10, 15]}
inner_cv = 5
grid_search = GridSearchCV(RandomForestRegressor(), param_grid, cv=inner_cv)

# å¤–å±¤ CVï¼šè©•ä¼°æ³›åŒ–èƒ½åŠ›
outer_cv = 5
scores = cross_val_score(grid_search, X, y, cv=outer_cv, scoring='r2')

print(f"Nested CV RÂ²: {scores.mean():.3f} Â± {scores.std():.3f}")
```

#### è¨ˆç®—æˆæœ¬

- å¤–å±¤ 5-Fold Ã— å…§å±¤ 5-Fold Ã— è¶…åƒæ•¸çµ„åˆ 9 å€‹ = **225 æ¬¡è¨“ç·´**
- é©åˆå°æ•¸æ“šé›†æˆ–é—œéµæ‡‰ç”¨
- å¤§æ•¸æ“šé›†å¯ç”¨å–®å±¤ CV + ç¨ç«‹æ¸¬è©¦é›†

### 5.6 äº¤å‰é©—è­‰æœ€ä½³å¯¦è¸

#### å¯¦è¸å»ºè­°

1. **æ¨™æº–æ•¸æ“šé›†** â†’ K-Fold (K=5 æˆ– 10)
2. **ä¸å¹³è¡¡åˆ†é¡** â†’ Stratified K-Fold
3. **æ™‚é–“åºåˆ—** â†’ Time Series Split
4. **å°æ•¸æ“šé›†** â†’ LOOCV æˆ– K=10
5. **èª¿åƒéœ€æ±‚** â†’ Nested CV æˆ– è¨“ç·´/é©—è­‰/æ¸¬è©¦ ä¸‰åˆ†æ³•

#### å¸¸è¦‹éŒ¯èª¤

âŒ **éŒ¯èª¤ 1**ï¼šåœ¨ CV ä¹‹å‰é€²è¡Œç‰¹å¾µé¸æ“‡
```python
# âŒ ç‰¹å¾µé¸æ“‡ä½¿ç”¨äº†å…¨éƒ¨æ•¸æ“š
selected_features = select_features(X, y)
X_selected = X[selected_features]
cv_score = cross_val_score(model, X_selected, y, cv=5)
```

âœ… **æ­£ç¢ºåšæ³•**ï¼šç‰¹å¾µé¸æ“‡æ‡‰åœ¨ CV çš„æ¯å€‹ fold å…§éƒ¨é€²è¡Œ
```python
# âœ… ä½¿ç”¨ Pipeline ç¢ºä¿ç‰¹å¾µé¸æ“‡åœ¨ CV å…§éƒ¨
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SelectKBest

pipe = Pipeline([
    ('feature_selection', SelectKBest(k=10)),
    ('model', RandomForestRegressor())
])
cv_score = cross_val_score(pipe, X, y, cv=5)
```

âŒ **éŒ¯èª¤ 2**ï¼šæ™‚é–“åºåˆ—ä½¿ç”¨éš¨æ©ŸåŠƒåˆ†
âŒ **éŒ¯èª¤ 3**ï¼šæ¸¬è©¦é›†åƒèˆ‡ CV (æ‡‰è©²å®Œå…¨ç¨ç«‹)

#### è©•ä¼°å ±å‘Šç¯„ä¾‹

```python
from sklearn.model_selection import cross_validate

cv_results = cross_validate(
    model, X, y, cv=5,
    scoring=['r2', 'neg_mean_absolute_error', 'neg_root_mean_squared_error'],
    return_train_score=True
)

print("Cross-Validation Results:")
print(f"Train RÂ²: {cv_results['train_r2'].mean():.3f} Â± {cv_results['train_r2'].std():.3f}")
print(f"Test RÂ²: {cv_results['test_r2'].mean():.3f} Â± {cv_results['test_r2'].std():.3f}")
print(f"Test MAE: {-cv_results['test_neg_mean_absolute_error'].mean():.3f}")
print(f"Test RMSE: {-cv_results['test_neg_root_mean_squared_error'].mean():.3f}")
```

**è¼¸å‡ºç¯„ä¾‹**ï¼š
```
Cross-Validation Results:
Train RÂ²: 0.958 Â± 0.012
Test RÂ²: 0.876 Â± 0.045
Test MAE: 2.34
Test RMSE: 3.21
```

**åˆ†æ**ï¼š
- Train RÂ² é¡¯è‘—é«˜æ–¼ Test RÂ² â†’ å¯èƒ½å­˜åœ¨è¼•å¾®éæ“¬åˆ
- Test RÂ² çš„æ¨™æº–å·® 0.045 â†’ æ¨¡å‹åœ¨ä¸åŒ fold ä¸Šè¡¨ç¾ç©©å®š

---

## 6. åå·®-æ–¹å·®æ¬Šè¡¡

### 6.1 æ ¸å¿ƒæ¦‚å¿µ

æ©Ÿå™¨å­¸ç¿’æ¨¡å‹çš„é æ¸¬èª¤å·®å¯åˆ†è§£ç‚ºä¸‰å€‹éƒ¨åˆ†ï¼š

$$
\text{Total Error} = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}
$$

#### åå·® (Bias)

**å®šç¾©**ï¼šæ¨¡å‹é æ¸¬çš„æœŸæœ›å€¼èˆ‡çœŸå¯¦å€¼ä¹‹é–“çš„å·®è·ã€‚

**å«ç¾©**ï¼š
- é«˜åå·® = **ç³»çµ±æ€§éŒ¯èª¤**
- æ¨¡å‹éæ–¼ç°¡åŒ–ï¼Œç„¡æ³•æ•æ‰æ•¸æ“šçš„çœŸå¯¦è¦å¾‹
- å°æ‡‰**æ¬ æ“¬åˆ (Underfitting)**

**åŒ–å·¥ä¾‹å­**ï¼š
- ç”¨ç·šæ€§æ¨¡å‹æ“¬åˆåŒ–å­¸åæ‡‰é€Ÿç‡ vs æº«åº¦ (å¯¦éš›æ˜¯ Arrhenius éç·šæ€§é—œä¿‚)
- æ¨¡å‹åœ¨è¨“ç·´é›†å’Œæ¸¬è©¦é›†ä¸Šéƒ½è¡¨ç¾ä¸ä½³

#### æ–¹å·® (Variance)

**å®šç¾©**ï¼šæ¨¡å‹é æ¸¬å€¼å°ä¸åŒè¨“ç·´é›†çš„æ•æ„Ÿç¨‹åº¦ã€‚

**å«ç¾©**ï¼š
- é«˜æ–¹å·® = **å°è¨“ç·´æ•¸æ“šéåº¦æ•æ„Ÿ**
- æ¨¡å‹éæ–¼è¤‡é›œï¼Œè¨˜ä½äº†è¨“ç·´æ•¸æ“šçš„é›œè¨Š
- å°æ‡‰**éæ“¬åˆ (Overfitting)**

**åŒ–å·¥ä¾‹å­**ï¼š
- ç”¨ 20 éšå¤šé …å¼æ“¬åˆ 30 å€‹å‚¬åŒ–åŠ‘å¯¦é©—æ•¸æ“š
- è¨“ç·´é›† RÂ²=0.99ï¼Œæ¸¬è©¦é›† RÂ²=0.45

#### ä¸å¯ç´„èª¤å·® (Irreducible Error)

**å®šç¾©**ï¼šæ•¸æ“šæœ¬èº«çš„é›œè¨Šï¼Œç„¡è«–æ¨¡å‹å¤šå¥½éƒ½ç„¡æ³•æ¶ˆé™¤ã€‚

**ä¾†æº**ï¼š
- æ¸¬é‡èª¤å·®
- æœªè§€æ¸¬çš„å½±éŸ¿å› ç´ 
- éš¨æ©Ÿæ³¢å‹•

**åŒ–å·¥ä¾‹å­**ï¼š
- æº«åº¦æ„Ÿæ¸¬å™¨ç²¾åº¦ Â±0.5Â°C
- åŸæ–™æ‰¹æ¬¡é–“çš„å¾®å°å·®ç•°

### 6.2 åå·®-æ–¹å·®æ¬Šè¡¡çš„è¦–è¦ºåŒ–ç†è§£

#### é¶å¿ƒæ¯”å–»

æƒ³åƒé æ¸¬æ˜¯å°„ç®­ï¼š
- **çœŸå¯¦å€¼** = é¶å¿ƒ
- **é æ¸¬å€¼** = ç®­çš„è½é»

| æ¨¡å‹é¡å‹ | åå·® | æ–¹å·® | åœ–ç¤º |
|---------|------|------|------|
| **ç†æƒ³æ¨¡å‹** | ä½ | ä½ | ğŸ¯ ç®­éƒ½é›†ä¸­åœ¨é¶å¿ƒ |
| **é«˜åå·®ä½æ–¹å·®** | é«˜ | ä½ | â­• ç®­é›†ä¸­ä½†åé›¢é¶å¿ƒ (æ¬ æ“¬åˆ) |
| **ä½åå·®é«˜æ–¹å·®** | ä½ | é«˜ | âŒ ç®­åˆ†æ•£åœ¨é¶å¿ƒå‘¨åœ (éæ“¬åˆ) |
| **é«˜åå·®é«˜æ–¹å·®** | é«˜ | é«˜ | âš ï¸ ç®­åˆ†æ•£ä¸”åé›¢ (æœ€å·®) |

#### æ¨¡å‹è¤‡é›œåº¦çš„å½±éŸ¿

```
æ¨¡å‹è¤‡é›œåº¦å¢åŠ  â†’
â”œâ”€ åå·® â†“ (æ¨¡å‹æ›´éˆæ´»ï¼Œèƒ½æ“¬åˆè¤‡é›œè¦å¾‹)
â””â”€ æ–¹å·® â†‘ (æ¨¡å‹å°è¨“ç·´æ•¸æ“šæ›´æ•æ„Ÿ)

æœ€ä½³æ¨¡å‹ = ç¸½èª¤å·®æœ€å°çš„å¹³è¡¡é»
```

**åœ–ç¤º**ï¼š

```
èª¤å·®
â”‚   
â”‚    Total Error
â”‚      /\
â”‚     /  \
â”‚    /    \____
â”‚   /          â”€â”€â”€â”€â”€â”€â”€â”€ Variance
â”‚  /      ____/
â”‚ /______/
â”‚/â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ BiasÂ²
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ æ¨¡å‹è¤‡é›œåº¦
  ç°¡å–®          è¤‡é›œ
(æ¬ æ“¬åˆ)    (æœ€ä½³)    (éæ“¬åˆ)
```

### 6.3 è¨ºæ–·æ¬ æ“¬åˆèˆ‡éæ“¬åˆ

#### åˆ¤æ–·ä¾æ“š

| ç—‡ç‹€ | è¨“ç·´èª¤å·® | æ¸¬è©¦èª¤å·® | è¨ºæ–· | è§£æ±ºæ–¹æ¡ˆ |
|------|---------|---------|------|---------|
| è¨“ç·´å’Œæ¸¬è©¦éƒ½å·® | é«˜ | é«˜ | **æ¬ æ“¬åˆ** (é«˜åå·®) | å¢åŠ æ¨¡å‹è¤‡é›œåº¦ |
| è¨“ç·´å¥½ï¼Œæ¸¬è©¦å·® | ä½ | é«˜ | **éæ“¬åˆ** (é«˜æ–¹å·®) | é™ä½æ¨¡å‹è¤‡é›œåº¦æˆ–å¢åŠ æ•¸æ“š |
| è¨“ç·´å’Œæ¸¬è©¦éƒ½å¥½ | ä½ | ä½ | âœ… **ç†æƒ³ç‹€æ…‹** | ç¶­æŒ |

#### å®šé‡æŒ‡æ¨™

**éæ“¬åˆæŒ‡æ•¸**ï¼š
$$
\text{Overfit Index} = \frac{\text{Train Score} - \text{Test Score}}{\text{Train Score}}
$$

**ç¶“é©—æ³•å‰‡**ï¼š
- < 5%ï¼šè¼•å¾®æˆ–ç„¡éæ“¬åˆ
- 5-15%ï¼šä¸­åº¦éæ“¬åˆ
- \> 15%ï¼šåš´é‡éæ“¬åˆ

**ä¾‹å­**ï¼š
```python
train_r2 = 0.95
test_r2 = 0.75
overfit_index = (train_r2 - test_r2) / train_r2 * 100
print(f"Overfit Index: {overfit_index:.1f}%")  # 21.1% â†’ åš´é‡éæ“¬åˆ
```

### 6.4 æ‡‰å°ç­–ç•¥

#### è§£æ±ºæ¬ æ“¬åˆ (é«˜åå·®)

| ç­–ç•¥ | æ–¹æ³• | sklearn ç¯„ä¾‹ |
|------|------|-------------|
| **å¢åŠ æ¨¡å‹è¤‡é›œåº¦** | å¤šé …å¼ç‰¹å¾µ | `PolynomialFeatures(degree=3)` |
|  | å¢åŠ ç¥ç¶“ç¶²è·¯å±¤æ•¸ |  |
| **å¢åŠ ç‰¹å¾µ** | ç‰¹å¾µå·¥ç¨‹ | äº¤äº’é …ã€é ˜åŸŸçŸ¥è­˜ç‰¹å¾µ |
| **æ¸›å°‘æ­£å‰‡åŒ–** | é™ä½æ‡²ç½°å¼·åº¦ | `Ridge(alpha=0.1)` â†’ `alpha=0.01` |
| **è¨“ç·´æ›´é•·æ™‚é–“** | å¢åŠ è¿­ä»£æ¬¡æ•¸ | `MLPRegressor(max_iter=1000)` |

#### è§£æ±ºéæ“¬åˆ (é«˜æ–¹å·®)

| ç­–ç•¥ | æ–¹æ³• | sklearn ç¯„ä¾‹ |
|------|------|-------------|
| **å¢åŠ è¨“ç·´æ•¸æ“š** | æ”¶é›†æ›´å¤šæ¨£æœ¬ | æœ€æœ‰æ•ˆä½†æˆæœ¬é«˜ |
|  | æ•¸æ“šå¢å¼· | åŒ–å·¥ï¼šä¸åŒæ“ä½œæ¢ä»¶ |
| **é™ä½æ¨¡å‹è¤‡é›œåº¦** | ç‰¹å¾µé¸æ“‡ | `SelectKBest(k=10)` |
|  | æ¸›å°‘å¤šé …å¼éšæ•¸ | `PolynomialFeatures(degree=2)` |
| **æ­£å‰‡åŒ–** | L1 (Lasso) | `Lasso(alpha=1.0)` |
|  | L2 (Ridge) | `Ridge(alpha=1.0)` |
|  | ElasticNet | `ElasticNet(alpha=1.0, l1_ratio=0.5)` |
| **Early Stopping** | æå‰åœæ­¢è¨“ç·´ | `MLPRegressor(early_stopping=True)` |
| **é›†æˆæ–¹æ³•** | Bagging | `RandomForestRegressor()` |
|  | Dropout (æ·±åº¦å­¸ç¿’) |  |
| **äº¤å‰é©—è­‰** | ç©©å¥è©•ä¼° | `cross_val_score(cv=10)` |

### 6.5 åŒ–å·¥æ¡ˆä¾‹ï¼šå‚¬åŒ–åŠ‘æ´»æ€§é æ¸¬

#### å ´æ™¯æè¿°

- **æ•¸æ“š**ï¼š50 å€‹å‚¬åŒ–åŠ‘å¯¦é©— (å°æ¨£æœ¬)
- **ç‰¹å¾µ**ï¼šé‡‘å±¬å«é‡ã€è¼‰é«”é¡å‹ã€ç„™ç‡’æº«åº¦ã€æ¯”è¡¨é¢ç© (4 å€‹ç‰¹å¾µ)
- **ç›®æ¨™**ï¼šå‚¬åŒ–åŠ‘æ´»æ€§ (è½‰åŒ–ç‡ %)

#### æ¨¡å‹æ¯”è¼ƒ

| æ¨¡å‹ | è¤‡é›œåº¦ | Train RÂ² | Test RÂ² | è¨ºæ–· |
|------|--------|---------|---------|------|
| **Linear Regression** | ä½ | 0.72 | 0.68 | è¼•å¾®æ¬ æ“¬åˆ |
| **Polynomial (degree=2)** | ä¸­ | 0.86 | 0.82 | âœ… å¹³è¡¡ |
| **Polynomial (degree=5)** | é«˜ | 0.98 | 0.54 | åš´é‡éæ“¬åˆ |
| **Random Forest (10 trees)** | ä¸­ | 0.91 | 0.78 | è¼•å¾®éæ“¬åˆ |
| **Random Forest (100 trees)** | é«˜ | 0.99 | 0.65 | éæ“¬åˆ |

#### åˆ†æèˆ‡æ±ºç­–

1. **Linear Regression**: åå·®ç•¥é«˜ï¼Œä½†æ–¹å·®ä½ï¼Œç©©å¥
2. **Polynomial (degree=2)**: æœ€ä½³å¹³è¡¡é»
3. **Polynomial (degree=5)**: è¨“ç·´é›†å®Œç¾ä½†æ¸¬è©¦é›†å´©æ½°ï¼Œå…¸å‹éæ“¬åˆ
4. **Random Forest**: æ¨¹æ•¸éå¤šå°è‡´è¨˜ä½è¨“ç·´æ•¸æ“š

**æœ€çµ‚é¸æ“‡**ï¼š
- **Polynomial (degree=2)** + **Ridge æ­£å‰‡åŒ–**
- ç†ç”±ï¼šå¹³è¡¡è¤‡é›œåº¦èˆ‡æ³›åŒ–èƒ½åŠ›ï¼Œä¸”å¯è§£é‡‹æ€§è¼ƒå¥½

**é€²ä¸€æ­¥å„ªåŒ–**ï¼š
```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import Ridge
from sklearn.pipeline import Pipeline

pipe = Pipeline([
    ('poly', PolynomialFeatures(degree=2)),
    ('ridge', Ridge(alpha=10.0))  # åŠ å…¥ L2 æ­£å‰‡åŒ–
])

cv_scores = cross_val_score(pipe, X, y, cv=5, scoring='r2')
print(f"CV RÂ²: {cv_scores.mean():.3f}")
```

### 6.6 å¯¦ç”¨å·¥å…·ï¼šåå·®-æ–¹å·®åˆ†è§£

é›–ç„¶ sklearn æ²’æœ‰å…§å»ºï¼Œä½†å¯ä»¥ç”¨ `mlxtend` å¥—ä»¶ï¼š

```python
from mlxtend.evaluate import bias_variance_decomp

avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(
    model, X_train, y_train, X_test, y_test,
    loss='mse', num_rounds=100, random_seed=42
)

print(f"Average BiasÂ²: {avg_bias:.3f}")
print(f"Average Variance: {avg_var:.3f}")
print(f"Total MSE: {avg_expected_loss:.3f}")
```

---

## 7. å­¸ç¿’æ›²ç·šèˆ‡é©—è­‰æ›²ç·š

å­¸ç¿’æ›²ç·šå’Œé©—è­‰æ›²ç·šæ˜¯è¨ºæ–·æ¨¡å‹å•é¡Œçš„å¼·å¤§è¦–è¦ºåŒ–å·¥å…·ã€‚

### 7.1 å­¸ç¿’æ›²ç·š (Learning Curve)

#### å®šç¾©

å­¸ç¿’æ›²ç·šé¡¯ç¤º**æ¨¡å‹æ€§èƒ½**å¦‚ä½•éš¨è‘—**è¨“ç·´æ¨£æœ¬æ•¸**è®ŠåŒ–ã€‚

**X è»¸**ï¼šè¨“ç·´æ¨£æœ¬æ•¸  
**Y è»¸**ï¼šæ¨¡å‹æ€§èƒ½ (å¦‚ RÂ², Accuracy)  
**å…©æ¢æ›²ç·š**ï¼šè¨“ç·´åˆ†æ•¸ vs äº¤å‰é©—è­‰åˆ†æ•¸

#### åŸç†

é€šéé€æ­¥å¢åŠ è¨“ç·´æ•¸æ“šï¼Œè§€å¯Ÿï¼š
1. æ¨¡å‹æ˜¯å¦å¾æ›´å¤šæ•¸æ“šä¸­å—ç›Šï¼Ÿ
2. ç•¶å‰æ•¸æ“šé‡æ˜¯å¦å……è¶³ï¼Ÿ
3. æ¨¡å‹æ˜¯å¦éæ“¬åˆæˆ–æ¬ æ“¬åˆï¼Ÿ

#### sklearn å¯¦ä½œ

```python
from sklearn.model_selection import learning_curve
import numpy as np
import matplotlib.pyplot as plt

train_sizes, train_scores, val_scores = learning_curve(
    model, X, y,
    train_sizes=np.linspace(0.1, 1.0, 10),
    cv=5,
    scoring='r2',
    n_jobs=-1
)

# è¨ˆç®—å¹³å‡å€¼å’Œæ¨™æº–å·®
train_mean = train_scores.mean(axis=1)
train_std = train_scores.std(axis=1)
val_mean = val_scores.mean(axis=1)
val_std = val_scores.std(axis=1)

# ç¹ªè£½å­¸ç¿’æ›²ç·š
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_mean, label='Training Score', marker='o')
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2)
plt.plot(train_sizes, val_mean, label='Cross-Validation Score', marker='s')
plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.2)

plt.xlabel('Training Set Size')
plt.ylabel('RÂ² Score')
plt.title('Learning Curve')
plt.legend(loc='best')
plt.grid(True)
plt.show()
```

#### å…¸å‹æ¨¡å¼è§£è®€

**æ¨¡å¼ 1: é«˜åå·® (æ¬ æ“¬åˆ)**

```
Score
â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• Train (ä½ä½†å¹³ç©©)
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• Val (ä½ä½†å¹³ç©©ï¼Œæ¥è¿‘ Train)
â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Training Size
```

**ç‰¹å¾µ**ï¼š
- è¨“ç·´åˆ†æ•¸å’Œé©—è­‰åˆ†æ•¸éƒ½**å¾ˆä½**
- å…©è€…**å¿«é€Ÿæ”¶æ–‚**ä¸”**å·®è·å°**
- å¢åŠ æ•¸æ“šç„¡æ³•é¡¯è‘—æ”¹å–„æ€§èƒ½

**è¨ºæ–·**ï¼šæ¨¡å‹å¤ªç°¡å–®ï¼Œç„¡æ³•æ•æ‰æ•¸æ“šè¦å¾‹

**è§£æ±º**ï¼šå¢åŠ æ¨¡å‹è¤‡é›œåº¦ (å¦‚å¤šé …å¼ç‰¹å¾µã€æ›´æ·±çš„ç¶²è·¯)

---

**æ¨¡å¼ 2: é«˜æ–¹å·® (éæ“¬åˆ)**

```
Score
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• Train (é«˜)
â”‚
â”‚              ___
â”‚         ____/   
â”‚    ____/        Val (ä½ï¼Œç·©æ…¢ä¸Šå‡)
â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Training Size
```

**ç‰¹å¾µ**ï¼š
- è¨“ç·´åˆ†æ•¸**å¾ˆé«˜**ä¸”ç©©å®š
- é©—è­‰åˆ†æ•¸**è¼ƒä½**ï¼Œéš¨æ•¸æ“šå¢åŠ ç·©æ…¢ä¸Šå‡
- å…©è€…**å·®è·å¤§**

**è¨ºæ–·**ï¼šæ¨¡å‹éåº¦æ“¬åˆè¨“ç·´æ•¸æ“š

**è§£æ±º**ï¼š
1. å¢åŠ è¨“ç·´æ•¸æ“š (æ›²ç·šé¡¯ç¤ºé‚„æœ‰ä¸Šå‡ç©ºé–“)
2. é™ä½æ¨¡å‹è¤‡é›œåº¦
3. å¢åŠ æ­£å‰‡åŒ–

---

**æ¨¡å¼ 3: ç†æƒ³ç‹€æ…‹**

```
Score
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• Train (é«˜)
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• Val (é«˜ï¼Œæ¥è¿‘ Train)
â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Training Size
```

**ç‰¹å¾µ**ï¼š
- è¨“ç·´åˆ†æ•¸å’Œé©—è­‰åˆ†æ•¸éƒ½**å¾ˆé«˜**
- å…©è€…**å·®è·å°**
- æ›²ç·šè¶¨æ–¼å¹³ç©©

**è¨ºæ–·**ï¼šæ¨¡å‹æ³›åŒ–è‰¯å¥½ âœ…

**è¡Œå‹•**ï¼šç„¡éœ€èª¿æ•´ï¼Œå¯è€ƒæ…®éƒ¨ç½²

---

**æ¨¡å¼ 4: æ•¸æ“šä¸è¶³**

```
Score
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• Train (é«˜)
â”‚              
â”‚          ___/
â”‚     ____/      
â”‚ ___/           Val (æŒçºŒä¸Šå‡ï¼Œæœªæ”¶æ–‚)
â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Training Size
```

**ç‰¹å¾µ**ï¼š
- é©—è­‰åˆ†æ•¸**æŒçºŒä¸Šå‡**ï¼Œæœªè¶¨æ–¼å¹³ç©©
- æš—ç¤ºæ›´å¤šæ•¸æ“šå¯é€²ä¸€æ­¥æ”¹å–„

**è¨ºæ–·**ï¼šç•¶å‰æ•¸æ“šé‡ä¸è¶³ä»¥å……åˆ†è¨“ç·´æ¨¡å‹

**è§£æ±º**ï¼šæ”¶é›†æ›´å¤šæ•¸æ“š

### 7.2 é©—è­‰æ›²ç·š (Validation Curve)

#### å®šç¾©

é©—è­‰æ›²ç·šé¡¯ç¤º**æ¨¡å‹æ€§èƒ½**å¦‚ä½•éš¨**å–®ä¸€è¶…åƒæ•¸**è®ŠåŒ–ã€‚

**X è»¸**ï¼šè¶…åƒæ•¸å€¼ (å¦‚æ­£å‰‡åŒ–å¼·åº¦ $\alpha$ )  
**Y è»¸**ï¼šæ¨¡å‹æ€§èƒ½  
**å…©æ¢æ›²ç·š**ï¼šè¨“ç·´åˆ†æ•¸ vs äº¤å‰é©—è­‰åˆ†æ•¸

#### ç”¨é€”

- è¦–è¦ºåŒ–è¶…åƒæ•¸å°æ€§èƒ½çš„å½±éŸ¿
- æ‰¾å‡ºéæ“¬åˆ/æ¬ æ“¬åˆçš„è½‰æŠ˜é»
- ç†è§£è¶…åƒæ•¸çš„ä½œç”¨æ©Ÿåˆ¶

#### sklearn å¯¦ä½œ

```python
from sklearn.model_selection import validation_curve

param_range = np.logspace(-4, 2, 10)  # alpha å¾ 0.0001 åˆ° 100

train_scores, val_scores = validation_curve(
    Ridge(), X, y,
    param_name='alpha',
    param_range=param_range,
    cv=5,
    scoring='r2',
    n_jobs=-1
)

train_mean = train_scores.mean(axis=1)
train_std = train_scores.std(axis=1)
val_mean = val_scores.mean(axis=1)
val_std = val_scores.std(axis=1)

# ç¹ªè£½é©—è­‰æ›²ç·š
plt.figure(figsize=(10, 6))
plt.semilogx(param_range, train_mean, label='Training Score', marker='o')
plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.2)
plt.semilogx(param_range, val_mean, label='Cross-Validation Score', marker='s')
plt.fill_between(param_range, val_mean - val_std, val_mean + val_std, alpha=0.2)

plt.xlabel('Alpha (Regularization Strength)')
plt.ylabel('RÂ² Score')
plt.title('Validation Curve (Ridge Regression)')
plt.legend(loc='best')
plt.grid(True)
plt.show()
```

#### å…¸å‹æ¨¡å¼è§£è®€

**Ridge Regression çš„ Alpha èª¿æ•´**ï¼š

```
RÂ² Score
â”‚
â”‚ Train â”€â”€â”€â”€â”€â•®
â”‚            â•²
â”‚             â•²____
â”‚               
â”‚        â•±â”€â”€â”€â”€â•²     Val
â”‚   ____â•±      â•²____
â”‚                    
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Alpha
  0.001   0.1   10   1000
  (å¼±æ­£å‰‡åŒ–)    (å¼·æ­£å‰‡åŒ–)
  
éæ“¬åˆå€åŸŸ â† æœ€ä½³é» â†’ æ¬ æ“¬åˆå€åŸŸ
```

**è§£è®€**ï¼š
- **Alpha å¤ªå°** (å·¦å´)ï¼šæ­£å‰‡åŒ–ä¸è¶³ â†’ éæ“¬åˆ (Train é«˜, Val ä½)
- **Alpha æœ€ä½³** (ä¸­é–“)ï¼šTrain å’Œ Val éƒ½é«˜ä¸”æ¥è¿‘ âœ…
- **Alpha å¤ªå¤§** (å³å´)ï¼šéåº¦æ‡²ç½° â†’ æ¬ æ“¬åˆ (Train å’Œ Val éƒ½ä½)

**æ±ºç­–**ï¼šé¸æ“‡ Val Score æœ€é«˜çš„ Alpha å€¼

### 7.3 åŒ–å·¥æ¡ˆä¾‹ï¼šåæ‡‰æº«åº¦é æ¸¬æ¨¡å‹è¨ºæ–·

#### å ´æ™¯

- **æ•¸æ“š**ï¼š200 å€‹åæ‡‰å¯¦é©—
- **ç›®æ¨™**ï¼šé æ¸¬åæ‡‰æº«åº¦
- **æ¨¡å‹**ï¼šSupport Vector Regression (SVR)
- **å•é¡Œ**ï¼šæ¸¬è©¦é›† RÂ² åªæœ‰ 0.65ï¼Œå¦‚ä½•æ”¹é€²ï¼Ÿ

#### æ­¥é©Ÿ 1: ç¹ªè£½å­¸ç¿’æ›²ç·š

```python
from sklearn.svm import SVR

model = SVR(kernel='rbf', C=1.0, epsilon=0.1)

# å­¸ç¿’æ›²ç·šåˆ†æ
train_sizes, train_scores, val_scores = learning_curve(
    model, X, y, train_sizes=np.linspace(0.1, 1.0, 10), cv=5, scoring='r2'
)
```

**ç™¼ç¾**ï¼šé©—è­‰åˆ†æ•¸éš¨è¨“ç·´æ¨£æœ¬å¢åŠ è€Œä¸Šå‡ï¼Œä¸”æœªæ”¶æ–‚ â†’ **æ•¸æ“šä¸è¶³**

**åˆæ­¥çµè«–**ï¼šéœ€è¦æ›´å¤šæ•¸æ“šï¼Œæˆ–é™ä½æ¨¡å‹è¤‡é›œåº¦

#### æ­¥é©Ÿ 2: ç¹ªè£½é©—è­‰æ›²ç·š (èª¿æ•´ C åƒæ•¸)

```python
param_range = np.logspace(-2, 3, 10)  # C å¾ 0.01 åˆ° 1000

train_scores, val_scores = validation_curve(
    SVR(kernel='rbf', epsilon=0.1), X, y,
    param_name='C',
    param_range=param_range,
    cv=5,
    scoring='r2'
)
```

**ç™¼ç¾**ï¼š
- ç•¶ C=1 æ™‚ï¼ŒTrain RÂ²=0.90, Val RÂ²=0.65 â†’ éæ“¬åˆ
- ç•¶ C=0.1 æ™‚ï¼ŒTrain RÂ²=0.78, Val RÂ²=0.72 â†’ æ”¹å–„ âœ…

**çµè«–**ï¼šé™ä½ C å€¼ (æ¸›å°‘æ¨¡å‹è¤‡é›œåº¦) å¯æ”¹å–„æ³›åŒ–

#### æ­¥é©Ÿ 3: æœ€çµ‚å„ªåŒ–

```python
# ä½¿ç”¨è¼ƒå°çš„ C å’Œäº¤å‰é©—è­‰é¸æ“‡æœ€ä½³ epsilon
from sklearn.model_selection import GridSearchCV

param_grid = {'C': [0.05, 0.1, 0.5], 'epsilon': [0.05, 0.1, 0.2]}
grid_search = GridSearchCV(SVR(kernel='rbf'), param_grid, cv=5, scoring='r2')
grid_search.fit(X_train, y_train)

best_model = grid_search.best_estimator_
print(f"Best params: {grid_search.best_params_}")
print(f"CV RÂ²: {grid_search.best_score_:.3f}")
print(f"Test RÂ²: {best_model.score(X_test, y_test):.3f}")
```

**çµæœ**ï¼š
- åŸå§‹æ¨¡å‹ (C=1.0): Test RÂ² = 0.65
- å„ªåŒ–æ¨¡å‹ (C=0.1, epsilon=0.1): Test RÂ² = 0.79 âœ…

### 7.4 æœ€ä½³å¯¦è¸

#### åˆ†ææµç¨‹

```
1. ç¹ªè£½å­¸ç¿’æ›²ç·š
   â†“
2. è¨ºæ–·å•é¡Œ (æ¬ æ“¬åˆ/éæ“¬åˆ/æ•¸æ“šä¸è¶³)
   â†“
3. ç¹ªè£½é©—è­‰æ›²ç·š (é‡å°æ€§èª¿æ•´è¶…åƒæ•¸)
   â†“
4. å†æ¬¡ç¹ªè£½å­¸ç¿’æ›²ç·šé©—è­‰æ”¹å–„
   â†“
5. æœ€çµ‚æ¸¬è©¦é›†è©•ä¼°
```

#### æ³¨æ„äº‹é …

- âš ï¸ å­¸ç¿’æ›²ç·šè¨ˆç®—æˆæœ¬é«˜ (å¤šæ¬¡è¨“ç·´)ï¼Œå…ˆç”¨å°è¦æ¨¡æ¸¬è©¦
- âš ï¸ é©—è­‰æ›²ç·šåªé¡¯ç¤ºå–®ä¸€è¶…åƒæ•¸å½±éŸ¿ï¼Œè¤‡é›œæ¨¡å‹éœ€ Grid Search
- âš ï¸ æ›²ç·šåªæ˜¯è¨ºæ–·å·¥å…·ï¼Œä¸èƒ½æ›¿ä»£æœ€çµ‚æ¸¬è©¦é›†è©•ä¼°

---

## 8. æ¨¡å‹æ¯”è¼ƒçš„çµ±è¨ˆæª¢å®š

ç•¶æ¯”è¼ƒå¤šå€‹æ¨¡å‹æ™‚ï¼Œæ€§èƒ½å·®ç•°å¯èƒ½æºæ–¼éš¨æ©Ÿæ€§ã€‚çµ±è¨ˆæª¢å®šå¹«åŠ©æˆ‘å€‘åˆ¤æ–·å·®ç•°æ˜¯å¦**é¡¯è‘—**ã€‚

### 8.1 ç‚ºä»€éº¼éœ€è¦çµ±è¨ˆæª¢å®šï¼Ÿ

**å ´æ™¯**ï¼šæ¯”è¼ƒå…©å€‹æ¨¡å‹

| æ¨¡å‹ | 5-Fold CV RÂ² |
|------|--------------|
| Model A | [0.82, 0.85, 0.81, 0.84, 0.83] â†’ å¹³å‡ 0.830 |
| Model B | [0.81, 0.83, 0.82, 0.85, 0.82] â†’ å¹³å‡ 0.826 |

**å•é¡Œ**ï¼šModel A å¹³å‡é«˜ 0.004ï¼Œä½†é€™å·®ç•°**çµ±è¨ˆé¡¯è‘—**å—ï¼Ÿé‚„æ˜¯éš¨æ©Ÿæ³¢å‹•ï¼Ÿ

**çµ±è¨ˆæª¢å®š**å¯çµ¦å‡ºç­”æ¡ˆï¼š
- **H0 (è™›ç„¡å‡è¨­)**ï¼šå…©æ¨¡å‹æ€§èƒ½ç„¡å·®ç•°
- **H1 (å°ç«‹å‡è¨­)**ï¼šå…©æ¨¡å‹æ€§èƒ½æœ‰å·®ç•°
- **p-value < 0.05** â†’ æ‹’çµ• H0ï¼Œå·®ç•°é¡¯è‘—

### 8.2 Paired t-test (é…å° t æª¢å®š)

#### é©ç”¨å ´æ™¯

- æ¯”è¼ƒ**å…©å€‹æ¨¡å‹**
- ä½¿ç”¨**ç›¸åŒçš„ CV folds** (é…å°æ•¸æ“š)
- å‡è¨­åˆ†æ•¸æœå¾**å¸¸æ…‹åˆ†ä½ˆ**

#### Python å¯¦ä½œ

```python
from scipy.stats import ttest_rel

model_a_scores = [0.82, 0.85, 0.81, 0.84, 0.83]
model_b_scores = [0.81, 0.83, 0.82, 0.85, 0.82]

t_stat, p_value = ttest_rel(model_a_scores, model_b_scores)

print(f"t-statistic: {t_stat:.3f}")
print(f"p-value: {p_value:.3f}")

if p_value < 0.05:
    print("å·®ç•°é¡¯è‘— âœ…")
else:
    print("å·®ç•°ä¸é¡¯è‘— âŒ")
```

**è¼¸å‡º**ï¼š
```
t-statistic: 0.931
p-value: 0.405
å·®ç•°ä¸é¡¯è‘— âŒ
```

**çµè«–**ï¼šé›–ç„¶ Model A å¹³å‡åˆ†æ•¸ç•¥é«˜ï¼Œä½†çµ±è¨ˆä¸Šç„¡é¡¯è‘—å·®ç•°ï¼Œå¯èƒ½æ˜¯éš¨æ©Ÿæ³¢å‹•ã€‚

### 8.3 Wilcoxon Signed-Rank Test (å¨çˆ¾ç§‘å…‹æ£®ç¬¦è™Ÿç§©æª¢å®š)

#### é©ç”¨å ´æ™¯

- æ¯”è¼ƒ**å…©å€‹æ¨¡å‹**
- **ä¸å‡è¨­å¸¸æ…‹åˆ†ä½ˆ** (æ›´ç©©å¥)
- ä½¿ç”¨ç›¸åŒçš„ CV folds

#### Python å¯¦ä½œ

```python
from scipy.stats import wilcoxon

stat, p_value = wilcoxon(model_a_scores, model_b_scores)

print(f"Wilcoxon statistic: {stat:.3f}")
print(f"p-value: {p_value:.3f}")
```

### 8.4 åŒ–å·¥æ¡ˆä¾‹ï¼šå‚¬åŒ–åŠ‘æ€§èƒ½æ¨¡å‹æ¯”è¼ƒ

**å ´æ™¯**ï¼šæ¯”è¼ƒ 3 å€‹æ¨¡å‹é æ¸¬å‚¬åŒ–åŠ‘è½‰åŒ–ç‡

```python
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR

models = {
    'RF': RandomForestRegressor(),
    'GBT': GradientBoostingRegressor(),
    'SVR': SVR()
}

scores = {}
for name, model in models.items():
    scores[name] = cross_val_score(model, X, y, cv=10, scoring='r2')
    print(f"{name}: {scores[name].mean():.3f} Â± {scores[name].std():.3f}")
```

**è¼¸å‡º**ï¼š
```
RF:  0.825 Â± 0.042
GBT: 0.831 Â± 0.038
SVR: 0.798 Â± 0.055
```

**çµ±è¨ˆæª¢å®š (RF vs GBT)**ï¼š
```python
t_stat, p_value = ttest_rel(scores['RF'], scores['GBT'])
print(f"RF vs GBT p-value: {p_value:.3f}")
```

**çµè«–**ï¼š
- GBT å¹³å‡åˆ†æ•¸æœ€é«˜ (0.831)
- GBT vs RF: p=0.234 (ä¸é¡¯è‘—)
- GBT vs SVR: p=0.018 (é¡¯è‘—)

**æ±ºç­–**ï¼šGBT å’Œ RF æ€§èƒ½ç›¸ç•¶ï¼Œä½† GBT æ¨™æº–å·®è¼ƒå° â†’ é¸æ“‡ **GBT**

### 8.5 æœ€ä½³å¯¦è¸

- âœ… ä½¿ç”¨ç›¸åŒçš„ CV åŠƒåˆ†ç¢ºä¿å¯æ¯”æ€§
- âœ… è‡³å°‘ 5 å€‹ fold æ‰æœ‰çµ±è¨ˆæ„ç¾© (10 fold æ›´å¥½)
- âœ… å°å·®ç•° (< 1%) å³ä½¿é¡¯è‘—ä¹Ÿå¯èƒ½ç„¡å¯¦éš›æ„ç¾©
- âš ï¸ å¤šé‡æ¯”è¼ƒæ™‚éœ€èª¿æ•´é¡¯è‘—æ°´æº– (Bonferroni æ ¡æ­£)

---

## 9. å¤šç›®æ¨™æ¨¡å‹é¸æ“‡

å¯¦å‹™ä¸­ï¼Œæ¨¡å‹é¸æ“‡ä¸åªçœ‹æº–ç¢ºåº¦ï¼Œé‚„éœ€è€ƒæ…®å¤šç¨®å› ç´ ã€‚

### 9.1 è©•ä¼°ç¶­åº¦

| ç¶­åº¦ | æŒ‡æ¨™ | åŒ–å·¥è€ƒé‡ |
|------|------|---------|
| **æº–ç¢ºåº¦** | RÂ², F1, RMSE | é æ¸¬å¯é æ€§ |
| **è¨ˆç®—æˆæœ¬** | è¨“ç·´æ™‚é–“ã€æ¨ç†æ™‚é–“ã€è¨˜æ†¶é«” | å³æ™‚æ§åˆ¶éœ€æ±‚ |
| **å¯è§£é‡‹æ€§** | æ¨¡å‹é€æ˜åº¦ | å·¥ç¨‹å¸«ç†è§£ã€æ³•è¦è¦æ±‚ |
| **ç©©å¥æ€§** | CV æ¨™æº–å·®ã€å°ç•°å¸¸å€¼æ•æ„Ÿåº¦ | æ“ä½œæ¢ä»¶è®ŠåŒ– |
| **éƒ¨ç½²é›£åº¦** | æ¨¡å‹è¤‡é›œåº¦ã€ä¾è³´å¥—ä»¶ | ç”Ÿç”¢ç’°å¢ƒé™åˆ¶ |

### 9.2 æ±ºç­–çŸ©é™£ç¯„ä¾‹

**å ´æ™¯**ï¼šåæ‡‰å™¨æº«åº¦é æ¸¬ç³»çµ±

| æ¨¡å‹ | RÂ² | æ¨ç†æ™‚é–“ (ms) | å¯è§£é‡‹æ€§ | ç©©å¥æ€§ | ç¸½åˆ† |
|------|----|--------------|---------|---------|----|
| Linear | 0.75 | 0.1 | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜…â˜…â˜† |
| RF | 0.83 | 5.2 | â˜…â˜…â˜…â˜†â˜† | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜…â˜…â˜† |
| XGBoost | 0.87 | 3.8 | â˜…â˜…â˜†â˜†â˜† | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜… |
| DNN | 0.89 | 12.5 | â˜…â˜†â˜†â˜†â˜† | â˜…â˜…â˜…â˜†â˜† | â˜…â˜…â˜…â˜†â˜† |

**æ¬Šé‡åˆ†é…** (ä¾éœ€æ±‚èª¿æ•´)ï¼š
- æº–ç¢ºåº¦ï¼š40%
- æ¨ç†é€Ÿåº¦ï¼š30% (å³æ™‚æ§åˆ¶é‡è¦)
- å¯è§£é‡‹æ€§ï¼š20% (å®‰å…¨æ³•è¦)
- ç©©å¥æ€§ï¼š10%

**åŠ æ¬Šç¸½åˆ†è¨ˆç®—**ï¼š
```python
weights = {'accuracy': 0.4, 'speed': 0.3, 'interpretability': 0.2, 'robustness': 0.1}
# æ¨™æº–åŒ–å„æŒ‡æ¨™åˆ° 0-1ï¼Œè¨ˆç®—åŠ æ¬Šå’Œ
```

**æ±ºç­–**ï¼šè‹¥é‡è¦–å³æ™‚æ€§å’Œå¯è§£é‡‹æ€§ â†’ **Random Forest**

### 9.3 Pareto æœ€ä½³åŒ– (å¤šç›®æ¨™æ¬Šè¡¡)

ç•¶ç„¡æ³•åŒæ™‚å„ªåŒ–æ‰€æœ‰ç›®æ¨™æ™‚ï¼Œå°‹æ‰¾ **Pareto å‰æ²¿** (ç„¡æ³•å†æ”¹å–„ä»»ä¸€ç›®æ¨™è€Œä¸çŠ§ç‰²å…¶ä»–)ã€‚

**åœ–ç¤º**ï¼šæº–ç¢ºåº¦ vs æ¨ç†æ™‚é–“

```
Accuracy
â”‚     â— XGBoost (Pareto æœ€ä½³)
â”‚   â—   â— RF (Pareto æœ€ä½³)
â”‚ â—       
â”‚   â—   Linear (Pareto æœ€ä½³)
â”‚     â—   (è¢«æ”¯é…)
â”‚       â— DNN (è¢«æ”¯é…ï¼Œå¤ªæ…¢)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Inference Time
```

**Pareto æœ€ä½³æ¨¡å‹**ï¼šLinear, RF, XGBoost (ä¾å ´æ™¯é¸æ“‡)

---

## 10. åŒ–å·¥é ˜åŸŸæ‡‰ç”¨æ¡ˆä¾‹

### 10.1 æ¡ˆä¾‹ 1ï¼šå‚¬åŒ–åŠ‘ç¯©é¸æœ€ä½³åŒ–

**ç›®æ¨™**ï¼šé æ¸¬æ–°å‚¬åŒ–åŠ‘çš„åæ‡‰è½‰åŒ–ç‡ï¼Œæ¸›å°‘å¯¦é©—æ¬¡æ•¸

**æ•¸æ“š**ï¼š
- æ­·å²æ•¸æ“šï¼š80 å€‹å‚¬åŒ–åŠ‘å¯¦é©—
- ç‰¹å¾µï¼šé‡‘å±¬çµ„æˆ (Pt, Pd, Rh å«é‡)ã€è¼‰é«” (Al2O3, SiO2)ã€ç„™ç‡’æº«åº¦ã€æ¯”è¡¨é¢ç©
- ç›®æ¨™ï¼šè½‰åŒ–ç‡ (%)

**è©•ä¼°ç­–ç•¥**ï¼š
- æŒ‡æ¨™ï¼šMAPE (æ¥­å‹™æ˜“ç†è§£) + RÂ² (çµ±è¨ˆè©•ä¼°)
- é©—è­‰ï¼š5-Fold CV (æ•¸æ“šé‡æœ‰é™)
- é—œæ³¨ï¼šMAPE < 3% ç‚ºå¯æ¥å— (å¯¦é©—èª¤å·®ç´„ Â±2%)

**æ¨¡å‹é¸æ“‡æµç¨‹**ï¼š
1. å¿«é€Ÿç¯©é¸ï¼šLinear, RF, XGBoost
2. å­¸ç¿’æ›²ç·šè¨ºæ–·ï¼šç™¼ç¾æ•¸æ“šç•¥ä¸è¶³ï¼Œéœ€æ­£å‰‡åŒ–
3. è¶…åƒæ•¸èª¿æ•´ï¼šNested CV é¿å…éæ“¬åˆ
4. çµ±è¨ˆæª¢å®šï¼šRF vs XGBoost (p=0.072, ä¸é¡¯è‘—)
5. æœ€çµ‚æ±ºç­–ï¼šRF (MAPE 2.8%, å¯è§£é‡‹æ€§ä½³)

### 10.2 æ¡ˆä¾‹ 2ï¼šç”¢å“å“è³ªé æ¸¬ç³»çµ±

**ç›®æ¨™**ï¼šå³æ™‚é æ¸¬ç”¢å“å“è³ªç­‰ç´š (A/B/C)

**æ•¸æ“š**ï¼š
- ç”Ÿç”¢æ•¸æ“šï¼š5000 æ‰¹æ¬¡
- é¡åˆ¥åˆ†ä½ˆï¼šA=70%, B=25%, C=5% (ä¸å¹³è¡¡)
- éœ€æ±‚ï¼šå³æ™‚é æ¸¬ (< 100 ms)

**è©•ä¼°ç­–ç•¥**ï¼š
- æŒ‡æ¨™ï¼šMacro F1 (å¹³ç­‰å°å¾…å„é¡åˆ¥) + C é¡ Recall (ä¸èƒ½æ¼æª¢ C ç´š)
- é©—è­‰ï¼šStratified 10-Fold CV
- é—œæ³¨ï¼šC é¡ Recall > 90%

**æ¨¡å‹æ¯”è¼ƒ**ï¼š

| æ¨¡å‹ | Macro F1 | C Recall | æ¨ç†æ™‚é–“ | æ±ºç­– |
|------|---------|---------|---------|------|
| Logistic | 0.72 | 0.75 | 0.5 ms | âŒ C Recall ä¸è¶³ |
| RF | 0.81 | 0.88 | 8 ms | âš ï¸ æ¥è¿‘ä½†æœªé”æ¨™ |
| XGBoost | 0.85 | 0.92 | 12 ms | âœ… ç¬¦åˆéœ€æ±‚ |
| LightGBM | 0.84 | 0.91 | 6 ms | âœ… é€Ÿåº¦æ›´å¿« |

**æœ€çµ‚é¸æ“‡**ï¼š**LightGBM**ï¼Œä¸¦èª¿æ•´åˆ†é¡é–¾å€¼é€²ä¸€æ­¥å„ªåŒ– C é¡ Recall åˆ° 94%

---

## 11. ç¸½çµèˆ‡æœ€ä½³å¯¦è¸

### 11.1 æ ¸å¿ƒè¦é»å›é¡§

âœ… **è©•ä¼°æŒ‡æ¨™**ï¼šæ ¹æ“šæ¥­å‹™ç›®æ¨™å’Œæ•¸æ“šç‰¹æ€§é¸æ“‡åˆé©æŒ‡æ¨™  
âœ… **äº¤å‰é©—è­‰**ï¼šä½¿ç”¨ CV è©•ä¼°æ³›åŒ–èƒ½åŠ›ï¼Œé¿å…éæ“¬åˆ  
âœ… **åå·®-æ–¹å·®æ¬Šè¡¡**ï¼šè¨ºæ–·æ¬ æ“¬åˆ/éæ“¬åˆï¼Œå¹³è¡¡æ¨¡å‹è¤‡é›œåº¦  
âœ… **å­¸ç¿’æ›²ç·š**ï¼šè¦–è¦ºåŒ–è¨ºæ–·ï¼ŒæŒ‡å°æ¨¡å‹æ”¹é€²æ–¹å‘  
âœ… **çµ±è¨ˆæª¢å®š**ï¼šç¢ºä¿æ¨¡å‹å·®ç•°çµ±è¨ˆé¡¯è‘—  
âœ… **å¤šç›®æ¨™æ±ºç­–**ï¼šç¶œåˆè€ƒé‡æº–ç¢ºåº¦ã€æˆæœ¬ã€å¯è§£é‡‹æ€§

### 11.2 åŒ–å·¥å»ºæ¨¡æª¢æŸ¥æ¸…å–®

**æ•¸æ“šæº–å‚™éšæ®µ**ï¼š
- [ ] æª¢æŸ¥æ˜¯å¦æœ‰æ•¸æ“šæ´©æ¼
- [ ] ç¢ºèªè¨“ç·´/é©—è­‰/æ¸¬è©¦é›†åŠƒåˆ†åˆç†
- [ ] æ™‚é–“åºåˆ—æ•¸æ“šä½¿ç”¨æ™‚é–“é †åºåŠƒåˆ†

**æ¨¡å‹è¨“ç·´éšæ®µ**ï¼š
- [ ] ä½¿ç”¨äº¤å‰é©—è­‰è©•ä¼°ç©©å¥æ€§
- [ ] è¨˜éŒ„æ‰€æœ‰å¯¦é©—çµæœ (é¿å…é¸æ“‡æ€§å ±å‘Š)
- [ ] ç¹ªè£½å­¸ç¿’æ›²ç·šè¨ºæ–·å•é¡Œ

**æ¨¡å‹é¸æ“‡éšæ®µ**ï¼š
- [ ] ä½¿ç”¨å¤šç¨®è©•ä¼°æŒ‡æ¨™ (ä¸åªçœ‹å–®ä¸€æŒ‡æ¨™)
- [ ] çµ±è¨ˆæª¢å®šç¢ºèªå·®ç•°é¡¯è‘—æ€§
- [ ] è€ƒæ…®æ¥­å‹™éœ€æ±‚ (é€Ÿåº¦ã€å¯è§£é‡‹æ€§)
- [ ] æ¸¬è©¦é›†åªç”¨ä¸€æ¬¡ä½œæœ€çµ‚è©•ä¼°

**éƒ¨ç½²å‰æª¢æŸ¥**ï¼š
- [ ] é©—è­‰æ¨¡å‹åœ¨æœ€æ–°æ•¸æ“šä¸Šçš„æ€§èƒ½
- [ ] æ¸¬è©¦æ¥µç«¯æ¢ä»¶ä¸‹çš„ç©©å¥æ€§
- [ ] æº–å‚™æ¨¡å‹ç›£æ§æŒ‡æ¨™
- [ ] å»ºç«‹æ¨¡å‹æ›´æ–°æ©Ÿåˆ¶

### 11.3 å¸¸è¦‹éŒ¯èª¤ç¸½çµ

| éŒ¯èª¤ | å¾Œæœ | æ­£ç¢ºåšæ³• |
|------|------|---------|
| åªçœ‹è¨“ç·´é›†æº–ç¢ºåº¦ | éæ“¬åˆ | ä½¿ç”¨äº¤å‰é©—è­‰ |
| ä¸å¹³è¡¡æ•¸æ“šç”¨ Accuracy | èª¤å° | ä½¿ç”¨ F1, Precision, Recall |
| æ™‚é–“åºåˆ—éš¨æ©ŸåŠƒåˆ† | é«˜ä¼°æ€§èƒ½ | Time Series Split |
| éåº¦èª¿åƒ | éæ“¬åˆé©—è­‰é›† | Nested CV |
| å¿½ç•¥è¨ˆç®—æˆæœ¬ | ç„¡æ³•éƒ¨ç½² | ç¶œåˆè©•ä¼°å¤šç›®æ¨™ |

### 11.4 é€²éšå­¸ç¿’è³‡æº

**æ›¸ç±**ï¼š
- *An Introduction to Statistical Learning* (James et al.)
- *The Elements of Statistical Learning* (Hastie et al.)

**ç·šä¸Šèª²ç¨‹**ï¼š
- Andrew Ng - Machine Learning (Coursera)
- Fast.ai - Practical Deep Learning

**sklearn å®˜æ–¹æ–‡æª”**ï¼š
- [Model Evaluation](https://scikit-learn.org/stable/modules/model_evaluation.html)
- [Cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html)

---

**ç‰ˆæ¬Šè²æ˜**ï¼šæœ¬æ•™æç”±é€¢ç”²å¤§å­¸åŒ–å­¸å·¥ç¨‹å­¸ç³»èŠæ›œç¦åŠ©ç†æ•™æˆç·¨å¯«ï¼Œåƒ…ä¾›æ•™å­¸ä½¿ç”¨ã€‚

**èª²ç¨‹ä»£ç¢¼**ï¼šCHE-AI-114  
**æ›´æ–°æ—¥æœŸ**ï¼š2026 å¹´ 1 æœˆ

---

**é…å¥—æª”æ¡ˆ**ï¼š
- è¬›ç¾©æª”æ¡ˆï¼š`Unit14_Model_Evaluation_Overview.md`
- ç¨‹å¼ç¢¼æª”æ¡ˆï¼š`Unit14_Model_Comparison.ipynb`, `Unit14_Hyperparameter_Tuning_GridSearch.ipynb`, `Unit14_Hyperparameter_Tuning_Bayesian.ipynb`, `Unit14_Model_Selection_Pipeline.ipynb`, `Unit14_Model_Interpretability_Basics.ipynb`
- ä½œæ¥­æª”æ¡ˆï¼š`Unit14_Homework.ipynb`
- è¼¸å‡ºè³‡æ–™å¤¾ï¼š`outputs/P3_Unit14_Model_Evaluation/`

---

**ä¸‹ä¸€å–®å…ƒ**ï¼šPart 4 æ·±åº¦å­¸ç¿’ - Unit15 DNN(MLP) æ¨¡å‹

