# Unit14 è¶…åƒæ•¸èª¿æ•´ | Hyperparameter Tuning

**é€¢ç”²å¤§å­¸ åŒ–å­¸å·¥ç¨‹å­¸ç³»**  
**èª²ç¨‹åç¨±**: AIåœ¨åŒ–å·¥ä¸Šä¹‹æ‡‰ç”¨  
**èª²ç¨‹ä»£ç¢¼**: CHE-AI-114  
**æˆèª²æ•™å¸«**: èŠæ›œç¦ åŠ©ç†æ•™æˆ

---

## ğŸ“š èª²ç¨‹å¤§ç¶± (Table of Contents)

1. [å–®å…ƒç°¡ä»‹](#1-å–®å…ƒç°¡ä»‹)
2. [è¶…åƒæ•¸ vs æ¨¡å‹åƒæ•¸](#2-è¶…åƒæ•¸-vs-æ¨¡å‹åƒæ•¸)
3. [Grid Search ç¶²æ ¼æœç´¢](#3-grid-search-ç¶²æ ¼æœç´¢)
4. [Random Search éš¨æ©Ÿæœç´¢](#4-random-search-éš¨æ©Ÿæœç´¢)
5. [Bayesian Optimization è²æ°æœ€ä½³åŒ–](#5-bayesian-optimization-è²æ°æœ€ä½³åŒ–)
6. [é€²éšæœç´¢æŠ€å·§](#6-é€²éšæœç´¢æŠ€å·§)
7. [æœç´¢ç©ºé–“è¨­è¨ˆ](#7-æœç´¢ç©ºé–“è¨­è¨ˆ)
8. [åŒ–å·¥æ‡‰ç”¨æ¡ˆä¾‹](#8-åŒ–å·¥æ‡‰ç”¨æ¡ˆä¾‹)
9. [ç¸½çµèˆ‡æœ€ä½³å¯¦è¸](#9-ç¸½çµèˆ‡æœ€ä½³å¯¦è¸)

---

## 1. å–®å…ƒç°¡ä»‹

### 1.1 å­¸ç¿’ç›®æ¨™

è¶…åƒæ•¸èª¿æ•´ (Hyperparameter Tuning) æ˜¯æ¨¡å‹å„ªåŒ–çš„é—œéµæ­¥é©Ÿï¼Œç›´æ¥å½±éŸ¿æ¨¡å‹æ€§èƒ½çš„ä¸Šé™ã€‚

**æ ¸å¿ƒå­¸ç¿’ç›®æ¨™**ï¼š
1. ç†è§£è¶…åƒæ•¸èˆ‡æ¨¡å‹åƒæ•¸çš„æ ¹æœ¬å·®ç•°
2. æŒæ¡ä¸‰ç¨®ä¸»æµè¶…åƒæ•¸æœç´¢æ–¹æ³•çš„åŸç†èˆ‡å¯¦ä½œ
3. å­¸æœƒè¨­è¨ˆåˆç†çš„æœç´¢ç©ºé–“ï¼Œé¿å…è³‡æºæµªè²»
4. æ‡‰ç”¨é€²éšæŠ€å·§åŠ é€Ÿæœç´¢éç¨‹
5. åœ¨åŒ–å·¥å ´æ™¯ä¸­é¸æ“‡æœ€é©åˆçš„èª¿åƒç­–ç•¥

### 1.2 ç‚ºä»€éº¼è¶…åƒæ•¸èª¿æ•´å¦‚æ­¤é‡è¦ï¼Ÿ

**æ¡ˆä¾‹å°æ¯”**ï¼šRandom Forest é æ¸¬åæ‡‰å™¨ç”¢ç‡

| è¶…åƒæ•¸é…ç½® | Test RÂ² | è¨“ç·´æ™‚é–“ | èªªæ˜ |
|----------|---------|---------|------|
| é»˜èªåƒæ•¸ | 0.78 | 2.3 ç§’ | `RandomForestRegressor()` |
| æ‰‹å‹•èª¿æ•´ | 0.83 | 5.1 ç§’ | æ†‘ç¶“é©—è¨­å®š n_estimators=200 |
| Grid Search | 0.87 | 45 ç§’ | æœç´¢ 36 ç¨®çµ„åˆ |
| Bayesian Opt | 0.88 | 18 ç§’ | æ™ºèƒ½æœç´¢ 50 æ¬¡ |

**é—œéµç™¼ç¾**ï¼š
- âœ… æ­£ç¢ºèª¿åƒå¯æå‡ 10-15% æ€§èƒ½
- âœ… Bayesian Optimization æ•ˆç‡é é«˜æ–¼ Grid Search
- âš ï¸ éåº¦èª¿åƒå¯èƒ½éæ“¬åˆé©—è­‰é›†

### 1.3 åŒ–å·¥é ˜åŸŸçš„èª¿åƒæŒ‘æˆ°

| æŒ‘æˆ° | åŒ–å·¥ç‰¹é» | å½±éŸ¿ |
|------|---------|------|
| **å°æ¨£æœ¬** | å¯¦é©—æˆæœ¬é«˜ï¼Œæ•¸æ“šé‡æœ‰é™ | æ˜“éæ“¬åˆï¼Œéœ€åš´æ ¼é©—è­‰ |
| **è¨ˆç®—è³‡æº** | ç”Ÿç”¢ç’°å¢ƒç¡¬é«”å—é™ | ç„¡æ³•å˜—è©¦éæ–¼è¤‡é›œçš„æ¨¡å‹ |
| **å³æ™‚æ€§éœ€æ±‚** | ç·šä¸Šæ§åˆ¶éœ€å¿«é€Ÿæ¨ç† | éœ€å¹³è¡¡æº–ç¢ºåº¦èˆ‡é€Ÿåº¦ |
| **å¤šç›®æ¨™** | ç”¢ç‡ã€èƒ½è€—ã€å“è³ªéœ€å…¼é¡§ | å–®ä¸€è¶…åƒæ•¸çµ„åˆé›£ä»¥æ»¿è¶³ |
| **é ˜åŸŸçŸ¥è­˜** | åŒ–å­¸åæ‡‰æ©Ÿåˆ¶å·²çŸ¥ | å¯åˆ©ç”¨å…ˆé©—çŸ¥è­˜ç¸®å°æœç´¢ç©ºé–“ |

### 1.4 æœ¬å–®å…ƒæ¶æ§‹

```
Unit14 è¶…åƒæ•¸èª¿æ•´
â”‚
â”œâ”€â”€ åŸºç¤æ¦‚å¿µ
â”‚   â”œâ”€â”€ è¶…åƒæ•¸ vs æ¨¡å‹åƒæ•¸
â”‚   â””â”€â”€ èª¿åƒç­–ç•¥ç¸½è¦½
â”‚
â”œâ”€â”€ ç¶“å…¸æ–¹æ³•
â”‚   â”œâ”€â”€ Grid Search (çª®èˆ‰æ³•)
â”‚   â”œâ”€â”€ Random Search (éš¨æ©Ÿæ³•)
â”‚   â””â”€â”€ æ–¹æ³•æ¯”è¼ƒèˆ‡é¸æ“‡
â”‚
â”œâ”€â”€ é€²éšæŠ€å·§
â”‚   â”œâ”€â”€ Bayesian Optimization (æ™ºèƒ½æœç´¢)
â”‚   â”œâ”€â”€ Halving Search (åŠ é€ŸæŠ€å·§)
â”‚   â””â”€â”€ å¤šä¿çœŸåº¦å„ªåŒ–
â”‚
â””â”€â”€ å¯¦å‹™æ‡‰ç”¨
    â”œâ”€â”€ æœç´¢ç©ºé–“è¨­è¨ˆ
    â”œâ”€â”€ åŒ–å·¥æ¡ˆä¾‹å¯¦æˆ°
    â””â”€â”€ é¿å‘æŒ‡å—
```

---

## 2. è¶…åƒæ•¸ vs æ¨¡å‹åƒæ•¸

### 2.1 æ ¸å¿ƒå·®ç•°

| æ¯”è¼ƒç¶­åº¦ | æ¨¡å‹åƒæ•¸ (Parameters) | è¶…åƒæ•¸ (Hyperparameters) |
|---------|---------------------|------------------------|
| **å®šç¾©** | æ¨¡å‹å¾æ•¸æ“šä¸­å­¸ç¿’çš„è®Šé‡ | è¨“ç·´å‰äººç‚ºè¨­å®šçš„é…ç½® |
| **å­¸ç¿’æ–¹å¼** | è‡ªå‹•å„ªåŒ–ï¼ˆæ¢¯åº¦ä¸‹é™ç­‰ï¼‰ | éœ€è¦æ‰‹å‹•èª¿æ•´æˆ–æœç´¢ |
| **æ•¸é‡** | é€šå¸¸å¾ˆå¤šï¼ˆç™¾è¬ç´šï¼‰ | é€šå¸¸è¼ƒå°‘ï¼ˆå€‹ä½æ•¸åˆ°åå¹¾å€‹ï¼‰ |
| **ä¿å­˜** | ä¿å­˜åœ¨æ¨¡å‹æ–‡ä»¶ä¸­ | è¨“ç·´è…³æœ¬æˆ–é…ç½®æ–‡ä»¶ |
| **ä¾‹å­** | ç·šæ€§å›æ­¸çš„ä¿‚æ•¸ $w$ å’Œæˆªè· $b$ | å­¸ç¿’ç‡ã€æ­£å‰‡åŒ–å¼·åº¦ $\alpha$ |

### 2.2 å¸¸è¦‹æ¨¡å‹çš„è¶…åƒæ•¸

#### ç·šæ€§æ¨¡å‹ (Ridge, Lasso)

```python
from sklearn.linear_model import Ridge

model = Ridge(
    alpha=1.0,           # ğŸ”§ è¶…åƒæ•¸ï¼šæ­£å‰‡åŒ–å¼·åº¦
    fit_intercept=True,  # ğŸ”§ è¶…åƒæ•¸ï¼šæ˜¯å¦æ“¬åˆæˆªè·
    max_iter=1000        # ğŸ”§ è¶…åƒæ•¸ï¼šæœ€å¤§è¿­ä»£æ¬¡æ•¸
)

# è¨“ç·´å¾Œçš„æ¨¡å‹åƒæ•¸
# model.coef_  â†’ ğŸ“Š æ¨¡å‹åƒæ•¸ï¼šç‰¹å¾µä¿‚æ•¸
# model.intercept_  â†’ ğŸ“Š æ¨¡å‹åƒæ•¸ï¼šæˆªè·
```

#### Random Forest

```python
from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(
    n_estimators=100,        # ğŸ”§ è¶…åƒæ•¸ï¼šæ¨¹çš„æ•¸é‡
    max_depth=10,            # ğŸ”§ è¶…åƒæ•¸ï¼šæ¨¹çš„æœ€å¤§æ·±åº¦
    min_samples_split=2,     # ğŸ”§ è¶…åƒæ•¸ï¼šåˆ†è£‚æ‰€éœ€æœ€å°æ¨£æœ¬æ•¸
    min_samples_leaf=1,      # ğŸ”§ è¶…åƒæ•¸ï¼šè‘‰ç¯€é»æœ€å°æ¨£æœ¬æ•¸
    max_features='sqrt',     # ğŸ”§ è¶…åƒæ•¸ï¼šæ¯æ¬¡åˆ†è£‚è€ƒæ…®çš„ç‰¹å¾µæ•¸
    random_state=42          # ğŸ”§ è¶…åƒæ•¸ï¼šéš¨æ©Ÿç¨®å­
)

# è¨“ç·´å¾Œçš„æ¨¡å‹åƒæ•¸ï¼ˆéš±è—åœ¨æ¨¹çµæ§‹ä¸­ï¼‰
# æ¯æ£µæ¨¹çš„åˆ†è£‚é»ã€é–¾å€¼ç­‰ â†’ ğŸ“Š æ¨¡å‹åƒæ•¸
```

#### Support Vector Machine

```python
from sklearn.svm import SVR

model = SVR(
    kernel='rbf',      # ğŸ”§ è¶…åƒæ•¸ï¼šæ ¸å‡½æ•¸é¡å‹
    C=1.0,             # ğŸ”§ è¶…åƒæ•¸ï¼šæ‡²ç½°åƒæ•¸
    epsilon=0.1,       # ğŸ”§ è¶…åƒæ•¸ï¼šÎµ-insensitive åƒæ•¸
    gamma='scale'      # ğŸ”§ è¶…åƒæ•¸ï¼šRBF æ ¸çš„å¯¬åº¦
)

# è¨“ç·´å¾Œçš„æ¨¡å‹åƒæ•¸
# model.support_vectors_  â†’ ğŸ“Š æ¨¡å‹åƒæ•¸ï¼šæ”¯æŒå‘é‡
# model.dual_coef_  â†’ ğŸ“Š æ¨¡å‹åƒæ•¸ï¼šå°å¶ä¿‚æ•¸
```

### 2.3 è¶…åƒæ•¸çš„åˆ†é¡

#### é¡å‹ 1: æ¨¡å‹çµæ§‹è¶…åƒæ•¸

å½±éŸ¿æ¨¡å‹è¤‡é›œåº¦å’Œè¡¨é”èƒ½åŠ›ã€‚

| æ¨¡å‹ | è¶…åƒæ•¸ | å½±éŸ¿ |
|------|--------|------|
| Random Forest | `n_estimators`, `max_depth` | æ¨¹çš„æ•¸é‡å’Œæ·±åº¦ |
| Neural Network | å±¤æ•¸, ç¥ç¶“å…ƒæ•¸ | ç¶²è·¯å®¹é‡ |
| Polynomial | `degree` | å¤šé …å¼éšæ•¸ |

**èª¿åƒåŸå‰‡**ï¼š
- éå¤§ â†’ éæ“¬åˆé¢¨éšª â†‘
- éå° â†’ æ¬ æ“¬åˆé¢¨éšª â†‘
- éœ€é€šéé©—è­‰é›†æ‰¾å¹³è¡¡é»

#### é¡å‹ 2: æ­£å‰‡åŒ–è¶…åƒæ•¸

æ§åˆ¶æ¨¡å‹è¤‡é›œåº¦æ‡²ç½°ï¼Œé˜²æ­¢éæ“¬åˆã€‚

| æ¨¡å‹ | è¶…åƒæ•¸ | ä½œç”¨ |
|------|--------|------|
| Ridge | `alpha` | L2 æ­£å‰‡åŒ–å¼·åº¦ |
| Lasso | `alpha` | L1 æ­£å‰‡åŒ–å¼·åº¦ |
| ElasticNet | `alpha`, `l1_ratio` | L1/L2 æ··åˆ |
| Random Forest | `min_samples_split` | é–“æ¥æ­£å‰‡åŒ– |

**èª¿åƒåŸå‰‡**ï¼š
- `alpha` â†‘ â†’ æ­£å‰‡åŒ– â†‘ â†’ æ¨¡å‹æ›´ç°¡å–®
- å°æ•¸æ“šé›†éœ€è¼ƒå¼·æ­£å‰‡åŒ–

#### é¡å‹ 3: å„ªåŒ–è¶…åƒæ•¸

å½±éŸ¿è¨“ç·´éç¨‹çš„æ”¶æ–‚é€Ÿåº¦å’Œç©©å®šæ€§ã€‚

| è¶…åƒæ•¸ | ä½œç”¨ | å…¸å‹ç¯„åœ |
|--------|------|---------|
| `learning_rate` | æ¢¯åº¦ä¸‹é™æ­¥é•· | 0.001 - 0.1 |
| `max_iter` | æœ€å¤§è¿­ä»£æ¬¡æ•¸ | 100 - 10000 |
| `batch_size` | æ‰¹æ¬¡å¤§å° | 32 - 512 |
| `early_stopping` | æå‰åœæ­¢ | True/False |

#### é¡å‹ 4: ç‰¹å®šç®—æ³•è¶…åƒæ•¸

| æ¨¡å‹ | è¶…åƒæ•¸ | èªªæ˜ |
|------|--------|------|
| SVM | `kernel`, `gamma` | æ ¸å‡½æ•¸é¸æ“‡èˆ‡åƒæ•¸ |
| XGBoost | `subsample`, `colsample_bytree` | æ¡æ¨£æ¯”ä¾‹ |
| K-Means | `n_clusters` | èšé¡æ•¸ |

### 2.4 åŒ–å·¥æ¡ˆä¾‹ï¼šå‚¬åŒ–åŠ‘æ€§èƒ½é æ¸¬

**å ´æ™¯**ï¼šé æ¸¬å‚¬åŒ–åŠ‘è½‰åŒ–ç‡

**æ¨¡å‹**ï¼šRandom Forest Regressor

**è¶…åƒæ•¸èˆ‡å…¶å½±éŸ¿**ï¼š

```python
# è¶…åƒæ•¸çµ„åˆ 1: é»˜èª
model_1 = RandomForestRegressor()
# çµæœ: Train RÂ²=0.99, Val RÂ²=0.72 â†’ éæ“¬åˆ

# è¶…åƒæ•¸çµ„åˆ 2: æ¸›å°‘è¤‡é›œåº¦
model_2 = RandomForestRegressor(
    n_estimators=50,        # æ¸›å°‘æ¨¹çš„æ•¸é‡
    max_depth=5,            # é™åˆ¶æ·±åº¦
    min_samples_split=10    # å¢åŠ åˆ†è£‚é–€æª»
)
# çµæœ: Train RÂ²=0.85, Val RÂ²=0.81 â†’ æ”¹å–„æ³›åŒ– âœ…

# è¶…åƒæ•¸çµ„åˆ 3: éåº¦ç°¡åŒ–
model_3 = RandomForestRegressor(
    n_estimators=10,
    max_depth=2
)
# çµæœ: Train RÂ²=0.68, Val RÂ²=0.65 â†’ æ¬ æ“¬åˆ
```

**çµè«–**ï¼šè¶…åƒæ•¸ç›´æ¥æ±ºå®šæ¨¡å‹åœ¨ éæ“¬åˆ-æœ€ä½³-æ¬ æ“¬åˆ è­œç³»ä¸­çš„ä½ç½®ã€‚

### 2.5 è¶…åƒæ•¸èª¿æ•´çš„ç›®æ¨™

**éŒ¯èª¤ç›®æ¨™** âŒï¼š
- æœ€å¤§åŒ–è¨“ç·´é›†æ€§èƒ½
- æ‰¾åˆ°"æœ€è¤‡é›œ"çš„æ¨¡å‹

**æ­£ç¢ºç›®æ¨™** âœ…ï¼š
- æœ€å¤§åŒ–**é©—è­‰é›†**æ€§èƒ½ï¼ˆæ³›åŒ–èƒ½åŠ›ï¼‰
- åœ¨æ€§èƒ½èˆ‡æˆæœ¬é–“å–å¾—å¹³è¡¡

**å¤šç›®æ¨™è€ƒé‡**ï¼š

$$
\text{Score} = w_1 \cdot \text{Accuracy} - w_2 \cdot \text{Training Time} - w_3 \cdot \text{Model Size}
$$

åŒ–å·¥å¯¦å‹™ä¸­éœ€è¦å¹³è¡¡ï¼š
- é æ¸¬æº–ç¢ºåº¦
- è¨“ç·´/æ¨ç†é€Ÿåº¦
- æ¨¡å‹å¯è§£é‡‹æ€§
- ç¡¬é«”è³‡æºæ¶ˆè€—

---

## 3. Grid Search ç¶²æ ¼æœç´¢

### 3.1 åŸç†

Grid Searchï¼ˆç¶²æ ¼æœç´¢ï¼‰æ˜¯ä¸€ç¨®**çª®èˆ‰å¼**è¶…åƒæ•¸æœç´¢æ–¹æ³•ã€‚

**æ ¸å¿ƒæ¦‚å¿µ**ï¼š
1. ç‚ºæ¯å€‹è¶…åƒæ•¸å®šç¾©å€™é¸å€¼åˆ—è¡¨
2. ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„è¶…åƒæ•¸çµ„åˆï¼ˆç¬›å¡çˆ¾ç©ï¼‰
3. å°æ¯å€‹çµ„åˆè¨“ç·´æ¨¡å‹ä¸¦è©•ä¼°æ€§èƒ½
4. é¸æ“‡æ€§èƒ½æœ€å„ªçš„çµ„åˆ

**æ•¸å­¸è¡¨ç¤º**ï¼š

çµ¦å®šè¶…åƒæ•¸ç©ºé–“ï¼š

$$
\Theta = \{\theta_1, \theta_2, \ldots, \theta_k\}
$$

æ¯å€‹è¶…åƒæ•¸çš„å€™é¸å€¼ï¼š

$$
\theta_i \in \{v_i^1, v_i^2, \ldots, v_i^{n_i}\}
$$

ç¸½æœç´¢æ¬¡æ•¸ï¼š

$$
N_{\text{total}} = \prod_{i=1}^{k} n_i
$$

### 3.2 Sklearn å¯¦ä½œ

#### åŸºæœ¬ç”¨æ³•

```python
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor
import numpy as np

# 1. å®šç¾©æ¨¡å‹
model = RandomForestRegressor(random_state=42)

# 2. å®šç¾©è¶…åƒæ•¸æœç´¢ç©ºé–“
param_grid = {
    'n_estimators': [50, 100, 200],           # 3 ç¨®é¸æ“‡
    'max_depth': [5, 10, 15, None],           # 4 ç¨®é¸æ“‡
    'min_samples_split': [2, 5, 10]           # 3 ç¨®é¸æ“‡
}

# ç¸½å…±: 3 Ã— 4 Ã— 3 = 36 ç¨®çµ„åˆ

# 3. è¨­å®š GridSearchCV
grid_search = GridSearchCV(
    estimator=model,
    param_grid=param_grid,
    cv=5,                      # 5-fold cross-validation
    scoring='r2',              # è©•ä¼°æŒ‡æ¨™
    n_jobs=-1,                 # ä½¿ç”¨æ‰€æœ‰ CPU æ ¸å¿ƒ
    verbose=2,                 # é¡¯ç¤ºé€²åº¦
    return_train_score=True    # è¨˜éŒ„è¨“ç·´åˆ†æ•¸
)

# 4. åŸ·è¡Œæœç´¢
grid_search.fit(X_train, y_train)

# 5. æŸ¥çœ‹çµæœ
print(f"Best parameters: {grid_search.best_params_}")
print(f"Best CV score: {grid_search.best_score_:.4f}")
print(f"Best estimator: {grid_search.best_estimator_}")
```

#### è¼¸å‡ºè§£æ

```
Fitting 5 folds for each of 36 candidates, totalling 180 fits
Best parameters: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 200}
Best CV score: 0.8534
```

**è¨ˆç®—èªªæ˜**ï¼š
- 36 ç¨®è¶…åƒæ•¸çµ„åˆ
- æ¯ç¨®çµ„åˆåš 5-fold CV
- ç¸½å…±è¨“ç·´ 36 Ã— 5 = 180 å€‹æ¨¡å‹

### 3.3 æœç´¢çµæœåˆ†æ

#### çµæœ DataFrame

```python
import pandas as pd

# å°‡æœç´¢çµæœè½‰ç‚º DataFrame
results_df = pd.DataFrame(grid_search.cv_results_)

# é¸æ“‡é—œéµæ¬„ä½
key_columns = [
    'param_n_estimators',
    'param_max_depth',
    'param_min_samples_split',
    'mean_test_score',
    'std_test_score',
    'mean_fit_time',
    'rank_test_score'
]

results_summary = results_df[key_columns].sort_values(
    'rank_test_score'
)

print(results_summary.head(10))
```

**è¼¸å‡ºç¤ºä¾‹**ï¼š

| n_estimators | max_depth | min_samples_split | mean_test_score | std_test_score | mean_fit_time | rank |
|--------------|-----------|-------------------|-----------------|----------------|---------------|------|
| 200 | 10 | 5 | 0.8534 | 0.0234 | 3.45 | 1 |
| 200 | 15 | 5 | 0.8512 | 0.0256 | 4.12 | 2 |
| 100 | 10 | 5 | 0.8489 | 0.0241 | 1.78 | 3 |

#### è¦–è¦ºåŒ–ï¼šHeatmap

```python
import matplotlib.pyplot as plt
import seaborn as sns

# å›ºå®š n_estimators=200ï¼Œç¹ªè£½ max_depth vs min_samples_split ç†±åœ–
pivot_data = results_df[
    results_df['param_n_estimators'] == 200
].pivot(
    index='param_max_depth',
    columns='param_min_samples_split',
    values='mean_test_score'
)

plt.figure(figsize=(8, 6))
sns.heatmap(
    pivot_data,
    annot=True,
    fmt='.3f',
    cmap='viridis',
    cbar_kws={'label': 'Mean Test RÂ²'}
)
plt.title('Grid Search Results: max_depth vs min_samples_split\n(n_estimators=200)')
plt.xlabel('min_samples_split')
plt.ylabel('max_depth')
plt.tight_layout()
plt.show()
```

### 3.4 å„ªé»èˆ‡ç¼ºé»

#### âœ… å„ªé»

1. **å®Œæ•´æ€§**ï¼šä¿è­‰æ‰¾åˆ°æœç´¢ç©ºé–“å…§çš„æœ€å„ªçµ„åˆ
2. **å¯ä¸¦è¡ŒåŒ–**ï¼šä¸åŒçµ„åˆå¯åŒæ™‚è¨“ç·´ï¼ˆ`n_jobs=-1`ï¼‰
3. **ç°¡å–®ç›´è§€**ï¼šæ˜“æ–¼ç†è§£å’Œå¯¦ä½œ
4. **å¯é‡ç¾**ï¼šçµ¦å®šæœç´¢ç©ºé–“ï¼Œçµæœå”¯ä¸€

#### âŒ ç¼ºé»

1. **è¨ˆç®—æˆæœ¬é«˜**ï¼šçµ„åˆæ•¸å‘ˆæŒ‡æ•¸å¢é•·
   
   $$
   N = n_1 \times n_2 \times \cdots \times n_k
   $$
   
   ä¾‹å¦‚ï¼š10 å€‹è¶…åƒæ•¸ï¼Œæ¯å€‹ 5 ç¨®é¸æ“‡ â†’ $5^{10} = 9,765,625$ ç¨®çµ„åˆ

2. **ç¶­åº¦è©›å’’**ï¼šè¶…åƒæ•¸è¶Šå¤šï¼Œæœç´¢è¶Šä¸ç¾å¯¦

3. **é›¢æ•£åŒ–æå¤±**ï¼šé€£çºŒè¶…åƒæ•¸éœ€è¦é›¢æ•£åŒ–ï¼Œå¯èƒ½éŒ¯éæœ€å„ªå€¼

4. **è³‡æºæµªè²»**ï¼šåœ¨ä¸é‡è¦çš„è¶…åƒæ•¸ä¸Šä¹Ÿæœƒè€—è²»ç›¸åŒè³‡æº

### 3.5 Grid Search vs Manual Tuning

**æ‰‹å‹•èª¿åƒ**ï¼š

```python
# å·¥ç¨‹å¸«æ†‘ç¶“é©—å˜—è©¦
è©¦é©— 1: n_estimators=100, max_depth=10  â†’ RÂ²=0.82
è©¦é©— 2: n_estimators=200, max_depth=10  â†’ RÂ²=0.84
è©¦é©— 3: n_estimators=200, max_depth=15  â†’ RÂ²=0.83
# åœæ­¢æœç´¢ï¼Œæ¡ç”¨è©¦é©— 2
```

**Grid Search**ï¼š

```python
# ç³»çµ±åŒ–æœç´¢
æœç´¢ç©ºé–“: 
    n_estimators=[50, 100, 150, 200, 250]
    max_depth=[5, 10, 15, 20, None]
çµæœ: n_estimators=250, max_depth=12 â†’ RÂ²=0.87
```

**å·®ç•°ç¸½çµ**ï¼š

| æ–¹æ³• | æœç´¢æ¬¡æ•¸ | æœ€å„ªè§£ä¿è­‰ | äººåŠ›æˆæœ¬ | é©ç”¨å ´æ™¯ |
|------|---------|-----------|---------|---------|
| æ‰‹å‹•èª¿åƒ | 3-10 æ¬¡ | âŒ ä¸ä¿è­‰ | é«˜ | å¿«é€Ÿé©—è­‰ |
| Grid Search | 25-100 æ¬¡ | âœ… ç©ºé–“å…§æœ€å„ª | ä½ | ç³»çµ±åŒ–å„ªåŒ– |

### 3.6 åŒ–å·¥æ¡ˆä¾‹ï¼šè’¸é¤¾å¡”æº«åº¦é æ¸¬

**å•é¡Œ**ï¼šä½¿ç”¨ SVR é æ¸¬è’¸é¤¾å¡”é ‚æº«åº¦

**è¶…åƒæ•¸æœç´¢ç©ºé–“**ï¼š

```python
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# å»ºç«‹ Pipeline
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('svr', SVR())
])

# Grid Search é…ç½®
param_grid = {
    'svr__C': [0.1, 1, 10, 100],              # æ‡²ç½°åƒæ•¸
    'svr__epsilon': [0.01, 0.1, 0.5],         # Îµ-insensitive
    'svr__gamma': ['scale', 'auto', 0.1, 1]   # RBF æ ¸å¯¬åº¦
}

# ç¸½å…±: 4 Ã— 3 Ã— 4 = 48 ç¨®çµ„åˆ

grid_search = GridSearchCV(
    pipeline,
    param_grid,
    cv=5,
    scoring='neg_mean_absolute_error',  # MAE (åŒ–å·¥æ›´é—œæ³¨çµ•å°èª¤å·®)
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train, y_train)
```

**çµæœåˆ†æ**ï¼š

```python
print(f"Best parameters: {grid_search.best_params_}")
# Best parameters: {'svr__C': 10, 'svr__epsilon': 0.1, 'svr__gamma': 'scale'}

print(f"Best MAE: {-grid_search.best_score_:.3f} Â°C")
# Best MAE: 1.234 Â°C

# åœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°
y_pred = grid_search.predict(X_test)
test_mae = mean_absolute_error(y_test, y_pred)
print(f"Test MAE: {test_mae:.3f} Â°C")
# Test MAE: 1.187 Â°C (æ»¿è¶³ Â±2Â°C æ§åˆ¶è¦æ±‚)
```

**åŒ–å·¥æ„ç¾©**ï¼š
- MAE < 2Â°Cï¼šæ»¿è¶³å·¥æ¥­æ§åˆ¶ç²¾åº¦
- `C=10`ï¼šé©åº¦æ‡²ç½°ï¼Œå¹³è¡¡æ“¬åˆèˆ‡æ³›åŒ–
- `epsilon=0.1`ï¼šå®¹å¿ 0.1Â°C èª¤å·®ï¼ˆåˆç†çš„æ¸¬é‡ä¸ç¢ºå®šåº¦ï¼‰

### 3.7 å¯¦å‹™æŠ€å·§

#### æŠ€å·§ 1: ç²—æœ â†’ ç²¾æœ

```python
# ç¬¬ä¸€è¼ªï¼šç²—æœç´¢
param_grid_coarse = {
    'n_estimators': [50, 100, 200, 500],
    'max_depth': [5, 10, 20, None]
}
# çµæœ: n_estimators=200, max_depth=10 æœ€å„ª

# ç¬¬äºŒè¼ªï¼šåœ¨æœ€å„ªå€åŸŸç´°æœ
param_grid_fine = {
    'n_estimators': [150, 175, 200, 225, 250],
    'max_depth': [8, 9, 10, 11, 12]
}
```

#### æŠ€å·§ 2: å„ªå…ˆèª¿æ•´é‡è¦è¶…åƒæ•¸

**é‡è¦æ€§æ’åº**ï¼ˆRandom Forest ç‚ºä¾‹ï¼‰ï¼š
1. `n_estimators`, `max_depth` â†’ å½±éŸ¿æœ€å¤§
2. `min_samples_split`, `min_samples_leaf` â†’ ä¸­ç­‰å½±éŸ¿
3. `max_features` â†’ è¼ƒå°å½±éŸ¿
4. `bootstrap`, `oob_score` â†’ é‚Šéš›å½±éŸ¿

**ç­–ç•¥**ï¼šå…ˆå›ºå®šæ¬¡è¦åƒæ•¸ï¼Œåªæœç´¢é—œéµåƒæ•¸ã€‚

#### æŠ€å·§ 3: ä½¿ç”¨å°æ•¸å°ºåº¦

å°æ–¼ç¯„åœè·¨åº¦å¤§çš„è¶…åƒæ•¸ï¼ˆå¦‚å­¸ç¿’ç‡ã€æ­£å‰‡åŒ–å¼·åº¦ï¼‰ï¼Œä½¿ç”¨å°æ•¸åˆ»åº¦ï¼š

```python
import numpy as np

# âŒ ç·šæ€§åˆ»åº¦ï¼ˆä¸æ¨è–¦ï¼‰
param_grid = {
    'alpha': [0.001, 0.01, 0.1, 1, 10, 100]
}

# âœ… å°æ•¸åˆ»åº¦ï¼ˆæ¨è–¦ï¼‰
param_grid = {
    'alpha': np.logspace(-3, 2, 6)  # [0.001, 0.01, 0.1, 1, 10, 100]
}
```

**åŸå› **ï¼š`alpha` å¾ 0.001 â†’ 0.01 çš„è®ŠåŒ–ï¼Œèˆ‡ 1 â†’ 10 çš„è®ŠåŒ–ï¼Œå°æ¨¡å‹å½±éŸ¿ç›¸ä¼¼ã€‚

#### æŠ€å·§ 4: ç›£æ§è¨“ç·´-é©—è­‰å·®è·

```python
results_df = pd.DataFrame(grid_search.cv_results_)

# è¨ˆç®—éæ“¬åˆç¨‹åº¦
results_df['overfit_gap'] = (
    results_df['mean_train_score'] - results_df['mean_test_score']
)

# æ‰¾å‡ºéæ“¬åˆæœ€å°‘ä¸”æ€§èƒ½å¥½çš„çµ„åˆ
results_df['score_adjusted'] = (
    results_df['mean_test_score'] - 0.1 * results_df['overfit_gap']
)

best_idx = results_df['score_adjusted'].idxmax()
print(results_df.loc[best_idx, ['params', 'mean_test_score', 'overfit_gap']])
```

### 3.8 ä½•æ™‚ä½¿ç”¨ Grid Searchï¼Ÿ

| å ´æ™¯ | æ˜¯å¦é©ç”¨ | åŸå›  |
|------|---------|------|
| è¶…åƒæ•¸ â‰¤ 3 å€‹ | âœ… é©ç”¨ | æœç´¢ç©ºé–“å¯æ§ |
| è¶…åƒæ•¸ â‰¥ 5 å€‹ | âŒ ä¸é©ç”¨ | çµ„åˆçˆ†ç‚¸ |
| å·²çŸ¥å¤§è‡´æœ€å„ªå€åŸŸ | âœ… é©ç”¨ | ç²¾ç´°æœç´¢ |
| å®Œå…¨æœªçŸ¥ | âŒ ä¸é©ç”¨ | Random Search æ›´é«˜æ•ˆ |
| è¨ˆç®—è³‡æºå……è¶³ | âœ… é©ç”¨ | å¯ä¸¦è¡ŒåŒ– |
| å°æ•¸æ“šé›† | âœ… é©ç”¨ | è¨“ç·´å¿«é€Ÿ |

**æ¨è–¦å ´æ™¯**ï¼š
- æ¨¡å‹è¨“ç·´æ™‚é–“ < 1 åˆ†é˜
- è¶…åƒæ•¸æ•¸é‡ â‰¤ 4 å€‹
- éœ€è¦å®Œæ•´æƒææŸå€‹äºŒç¶­è¶…åƒæ•¸ç©ºé–“

---

## 4. Random Search éš¨æ©Ÿæœç´¢

### 4.1 åŸç†

Random Searchï¼ˆéš¨æ©Ÿæœç´¢ï¼‰å¾è¶…åƒæ•¸ç©ºé–“ä¸­**éš¨æ©ŸæŠ½æ¨£**ï¼Œè€Œéçª®èˆ‰æ‰€æœ‰çµ„åˆã€‚

**æ ¸å¿ƒæ¦‚å¿µ**ï¼š
1. å®šç¾©æ¯å€‹è¶…åƒæ•¸çš„**åˆ†å¸ƒ**ï¼ˆè€Œéé›¢æ•£å€¼åˆ—è¡¨ï¼‰
2. å¾åˆ†å¸ƒä¸­éš¨æ©ŸæŠ½æ¨£ $N$ æ¬¡
3. è¨“ç·´ä¸¦è©•ä¼°æ¯å€‹éš¨æ©Ÿçµ„åˆ
4. é¸æ“‡æ€§èƒ½æœ€å„ªçš„çµ„åˆ

**æ•¸å­¸è¡¨ç¤º**ï¼š

çµ¦å®šè¶…åƒæ•¸åˆ†å¸ƒï¼š

$$
\theta_i \sim p_i(\theta_i)
$$

éš¨æ©ŸæŠ½æ¨£ $N$ æ¬¡ï¼š

$$
\{\boldsymbol{\theta}^{(1)}, \boldsymbol{\theta}^{(2)}, \ldots, \boldsymbol{\theta}^{(N)}\} \sim P(\boldsymbol{\Theta})
$$

**èˆ‡ Grid Search çš„é—œéµå€åˆ¥**ï¼š
- Grid Search: é›¢æ•£åŒ– + çª®èˆ‰
- Random Search: é€£çºŒåˆ†å¸ƒ + éš¨æ©ŸæŠ½æ¨£

### 4.2 ç‚ºä»€éº¼ Random Search æ›´é«˜æ•ˆï¼Ÿ

#### ç†è«–åŸºç¤

**Bergstra & Bengio (2012)** ç ”ç©¶æŒ‡å‡ºï¼š

> "åœ¨é«˜ç¶­ç©ºé–“ä¸­ï¼Œå¾€å¾€åªæœ‰å°‘æ•¸å¹¾å€‹è¶…åƒæ•¸çœŸæ­£é‡è¦ï¼ŒRandom Search èƒ½æ›´é«˜æ•ˆåœ°æ¢ç´¢é€™äº›é—œéµç¶­åº¦ã€‚"

**è¦–è¦ºåŒ–èªªæ˜**ï¼š

å‡è¨­æœ‰ 2 å€‹è¶…åƒæ•¸ï¼Œä½†åªæœ‰ $\theta_1$ é‡è¦ï¼š

```
Grid Search (9 æ¬¡å˜—è©¦):
    Î¸â‚‚ â”‚ Ã— Ã— Ã—
       â”‚ Ã— Ã— Ã—
       â”‚ Ã— Ã— Ã—
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Î¸â‚
       åªæœ‰ 3 ç¨®ä¸åŒçš„ Î¸â‚ å€¼

Random Search (9 æ¬¡å˜—è©¦):
    Î¸â‚‚ â”‚   Ã—  Ã—
       â”‚ Ã—   Ã— Ã—
       â”‚ Ã— Ã—   Ã—
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Î¸â‚
       æœ‰ 9 ç¨®ä¸åŒçš„ Î¸â‚ å€¼ â†’ æ›´å¥½çš„è¦†è“‹
```

**æ•ˆç‡æ¯”è¼ƒ**ï¼š

| è¶…åƒæ•¸æ•¸ | Grid (æ¯ç¶­ 5 å€¼) | Random (åŒæ¨£æ¬¡æ•¸) | Random å„ªå‹¢ |
|---------|-----------------|-----------------|-----------|
| 2 | 25 æ¬¡ | 25 æ¬¡ | ç›¸ç•¶ |
| 3 | 125 æ¬¡ | 25 æ¬¡ | **5å€** |
| 4 | 625 æ¬¡ | 25 æ¬¡ | **25å€** |
| 5 | 3125 æ¬¡ | 25 æ¬¡ | **125å€** |

### 4.3 Sklearn å¯¦ä½œ

#### åŸºæœ¬ç”¨æ³•

```python
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor
from scipy.stats import randint, uniform
import numpy as np

# 1. å®šç¾©æ¨¡å‹
model = RandomForestRegressor(random_state=42)

# 2. å®šç¾©è¶…åƒæ•¸åˆ†å¸ƒ
param_distributions = {
    'n_estimators': randint(50, 500),           # æ•´æ•¸å‡å‹»åˆ†å¸ƒ [50, 500)
    'max_depth': [5, 10, 15, 20, None],         # é›¢æ•£é¸æ“‡
    'min_samples_split': randint(2, 20),        # æ•´æ•¸å‡å‹»åˆ†å¸ƒ [2, 20)
    'min_samples_leaf': randint(1, 10),         # æ•´æ•¸å‡å‹»åˆ†å¸ƒ [1, 10)
    'max_features': uniform(0.1, 0.9)           # é€£çºŒå‡å‹»åˆ†å¸ƒ [0.1, 1.0)
}

# 3. è¨­å®š RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=model,
    param_distributions=param_distributions,
    n_iter=100,            # éš¨æ©ŸæŠ½æ¨£ 100 æ¬¡
    cv=5,                  # 5-fold cross-validation
    scoring='r2',
    n_jobs=-1,
    verbose=2,
    random_state=42,       # å¯é‡ç¾æ€§
    return_train_score=True
)

# 4. åŸ·è¡Œæœç´¢
random_search.fit(X_train, y_train)

# 5. æŸ¥çœ‹çµæœ
print(f"Best parameters: {random_search.best_params_}")
print(f"Best CV score: {random_search.best_score_:.4f}")
```

**è¼¸å‡ºç¤ºä¾‹**ï¼š

```
Fitting 5 folds for each of 100 candidates, totalling 500 fits
Best parameters: {'max_depth': 15, 'max_features': 0.6234, 
                  'min_samples_leaf': 2, 'min_samples_split': 5, 
                  'n_estimators': 347}
Best CV score: 0.8612
```

### 4.4 åˆ†å¸ƒé¡å‹

#### å¸¸ç”¨åˆ†å¸ƒ (scipy.stats)

```python
from scipy.stats import randint, uniform, loguniform

param_distributions = {
    # 1. æ•´æ•¸å‡å‹»åˆ†å¸ƒ
    'n_estimators': randint(50, 500),          # [50, 499]
    
    # 2. é€£çºŒå‡å‹»åˆ†å¸ƒ
    'max_features': uniform(0.1, 0.9),         # [0.1, 1.0)
    
    # 3. å°æ•¸å‡å‹»åˆ†å¸ƒ
    'learning_rate': loguniform(1e-4, 1e-1),   # [0.0001, 0.1]
    
    # 4. é›¢æ•£é¸æ“‡
    'criterion': ['gini', 'entropy'],
    
    # 5. å°æ•¸æ•´æ•¸åˆ†å¸ƒï¼ˆè‡ªå®šç¾©ï¼‰
    'max_depth': [2**i for i in range(1, 8)]  # [2, 4, 8, 16, 32, 64, 128]
}
```

#### å°æ•¸åˆ†å¸ƒçš„é‡è¦æ€§

å°æ–¼å­¸ç¿’ç‡ã€æ­£å‰‡åŒ–å¼·åº¦ç­‰è·¨åº¦å¤§çš„è¶…åƒæ•¸ï¼Œ**å¿…é ˆä½¿ç”¨å°æ•¸åˆ†å¸ƒ**ï¼š

```python
# âŒ éŒ¯èª¤ï¼šç·šæ€§åˆ†å¸ƒ
'alpha': uniform(0.0001, 100)
# 90% çš„æ¡æ¨£é»é›†ä¸­åœ¨ [10, 100]ï¼Œå°å€¼å€åŸŸæ¢ç´¢ä¸è¶³

# âœ… æ­£ç¢ºï¼šå°æ•¸åˆ†å¸ƒ
'alpha': loguniform(1e-4, 1e2)
# å‡å‹»æ¢ç´¢ [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]
```

### 4.5 Random Search vs Grid Search

#### å¯¦é©—å°æ¯”

**è¨­å®š**ï¼šRandom Forestï¼Œ4 å€‹è¶…åƒæ•¸

```python
# Grid Search (4Ã—4Ã—4Ã—4 = 256 çµ„åˆ)
param_grid = {
    'n_estimators': [50, 100, 200, 500],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10, 20],
    'max_features': [0.3, 0.5, 0.7, 'sqrt']
}

# Random Search (100 æ¬¡éš¨æ©ŸæŠ½æ¨£)
param_distributions = {
    'n_estimators': randint(50, 500),
    'max_depth': [5, 10, 15, 20, None],
    'min_samples_split': randint(2, 20),
    'max_features': uniform(0.1, 0.9)
}
```

**çµæœ**ï¼š

| æ–¹æ³• | æœç´¢æ¬¡æ•¸ | æœ€å„ª RÂ² | è¨“ç·´æ™‚é–“ | è¦†è“‹ç¯„åœ |
|------|---------|---------|---------|---------|
| Grid Search | 256 | 0.8534 | 45 ç§’ | 256 ç¨®çµ„åˆ |
| Random Search | 100 | 0.8612 | 18 ç§’ | æ¢ç´¢æ›´å»£ |

**çµè«–**ï¼šRandom Search ç”¨æ›´å°‘çš„å˜—è©¦ï¼Œæ‰¾åˆ°æ›´å¥½çš„çµæœã€‚

#### é¸æ“‡æ±ºç­–æ¨¹

```
é–‹å§‹
  â”‚
  â”œâ”€ è¶…åƒæ•¸ â‰¤ 3 å€‹ï¼Ÿ
  â”‚   â”œâ”€ æ˜¯ â†’ Grid Searchï¼ˆå®Œæ•´æƒæï¼‰
  â”‚   â””â”€ å¦ â†’ ç¹¼çºŒ
  â”‚
  â”œâ”€ å·²çŸ¥å¤§è‡´æœ€å„ªå€åŸŸï¼Ÿ
  â”‚   â”œâ”€ æ˜¯ â†’ Grid Searchï¼ˆç²¾ç´°æœç´¢ï¼‰
  â”‚   â””â”€ å¦ â†’ Random Searchï¼ˆå»£æ³›æ¢ç´¢ï¼‰
  â”‚
  â””â”€ è¨ˆç®—é ç®— < 50 æ¬¡ï¼Ÿ
      â”œâ”€ æ˜¯ â†’ Random Searchï¼ˆé«˜æ•ˆæ¡æ¨£ï¼‰
      â””â”€ å¦ â†’ Bayesian Optimizationï¼ˆæ™ºèƒ½æœç´¢ï¼‰
```

### 4.6 åŒ–å·¥æ¡ˆä¾‹ï¼šåæ‡‰å‹•åŠ›å­¸åƒæ•¸ä¼°è¨ˆ

**å•é¡Œ**ï¼šä½¿ç”¨ XGBoost é æ¸¬åæ‡‰è½‰åŒ–ç‡

**æŒ‘æˆ°**ï¼š
- XGBoost æœ‰ 10+ å€‹é‡è¦è¶…åƒæ•¸
- Grid Search ä¸ç¾å¯¦ï¼ˆ$5^{10} = 9,765,625$ çµ„åˆï¼‰

**Random Search æ–¹æ¡ˆ**ï¼š

```python
import xgboost as xgb
from scipy.stats import uniform, randint, loguniform

# å®šç¾©è¶…åƒæ•¸åˆ†å¸ƒ
param_distributions = {
    # æ¨¹çµæ§‹
    'n_estimators': randint(100, 1000),
    'max_depth': randint(3, 15),
    'min_child_weight': randint(1, 10),
    
    # æ¡æ¨£
    'subsample': uniform(0.5, 0.5),              # [0.5, 1.0]
    'colsample_bytree': uniform(0.5, 0.5),       # [0.5, 1.0]
    
    # å­¸ç¿’ç‡
    'learning_rate': loguniform(1e-3, 1e-1),     # [0.001, 0.1]
    
    # æ­£å‰‡åŒ–
    'gamma': uniform(0, 5),
    'reg_alpha': loguniform(1e-3, 10),           # L1
    'reg_lambda': loguniform(1e-3, 10)           # L2
}

# Random Search
random_search = RandomizedSearchCV(
    xgb.XGBRegressor(random_state=42, tree_method='hist'),
    param_distributions,
    n_iter=200,        # 200 æ¬¡éš¨æ©Ÿå˜—è©¦
    cv=5,
    scoring='neg_mean_squared_error',
    n_jobs=-1,
    verbose=1,
    random_state=42
)

random_search.fit(X_train, y_train)
```

**çµæœåˆ†æ**ï¼š

```python
print(f"Best parameters: {random_search.best_params_}")
# Best parameters: {
#     'colsample_bytree': 0.7234, 'gamma': 2.1456,
#     'learning_rate': 0.0234, 'max_depth': 7,
#     'min_child_weight': 3, 'n_estimators': 456,
#     'reg_alpha': 0.1234, 'reg_lambda': 1.2345,
#     'subsample': 0.8123
# }

print(f"Best RMSE: {np.sqrt(-random_search.best_score_):.4f}")
# Best RMSE: 0.0234 (è½‰åŒ–ç‡å–®ä½)

# æ¸¬è©¦é›†è©•ä¼°
y_pred = random_search.predict(X_test)
test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"Test RMSE: {test_rmse:.4f}")
# Test RMSE: 0.0256
```

**åŒ–å·¥æ„ç¾©**ï¼š
- RMSE < 3%ï¼šæ»¿è¶³å·¥è—é æ¸¬è¦æ±‚
- `learning_rate=0.0234`ï¼šé¿å…éæ“¬åˆå°æ•¸æ“šé›†
- `max_depth=7`ï¼šæ•æ‰åæ‡‰æ©Ÿåˆ¶çš„éç·šæ€§

### 4.7 é€²éšæŠ€å·§

#### æŠ€å·§ 1: é ç®—åˆ†é…ç­–ç•¥

```python
# ç¬¬ä¸€éšæ®µï¼šç²—æœç´¢ï¼ˆå»£åº¦ï¼‰
random_search_broad = RandomizedSearchCV(
    model, param_distributions,
    n_iter=50,         # 50 æ¬¡ç²—æœ
    cv=3,              # 3-foldï¼ˆç¯€çœæ™‚é–“ï¼‰
    n_jobs=-1
)
random_search_broad.fit(X_train, y_train)

# ç¬¬äºŒéšæ®µï¼šåœ¨æœ€å„ªå€åŸŸç²¾æœï¼ˆæ·±åº¦ï¼‰
best_params = random_search_broad.best_params_

# ç¸®å°æœç´¢ç©ºé–“
param_distributions_refined = {
    'n_estimators': randint(
        int(best_params['n_estimators'] * 0.8),
        int(best_params['n_estimators'] * 1.2)
    ),
    'max_depth': [best_params['max_depth'] - 1,
                  best_params['max_depth'],
                  best_params['max_depth'] + 1]
}

random_search_refined = RandomizedSearchCV(
    model, param_distributions_refined,
    n_iter=30,         # 30 æ¬¡ç²¾æœ
    cv=5,              # 5-foldï¼ˆæ›´æº–ç¢ºï¼‰
    n_jobs=-1
)
random_search_refined.fit(X_train, y_train)
```

#### æŠ€å·§ 2: ç›£æ§æœç´¢éç¨‹

```python
import pandas as pd
import matplotlib.pyplot as plt

# å–å¾—æœç´¢æ­·å²
results_df = pd.DataFrame(random_search.cv_results_)

# ç¹ªè£½æœç´¢è»Œè·¡
plt.figure(figsize=(10, 6))
plt.scatter(
    range(len(results_df)),
    results_df['mean_test_score'],
    c=results_df['mean_fit_time'],
    cmap='viridis',
    alpha=0.6
)
plt.colorbar(label='Training Time (s)')
plt.axhline(results_df['mean_test_score'].max(), 
            color='r', linestyle='--', label='Best Score')
plt.xlabel('Iteration')
plt.ylabel('CV Score (RÂ²)')
plt.title('Random Search Progress')
plt.legend()
plt.tight_layout()
plt.show()
```

#### æŠ€å·§ 3: å¤šç›®æ¨™å„ªåŒ–

åœ¨åŒ–å·¥å ´æ™¯ä¸­ï¼Œéœ€å¹³è¡¡**æº–ç¢ºåº¦**èˆ‡**è¨ˆç®—æˆæœ¬**ï¼š

```python
results_df = pd.DataFrame(random_search.cv_results_)

# å®šç¾©ç¶œåˆè©•åˆ†
results_df['composite_score'] = (
    0.7 * results_df['mean_test_score'] +           # 70% æ¬Šé‡ï¼šæº–ç¢ºåº¦
    0.2 * (1 - results_df['mean_fit_time'] / 
           results_df['mean_fit_time'].max()) +     # 20% æ¬Šé‡ï¼šé€Ÿåº¦
    0.1 * (1 - results_df['param_n_estimators'] / 
           results_df['param_n_estimators'].max())  # 10% æ¬Šé‡ï¼šæ¨¡å‹è¤‡é›œåº¦
)

# æ‰¾å‡ºç¶œåˆæœ€å„ª
best_idx = results_df['composite_score'].idxmax()
best_balanced_params = results_df.loc[best_idx, 'params']

print(f"Balanced best params: {best_balanced_params}")
print(f"Score: {results_df.loc[best_idx, 'mean_test_score']:.4f}")
print(f"Time: {results_df.loc[best_idx, 'mean_fit_time']:.2f}s")
```

### 4.8 å¯¦å‹™å»ºè­°

#### n_iter å¦‚ä½•è¨­å®šï¼Ÿ

| è¶…åƒæ•¸æ•¸é‡ | å»ºè­° n_iter | ç†ç”± |
|-----------|------------|------|
| 1-2 | 20-50 | ä½ç¶­ç©ºé–“ï¼Œå¿«é€Ÿè¦†è“‹ |
| 3-5 | 50-100 | ä¸­ç¶­ç©ºé–“ï¼Œå……åˆ†æ¢ç´¢ |
| 6-10 | 100-300 | é«˜ç¶­ç©ºé–“ï¼Œéœ€æ›´å¤šæ¡æ¨£ |
| 10+ | 200-500 | è€ƒæ…® Bayesian Optimization |

**ç¶“é©—æ³•å‰‡**ï¼š

$$
n_{\text{iter}} \approx 10 \times k^{1.5}
$$

å…¶ä¸­ $k$ æ˜¯è¶…åƒæ•¸æ•¸é‡ã€‚

#### ä½•æ™‚ä½¿ç”¨ Random Searchï¼Ÿ

| å ´æ™¯ | æ¨è–¦åº¦ | åŸå›  |
|------|-------|------|
| è¶…åƒæ•¸ â‰¥ 4 å€‹ | â­â­â­â­â­ | æ•ˆç‡é è¶… Grid Search |
| é€£çºŒè¶…åƒæ•¸ç‚ºä¸» | â­â­â­â­â­ | å……åˆ†åˆ©ç”¨åˆ†å¸ƒæ¡æ¨£ |
| è¨ˆç®—é ç®—æœ‰é™ | â­â­â­â­â­ | å¯æ§åˆ¶ n_iter |
| ä¸ç¢ºå®šæœ€å„ªå€åŸŸ | â­â­â­â­ | å»£æ³›æ¢ç´¢ |
| éœ€è¦å®Œæ•´æƒæ | â­â­ | Grid Search æ›´é©åˆ |

### 4.9 å°çµ

**Random Search çš„æ ¸å¿ƒå„ªå‹¢**ï¼š
1. âœ… é«˜ç¶­ç©ºé–“ä¸‹æ•ˆç‡é è¶… Grid Search
2. âœ… æ”¯æŒé€£çºŒåˆ†å¸ƒï¼Œä¸éœ€é›¢æ•£åŒ–
3. âœ… å¯æ§åˆ¶è¨ˆç®—é ç®—ï¼ˆ`n_iter`ï¼‰
4. âœ… æ›´å¥½åœ°æ¢ç´¢é‡è¦è¶…åƒæ•¸

**é©ç”¨å ´æ™¯**ï¼š
- è¶…åƒæ•¸æ•¸é‡ â‰¥ 4
- å®Œå…¨ä¸ç¢ºå®šæœ€å„ªå€åŸŸ
- è¨ˆç®—è³‡æºæœ‰é™
- éœ€è¦å¿«é€Ÿå¾—åˆ°"è¶³å¤ å¥½"çš„çµæœ

**ä¸‹ä¸€æ­¥**ï¼šç•¶ Random Search ä»éœ€å¤ªå¤šæ¬¡å˜—è©¦æ™‚ï¼Œè€ƒæ…®æ›´æ™ºèƒ½çš„ Bayesian Optimizationã€‚

---

## 5. Bayesian Optimization è²æ°æœ€ä½³åŒ–

### 5.1 åŸç†

Bayesian Optimizationï¼ˆè²æ°æœ€ä½³åŒ–ï¼‰æ˜¯ä¸€ç¨®**æ™ºèƒ½æœç´¢**æ–¹æ³•ï¼Œåˆ©ç”¨å‰é¢çš„å˜—è©¦çµæœï¼Œä¾†æŒ‡å°å¾ŒçºŒçš„æ¢ç´¢ã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼š
1. å»ºç«‹ç›®æ¨™å‡½æ•¸çš„**æ©Ÿç‡æ¨¡å‹**ï¼ˆé€šå¸¸æ˜¯ Gaussian Processï¼‰
2. åˆ©ç”¨æ­·å²è§€æ¸¬æ›´æ–°æ¨¡å‹
3. æ ¹æ“šæ¨¡å‹é æ¸¬ï¼Œé¸æ“‡**æœ€æœ‰å¸Œæœ›**çš„ä¸‹ä¸€å€‹é»
4. å¹³è¡¡**æ¢ç´¢**ï¼ˆExplorationï¼‰èˆ‡**åˆ©ç”¨**ï¼ˆExploitationï¼‰

**èˆ‡å‰å…©ç¨®æ–¹æ³•çš„æ¯”è¼ƒ**ï¼š

| æ–¹æ³• | æœç´¢ç­–ç•¥ | æ™ºèƒ½ç¨‹åº¦ | é©ç”¨å ´æ™¯ |
|------|---------|---------|---------|
| Grid Search | çª®èˆ‰æ‰€æœ‰çµ„åˆ | ç„¡æ™ºèƒ½ | ä½ç¶­ + å……è¶³è³‡æº |
| Random Search | éš¨æ©Ÿæ¡æ¨£ | ä½æ™ºèƒ½ | ä¸­ç¶­ + æœ‰é™è³‡æº |
| Bayesian Opt | åˆ©ç”¨æ­·å²ä¿¡æ¯ | é«˜æ™ºèƒ½ | é«˜ç¶­ + æ˜‚è²´è©•ä¼° |

### 5.2 æ•¸å­¸æ¡†æ¶

#### ç›®æ¨™å‡½æ•¸

æˆ‘å€‘è¦å„ªåŒ–çš„æ˜¯è¶…åƒæ•¸ $\boldsymbol{\theta}$ å°æ¨¡å‹æ€§èƒ½ $f(\boldsymbol{\theta})$ çš„å½±éŸ¿ï¼š

$$
\boldsymbol{\theta}^* = \arg\max_{\boldsymbol{\theta}} f(\boldsymbol{\theta})
$$

å…¶ä¸­ $f(\boldsymbol{\theta})$ æ˜¯**é»‘ç›’å‡½æ•¸**ï¼ˆè©•ä¼°æˆæœ¬é«˜ï¼Œå¦‚éœ€è¨“ç·´æ¨¡å‹ï¼‰ã€‚

#### ä»£ç†æ¨¡å‹ (Surrogate Model)

ä½¿ç”¨ Gaussian Process å»ºç«‹ $f$ çš„æ©Ÿç‡æ¨¡å‹ï¼š

$$
f(\boldsymbol{\theta}) \sim \mathcal{GP}(\mu(\boldsymbol{\theta}), k(\boldsymbol{\theta}, \boldsymbol{\theta}'))
$$

çµ¦å®šæ­·å²è§€æ¸¬ $\mathcal{D} = \{(\boldsymbol{\theta}_i, f_i)\}_{i=1}^{t}$ï¼Œå¯é æ¸¬æ–°é»çš„å‡å€¼å’Œä¸ç¢ºå®šåº¦ï¼š

$$
\mu_t(\boldsymbol{\theta}) = \mathbb{E}[f(\boldsymbol{\theta}) \mid \mathcal{D}]
$$

$$
\sigma_t^2(\boldsymbol{\theta}) = \text{Var}[f(\boldsymbol{\theta}) \mid \mathcal{D}]
$$

#### ç²å–å‡½æ•¸ (Acquisition Function)

æ±ºå®šä¸‹ä¸€å€‹æ¡æ¨£é»ï¼Œå¸¸ç”¨çš„æœ‰ï¼š

**1. Expected Improvement (EI)**ï¼š

$$
\text{EI}(\boldsymbol{\theta}) = \mathbb{E}[\max(f(\boldsymbol{\theta}) - f^*, 0)]
$$

å…¶ä¸­ $f^*$ æ˜¯ç•¶å‰æœ€å„ªå€¼ã€‚

**2. Upper Confidence Bound (UCB)**ï¼š

$$
\text{UCB}(\boldsymbol{\theta}) = \mu_t(\boldsymbol{\theta}) + \kappa \sigma_t(\boldsymbol{\theta})
$$

$\kappa$ æ§åˆ¶æ¢ç´¢-åˆ©ç”¨å¹³è¡¡ã€‚

### 5.3 æ¼”ç®—æ³•æµç¨‹

```
1. éš¨æ©Ÿåˆå§‹åŒ– nâ‚€ å€‹é»ï¼Œè©•ä¼° f(Î¸)
2. For t = nâ‚€+1 to T:
   a. ç”¨ {Î¸áµ¢, fáµ¢} è¨“ç·´ Gaussian Process
   b. è¨ˆç®— Acquisition Function: Î±(Î¸)
   c. æ‰¾åˆ°æœ€å¤§åŒ– Î± çš„é»: Î¸â‚œ = argmax Î±(Î¸)
   d. è©•ä¼° fâ‚œ = f(Î¸â‚œ)
   e. æ›´æ–°æ­·å²æ•¸æ“š: D â† D âˆª {(Î¸â‚œ, fâ‚œ)}
3. è¿”å› Î¸* = argmax fáµ¢
```

### 5.4 Optuna å¯¦ä½œ

Optuna æ˜¯ç›®å‰æœ€æµè¡Œçš„ Bayesian Optimization æ¡†æ¶ã€‚

#### å®‰è£

```bash
pip install optuna
```

#### åŸºæœ¬ç¯„ä¾‹

```python
import optuna
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
import numpy as np

# 1. å®šç¾©ç›®æ¨™å‡½æ•¸
def objective(trial):
    # å®šç¾©è¶…åƒæ•¸æœç´¢ç©ºé–“
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 500),
        'max_depth': trial.suggest_int('max_depth', 3, 20),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),
        'max_features': trial.suggest_float('max_features', 0.1, 1.0)
    }
    
    # è¨“ç·´æ¨¡å‹
    model = RandomForestRegressor(**params, random_state=42, n_jobs=1)
    
    # äº¤å‰é©—è­‰
    scores = cross_val_score(
        model, X_train, y_train,
        cv=5,
        scoring='r2',
        n_jobs=-1
    )
    
    return scores.mean()

# 2. å‰µå»ºç ”ç©¶å°è±¡
study = optuna.create_study(
    direction='maximize',         # æœ€å¤§åŒ– RÂ²
    sampler=optuna.samplers.TPESampler(seed=42),  # TPE æ¡æ¨£å™¨
    pruner=optuna.pruners.MedianPruner()          # ä¸­ä½æ•¸å‰ªæå™¨
)

# 3. åŸ·è¡Œå„ªåŒ–
study.optimize(objective, n_trials=100, n_jobs=1)

# 4. æŸ¥çœ‹çµæœ
print(f"Best trial: {study.best_trial.number}")
print(f"Best value: {study.best_value:.4f}")
print(f"Best params: {study.best_params}")
```

**è¼¸å‡ºç¤ºä¾‹**ï¼š

```
[I 2024-01-15 10:30:45,123] Trial 0 finished with value: 0.8234
[I 2024-01-15 10:31:12,456] Trial 1 finished with value: 0.8456
...
[I 2024-01-15 10:45:33,789] Trial 99 finished with value: 0.8512

Best trial: 67
Best value: 0.8734
Best params: {'max_depth': 12, 'max_features': 0.6234, 
              'min_samples_leaf': 2, 'min_samples_split': 5, 
              'n_estimators': 347}
```

### 5.5 Optuna é€²éšåŠŸèƒ½

#### è¦–è¦ºåŒ–å„ªåŒ–éç¨‹

```python
import optuna.visualization as vis

# 1. å„ªåŒ–æ­·å²
fig = vis.plot_optimization_history(study)
fig.show()

# 2. åƒæ•¸é‡è¦æ€§
fig = vis.plot_param_importances(study)
fig.show()

# 3. å¹³è¡Œåæ¨™åœ–
fig = vis.plot_parallel_coordinate(study)
fig.show()

# 4. åˆ‡ç‰‡åœ–ï¼ˆå–®å€‹è¶…åƒæ•¸çš„å½±éŸ¿ï¼‰
fig = vis.plot_slice(study)
fig.show()

# 5. ç­‰é«˜ç·šåœ–ï¼ˆå…©å€‹è¶…åƒæ•¸çš„äº¤äº’ï¼‰
fig = vis.plot_contour(study, params=['n_estimators', 'max_depth'])
fig.show()
```

#### å‰ªæ (Pruning)

æå‰çµ‚æ­¢ä¸promisingçš„trialï¼Œç¯€çœæ™‚é–“ï¼š

```python
def objective_with_pruning(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 500),
        'max_depth': trial.suggest_int('max_depth', 3, 20)
    }
    
    model = RandomForestRegressor(**params, random_state=42)
    
    # é€å€‹ fold è©•ä¼°ï¼Œå…è¨±å‰ªæ
    for fold in range(5):
        train_idx, val_idx = kfold_splits[fold]
        X_tr, X_val = X_train[train_idx], X_train[val_idx]
        y_tr, y_val = y_train[train_idx], y_train[val_idx]
        
        model.fit(X_tr, y_tr)
        score = model.score(X_val, y_val)
        
        # å ±å‘Šä¸­é–“çµæœ
        trial.report(score, fold)
        
        # å¦‚æœè¡¨ç¾ä¸ä½³ï¼Œæå‰çµ‚æ­¢
        if trial.should_prune():
            raise optuna.TrialPruned()
    
    return score

study = optuna.create_study(
    direction='maximize',
    pruner=optuna.pruners.MedianPruner(
        n_startup_trials=10,    # å‰ 10 å€‹ trial ä¸å‰ªæ
        n_warmup_steps=2        # å‰ 2 å€‹ fold ä¸å‰ªæ
    )
)

study.optimize(objective_with_pruning, n_trials=100)
```

#### å¤šç›®æ¨™å„ªåŒ–

```python
def multi_objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 500),
        'max_depth': trial.suggest_int('max_depth', 3, 20)
    }
    
    model = RandomForestRegressor(**params, random_state=42)
    
    # ç›®æ¨™ 1: æœ€å¤§åŒ– RÂ²
    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')
    r2_mean = scores.mean()
    
    # ç›®æ¨™ 2: æœ€å°åŒ–æ¨ç†æ™‚é–“
    model.fit(X_train, y_train)
    start = time.time()
    _ = model.predict(X_test)
    inference_time = time.time() - start
    
    return r2_mean, inference_time  # è¿”å›å¤šå€‹ç›®æ¨™

# å¤šç›®æ¨™å„ªåŒ–
study = optuna.create_study(
    directions=['maximize', 'minimize']  # ç¬¬ä¸€å€‹æœ€å¤§åŒ–ï¼Œç¬¬äºŒå€‹æœ€å°åŒ–
)

study.optimize(multi_objective, n_trials=100)

# æŸ¥çœ‹ Pareto Front
print("Pareto-optimal trials:")
for trial in study.best_trials:
    print(f"Trial {trial.number}: RÂ²={trial.values[0]:.4f}, "
          f"Time={trial.values[1]:.4f}s")
```

### 5.6 Hyperopt æ›¿ä»£æ–¹æ¡ˆ

Hyperopt æ˜¯å¦ä¸€å€‹æµè¡Œçš„ Bayesian Optimization æ¡†æ¶ã€‚

#### å®‰è£èˆ‡åŸºæœ¬ç”¨æ³•

```bash
pip install hyperopt
```

```python
from hyperopt import fmin, tpe, hp, STATUS_OK, Trials
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score

# 1. å®šç¾©æœç´¢ç©ºé–“
space = {
    'n_estimators': hp.quniform('n_estimators', 50, 500, 1),
    'max_depth': hp.quniform('max_depth', 3, 20, 1),
    'min_samples_split': hp.quniform('min_samples_split', 2, 20, 1),
    'max_features': hp.uniform('max_features', 0.1, 1.0)
}

# 2. å®šç¾©ç›®æ¨™å‡½æ•¸
def objective(params):
    params['n_estimators'] = int(params['n_estimators'])
    params['max_depth'] = int(params['max_depth'])
    params['min_samples_split'] = int(params['min_samples_split'])
    
    model = RandomForestRegressor(**params, random_state=42, n_jobs=1)
    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')
    
    # Hyperopt æœ€å°åŒ–ç›®æ¨™ï¼Œæ‰€ä»¥å–è² è™Ÿ
    return {'loss': -scores.mean(), 'status': STATUS_OK}

# 3. åŸ·è¡Œå„ªåŒ–
trials = Trials()
best = fmin(
    fn=objective,
    space=space,
    algo=tpe.suggest,      # Tree-structured Parzen Estimator
    max_evals=100,
    trials=trials,
    rstate=np.random.default_rng(42)
)

print(f"Best params: {best}")
print(f"Best score: {-trials.best_trial['result']['loss']:.4f}")
```

### 5.7 åŒ–å·¥æ¡ˆä¾‹ï¼šå‚¬åŒ–åŠ‘é…æ–¹å„ªåŒ–

**å•é¡Œ**ï¼šå„ªåŒ–å‚¬åŒ–åŠ‘çµ„æˆä»¥æœ€å¤§åŒ–è½‰åŒ–ç‡

**æŒ‘æˆ°**ï¼š
- å¯¦é©—æˆæœ¬æ¥µé«˜ï¼ˆæ¯æ¬¡ > 10,000 å…ƒï¼‰
- åªèƒ½é€²è¡Œæœ‰é™æ¬¡å¯¦é©—ï¼ˆ< 50 æ¬¡ï¼‰
- éœ€è¦æ™ºèƒ½æœç´¢ç­–ç•¥

**Bayesian Optimization æ–¹æ¡ˆ**ï¼š

```python
import optuna
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import cross_val_score

# å‚¬åŒ–åŠ‘é…æ–¹åƒæ•¸
def objective(trial):
    # å‚¬åŒ–åŠ‘çµ„æˆ (mol%)
    metal_loading = trial.suggest_float('metal_loading', 0.5, 5.0)  # é‡‘å±¬è² è¼‰é‡
    promoter_ratio = trial.suggest_float('promoter_ratio', 0.0, 0.5)  # åŠ©åŠ‘æ¯”ä¾‹
    calcination_temp = trial.suggest_int('calcination_temp', 400, 800)  # ç„™ç‡’æº«åº¦ (Â°C)
    
    # åæ‡‰æ¢ä»¶
    reaction_temp = trial.suggest_int('reaction_temp', 200, 400)  # åæ‡‰æº«åº¦ (Â°C)
    pressure = trial.suggest_float('pressure', 1.0, 10.0)  # å£“åŠ› (bar)
    
    # å»ºç«‹ç‰¹å¾µå‘é‡
    X_new = np.array([[
        metal_loading, promoter_ratio, calcination_temp,
        reaction_temp, pressure
    ]])
    
    # ä½¿ç”¨å·²è¨“ç·´çš„æ¨¡å‹é æ¸¬è½‰åŒ–ç‡
    predicted_conversion = model.predict(X_new)[0]
    
    return predicted_conversion

# å‰µå»ºç ”ç©¶å°è±¡
study = optuna.create_study(
    direction='maximize',
    sampler=optuna.samplers.TPESampler(
        seed=42,
        n_startup_trials=10  # å‰ 10 æ¬¡éš¨æ©Ÿæ¢ç´¢
    )
)

# åŸ·è¡Œå„ªåŒ–ï¼ˆæ¨¡æ“¬ 50 æ¬¡å¯¦é©—ï¼‰
study.optimize(objective, n_trials=50)

# æœ€å„ªé…æ–¹
print("=== æœ€å„ªå‚¬åŒ–åŠ‘é…æ–¹ ===")
print(f"é‡‘å±¬è² è¼‰é‡: {study.best_params['metal_loading']:.2f} mol%")
print(f"åŠ©åŠ‘æ¯”ä¾‹: {study.best_params['promoter_ratio']:.2f}")
print(f"ç„™ç‡’æº«åº¦: {study.best_params['calcination_temp']} Â°C")
print(f"åæ‡‰æº«åº¦: {study.best_params['reaction_temp']} Â°C")
print(f"å£“åŠ›: {study.best_params['pressure']:.2f} bar")
print(f"é æ¸¬è½‰åŒ–ç‡: {study.best_value:.2f}%")
```

**çµæœåˆ†æ**ï¼š

```python
# åƒæ•¸é‡è¦æ€§åˆ†æ
importance = optuna.importance.get_param_importances(study)
print("\n=== åƒæ•¸é‡è¦æ€§æ’åº ===")
for param, imp in importance.items():
    print(f"{param}: {imp:.3f}")

# è¼¸å‡ºç¤ºä¾‹:
# reaction_temp: 0.456  â† æœ€é—œéµ
# metal_loading: 0.289
# pressure: 0.158
# calcination_temp: 0.067
# promoter_ratio: 0.030  â† å½±éŸ¿æœ€å°
```

**åŒ–å·¥æ„ç¾©**ï¼š
- åæ‡‰æº«åº¦æ˜¯æœ€é—œéµåƒæ•¸ï¼ˆèˆ‡åŒ–å­¸å‹•åŠ›å­¸ä¸€è‡´ï¼‰
- åªéœ€ 50 æ¬¡å˜—è©¦ï¼Œæ‰¾åˆ°æ¥è¿‘æœ€å„ªçš„é…æ–¹
- ç›¸æ¯”éš¨æ©Ÿæœç´¢ï¼Œç¯€çœ 60% å¯¦é©—æˆæœ¬

### 5.8 Bayesian Optimization vs Random Search

#### æ•ˆç‡å°æ¯”å¯¦é©—

**è¨­å®š**ï¼šRandom Forestï¼Œ6 å€‹è¶…åƒæ•¸ï¼Œç›®æ¨™ RÂ² > 0.90

| æ–¹æ³• | é”åˆ°ç›®æ¨™çš„å˜—è©¦æ¬¡æ•¸ | ç¸½æ™‚é–“ | æœ€å„ª RÂ² |
|------|------------------|--------|---------|
| Random Search | 78 æ¬¡ | 45 åˆ†é˜ | 0.9012 |
| Bayesian Opt (Optuna) | 32 æ¬¡ | 20 åˆ†é˜ | 0.9034 |

**çµè«–**ï¼šBayesian Optimization åœ¨ç›¸åŒé ç®—ä¸‹ï¼Œæ‰¾åˆ°æ›´å¥½çš„è§£ã€‚

#### æ”¶æ–‚æ›²ç·š

```python
import matplotlib.pyplot as plt

# Random Search çµæœ
random_scores = [éš¨æ©Ÿè©¦é©—çš„åˆ†æ•¸åˆ—è¡¨]
random_best = np.maximum.accumulate(random_scores)

# Bayesian Optimization çµæœ
bayesian_scores = [trial.value for trial in study.trials]
bayesian_best = np.maximum.accumulate(bayesian_scores)

plt.figure(figsize=(10, 6))
plt.plot(random_best, label='Random Search', alpha=0.7)
plt.plot(bayesian_best, label='Bayesian Optimization', linewidth=2)
plt.axhline(0.90, color='r', linestyle='--', label='Target: RÂ²=0.90')
plt.xlabel('Number of Trials')
plt.ylabel('Best Score (RÂ²)')
plt.title('Convergence Comparison')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

**å…¸å‹æ›²ç·š**ï¼š
- Random Search: é‹¸é½’ç‹€ï¼Œç·©æ…¢æ”¹é€²
- Bayesian Opt: å¿«é€Ÿæ”¶æ–‚ï¼Œæœ‰æ˜ç¢ºè¶¨å‹¢

### 5.9 å¯¦å‹™å»ºè­°

#### ä½•æ™‚ä½¿ç”¨ Bayesian Optimizationï¼Ÿ

| å ´æ™¯ | æ¨è–¦åº¦ | åŸå›  |
|------|-------|------|
| è©•ä¼°æˆæœ¬é«˜ï¼ˆè¨“ç·´æ™‚é–“ > 10 åˆ†é˜ï¼‰ | â­â­â­â­â­ | æ ¸å¿ƒå„ªå‹¢ |
| è¶…åƒæ•¸ â‰¥ 5 å€‹ | â­â­â­â­â­ | é«˜ç¶­æœç´¢ |
| é ç®—åš´æ ¼é™åˆ¶ï¼ˆ< 50 æ¬¡ï¼‰ | â­â­â­â­â­ | æ¨£æœ¬æ•ˆç‡é«˜ |
| å¯¦é©—æˆæœ¬æ˜‚è²´ï¼ˆåŒ–å·¥å¯¦é©—ï¼‰ | â­â­â­â­â­ | æ¸›å°‘è©¦é©—æ¬¡æ•¸ |
| æ¨¡å‹è¨“ç·´å¿«é€Ÿï¼ˆ< 1 ç§’ï¼‰ | â­â­ | Random Search è¶³å¤  |

#### å¸¸è¦‹é™·é˜±

**1. éåº¦ä¾è³´åˆå§‹åŒ–**ï¼š

```python
# âŒ ä¸æ¨è–¦ï¼šåªæœ‰ 5 å€‹åˆå§‹é»
study.optimize(objective, n_trials=50)
# å‰ 5 å€‹æ˜¯éš¨æ©Ÿçš„ï¼Œå¯èƒ½é™·å…¥å±€éƒ¨æœ€å„ª

# âœ… æ¨è–¦ï¼šè‡³å°‘ 10-20 å€‹åˆå§‹é»
study = optuna.create_study(
    sampler=optuna.samplers.TPESampler(
        n_startup_trials=20  # å‰ 20 å€‹éš¨æ©Ÿæ¢ç´¢
    )
)
```

**2. å¿½ç•¥ä¸ç¢ºå®šåº¦**ï¼š

```python
# è©•ä¼°å¤šæ¬¡ï¼Œå–å¹³å‡
scores = []
for _ in range(3):
    model.fit(X_train, y_train)
    score = model.score(X_val, y_val)
    scores.append(score)
return np.mean(scores)  # æ›´ç©©å®šçš„ä¼°è¨ˆ
```

**3. æœç´¢ç©ºé–“è¨­å®šä¸ç•¶**ï¼š

```python
# âŒ æœç´¢ç©ºé–“éå¤§
'learning_rate': trial.suggest_float('learning_rate', 1e-10, 1.0)
# å¤§éƒ¨åˆ†å€åŸŸç„¡æ„ç¾©

# âœ… åˆç†ç¯„åœ
'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)
```

### 5.10 å°çµ

**Bayesian Optimization çš„æ ¸å¿ƒå„ªå‹¢**ï¼š
1. âœ… æ¨£æœ¬æ•ˆç‡æ¥µé«˜ï¼ˆç›¸æ¯” Random Search ç¯€çœ 50-70%ï¼‰
2. âœ… åˆ©ç”¨æ­·å²ä¿¡æ¯ï¼Œæ™ºèƒ½é¸æ“‡ä¸‹ä¸€å€‹è©¦é©—é»
3. âœ… é©åˆæ˜‚è²´çš„è©•ä¼°ï¼ˆè¨“ç·´æ™‚é–“é•·ã€å¯¦é©—æˆæœ¬é«˜ï¼‰
4. âœ… æä¾›è±å¯Œçš„è¨ºæ–·å·¥å…·ï¼ˆé‡è¦æ€§åˆ†æã€æ”¶æ–‚æ›²ç·šï¼‰

**é©ç”¨å ´æ™¯**ï¼š
- æ¨¡å‹è¨“ç·´æ™‚é–“ > 10 åˆ†é˜
- åŒ–å·¥å¯¦é©—ç­‰æ˜‚è²´è©•ä¼°
- é ç®—åš´æ ¼é™åˆ¶ï¼ˆ< 100 æ¬¡å˜—è©¦ï¼‰
- è¶…åƒæ•¸æ•¸é‡ â‰¥ 5

**æ¡†æ¶é¸æ“‡**ï¼š
- **Optuna**ï¼šåŠŸèƒ½æœ€å…¨ï¼Œç¤¾ç¾¤æ´»èºï¼Œè¦–è¦ºåŒ–è±å¯Œï¼ˆæ¨è–¦ï¼‰
- **Hyperopt**ï¼šè€ç‰Œæ¡†æ¶ï¼Œæ–‡ç»å¼•ç”¨å¤šï¼Œä½†èªæ³•è¼ƒè¤‡é›œ

---

## 6. é€²éšæœç´¢æŠ€å·§

### 6.1 Halving Search

**Successive Halving** æ˜¯ä¸€ç¨®åŠ é€Ÿç­–ç•¥ï¼Œé€šéé€æ­¥æ·˜æ±°è¡¨ç¾å·®çš„å€™é¸è€…ï¼Œé›†ä¸­è³‡æºåœ¨æœ‰å¸Œæœ›çš„çµ„åˆä¸Šã€‚

#### åŸç†

```
åˆå§‹: 64 å€‹å€™é¸è€…ï¼Œæ¯å€‹ç”¨ 10% æ•¸æ“šè©•ä¼°
  â†“ æ·˜æ±°æœ€å·®çš„ä¸€åŠ
ç¬¬ 2 è¼ª: 32 å€‹å€™é¸è€…ï¼Œæ¯å€‹ç”¨ 20% æ•¸æ“šè©•ä¼°
  â†“ æ·˜æ±°æœ€å·®çš„ä¸€åŠ
ç¬¬ 3 è¼ª: 16 å€‹å€™é¸è€…ï¼Œæ¯å€‹ç”¨ 40% æ•¸æ“šè©•ä¼°
  â†“ æ·˜æ±°æœ€å·®çš„ä¸€åŠ
ç¬¬ 4 è¼ª: 8 å€‹å€™é¸è€…ï¼Œæ¯å€‹ç”¨ 80% æ•¸æ“šè©•ä¼°
  â†“ é¸å‡ºæœ€å„ª
æœ€çµ‚: 1 å€‹å€™é¸è€…ï¼Œç”¨ 100% æ•¸æ“šè©•ä¼°
```

**è¨ˆç®—æ•ˆç‡**ï¼š

å‚³çµ±æ–¹æ³•ï¼š64 å€‹å€™é¸è€… Ã— 100% æ•¸æ“š = **6400 å–®ä½**

Halving æ–¹æ³•ï¼š

$$
64 \times 10\% + 32 \times 20\% + 16 \times 40\% + 8 \times 80\% = 25.6 \text{ å–®ä½}
$$

**åŠ é€Ÿæ¯”**ï¼š6400 / 25.6 â‰ˆ **250 å€** âœ…

#### Sklearn å¯¦ä½œ

```python
from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import HalvingGridSearchCV, HalvingRandomSearchCV
from sklearn.ensemble import RandomForestRegressor

# Halving Grid Search
model = RandomForestRegressor(random_state=42)

param_grid = {
    'n_estimators': [50, 100, 200, 500],
    'max_depth': [5, 10, 15, 20, None],
    'min_samples_split': [2, 5, 10, 20]
}

halving_grid = HalvingGridSearchCV(
    model,
    param_grid,
    factor=3,                # æ¯è¼ªä¿ç•™ 1/3
    resource='n_samples',    # é€æ­¥å¢åŠ æ¨£æœ¬æ•¸
    max_resources='auto',    # æœ€çµ‚ä½¿ç”¨å…¨éƒ¨æ•¸æ“š
    cv=5,
    scoring='r2',
    n_jobs=-1,
    random_state=42
)

halving_grid.fit(X_train, y_train)

print(f"Best params: {halving_grid.best_params_}")
print(f"Best score: {halving_grid.best_score_:.4f}")
print(f"Number of candidates: {halving_grid.n_candidates_}")
print(f"Number of resources (samples): {halving_grid.n_resources_}")
```

#### Halving Random Search

```python
from scipy.stats import randint, uniform

param_distributions = {
    'n_estimators': randint(50, 500),
    'max_depth': [5, 10, 15, 20, None],
    'min_samples_split': randint(2, 20),
    'max_features': uniform(0.1, 0.9)
}

halving_random = HalvingRandomSearchCV(
    model,
    param_distributions,
    n_candidates='exhaust',  # ç›¡å¯èƒ½å¤šçš„åˆå§‹å€™é¸è€…
    factor=3,
    resource='n_samples',
    max_resources='auto',
    cv=5,
    scoring='r2',
    n_jobs=-1,
    random_state=42
)

halving_random.fit(X_train, y_train)
```

### 6.2 æ—©åœ (Early Stopping)

å°æ–¼æ”¯æŒå¢é‡è¨“ç·´çš„æ¨¡å‹ï¼ˆå¦‚ XGBoostï¼‰ï¼Œå¯ä»¥æå‰çµ‚æ­¢ç„¡å¸Œæœ›çš„è©¦é©—ã€‚

#### XGBoost ç¯„ä¾‹

```python
import xgboost as xgb

def objective_with_early_stopping(trial):
    params = {
        'n_estimators': 1000,  # è¨­å®šå¤§å€¼
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),
        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),
        'early_stopping_rounds': 50  # 50 è¼ªç„¡æ”¹å–„å‰‡åœæ­¢
    }
    
    model = xgb.XGBRegressor(**params, random_state=42)
    
    model.fit(
        X_train, y_train,
        eval_set=[(X_val, y_val)],
        verbose=False
    )
    
    # å¯¦éš›è¨“ç·´çš„è¼ªæ•¸ï¼ˆå¯èƒ½é å°‘æ–¼ 1000ï¼‰
    best_iteration = model.best_iteration
    score = model.score(X_val, y_val)
    
    return score

study = optuna.create_study(direction='maximize')
study.optimize(objective_with_early_stopping, n_trials=50)
```

### 6.3 å¤šä¿çœŸåº¦å„ªåŒ– (Multi-Fidelity Optimization)

ä½¿ç”¨ä¸åŒä¿çœŸåº¦ï¼ˆæ¨£æœ¬é‡ã€è¨“ç·´è¼ªæ•¸ã€åˆ†è¾¨ç‡ç­‰ï¼‰åŠ é€Ÿæœç´¢ã€‚

#### ç­–ç•¥

| ä¿çœŸåº¦ç´šåˆ¥ | æ¨£æœ¬é‡ | CV Folds | è¨“ç·´è¼ªæ•¸ | ç”¨é€” |
|-----------|--------|----------|---------|------|
| ä½ | 10% | 2 | 10 | åˆæ­¥ç¯©é¸ |
| ä¸­ | 30% | 3 | 50 | ç²¾é¸å€™é¸ |
| é«˜ | 100% | 5 | 1000 | æœ€çµ‚è©•ä¼° |

#### å¯¦ä½œç¯„ä¾‹

```python
def multi_fidelity_objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 500),
        'max_depth': trial.suggest_int('max_depth', 3, 20)
    }
    
    # ç¬¬ä¸€éšæ®µï¼šå¿«é€Ÿè©•ä¼°ï¼ˆ30% æ•¸æ“šï¼Œ3-foldï¼‰
    X_sample, _, y_sample, _ = train_test_split(
        X_train, y_train, train_size=0.3, random_state=42
    )
    
    model = RandomForestRegressor(**params, random_state=42, n_jobs=1)
    scores = cross_val_score(model, X_sample, y_sample, cv=3, scoring='r2')
    score_low = scores.mean()
    
    # å¦‚æœè¡¨ç¾å¤ªå·®ï¼Œæå‰çµ‚æ­¢
    if score_low < 0.70:
        return score_low
    
    # ç¬¬äºŒéšæ®µï¼šå®Œæ•´è©•ä¼°ï¼ˆ100% æ•¸æ“šï¼Œ5-foldï¼‰
    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')
    score_high = scores.mean()
    
    return score_high

study = optuna.create_study(direction='maximize')
study.optimize(multi_fidelity_objective, n_trials=100)
```

### 6.4 æš–å•Ÿå‹• (Warm Start)

åˆ©ç”¨ä¹‹å‰çš„æœç´¢çµæœï¼ŒåŠ é€Ÿæ–°ä¸€è¼ªå„ªåŒ–ã€‚

```python
# ç¬¬ä¸€è¼ªå„ªåŒ–
study = optuna.create_study(direction='maximize', study_name='rf_tuning')
study.optimize(objective, n_trials=50)

# ä¿å­˜çµæœ
import joblib
joblib.dump(study, 'study_checkpoint.pkl')

# ç¬¬äºŒè¼ªå„ªåŒ–ï¼ˆçºŒæ¥ï¼‰
study_loaded = joblib.load('study_checkpoint.pkl')
study_loaded.optimize(objective, n_trials=50)  # å†å„ªåŒ– 50 æ¬¡

print(f"Total trials: {len(study_loaded.trials)}")  # 100 æ¬¡
```

### 6.5 å¹³è¡Œåˆ†ä½ˆå¼å„ªåŒ–

```python
import optuna

# å‰µå»ºå…±äº«ç ”ç©¶å°è±¡ï¼ˆä½¿ç”¨è³‡æ–™åº«ï¼‰
study = optuna.create_study(
    study_name='distributed_optimization',
    storage='sqlite:///optuna_study.db',  # å…±äº«è³‡æ–™åº«
    direction='maximize',
    load_if_exists=True  # å¦‚æœå·²å­˜åœ¨å‰‡è¼‰å…¥
)

# åœ¨å¤šå°æ©Ÿå™¨æˆ–å¤šå€‹é€²ç¨‹ä¸ŠåŒæ™‚é‹è¡Œ
study.optimize(objective, n_trials=100)
```

**å¤šé€²ç¨‹ç¯„ä¾‹**ï¼š

```python
from multiprocessing import Pool

def run_optimization(worker_id):
    study = optuna.load_study(
        study_name='distributed_optimization',
        storage='sqlite:///optuna_study.db'
    )
    study.optimize(objective, n_trials=25)

if __name__ == '__main__':
    with Pool(4) as pool:  # 4 å€‹å·¥ä½œé€²ç¨‹
        pool.map(run_optimization, range(4))
```

### 6.6 åŒ–å·¥æ¡ˆä¾‹ï¼šåŠ é€Ÿè’¸é¤¾å¡”æ§åˆ¶å™¨èª¿åƒ

**å ´æ™¯**ï¼šèª¿æ•´ PID æ§åˆ¶å™¨åƒæ•¸ä»¥æœ€å°åŒ–æº«åº¦æ³¢å‹•

**æŒ‘æˆ°**ï¼šæ¯æ¬¡æ¨¡æ“¬éœ€è¦ 5 åˆ†é˜

**Halving Random Search æ–¹æ¡ˆ**ï¼š

```python
from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import HalvingRandomSearchCV

# æ¨¡æ“¬æ§åˆ¶å™¨æ€§èƒ½ï¼ˆå¯¦å‹™ä¸­éœ€èª¿ç”¨æ¨¡æ“¬å™¨ï¼‰
def evaluate_controller(Kp, Ki, Kd, simulation_time):
    """
    simulation_time: æ¨¡æ“¬æ™‚é•·ï¼ˆç§’ï¼‰
    è¿”å›: æº«åº¦æ¨™æº–å·®ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
    """
    # å¯¦éš›æ‡‰ç”¨ä¸­æœƒèª¿ç”¨ Aspen/HYSYS ç­‰
    # æ­¤è™•ç°¡åŒ–ç‚ºå¿«é€Ÿè©•ä¼°
    return some_simulation_function(Kp, Ki, Kd, simulation_time)

# ä½¿ç”¨ Halving ç­–ç•¥
param_distributions = {
    'Kp': uniform(0.1, 10),
    'Ki': uniform(0.01, 1),
    'Kd': uniform(0.001, 0.1)
}

# åˆå§‹ç”¨çŸ­æ™‚é–“æ¨¡æ“¬ï¼Œé€æ­¥å¢åŠ 
halving_search = HalvingRandomSearchCV(
    CustomController(),
    param_distributions,
    resource='simulation_time',  # é€æ­¥å¢åŠ æ¨¡æ“¬æ™‚é•·
    min_resources=10,            # æœ€å° 10 ç§’
    max_resources=300,           # æœ€å¤§ 5 åˆ†é˜
    factor=3,
    cv=3,
    scoring='neg_std',           # æœ€å°åŒ–æ¨™æº–å·®
    random_state=42
)

halving_search.fit(X_dummy, y_dummy)
```

**æ•ˆæœ**ï¼š
- å‚³çµ±æ–¹æ³•ï¼š100 å€‹çµ„åˆ Ã— 5 åˆ†é˜ = **8.3 å°æ™‚**
- Halving Searchï¼šåŠ é€Ÿ **50 å€** â†’ **10 åˆ†é˜**

---

## 7. æœç´¢ç©ºé–“è¨­è¨ˆ

### 7.1 æœç´¢ç©ºé–“çš„é‡è¦æ€§

> "Garbage in, garbage out" â€” æœç´¢ç©ºé–“è¨­è¨ˆä¸ç•¶ï¼Œå†å¥½çš„ç®—æ³•ä¹Ÿç„¡æ¿Ÿæ–¼äº‹ã€‚

**å¸¸è¦‹éŒ¯èª¤**ï¼š

```python
# âŒ éŒ¯èª¤ 1: ç¯„åœéå¤§
'learning_rate': uniform(1e-10, 10)  # å¤§éƒ¨åˆ†å€åŸŸç„¡æ„ç¾©

# âŒ éŒ¯èª¤ 2: ç¯„åœéå°
'n_estimators': [90, 95, 100, 105, 110]  # å¯èƒ½éŒ¯éæœ€å„ªå€åŸŸ

# âŒ éŒ¯èª¤ 3: åˆ»åº¦ä¸ç•¶
'alpha': [0.001, 0.01, 0.1, 1, 10]  # æ‡‰ä½¿ç”¨å°æ•¸åˆ»åº¦

# âŒ éŒ¯èª¤ 4: å¿½ç•¥ç›¸é—œæ€§
# max_depth å’Œ min_samples_leaf ç›¸äº’å½±éŸ¿ï¼Œéœ€åŒæ™‚èª¿æ•´
```

### 7.2 é€£çºŒ vs é›¢æ•£

| è¶…åƒæ•¸é¡å‹ | æœç´¢ç©ºé–“è¨­è¨ˆ | ç¯„ä¾‹ |
|-----------|------------|------|
| é€£çºŒå‹ | ä½¿ç”¨åˆ†å¸ƒï¼ˆuniform, loguniformï¼‰ | learning_rate, alpha |
| æ•´æ•¸å‹ | ä½¿ç”¨ randint æˆ–æ•´æ•¸åˆ†å¸ƒ | n_estimators, max_depth |
| é¡åˆ¥å‹ | åˆ—èˆ‰æ‰€æœ‰é¸é … | kernel, criterion |

**Optuna ç¯„ä¾‹**ï¼š

```python
def objective(trial):
    # é€£çºŒå‹
    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)
    
    # æ•´æ•¸å‹
    n_estimators = trial.suggest_int('n_estimators', 50, 500)
    
    # é¡åˆ¥å‹
    kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly'])
    
    # æ¢ä»¶å‹ï¼ˆä¾è³´å…¶ä»–è¶…åƒæ•¸ï¼‰
    if kernel == 'poly':
        degree = trial.suggest_int('degree', 2, 5)
    else:
        degree = None
    
    return train_and_evaluate(learning_rate, n_estimators, kernel, degree)
```

### 7.3 å°æ•¸å°ºåº¦çš„æ‡‰ç”¨

**ä½•æ™‚ä½¿ç”¨å°æ•¸å°ºåº¦ï¼Ÿ**

ç•¶è¶…åƒæ•¸ç¯„åœè·¨è¶Šå¤šå€‹æ•¸é‡ç´šæ™‚ï¼ˆå¦‚ 0.001 â†’ 100ï¼‰ã€‚

| è¶…åƒæ•¸ | ç·šæ€§ç¯„åœ | å°æ•¸ç¯„åœ |
|--------|---------|---------|
| learning_rate | âŒ | âœ… [1e-4, 1e-1] |
| alpha (æ­£å‰‡åŒ–) | âŒ | âœ… [1e-5, 1e2] |
| C (SVM) | âŒ | âœ… [1e-3, 1e3] |
| n_estimators | âœ… [50, 500] | âŒ |
| max_depth | âœ… [3, 20] | âŒ |

**Python å¯¦ä½œ**ï¼š

```python
# Optuna
'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-1)

# Sklearn (Random Search)
from scipy.stats import loguniform
'learning_rate': loguniform(1e-4, 1e-1)

# æ‰‹å‹•ç”Ÿæˆå°æ•¸åˆ»åº¦
import numpy as np
np.logspace(-4, -1, 10)  # [0.0001, ..., 0.1]
```

### 7.4 å¸¸è¦‹æ¨¡å‹çš„æ¨è–¦æœç´¢ç©ºé–“

#### Random Forest

```python
# Grid Search ç‰ˆæœ¬
param_grid_rf = {
    'n_estimators': [100, 200, 300, 500],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2', None]
}

# Random/Bayesian Search ç‰ˆæœ¬
param_dist_rf = {
    'n_estimators': randint(50, 500),
    'max_depth': randint(5, 50),
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 10),
    'max_features': uniform(0.1, 0.9)
}
```

#### XGBoost

```python
param_dist_xgb = {
    # æ¨¹çµæ§‹
    'n_estimators': randint(100, 1000),
    'max_depth': randint(3, 10),
    'min_child_weight': randint(1, 10),
    
    # å­¸ç¿’ç‡
    'learning_rate': loguniform(1e-3, 1e-1),
    
    # æ¡æ¨£
    'subsample': uniform(0.5, 0.5),           # [0.5, 1.0]
    'colsample_bytree': uniform(0.5, 0.5),    # [0.5, 1.0]
    
    # æ­£å‰‡åŒ–
    'gamma': uniform(0, 5),
    'reg_alpha': loguniform(1e-4, 10),        # L1
    'reg_lambda': loguniform(1e-4, 10)        # L2
}
```

#### Support Vector Machine

```python
param_dist_svm = {
    'C': loguniform(1e-2, 1e3),               # æ‡²ç½°åƒæ•¸
    'epsilon': loguniform(1e-3, 1),           # Îµ-tube å¯¬åº¦
    'gamma': loguniform(1e-4, 1),             # RBF æ ¸å¯¬åº¦
    'kernel': ['rbf', 'poly', 'sigmoid']
}
```

#### Gradient Boosting

```python
param_dist_gb = {
    'n_estimators': randint(50, 500),
    'learning_rate': loguniform(1e-3, 1),
    'max_depth': randint(3, 10),
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 10),
    'subsample': uniform(0.5, 0.5),
    'max_features': uniform(0.5, 0.5)
}
```

### 7.5 åˆ©ç”¨é ˜åŸŸçŸ¥è­˜ç¸®å°ç©ºé–“

#### åŒ–å·¥æ¡ˆä¾‹ï¼šåæ‡‰æº«åº¦é æ¸¬

**å ´æ™¯**ï¼šé æ¸¬åŒ–å­¸åæ‡‰çš„æœ€å„ªæº«åº¦

**ç‰©ç†ç´„æŸ**ï¼š
- åæ‡‰æº«åº¦ç¯„åœ: 50-300Â°Cï¼ˆå·²çŸ¥ï¼‰
- å£“åŠ›ç¯„åœ: 1-50 barï¼ˆå·²çŸ¥ï¼‰
- æŸäº›ç‰¹å¾µçµ„åˆç‰©ç†ä¸Šä¸å¯è¡Œ

**å„ªåŒ–æœç´¢ç©ºé–“**ï¼š

```python
def objective_with_constraints(trial):
    # åŸºæ–¼åŒ–å­¸å‹•åŠ›å­¸ï¼Œç¸®å°å­¸ç¿’ç‡ç¯„åœ
    learning_rate = trial.suggest_loguniform('learning_rate', 1e-3, 1e-1)
    # ï¼ˆç¶“é©—ï¼šå¤ªå°çš„å­¸ç¿’ç‡åœ¨æ­¤å•é¡Œç„¡æ•ˆï¼‰
    
    # å·²çŸ¥æ¨¹æ·±åº¦è¶…é 15 æ²’æœ‰å¹«åŠ©ï¼ˆåæ‡‰æ©Ÿåˆ¶ä¸é‚£éº¼è¤‡é›œï¼‰
    max_depth = trial.suggest_int('max_depth', 3, 15)
    
    # æº«åº¦å’Œå£“åŠ›å¿…é ˆæ»¿è¶³ç‰©ç†ç´„æŸ
    temp = trial.suggest_float('temp', 50, 300)
    pressure = trial.suggest_float('pressure', 1, 50)
    
    # ç„¡æ•ˆçµ„åˆï¼šé«˜æº«ä½å£“ï¼ˆæº¶åŠ‘æœƒæ±½åŒ–ï¼‰
    if temp > 200 and pressure < 5:
        return float('-inf')  # æ‡²ç½°ç„¡æ•ˆçµ„åˆ
    
    return train_and_evaluate(...)
```

### 7.6 æ¢ä»¶è¶…åƒæ•¸

æŸäº›è¶…åƒæ•¸åªåœ¨ç‰¹å®šæ¢ä»¶ä¸‹æœ‰æ„ç¾©ã€‚

```python
def objective_conditional(trial):
    # é¸æ“‡æ¨¡å‹é¡å‹
    model_type = trial.suggest_categorical('model_type', ['rf', 'xgb', 'svm'])
    
    if model_type == 'rf':
        n_estimators = trial.suggest_int('rf_n_estimators', 50, 500)
        max_depth = trial.suggest_int('rf_max_depth', 5, 30)
        model = RandomForestRegressor(
            n_estimators=n_estimators,
            max_depth=max_depth,
            random_state=42
        )
    
    elif model_type == 'xgb':
        n_estimators = trial.suggest_int('xgb_n_estimators', 50, 500)
        learning_rate = trial.suggest_loguniform('xgb_learning_rate', 1e-3, 1e-1)
        model = xgb.XGBRegressor(
            n_estimators=n_estimators,
            learning_rate=learning_rate,
            random_state=42
        )
    
    else:  # svm
        C = trial.suggest_loguniform('svm_C', 1e-2, 1e2)
        gamma = trial.suggest_loguniform('svm_gamma', 1e-4, 1)
        model = SVR(C=C, gamma=gamma)
    
    return cross_val_score(model, X_train, y_train, cv=5).mean()
```

### 7.7 é¿å‘æŒ‡å—

#### é™·é˜± 1: ä¸åˆ‡å¯¦éš›çš„ç¯„åœ

```python
# âŒ éŒ¯èª¤ï¼šmax_depth è¨­ç‚º 100ï¼ˆå¹¾ä¹ä¸€å®šéæ“¬åˆï¼‰
'max_depth': randint(1, 100)

# âœ… æ­£ç¢ºï¼šåŸºæ–¼æ•¸æ“šé‡å’Œç¶“é©—
# å°æ–¼ n=1000 çš„æ•¸æ“šé›†
'max_depth': randint(3, 20)
```

#### é™·é˜± 2: å¿½ç•¥è¨ˆç®—æˆæœ¬

```python
# âŒ éŒ¯èª¤ï¼šn_estimators éå¤§æµªè²»æ™‚é–“
'n_estimators': randint(50, 10000)  # è¨“ç·´ 10000 æ£µæ¨¹æ²’å¿…è¦

# âœ… æ­£ç¢ºï¼šåˆç†ä¸Šé™
'n_estimators': randint(50, 500)
```

#### é™·é˜± 3: éåº¦é›¢æ•£åŒ–

```python
# âŒ éŒ¯èª¤ï¼šåˆ»åº¦å¤ªå¯†é›†
'alpha': [0.1, 0.11, 0.12, 0.13, ..., 1.0]  # 90 å€‹å€¼

# âœ… æ­£ç¢ºï¼šå°æ•¸åˆ»åº¦ + åˆç†æ•¸é‡
'alpha': np.logspace(-2, 1, 10)  # [0.01, 0.02, ..., 10]
```

---

## 8. åŒ–å·¥æ‡‰ç”¨æ¡ˆä¾‹

### 8.1 æ¡ˆä¾‹ 1ï¼šç²¾é¤¾å¡”æº«åº¦è»Ÿæ¸¬é‡æ¨¡å‹

**èƒŒæ™¯**ï¼š
- ç›®æ¨™ï¼šé æ¸¬ç²¾é¤¾å¡”ç¬¬ 15 å±¤æº«åº¦
- è¼¸å…¥ï¼šé€²æ–™æµé‡ã€å›æµæ¯”ã€å†æ²¸å™¨ç†±è² è·ç­‰ 12 å€‹è®Šé‡
- æ•¸æ“šï¼š1200 ç­†æ­·å²æ•¸æ“š
- è¦æ±‚ï¼šMAE < 1Â°Cï¼Œæ¨ç†æ™‚é–“ < 100ms

**æ¨¡å‹é¸æ“‡**ï¼šXGBoostï¼ˆå…¼é¡§æº–ç¢ºåº¦èˆ‡é€Ÿåº¦ï¼‰

**è¶…åƒæ•¸èª¿æ•´ç­–ç•¥**ï¼š

```python
import xgboost as xgb
import optuna

def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 500),
        'max_depth': trial.suggest_int('max_depth', 3, 8),  # é™åˆ¶æ·±åº¦é˜²éæ“¬åˆ
        'learning_rate': trial.suggest_loguniform('learning_rate', 5e-3, 5e-1),
        'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),
        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10),
        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10),
        'gamma': trial.suggest_uniform('gamma', 0, 5)
    }
    
    model = xgb.XGBRegressor(
        **params,
        random_state=42,
        tree_method='hist',  # åŠ é€Ÿè¨“ç·´
        n_jobs=1
    )
    
    # æ™‚é–“åºåˆ—äº¤å‰é©—è­‰
    scores = []
    for train_idx, val_idx in TimeSeriesSplit(n_splits=5).split(X_train):
        X_tr, X_val = X_train[train_idx], X_train[val_idx]
        y_tr, y_val = y_train[train_idx], y_train[val_idx]
        
        model.fit(X_tr, y_tr)
        y_pred = model.predict(X_val)
        mae = mean_absolute_error(y_val, y_pred)
        scores.append(-mae)  # è² è™Ÿï¼šæœ€å°åŒ– MAE
    
    return np.mean(scores)

# Bayesian Optimization
study = optuna.create_study(
    direction='maximize',  # æœ€å¤§åŒ– -MAEï¼ˆå³æœ€å°åŒ– MAEï¼‰
    sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=20)
)

study.optimize(objective, n_trials=100, n_jobs=1)

# æœ€å„ªæ¨¡å‹
best_params = study.best_params
final_model = xgb.XGBRegressor(**best_params, random_state=42)
final_model.fit(X_train, y_train)

# æ¸¬è©¦é›†è©•ä¼°
y_pred = final_model.predict(X_test)
test_mae = mean_absolute_error(y_test, y_pred)
print(f"Test MAE: {test_mae:.3f} Â°C")

# æ¨ç†é€Ÿåº¦æ¸¬è©¦
import time
start = time.time()
_ = final_model.predict(X_test[:1000])
inference_time = (time.time() - start) / 1000 * 1000  # ms per sample
print(f"Inference time: {inference_time:.2f} ms")
```

**çµæœ**ï¼š
- Test MAE: **0.87Â°C** âœ…ï¼ˆæ»¿è¶³ < 1Â°Cï¼‰
- Inference time: **0.23 ms** âœ…ï¼ˆæ»¿è¶³ < 100 msï¼‰
- æœ€å„ªåƒæ•¸ï¼š`{'n_estimators': 287, 'max_depth': 6, 'learning_rate': 0.0342, ...}`

### 8.2 æ¡ˆä¾‹ 2ï¼šåæ‡‰å™¨ç”¢ç‡å„ªåŒ–

**èƒŒæ™¯**ï¼š
- ç›®æ¨™ï¼šæœ€å¤§åŒ–åŒ–å­¸åæ‡‰ç”¢ç‡
- è¼¸å…¥ï¼šåæ‡‰æº«åº¦ã€å£“åŠ›ã€å‚¬åŒ–åŠ‘è² è¼‰é‡ã€åœç•™æ™‚é–“
- æŒ‘æˆ°ï¼šå¯¦é©—æˆæœ¬é«˜ï¼ˆæ¯æ¬¡ > 10,000 å…ƒï¼‰ï¼Œåªèƒ½é€²è¡Œ 30 æ¬¡å¯¦é©—
- è¦æ±‚ï¼šæ‰¾åˆ°ç”¢ç‡ > 85% çš„æ“ä½œæ¢ä»¶

**ç­–ç•¥**ï¼šBayesian Optimization with Gaussian Process

```python
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
import optuna

# å·²æœ‰çš„å°‘é‡å¯¦é©—æ•¸æ“šï¼ˆ10 ç­†åˆå§‹æ•¸æ“šï¼‰
initial_experiments = pd.DataFrame({
    'temperature': [200, 220, 250, 180, 230, 240, 210, 260, 190, 225],
    'pressure': [5, 8, 10, 3, 7, 9, 6, 12, 4, 8],
    'catalyst_loading': [2.0, 2.5, 3.0, 1.5, 2.2, 2.8, 2.1, 3.5, 1.8, 2.6],
    'residence_time': [10, 15, 20, 8, 12, 18, 11, 22, 9, 14],
    'yield': [72, 78, 81, 65, 76, 80, 74, 79, 68, 77]  # %
})

# è¨“ç·´ Gaussian Process ä»£ç†æ¨¡å‹
X_init = initial_experiments[['temperature', 'pressure', 'catalyst_loading', 'residence_time']]
y_init = initial_experiments['yield']

kernel = C(1.0, (1e-3, 1e3)) * RBF([10, 2, 0.5, 5], (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, alpha=0.1)
gp.fit(X_init, y_init)

# ä½¿ç”¨ Optuna æ‰¾ä¸‹ä¸€å€‹å¯¦é©—é»
def suggest_next_experiment(trial):
    temp = trial.suggest_float('temperature', 180, 280)
    pressure = trial.suggest_float('pressure', 3, 15)
    catalyst = trial.suggest_float('catalyst_loading', 1.5, 4.0)
    residence = trial.suggest_float('residence_time', 8, 25)
    
    X_new = np.array([[temp, pressure, catalyst, residence]])
    
    # Upper Confidence Bound
    mean, std = gp.predict(X_new, return_std=True)
    ucb = mean + 2.0 * std  # kappa=2.0
    
    return ucb[0]

study = optuna.create_study(direction='maximize')
study.optimize(suggest_next_experiment, n_trials=20)  # å»ºè­° 20 å€‹æ–°å¯¦é©—

# æœ€æœ‰å¸Œæœ›çš„å¯¦é©—æ¢ä»¶
best_condition = study.best_params
print("=== å»ºè­°çš„ä¸‹ä¸€å€‹å¯¦é©—æ¢ä»¶ ===")
print(f"Temperature: {best_condition['temperature']:.1f} Â°C")
print(f"Pressure: {best_condition['pressure']:.1f} bar")
print(f"Catalyst Loading: {best_condition['catalyst_loading']:.2f} wt%")
print(f"Residence Time: {best_condition['residence_time']:.1f} min")

# é æ¸¬ç”¢ç‡
X_pred = np.array([[
    best_condition['temperature'],
    best_condition['pressure'],
    best_condition['catalyst_loading'],
    best_condition['residence_time']
]])
predicted_yield, uncertainty = gp.predict(X_pred, return_std=True)
print(f"Predicted Yield: {predicted_yield[0]:.1f}% Â± {uncertainty[0]:.1f}%")
```

**è¿­ä»£æµç¨‹**ï¼š
1. ç”¨ 10 ç­†åˆå§‹æ•¸æ“šè¨“ç·´ GP
2. æ‰¾ UCB æœ€å¤§çš„é» â†’ é€²è¡Œå¯¦é©—
3. æ›´æ–° GP æ¨¡å‹ â†’ å†æ‰¾ä¸‹ä¸€å€‹é»
4. é‡è¤‡ç›´åˆ°é”åˆ°ç›®æ¨™æˆ–é ç®—ç”¨ç›¡

**çµæœ**ï¼š
- ç¬¬ 25 æ¬¡å¯¦é©—é”åˆ°ç”¢ç‡ **86.3%** âœ…
- ç›¸æ¯”éš¨æ©Ÿå¯¦é©—ç¯€çœ **40%** æˆæœ¬

### 8.3 æ¡ˆä¾‹ 3ï¼šå‚¬åŒ–åŠ‘ç¯©é¸åŠ é€Ÿ

**èƒŒæ™¯**ï¼š
- ç›®æ¨™ï¼šå¾ 500 ç¨®å‚¬åŒ–åŠ‘é…æ–¹ä¸­æ‰¾å‡ºæœ€å„ªçµ„åˆ
- è©•ä¼°æ–¹æ³•ï¼šé«˜é€šé‡å¯¦é©—ï¼ˆHTEï¼‰
- é™åˆ¶ï¼šåªèƒ½æ¸¬è©¦ 50 ç¨®é…æ–¹
- å„ªåŒ–ç›®æ¨™ï¼šè½‰åŒ–ç‡ã€é¸æ“‡æ€§ã€ç©©å®šæ€§ï¼ˆå¤šç›®æ¨™ï¼‰

**å¤šç›®æ¨™ Bayesian Optimization**ï¼š

```python
def multi_objective_catalyst(trial):
    # å‚¬åŒ–åŠ‘çµ„æˆï¼ˆmol%ï¼‰
    metal_A = trial.suggest_float('metal_A', 0, 100)
    metal_B = trial.suggest_float('metal_B', 0, 100 - metal_A)
    support_type = trial.suggest_categorical('support_type', ['Al2O3', 'SiO2', 'TiO2', 'ZrO2'])
    
    # è£½å‚™æ¢ä»¶
    calcination_temp = trial.suggest_int('calcination_temp', 400, 800)
    calcination_time = trial.suggest_int('calcination_time', 2, 8)
    
    # æ¨¡æ“¬å¯¦é©—ï¼ˆå¯¦å‹™ä¸­èª¿ç”¨ HTE è¨­å‚™ï¼‰
    conversion = simulate_conversion(metal_A, metal_B, support_type, calcination_temp, calcination_time)
    selectivity = simulate_selectivity(metal_A, metal_B, support_type, calcination_temp, calcination_time)
    stability = simulate_stability(metal_A, metal_B, support_type, calcination_temp, calcination_time)
    
    return conversion, selectivity, stability  # ä¸‰å€‹ç›®æ¨™

# å¤šç›®æ¨™å„ªåŒ–
study = optuna.create_study(
    directions=['maximize', 'maximize', 'maximize']
)

study.optimize(multi_objective_catalyst, n_trials=50)

# Pareto æœ€å„ªè§£
print(f"Number of Pareto-optimal solutions: {len(study.best_trials)}")
for i, trial in enumerate(study.best_trials[:5]):  # é¡¯ç¤ºå‰ 5 å€‹
    print(f"\n=== Solution {i+1} ===")
    print(f"Conversion: {trial.values[0]:.1f}%")
    print(f"Selectivity: {trial.values[1]:.1f}%")
    print(f"Stability: {trial.values[2]:.1f} hours")
    print(f"Parameters: {trial.params}")
```

**çµæœ**ï¼š
- æ‰¾åˆ° 8 å€‹ Pareto æœ€å„ªé…æ–¹
- æœ€å„ªé…æ–¹ï¼šè½‰åŒ–ç‡ 92%, é¸æ“‡æ€§ 88%, ç©©å®šæ€§ 120 å°æ™‚
- ç›¸æ¯”å…¨é¢ç¯©é¸ï¼Œç¯€çœ **90%** å¯¦é©—æ¬¡æ•¸

### 8.4 æ¡ˆä¾‹ç¸½çµ

| æ¡ˆä¾‹ | æ¨¡å‹ | èª¿åƒæ–¹æ³• | é—œéµæŒ‘æˆ° | æˆæœ |
|------|------|---------|---------|------|
| ç²¾é¤¾å¡”è»Ÿæ¸¬é‡ | XGBoost | Bayesian Opt | é€Ÿåº¦èˆ‡æº–ç¢ºåº¦å¹³è¡¡ | MAE 0.87Â°C, 0.23ms |
| åæ‡‰å™¨ç”¢ç‡ | GP | Active Learning | å¯¦é©—æˆæœ¬é«˜ | 25 æ¬¡é”æ¨™ï¼Œç¯€çœ 40% |
| å‚¬åŒ–åŠ‘ç¯©é¸ | Multi-objective | Pareto Opt | å¤šç›®æ¨™è¡çª | 8 å€‹æœ€å„ªè§£ï¼Œç¯€çœ 90% |

**å…±åŒç‰¹é»**ï¼š
- å……åˆ†åˆ©ç”¨é ˜åŸŸçŸ¥è­˜è¨­è¨ˆæœç´¢ç©ºé–“
- é¸æ“‡åˆé©çš„å„ªåŒ–ç­–ç•¥ï¼ˆæˆæœ¬ vs æ€§èƒ½ï¼‰
- åš´æ ¼é©—è­‰ï¼ˆæ™‚é–“åºåˆ— CV, å¯¦é©—é©—è­‰ï¼‰

---

## 9. ç¸½çµèˆ‡æœ€ä½³å¯¦è¸

### 9.1 æ–¹æ³•é¸æ“‡æŒ‡å—

```
é–‹å§‹
  â”‚
  â”œâ”€ è¶…åƒæ•¸æ•¸é‡ â‰¤ 3ï¼Ÿ
  â”‚   â”œâ”€ Yes â†’ Grid Search
  â”‚   â””â”€ No â†’ ç¹¼çºŒ
  â”‚
  â”œâ”€ æ¨¡å‹è¨“ç·´æ™‚é–“ < 1 ç§’ï¼Ÿ
  â”‚   â”œâ”€ Yes â†’ Random Search (n_iter=100)
  â”‚   â””â”€ No â†’ ç¹¼çºŒ
  â”‚
  â”œâ”€ æ¨¡å‹è¨“ç·´æ™‚é–“ > 10 åˆ†é˜ï¼Ÿ
  â”‚   â”œâ”€ Yes â†’ Bayesian Optimization (n_trials=50)
  â”‚   â””â”€ No â†’ ç¹¼çºŒ
  â”‚
  â”œâ”€ é ç®— < 50 æ¬¡ï¼Ÿ
  â”‚   â”œâ”€ Yes â†’ Bayesian Optimization
  â”‚   â””â”€ No â†’ Random Search + Halving
  â”‚
  â””â”€ å¯¦é©—æˆæœ¬æ¥µé«˜ï¼ˆåŒ–å·¥å¯¦é©—ï¼‰ï¼Ÿ
      â””â”€ Yes â†’ Bayesian Optimization with GP
```

### 9.2 æœ€ä½³å¯¦è¸æ¸…å–®

#### âœ… æœç´¢ç©ºé–“è¨­è¨ˆ

- [ ] ä½¿ç”¨å°æ•¸å°ºåº¦æ–¼è·¨åº¦å¤§çš„è¶…åƒæ•¸ï¼ˆlearning_rate, alphaï¼‰
- [ ] è¨­å®šåˆç†çš„ç¯„åœï¼ˆé¿å…éå¤§æˆ–éå°ï¼‰
- [ ] åˆ©ç”¨é ˜åŸŸçŸ¥è­˜æ’é™¤ç„¡æ•ˆå€åŸŸ
- [ ] è™•ç†æ¢ä»¶è¶…åƒæ•¸ï¼ˆåªåœ¨ç‰¹å®šæ¨¡å‹æœ‰æ„ç¾©ï¼‰

#### âœ… é©—è­‰ç­–ç•¥

- [ ] ä½¿ç”¨äº¤å‰é©—è­‰ï¼ˆè‡³å°‘ 5-foldï¼‰
- [ ] æ™‚é–“åºåˆ—æ•¸æ“šå¿…é ˆç”¨ TimeSeriesSplit
- [ ] å°æ•¸æ“šé›†è€ƒæ…® LOOCV æˆ– Stratified CV
- [ ] ç›£æ§è¨“ç·´-é©—è­‰å·®è·ï¼ˆéæ“¬åˆæª¢æ¸¬ï¼‰

#### âœ… è¨ˆç®—æ•ˆç‡

- [ ] è¨­å®š `n_jobs=-1` å……åˆ†åˆ©ç”¨å¤šæ ¸
- [ ] è€ƒæ…® Halving Search åŠ é€Ÿ
- [ ] ä½¿ç”¨ Early Stoppingï¼ˆXGBoost ç­‰ï¼‰
- [ ] å¤šä¿çœŸåº¦è©•ä¼°ï¼ˆå…ˆç”¨å°æ•¸æ“šç¯©é¸ï¼‰

#### âœ… çµæœåˆ†æ

- [ ] å¯è¦–åŒ–æœç´¢éç¨‹ï¼ˆæ”¶æ–‚æ›²ç·šã€åƒæ•¸é‡è¦æ€§ï¼‰
- [ ] åˆ†ææœ€å„ªè§£çš„ç©©å®šæ€§ï¼ˆå¤šæ¬¡é‹è¡Œï¼‰
- [ ] æª¢æŸ¥æ˜¯å¦è§¸åŠæœç´¢é‚Šç•Œï¼ˆéœ€æ“´å¤§ç¯„åœï¼‰
- [ ] è¨˜éŒ„æ‰€æœ‰å˜—è©¦ï¼ˆä¾¿æ–¼å¾ŒçºŒåˆ†æï¼‰

#### âœ… åŒ–å·¥ç‰¹æ®Šè€ƒé‡

- [ ] å¹³è¡¡æº–ç¢ºåº¦èˆ‡å¯è§£é‡‹æ€§
- [ ] è€ƒæ…®æ¨ç†é€Ÿåº¦ï¼ˆç·šä¸Šæ§åˆ¶éœ€æ±‚ï¼‰
- [ ] æ¨¡å‹å¤§å°é™åˆ¶ï¼ˆåµŒå…¥å¼ç³»çµ±ï¼‰
- [ ] å¤šç›®æ¨™å„ªåŒ–ï¼ˆç”¢ç‡ã€èƒ½è€—ã€å®‰å…¨ï¼‰

### 9.3 å¸¸è¦‹éŒ¯èª¤

| éŒ¯èª¤ | å¾Œæœ | æ­£ç¢ºåšæ³• |
|------|------|---------|
| åœ¨è¨“ç·´é›†ä¸Šèª¿åƒ | éæ“¬åˆ | ä½¿ç”¨é©—è­‰é›†æˆ–äº¤å‰é©—è­‰ |
| å¿½ç•¥éš¨æ©Ÿæ€§ | çµæœä¸ç©©å®š | å¤šæ¬¡é‹è¡Œå–å¹³å‡ |
| éåº¦èª¿åƒ | éæ“¬åˆé©—è­‰é›† | ä¿ç•™æ¸¬è©¦é›†æœ€çµ‚è©•ä¼° |
| æœç´¢ç©ºé–“éå¤§ | æµªè²»è³‡æº | åˆ©ç”¨å…ˆé©—çŸ¥è­˜ç¸®å° |
| åªçœ‹å–®ä¸€æŒ‡æ¨™ | å¿½ç•¥å…¶ä»–é‡è¦å› ç´  | å¤šç›®æ¨™å¹³è¡¡ |

### 9.4 å·¥å…·å°æ¯”

| å·¥å…· | å„ªé» | ç¼ºé» | æ¨è–¦å ´æ™¯ |
|------|------|------|---------|
| GridSearchCV | ç°¡å–®ã€å®Œæ•´ | çµ„åˆçˆ†ç‚¸ | â‰¤ 3 å€‹è¶…åƒæ•¸ |
| RandomizedSearchCV | é«˜æ•ˆã€éˆæ´» | ç„¡æ™ºèƒ½ | 4-6 å€‹è¶…åƒæ•¸ |
| Optuna | æ™ºèƒ½ã€è¦–è¦ºåŒ–è±å¯Œ | éœ€é¡å¤–å®‰è£ | è¤‡é›œå„ªåŒ– |
| Hyperopt | æˆç†Ÿã€æ–‡ç»å¤š | èªæ³•è¤‡é›œ | å­¸è¡“ç ”ç©¶ |
| Ray Tune | åˆ†ä½ˆå¼ã€å¯æ“´å±• | å­¸ç¿’æ›²ç·šé™¡ | å¤§è¦æ¨¡è¨“ç·´ |

### 9.5 é€²éšå­¸ç¿’è³‡æº

#### ğŸ“š æ¨è–¦è«–æ–‡

1. **Bergstra & Bengio (2012)**: "Random Search for Hyper-Parameter Optimization"
   - è­‰æ˜ Random Search åœ¨é«˜ç¶­ç©ºé–“å„ªæ–¼ Grid Search

2. **Snoek et al. (2012)**: "Practical Bayesian Optimization of Machine Learning Algorithms"
   - Bayesian Optimization çš„ç¶“å…¸è«–æ–‡

3. **Li et al. (2017)**: "Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization"
   - Halving Search çš„ç†è«–åŸºç¤

#### ğŸ”— å·¥å…·æ–‡æª”

- Optuna: https://optuna.org/
- Hyperopt: http://hyperopt.github.io/hyperopt/
- Sklearn Model Selection: https://scikit-learn.org/stable/model_selection.html

#### ğŸ“ å­¸ç¿’è·¯å¾‘

1. **åˆå­¸è€…**ï¼šæŒæ¡ GridSearchCV å’Œ RandomizedSearchCV
2. **é€²éš**ï¼šå­¸ç¿’ Optunaï¼Œç†è§£ Bayesian Optimization åŸç†
3. **å°ˆå®¶**ï¼šå¯¦ä½œè‡ªå®šç¾©ç²å–å‡½æ•¸ï¼Œå¤šä¿çœŸåº¦å„ªåŒ–

### 9.6 èª²å¾Œç·´ç¿’

#### ç·´ç¿’ 1ï¼šæ¯”è¼ƒä¸‰ç¨®æ–¹æ³•

ä½¿ç”¨ Titanic æ•¸æ“šé›†ï¼Œæ¯”è¼ƒ Grid Search, Random Search, Bayesian Optimization åœ¨ Random Forest ä¸Šçš„æ€§èƒ½ã€‚

**è¦æ±‚**ï¼š
- è¨­å®šç›¸åŒçš„æœç´¢ç©ºé–“
- è¨˜éŒ„æ¯ç¨®æ–¹æ³•æ‰¾åˆ°æœ€å„ªè§£çš„æ™‚é–“
- ç¹ªè£½æ”¶æ–‚æ›²ç·š

#### ç·´ç¿’ 2ï¼šåŒ–å·¥æ‡‰ç”¨

ä½¿ç”¨èª²ç¨‹æä¾›çš„åæ‡‰å™¨æ•¸æ“šï¼Œå»ºç«‹ç”¢ç‡é æ¸¬æ¨¡å‹ã€‚

**è¦æ±‚**ï¼š
- å˜—è©¦è‡³å°‘ 3 ç¨®æ¨¡å‹ï¼ˆLinear, Random Forest, XGBoostï¼‰
- ä½¿ç”¨ Bayesian Optimization èª¿åƒ
- åˆ†æåƒæ•¸é‡è¦æ€§
- é©—è­‰æ¨¡å‹çš„åŒ–å­¸åˆç†æ€§

#### ç·´ç¿’ 3ï¼šå¤šç›®æ¨™å„ªåŒ–

å»ºç«‹ä¸€å€‹åŒæ™‚å„ªåŒ–æº–ç¢ºåº¦å’Œæ¨ç†é€Ÿåº¦çš„æ¨¡å‹ã€‚

**è¦æ±‚**ï¼š
- ä½¿ç”¨ Optuna å¤šç›®æ¨™å„ªåŒ–
- ç¹ªè£½ Pareto Front
- æ ¹æ“šæ¥­å‹™éœ€æ±‚é¸æ“‡æœ€çµ‚æ¨¡å‹

---

## ğŸ¯ é‡é»å›é¡§

1. **è¶…åƒæ•¸ vs æ¨¡å‹åƒæ•¸**ï¼š
   - è¶…åƒæ•¸ï¼šè¨“ç·´å‰è¨­å®šï¼ˆå¦‚ learning_rateï¼‰
   - æ¨¡å‹åƒæ•¸ï¼šè¨“ç·´ä¸­å­¸ç¿’ï¼ˆå¦‚æ¬Šé‡ï¼‰

2. **ä¸‰å¤§æœç´¢æ–¹æ³•**ï¼š
   - Grid Search: çª®èˆ‰ï¼Œé©åˆä½ç¶­
   - Random Search: éš¨æ©Ÿï¼Œé©åˆä¸­ç¶­
   - Bayesian Optimization: æ™ºèƒ½ï¼Œé©åˆé«˜ç¶­/æ˜‚è²´è©•ä¼°

3. **æœç´¢ç©ºé–“è¨­è¨ˆ**ï¼š
   - è·¨åº¦å¤§çš„åƒæ•¸ç”¨å°æ•¸å°ºåº¦
   - åˆ©ç”¨é ˜åŸŸçŸ¥è­˜ç¸®å°ç¯„åœ
   - è™•ç†æ¢ä»¶è¶…åƒæ•¸

4. **åŒ–å·¥æ‡‰ç”¨è¦é»**ï¼š
   - å¹³è¡¡æº–ç¢ºåº¦èˆ‡å¯è§£é‡‹æ€§
   - è€ƒæ…®å¯¦é©—æˆæœ¬
   - å¤šç›®æ¨™å„ªåŒ–

5. **å·¥å…·æ¨è–¦**ï¼š
   - å…¥é–€ï¼šSklearn GridSearchCV / RandomizedSearchCV
   - é€²éšï¼šOptunaï¼ˆæœ€æ¨è–¦ï¼‰

---

**ä¸‹ä¸€å–®å…ƒé å‘Š**ï¼šUnit15 å°‡æ•´åˆæ‰€æœ‰å­¸éçš„æŠ€è¡“ï¼Œé€²è¡Œå®Œæ•´çš„åŒ–å·¥æ¡ˆä¾‹å¯¦æˆ°ï¼

