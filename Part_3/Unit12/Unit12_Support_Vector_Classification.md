# Unit12 æ”¯æŒå‘é‡åˆ†é¡ | Support Vector Classification (SVC)

> **æœ€å¾Œæ›´æ–°**ï¼š2026-01-17

---

## å­¸ç¿’ç›®æ¨™

æœ¬ç¯€èª²å°‡æ·±å…¥å­¸ç¿’**æ”¯æŒå‘é‡åˆ†é¡ (Support Vector Classification, SVC)** æ¨¡å‹ï¼Œé€™æ˜¯æ©Ÿå™¨å­¸ç¿’ä¸­æœ€å¼·å¤§ä¸”æ‡‰ç”¨å»£æ³›çš„åˆ†é¡æ–¹æ³•ä¹‹ä¸€ã€‚é€šéæœ¬ç¯€èª²ï¼Œæ‚¨å°‡èƒ½å¤ ï¼š

- ç†è§£æ”¯æŒå‘é‡æ©Ÿçš„æ ¸å¿ƒæ¦‚å¿µèˆ‡å¹¾ä½•ç›´è§€
- æŒæ¡æ ¸å‡½æ•¸ (Kernel Function) çš„åŸç†èˆ‡æ‡‰ç”¨
- å­¸ç¿’è»Ÿé–“éš”èˆ‡ç¡¬é–“éš”çš„å·®ç•°
- æŒæ¡ sklearn ä¸­ `SVC` çš„ä½¿ç”¨æ–¹æ³•èˆ‡åƒæ•¸èª¿æ•´
- æ‡‰ç”¨ SVC è§£æ±ºåŒ–å·¥é ˜åŸŸçš„åˆ†é¡å•é¡Œ
- ç†è§£æ¨¡å‹çš„å„ªå‹¢ã€é™åˆ¶èˆ‡é©ç”¨å ´æ™¯

---

## 1. æ”¯æŒå‘é‡åˆ†é¡åŸºæœ¬æ¦‚å¿µ

### 1.1 ä»€éº¼æ˜¯æ”¯æŒå‘é‡åˆ†é¡ï¼Ÿ

**æ”¯æŒå‘é‡åˆ†é¡ (Support Vector Classification, SVC)** æ˜¯ä¸€ç¨®å¼·å¤§çš„ç›£ç£å­¸ç¿’ç®—æ³•ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨ç‰¹å¾µç©ºé–“ä¸­å°‹æ‰¾ä¸€å€‹**æœ€å„ªè¶…å¹³é¢ (Optimal Hyperplane)**ï¼Œä½¿å¾—ä¸åŒé¡åˆ¥ä¹‹é–“çš„**é–“éš” (Margin)** æœ€å¤§åŒ–ã€‚

### 1.2 æ ¸å¿ƒæ¦‚å¿µ

#### 1.2.1 æ±ºç­–é‚Šç•Œ (Decision Boundary)

åœ¨äºŒå…ƒåˆ†é¡å•é¡Œä¸­ï¼ŒSVC å°‹æ‰¾ä¸€å€‹è¶…å¹³é¢ä¾†åˆ†éš”å…©å€‹é¡åˆ¥ï¼š

$$
\mathbf{w}^T \mathbf{x} + b = 0
$$

å…¶ä¸­ï¼š
- $\mathbf{w}$ : æ³•å‘é‡ (Normal Vector)ï¼Œæ±ºå®šè¶…å¹³é¢çš„æ–¹å‘
- $b$ : åç½®é … (Bias)ï¼Œæ±ºå®šè¶…å¹³é¢çš„ä½ç½®
- $\mathbf{x}$ : ç‰¹å¾µå‘é‡

#### 1.2.2 é–“éš” (Margin)

**é–“éš”**æ˜¯æŒ‡æ±ºç­–é‚Šç•Œåˆ°æœ€è¿‘çš„è¨“ç·´æ¨£æœ¬çš„è·é›¢ã€‚SVC çš„ç›®æ¨™æ˜¯æœ€å¤§åŒ–é€™å€‹é–“éš”ã€‚

å°æ–¼é¡åˆ¥ $y_i \in \{-1, +1\}$ ï¼Œæˆ‘å€‘å¸Œæœ›ï¼š

$$
y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1, \quad \forall i
$$

é–“éš”å¯¬åº¦ç‚ºï¼š

$$
\text{Margin} = \frac{2}{\|\mathbf{w}\|}
$$

#### 1.2.3 æ”¯æŒå‘é‡ (Support Vectors)

**æ”¯æŒå‘é‡**æ˜¯é‚£äº›ä½æ–¼é–“éš”é‚Šç•Œä¸Šçš„è¨“ç·´æ¨£æœ¬ï¼Œå³æ»¿è¶³ï¼š

$$
y_i (\mathbf{w}^T \mathbf{x}_i + b) = 1
$$

é€™äº›æ¨£æœ¬å°æ±ºç­–é‚Šç•Œçš„ç¢ºå®šè‡³é—œé‡è¦ï¼Œå…¶ä»–æ¨£æœ¬å°æ¨¡å‹æ²’æœ‰å½±éŸ¿ã€‚

### 1.3 ç¡¬é–“éš” vs è»Ÿé–“éš”

#### ç¡¬é–“éš” SVM (Hard Margin SVM)

è¦æ±‚æ‰€æœ‰è¨“ç·´æ¨£æœ¬éƒ½è¢«æ­£ç¢ºåˆ†é¡ï¼Œä¸”åœ¨é–“éš”ä¹‹å¤–ï¼š

$$
\min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2
$$

$$
\text{s.t.} \quad y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1, \quad \forall i
$$

**é™åˆ¶**ï¼šåªé©ç”¨æ–¼ç·šæ€§å¯åˆ†æ•¸æ“šï¼Œå°å™ªè²å’Œç•°å¸¸å€¼æ¥µåº¦æ•æ„Ÿã€‚

#### è»Ÿé–“éš” SVM (Soft Margin SVM)

å…è¨±éƒ¨åˆ†æ¨£æœ¬é•åé–“éš”ç´„æŸï¼Œå¼•å…¥é¬†å¼›è®Šæ•¸ $\xi_i$ ï¼š

$$
\min_{\mathbf{w}, b, \xi} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^{m} \xi_i
$$

$$
\text{s.t.} \quad y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0, \quad \forall i
$$

å…¶ä¸­ï¼š
- $\xi_i$ : é¬†å¼›è®Šæ•¸ï¼Œè¡¨ç¤ºæ¨£æœ¬ $i$ é•åé–“éš”çš„ç¨‹åº¦
- $C$ : æ‡²ç½°åƒæ•¸ï¼Œæ§åˆ¶é–“éš”æœ€å¤§åŒ–èˆ‡èª¤åˆ†é¡ä¹‹é–“çš„æ¬Šè¡¡

**C åƒæ•¸çš„ä½œç”¨**ï¼š
- **C å¾ˆå¤§**ï¼šåš´æ ¼æ‡²ç½°é•åé–“éš”çš„æ¨£æœ¬ï¼Œå¯èƒ½éæ“¬åˆ
- **C å¾ˆå°**ï¼šå…è¨±æ›´å¤šé•åï¼Œé–“éš”æ›´å¤§ï¼Œå¯èƒ½æ¬ æ“¬åˆ

---

## 2. æ ¸å‡½æ•¸ (Kernel Functions)

### 2.1 æ ¸æŠ€å·§ (Kernel Trick)

å°æ–¼éç·šæ€§å¯åˆ†çš„æ•¸æ“šï¼ŒSVC ä½¿ç”¨**æ ¸æŠ€å·§**å°‡åŸå§‹ç‰¹å¾µç©ºé–“æ˜ å°„åˆ°æ›´é«˜ç¶­çš„ç‰¹å¾µç©ºé–“ï¼Œä½¿æ•¸æ“šåœ¨é«˜ç¶­ç©ºé–“ä¸­è®Šå¾—ç·šæ€§å¯åˆ†ã€‚

#### ç‰¹å¾µæ˜ å°„

å®šç¾©æ˜ å°„å‡½æ•¸ $\phi: \mathbb{R}^n \to \mathbb{R}^m$ ( $m >> n$ )ï¼Œå°‡ä½ç¶­æ•¸æ“šæ˜ å°„åˆ°é«˜ç¶­ç©ºé–“ã€‚

æ±ºç­–å‡½æ•¸è®Šç‚ºï¼š

$$
f(\mathbf{x}) = \text{sign}(\mathbf{w}^T \phi(\mathbf{x}) + b)
$$

#### æ ¸å‡½æ•¸å®šç¾©

æ ¸å‡½æ•¸ $K(\mathbf{x}_i, \mathbf{x}_j)$ ç›´æ¥è¨ˆç®—é«˜ç¶­ç©ºé–“ä¸­çš„å…§ç©ï¼Œè€Œç„¡éœ€é¡¯å¼è¨ˆç®—æ˜ å°„ $\phi$ ï¼š

$$
K(\mathbf{x}_i, \mathbf{x}_j) = \phi(\mathbf{x}_i)^T \phi(\mathbf{x}_j)
$$

é€™é¿å…äº†é«˜ç¶­è¨ˆç®—çš„å·¨å¤§é–‹éŠ·ï¼ˆç¶­åº¦ç½é›£ï¼‰ã€‚

### 2.2 å¸¸ç”¨æ ¸å‡½æ•¸

#### 2.2.1 ç·šæ€§æ ¸ (Linear Kernel)

$$
K(\mathbf{x}_i, \mathbf{x}_j) = \mathbf{x}_i^T \mathbf{x}_j
$$

- é©ç”¨æ–¼ç·šæ€§å¯åˆ†æˆ–è¿‘ä¼¼ç·šæ€§å¯åˆ†çš„æ•¸æ“š
- è¨ˆç®—é€Ÿåº¦æœ€å¿«
- ç­‰åŒæ–¼ä¸ä½¿ç”¨æ ¸å‡½æ•¸

#### 2.2.2 å¤šé …å¼æ ¸ (Polynomial Kernel)

$$
K(\mathbf{x}_i, \mathbf{x}_j) = (\gamma \mathbf{x}_i^T \mathbf{x}_j + r)^d
$$

åƒæ•¸ï¼š
- $d$ : å¤šé …å¼éšæ•¸ (degree)
- $\gamma$ : ç¸®æ”¾ä¿‚æ•¸
- $r$ : ç¨ç«‹é …ä¿‚æ•¸ (coef0)

ç‰¹é»ï¼š
- $d=1$ : ç­‰åŒæ–¼ç·šæ€§æ ¸
- $d=2$ : äºŒæ¬¡æ±ºç­–é‚Šç•Œ
- $d$ è¶Šå¤§ï¼Œæ¨¡å‹è¶Šè¤‡é›œï¼Œå®¹æ˜“éæ“¬åˆ

#### 2.2.3 é«˜æ–¯å¾‘å‘åŸºæ ¸ (RBF Kernel / Gaussian Kernel)

$$
K(\mathbf{x}_i, \mathbf{x}_j) = \exp\left(-\gamma \|\mathbf{x}_i - \mathbf{x}_j\|^2\right)
$$

å…¶ä¸­ $\gamma = \frac{1}{2\sigma^2}$ ã€‚

åƒæ•¸ï¼š
- $\gamma$ : æ±ºå®šæ¨£æœ¬å½±éŸ¿ç¯„åœ
  - **Î³ å¾ˆå¤§**ï¼šå½±éŸ¿ç¯„åœå°ï¼Œæ±ºç­–é‚Šç•Œè¤‡é›œï¼Œæ˜“éæ“¬åˆ
  - **Î³ å¾ˆå°**ï¼šå½±éŸ¿ç¯„åœå¤§ï¼Œæ±ºç­–é‚Šç•Œå¹³æ»‘ï¼Œå¯èƒ½æ¬ æ“¬åˆ

ç‰¹é»ï¼š
- **æœ€å¸¸ç”¨çš„æ ¸å‡½æ•¸**
- å¯ä»¥è™•ç†ä»»æ„è¤‡é›œçš„éç·šæ€§é—œä¿‚
- å°‡æ•¸æ“šæ˜ å°„åˆ°ç„¡é™ç¶­ç©ºé–“

#### 2.2.4 Sigmoid æ ¸ (Sigmoid Kernel)

$$
K(\mathbf{x}_i, \mathbf{x}_j) = \tanh(\gamma \mathbf{x}_i^T \mathbf{x}_j + r)
$$

ç‰¹é»ï¼š
- é¡ä¼¼ç¥ç¶“ç¶²çµ¡çš„æ¿€æ´»å‡½æ•¸
- ä¸ä¸€å®šæ»¿è¶³ Mercer æ¢ä»¶ï¼ˆæ­£å®šæ€§ï¼‰
- å¯¦éš›æ‡‰ç”¨è¼ƒå°‘

### 2.3 æ ¸å‡½æ•¸é¸æ“‡å»ºè­°

| æ•¸æ“šç‰¹æ€§ | æ¨è–¦æ ¸å‡½æ•¸ | åŸå›  |
|---------|-----------|------|
| ç·šæ€§å¯åˆ† | Linear | è¨ˆç®—å¿«é€Ÿï¼Œé¿å…éæ“¬åˆ |
| ç‰¹å¾µæ•¸ >> æ¨£æœ¬æ•¸ | Linear | é«˜ç¶­ç©ºé–“å·²ç¶“è¶³å¤  |
| å°æ¨£æœ¬ï¼Œéç·šæ€§ | RBF | éˆæ´»é©æ‡‰è¤‡é›œé‚Šç•Œ |
| æ˜ç¢ºçš„å¤šé …å¼é—œä¿‚ | Polynomial | ç¬¦åˆæ•¸æ“šç‰¹æ€§ |
| ä¸ç¢ºå®š | RBF (å…ˆè©¦) | é€šç”¨æ€§å¼·ï¼Œå¸¸ç‚ºæœ€ä½³é¸æ“‡ |

---

## 3. sklearn ä¸­çš„ SVC

### 3.1 åŸºæœ¬ä½¿ç”¨æ–¹æ³•

```python
from sklearn.svm import SVC

# å‰µå»ºæ¨¡å‹
model = SVC(kernel='rbf', C=1.0, gamma='scale')

# è¨“ç·´æ¨¡å‹
model.fit(X_train, y_train)

# é æ¸¬é¡åˆ¥
y_pred = model.predict(X_test)

# é æ¸¬æ©Ÿç‡ï¼ˆéœ€è¦è¨­ç½® probability=Trueï¼‰
model_proba = SVC(kernel='rbf', C=1.0, probability=True)
model_proba.fit(X_train, y_train)
y_proba = model_proba.predict_proba(X_test)
```

### 3.2 é‡è¦åƒæ•¸è©³è§£

#### 3.2.1 æ ¸å‡½æ•¸åƒæ•¸

**kernel** (é è¨­='rbf')
- `'linear'`: ç·šæ€§æ ¸ï¼Œ $K(\mathbf{x}, \mathbf{x'}) = \mathbf{x}^T \mathbf{x'}$ 
- `'poly'`: å¤šé …å¼æ ¸
- `'rbf'`: é«˜æ–¯å¾‘å‘åŸºæ ¸ï¼ˆé è¨­ï¼Œæœ€å¸¸ç”¨ï¼‰
- `'sigmoid'`: Sigmoid æ ¸
- è‡ªå®šç¾©æ ¸å‡½æ•¸ï¼ˆé€²éšï¼‰

**gamma** (é è¨­='scale')
- æ§åˆ¶ RBF, poly, sigmoid æ ¸çš„å½±éŸ¿ç¯„åœ
- `'scale'`: $\gamma = \frac{1}{n_{\text{features}} \times \text{Var}(X)}$ ï¼ˆæ¨è–¦ï¼‰
- `'auto'`: $\gamma = \frac{1}{n_{\text{features}}}$ 
- æ•¸å€¼ï¼šç›´æ¥æŒ‡å®š Î³ å€¼
- **Î³ å¤§**ï¼šæ¨¡å‹è¤‡é›œï¼Œæ˜“éæ“¬åˆ
- **Î³ å°**ï¼šæ¨¡å‹ç°¡å–®ï¼Œå¯èƒ½æ¬ æ“¬åˆ

**degree** (é è¨­=3ï¼Œåƒ…ç”¨æ–¼ poly æ ¸)
- å¤šé …å¼æ ¸çš„éšæ•¸

**coef0** (é è¨­=0.0ï¼Œç”¨æ–¼ poly å’Œ sigmoid æ ¸)
- æ ¸å‡½æ•¸ä¸­çš„ç¨ç«‹é …ä¿‚æ•¸

#### 3.2.2 æ­£å‰‡åŒ–åƒæ•¸

**C** (é è¨­=1.0)
- èª¤åˆ†é¡çš„æ‡²ç½°åƒæ•¸
- **C å¤§**ï¼š
  - åš´æ ¼æ‡²ç½°é•åé–“éš”çš„æ¨£æœ¬
  - é–“éš”è¼ƒå°ï¼Œè¤‡é›œæ±ºç­–é‚Šç•Œ
  - å®¹æ˜“éæ“¬åˆ
- **C å°**ï¼š
  - å…è¨±æ›´å¤šé•åï¼Œé–“éš”è¼ƒå¤§
  - å¹³æ»‘æ±ºç­–é‚Šç•Œ
  - å¯èƒ½æ¬ æ“¬åˆ
- èˆ‡ LogisticRegression ä¸­çš„ C æ„ç¾©ç›¸åŒï¼ˆéæ­£å‰‡åŒ–å¼·åº¦å€’æ•¸ï¼‰

#### 3.2.3 å¤šåˆ†é¡ç­–ç•¥

**decision_function_shape** (é è¨­='ovr')
- `'ovr'` (One-vs-Rest): æ¯å€‹é¡åˆ¥èˆ‡å…¶ä»–æ‰€æœ‰é¡åˆ¥æ¯”è¼ƒ
- `'ovo'` (One-vs-One): æ¯å…©å€‹é¡åˆ¥ä¹‹é–“æ§‹å»ºåˆ†é¡å™¨
  - å°æ–¼ k å€‹é¡åˆ¥ï¼Œéœ€è¦ $\frac{k(k-1)}{2}$ å€‹åˆ†é¡å™¨

#### 3.2.4 æ©Ÿç‡é æ¸¬

**probability** (é è¨­=False)
- æ˜¯å¦å•Ÿç”¨æ©Ÿç‡ä¼°è¨ˆ
- `True`: å¯ä½¿ç”¨ `predict_proba()` å’Œ `predict_log_proba()`
- ä½¿ç”¨ 5-fold äº¤å‰é©—è­‰é€²è¡Œæ ¡æº–ï¼ˆPlatt Scalingï¼‰
- æœƒå¢åŠ è¨“ç·´æ™‚é–“

#### 3.2.5 å…¶ä»–é‡è¦åƒæ•¸

**class_weight** (é è¨­=None)
- é¡åˆ¥æ¬Šé‡ï¼Œè™•ç†ä¸å¹³è¡¡æ•¸æ“š
- `'balanced'`: è‡ªå‹•èª¿æ•´æ¬Šé‡èˆ‡é¡åˆ¥é »ç‡æˆåæ¯”
- å­—å…¸ï¼šå¦‚ `{0: 1, 1: 3}` 

**random_state**
- éš¨æ©Ÿæ•¸ç¨®å­ï¼Œç¢ºä¿çµæœå¯é‡ç¾

**max_iter** (é è¨­=-1)
- æœ€å¤§è¿­ä»£æ¬¡æ•¸ï¼Œ-1 è¡¨ç¤ºç„¡é™åˆ¶

**cache_size** (é è¨­=200)
- æ ¸ç·©å­˜å¤§å°ï¼ˆMBï¼‰ï¼Œå¯åŠ é€Ÿè¨ˆç®—

**verbose**
- æ˜¯å¦è¼¸å‡ºè¨“ç·´éç¨‹ä¿¡æ¯

### 3.3 æ¨¡å‹å±¬æ€§

è¨“ç·´å®Œæˆå¾Œå¯å­˜å–ï¼š

```python
# æ”¯æŒå‘é‡
print(f'Support vectors: {model.support_vectors_}')  # shape: (n_support, n_features)

# æ”¯æŒå‘é‡çš„ç´¢å¼•
print(f'Support vector indices: {model.support_}')  # shape: (n_support,)

# æ¯å€‹é¡åˆ¥çš„æ”¯æŒå‘é‡æ•¸é‡
print(f'Number of support vectors per class: {model.n_support_}')

# å°å¶ä¿‚æ•¸
print(f'Dual coefficients: {model.dual_coef_}')  # shape: (n_classes-1, n_support)

# æˆªè·é …
print(f'Intercept: {model.intercept_}')  # shape: (n_classes * (n_classes-1) / 2,)

# é¡åˆ¥æ¨™ç±¤
print(f'Classes: {model.classes_}')
```

### 3.4 é æ¸¬æ–¹æ³•

```python
# é æ¸¬é¡åˆ¥æ¨™ç±¤
y_pred = model.predict(X_test)

# æ±ºç­–å‡½æ•¸å€¼ï¼ˆåˆ°æ±ºç­–é‚Šç•Œçš„è·é›¢ï¼‰
y_decision = model.decision_function(X_test)

# é æ¸¬æ©Ÿç‡ï¼ˆéœ€è¦ probability=Trueï¼‰
y_proba = model.predict_proba(X_test)  # shape: (n_samples, n_classes)

# é æ¸¬å°æ•¸æ©Ÿç‡
y_log_proba = model.predict_log_proba(X_test)
```

---

## 4. åŒ–å·¥é ˜åŸŸæ‡‰ç”¨å ´æ™¯

### 4.1 ç”¢å“å“è³ªäºŒå…ƒåˆ†é¡

**å•é¡Œ**ï¼šæ ¹æ“šè£½ç¨‹åƒæ•¸åˆ¤æ–·ç”¢å“æ˜¯å¦åˆæ ¼

**ç‰¹å¾µè®Šæ•¸**ï¼š
- åæ‡‰æº«åº¦ã€å£“åŠ›ã€æ™‚é–“
- åŸæ–™ç´”åº¦ã€æµé‡
- å‚¬åŒ–åŠ‘é¡å‹èˆ‡æ¿ƒåº¦

**ç›®æ¨™è®Šæ•¸**ï¼š
- åˆæ ¼ (1) / ä¸åˆæ ¼ (0)

**SVC å„ªå‹¢**ï¼š
- è™•ç†éç·šæ€§é—œä¿‚ï¼ˆä½¿ç”¨ RBF æ ¸ï¼‰
- å°ç•°å¸¸å€¼è¼ƒç‚ºç©©å¥ï¼ˆè»Ÿé–“éš”ï¼‰
- æ±ºç­–é‚Šç•Œæ¸…æ™°ï¼Œé©åˆäºŒå…ƒåˆ¤æ–·

### 4.2 å¤šç›¸æ…‹è­˜åˆ¥

**å•é¡Œ**ï¼šè­˜åˆ¥åŒ–å·¥æµç¨‹ä¸­çš„ä¸åŒç›¸æ…‹ï¼ˆæ°£ç›¸ã€æ¶²ç›¸ã€å…©ç›¸æµç­‰ï¼‰

**ç‰¹å¾µè®Šæ•¸**ï¼š
- å£“åŠ›æ³¢å‹•
- æº«åº¦æ¢¯åº¦
- å¯†åº¦æ¸¬é‡å€¼
- æµé€Ÿ

**ç›®æ¨™è®Šæ•¸**ï¼š
- ç›¸æ…‹é¡åˆ¥ï¼ˆæ°£ç›¸ / æ¶²ç›¸ / å…©ç›¸æµ / ä¸‰ç›¸æµï¼‰

**SVC é©ç”¨æ€§**ï¼š
- éç·šæ€§æ ¸èƒ½æ•æ‰è¤‡é›œç›¸è®Šç‰¹å¾µ
- å¤šåˆ†é¡å•é¡Œï¼ˆone-vs-rest æˆ– one-vs-oneï¼‰

### 4.3 è¨­å‚™æ•…éšœè¨ºæ–·

**å•é¡Œ**ï¼šæ ¹æ“šç›£æ¸¬ä¿¡è™Ÿåˆ¤æ–·è¨­å‚™æ˜¯å¦æ•…éšœ

**ç‰¹å¾µè®Šæ•¸**ï¼š
- æŒ¯å‹•é »è­œç‰¹å¾µ
- æº«åº¦è®ŠåŒ–æ¨¡å¼
- èƒ½è€—ç•°å¸¸æŒ‡æ¨™
- å™ªéŸ³ç‰¹å¾µ

**ç›®æ¨™è®Šæ•¸**ï¼š
- æ­£å¸¸ / æ•…éšœé¡å‹ 1 / æ•…éšœé¡å‹ 2 / ...

**SVC å„ªå‹¢**ï¼š
- æ”¯æŒé«˜ç¶­ç‰¹å¾µï¼ˆé »è­œæ•¸æ“šï¼‰
- å°æ¨£æœ¬ä¸‹è¡¨ç¾è‰¯å¥½
- æ±ºç­–å‡½æ•¸å€¼å¯ä½œç‚ºç•°å¸¸ç¨‹åº¦æŒ‡æ¨™

### 4.4 åæ‡‰è·¯å¾‘é¸æ“‡

**å•é¡Œ**ï¼šé æ¸¬åœ¨çµ¦å®šæ¢ä»¶ä¸‹åæ‡‰æœƒèµ°å“ªä¸€æ¢è·¯å¾‘

**ç‰¹å¾µè®Šæ•¸**ï¼š
- åæ‡‰ç‰©æ¿ƒåº¦æ¯”ä¾‹
- æº«åº¦ã€å£“åŠ›ã€æº¶åŠ‘æ€§è³ª
- å‚¬åŒ–åŠ‘ç¨®é¡

**ç›®æ¨™è®Šæ•¸**ï¼š
- ä¸»è¦ç”¢ç‰©è·¯å¾‘é¡åˆ¥

**SVC é©ç”¨åŸå› **ï¼š
- åŒ–å­¸åæ‡‰è·¯å¾‘é€šå¸¸å‘ˆç¾éç·šæ€§æ±ºç­–é‚Šç•Œ
- å¯è™•ç†é€£çºŒèˆ‡é¡åˆ¥æ··åˆç‰¹å¾µ

### 4.5 é¡è‰²/æ°£å‘³åˆ†é¡

**å•é¡Œ**ï¼šæ ¹æ“šå…‰è­œæˆ–æ„Ÿæ¸¬å™¨æ•¸æ“šå°ç”¢å“é¡è‰²æˆ–æ°£å‘³åˆ†é¡

**ç‰¹å¾µè®Šæ•¸**ï¼š
- å…‰è­œå¸æ”¶å³°ä½ç½®èˆ‡å¼·åº¦
- æ°£é«”æ„Ÿæ¸¬å™¨éŸ¿æ‡‰å€¼

**ç›®æ¨™è®Šæ•¸**ï¼š
- é¡è‰²ç­‰ç´š / æ°£å‘³é¡åˆ¥

**SVC å„ªå‹¢**ï¼š
- å…‰è­œæ•¸æ“šé«˜ç¶­ï¼ŒSVC è™•ç†æ•ˆæœå¥½
- å¯ç”¨ç·šæ€§æ ¸ï¼ˆPCAé™ç¶­å¾Œï¼‰æˆ–RBFæ ¸

---

## 5. å®Œæ•´å¯¦ä½œæ¡ˆä¾‹ï¼šåŒ–å­¸åæ‡‰æ¢ä»¶åˆ†é¡

### 5.1 å•é¡Œæè¿°

æŸåŒ–å·¥å» éœ€è¦æ ¹æ“šåæ‡‰æ¢ä»¶é æ¸¬åæ‡‰èƒ½å¦æˆåŠŸé€²è¡Œåˆ°ç›®æ¨™ç”¢ç‰©ã€‚æˆ‘å€‘å°‡ä½¿ç”¨ SVC å»ºç«‹åˆ†é¡æ¨¡å‹ï¼Œä¸¦æ¯”è¼ƒä¸åŒæ ¸å‡½æ•¸çš„æ•ˆæœã€‚

**ç‰¹å¾µè®Šæ•¸**ï¼š
- `temperature` (æº«åº¦, Â°C): åæ‡‰æº«åº¦
- `pressure` (å£“åŠ›, bar): åæ‡‰å£“åŠ›  
- `catalyst_concentration` (å‚¬åŒ–åŠ‘æ¿ƒåº¦, mol/L)
- `reactant_ratio` (åæ‡‰ç‰©æ¯”ä¾‹)
- `reaction_time` (åæ‡‰æ™‚é–“, hours)

**ç›®æ¨™è®Šæ•¸**ï¼š
- `success` (æˆåŠŸ, 0/1): 1 è¡¨ç¤ºåæ‡‰æˆåŠŸï¼Œ0 è¡¨ç¤ºå¤±æ•—

### 5.2 æ•¸æ“šç”Ÿæˆèˆ‡æ¢ç´¢

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import (
    classification_report, confusion_matrix, 
    roc_curve, roc_auc_score, accuracy_score
)

# è¨­å®šéš¨æ©Ÿç¨®å­
np.random.seed(42)

# ç”Ÿæˆæ¨¡æ“¬æ•¸æ“šï¼ˆå…·æœ‰éç·šæ€§é—œä¿‚ï¼‰
n_samples = 500

# ç”Ÿæˆç‰¹å¾µ
temperature = np.random.uniform(150, 250, n_samples)
pressure = np.random.uniform(5, 30, n_samples)
catalyst_concentration = np.random.uniform(0.01, 0.1, n_samples)
reactant_ratio = np.random.uniform(1.0, 3.0, n_samples)
reaction_time = np.random.uniform(2, 10, n_samples)

# ç”Ÿæˆéç·šæ€§ç›®æ¨™è®Šæ•¸ï¼ˆæ›´è¤‡é›œçš„æ±ºç­–é‚Šç•Œï¼‰
# ä½¿ç”¨å¤šé …å¼ç‰¹å¾µå’Œéç·šæ€§è®Šæ›
linear_comb = (
    0.05 * (temperature - 200) +
    0.15 * (pressure - 15) +
    50 * (catalyst_concentration - 0.055) +
    0.8 * (reactant_ratio - 2.0) +
    0.3 * (reaction_time - 6)
)

# æ·»åŠ éç·šæ€§é …
nonlinear_comb = (
    0.001 * (temperature - 200)**2 +
    0.01 * (pressure - 15)**2 +
    500 * (catalyst_concentration - 0.055)**2 +
    0.1 * (reactant_ratio - 2.0) * (pressure - 15) +  # äº¤äº’é …
    np.random.normal(0, 2, n_samples)  # å™ªéŸ³
)

# çµ„åˆç·šæ€§å’Œéç·šæ€§éƒ¨åˆ†
total_score = linear_comb + nonlinear_comb
probability = 1 / (1 + np.exp(-total_score))
success = (probability > 0.5).astype(int)

# å‰µå»º DataFrame
df = pd.DataFrame({
    'temperature': temperature,
    'pressure': pressure,
    'catalyst_concentration': catalyst_concentration,
    'reactant_ratio': reactant_ratio,
    'reaction_time': reaction_time,
    'success': success
})

print("="*60)
print("æ•¸æ“šé›†æ¦‚è¦½")
print("="*60)
print(df.head(10))
print(f"\næ•¸æ“šé›†å½¢ç‹€: {df.shape}")
print(f"\né¡åˆ¥åˆ†ä½ˆ:")
print(df['success'].value_counts())
print(f"æˆåŠŸç‡: {df['success'].mean():.2%}")
```

**åŸ·è¡Œçµæœ**ï¼š

```
============================================================
æ•¸æ“šé›†æ¦‚è¦½
============================================================
   temperature   pressure  catalyst_concentration  reactant_ratio  \
0   187.454012  22.454043                0.026662        2.038164   
1   245.071431  18.402409                0.058771        1.958364   
2   223.199394  12.738190                0.088565        1.051284   
3   209.865848  25.344875                0.075900        1.682496   
4   165.601864  22.118279                0.082591        1.760391   

   reaction_time  success  
0       4.093645        1  
1       3.975830        1  
2       9.250037        1  
3       3.996370        1  
4       4.175598        1  

æ•¸æ“šé›†å½¢ç‹€: (500, 6)

é¡åˆ¥åˆ†ä½ˆ:
success
1    365
0    135
Name: count, dtype: int64
æˆåŠŸç‡: 73.00%
```

**æ•¸æ“šçµ±è¨ˆåˆ†æ**ï¼š

| ç‰¹å¾µ | æœ€å°å€¼ | æœ€å¤§å€¼ | å–®ä½ |
|------|--------|--------|------|
| temperature | 150 | 250 | Â°C |
| pressure | 5 | 30 | bar |
| catalyst_concentration | 0.01 | 0.1 | mol/L |
| reactant_ratio | 1.0 | 3.0 | - |
| reaction_time | 2 | 10 | hours |

**é¡åˆ¥åˆ†ä½ˆç‰¹å¾µ**ï¼š
- **æˆåŠŸæ¨£æœ¬ (1)**ï¼š365 å€‹ (73.0%)
- **å¤±æ•—æ¨£æœ¬ (0)**ï¼š135 å€‹ (27.0%)
- **é¡åˆ¥ä¸å¹³è¡¡æ¯”ä¾‹**ï¼šç´„ 2.7:1

é€™æ˜¯ä¸€å€‹å…¸å‹çš„**ä¸å¹³è¡¡åˆ†é¡å•é¡Œ**ï¼Œå¤±æ•—æ¨£æœ¬æ•¸é‡åƒ…ç‚ºæˆåŠŸæ¨£æœ¬çš„ 1/3ã€‚åœ¨åŒ–å·¥é ˜åŸŸï¼Œé€™ç¨®æƒ…æ³å¾ˆå¸¸è¦‹ï¼Œå› ç‚ºï¼š
- æˆåŠŸçš„åæ‡‰æ¢ä»¶ç¶“éå„ªåŒ–ï¼Œå‡ºç¾é »ç‡è¼ƒé«˜
- å¤±æ•—æ¡ˆä¾‹è¼ƒå°‘ä½†æ›´éœ€è¦è¢«æº–ç¢ºè­˜åˆ¥ï¼ˆé¿å…è³‡æºæµªè²»ï¼‰
- éœ€è¦ç‰¹åˆ¥æ³¨æ„**Failure é¡åˆ¥çš„å¬å›ç‡** (Recall)ï¼Œè€Œéåƒ…é—œæ³¨æ•´é«”æº–ç¢ºç‡

### 5.3 æ•¸æ“šé è™•ç†

```python
# åˆ†é›¢ç‰¹å¾µå’Œç›®æ¨™è®Šæ•¸
X = df.drop('success', axis=1)
y = df['success']

# åˆ†å‰²è¨“ç·´é›†å’Œæ¸¬è©¦é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("="*60)
print("æ•¸æ“šåˆ†å‰²")
print("="*60)
print(f"è¨“ç·´é›†å¤§å°: {X_train.shape}")
print(f"æ¸¬è©¦é›†å¤§å°: {X_test.shape}")
print(f"è¨“ç·´é›†æˆåŠŸç‡: {y_train.mean():.2%}")
print(f"æ¸¬è©¦é›†æˆåŠŸç‡: {y_test.mean():.2%}")

# ç‰¹å¾µæ¨™æº–åŒ–ï¼ˆSVC å°ç‰¹å¾µå°ºåº¦æ•æ„Ÿï¼‰
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# è½‰æ›ç‚º DataFrame
X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)
```

**åŸ·è¡Œçµæœ**ï¼š

```
============================================================
æ•¸æ“šåˆ†å‰²
============================================================
è¨“ç·´é›†å¤§å°: (400, 5)
æ¸¬è©¦é›†å¤§å°: (100, 5)
è¨“ç·´é›†æˆåŠŸç‡: 73.00%
æ¸¬è©¦é›†æˆåŠŸç‡: 73.00%
```

**æ•¸æ“šå¯è¦–åŒ–**ï¼š

![ç‰¹å¾µåˆ†ä½ˆ](outputs/P3_Unit12_Support_Vector_Classification/figs/feature_distributions.png)

**åœ–è¡¨è§£é‡‹**ï¼š

1. **ç‰¹å¾µåˆ†ä½ˆ (å·¦å´ 5 å€‹å­åœ–)**ï¼š
   - æ‰€æœ‰ç‰¹å¾µå‘ˆç¾å‡å‹»åˆ†ä½ˆï¼ˆUniform Distributionï¼‰ï¼Œç¬¦åˆç”Ÿæˆè¨­å®š
   - **Temperature**ï¼šé›†ä¸­åœ¨ 150-250Â°Cï¼Œæ¶µè“‹å¸¸è¦‹çš„åŒ–å­¸åæ‡‰æº«åº¦ç¯„åœ
   - **Pressure**ï¼š5-30 barï¼Œè·¨è¶Šä½å£“åˆ°ä¸­å£“ç¯„åœ
   - **Catalyst Concentration**ï¼š0.01-0.1 mol/Lï¼Œä½æ¿ƒåº¦å‚¬åŒ–åŠ‘å…¸å‹ç¯„åœ
   - **Reactant Ratio**ï¼š1-3 å€ï¼Œæ¶µè“‹åŒ–å­¸è¨ˆé‡æ¯”èˆ‡éé‡åæ‡‰ç‰©æƒ…æ³
   - **Reaction Time**ï¼š2-10 å°æ™‚ï¼Œåˆç†çš„åæ‡‰æ™‚é–“çª—å£

2. **é¡åˆ¥æ¯”ä¾‹ (å³å´åœ“é¤…åœ–)**ï¼š
   - **Success (1)**: 73.0% - ç¶ è‰²å€åŸŸ
   - **Failure (0)**: 27.0% - ç´…è‰²å€åŸŸ
   - æ˜é¡¯çš„é¡åˆ¥ä¸å¹³è¡¡ï¼Œéœ€è¦åœ¨æ¨¡å‹è¨“ç·´æ™‚ç‰¹åˆ¥è™•ç†

![ç‰¹å¾µç›¸é—œæ€§ç†±åœ–](outputs/P3_Unit12_Support_Vector_Classification/figs/correlation_heatmap.png)

**ç›¸é—œæ€§åˆ†æ**ï¼š

- **æ‰€æœ‰ç‰¹å¾µä¹‹é–“çš„ç›¸é—œä¿‚æ•¸å‡æ¥µä½** (|r| < 0.1)
- è¡¨ç¤ºç‰¹å¾µä¹‹é–“ç›¸äº’ç¨ç«‹ï¼Œç„¡å…±ç·šæ€§å•é¡Œ
- é€™æ˜¯ç†æƒ³çš„ç‰¹å¾µé›†åˆï¼Œæ¯å€‹ç‰¹å¾µéƒ½æä¾›ç¨ç«‹çš„è³‡è¨Š
- **åŒ–å·¥æ„ç¾©**ï¼šæº«åº¦ã€å£“åŠ›ã€å‚¬åŒ–åŠ‘æ¿ƒåº¦ã€åæ‡‰ç‰©æ¯”ä¾‹ã€åæ‡‰æ™‚é–“éƒ½æ˜¯å¯ç¨ç«‹èª¿æ§çš„æ“ä½œè®Šæ•¸
- SVC åœ¨é€™ç¨®ä½ç›¸é—œæ€§ç‰¹å¾µä¸Šè¡¨ç¾è‰¯å¥½ï¼Œç„¡éœ€é€²è¡Œç‰¹å¾µé¸æ“‡æˆ–é™ç¶­

### 5.4 æ¨¡å‹è¨“ç·´ï¼šç·šæ€§æ ¸

```python
print("\n" + "="*60)
print("ç·šæ€§æ ¸ SVC è¨“ç·´")
print("="*60)

# å‰µå»ºç·šæ€§æ ¸ SVC
svc_linear = SVC(
    kernel='linear',
    C=1.0,
    random_state=42
)

# è¨“ç·´æ¨¡å‹
svc_linear.fit(X_train_scaled, y_train)

# é æ¸¬
y_train_pred_linear = svc_linear.predict(X_train_scaled)
y_test_pred_linear = svc_linear.predict(X_test_scaled)

# è©•ä¼°
train_acc_linear = accuracy_score(y_train, y_train_pred_linear)
test_acc_linear = accuracy_score(y_test, y_test_pred_linear)

print(f"è¨“ç·´é›†æº–ç¢ºç‡: {train_acc_linear:.4f}")
print(f"æ¸¬è©¦é›†æº–ç¢ºç‡: {test_acc_linear:.4f}")
print(f"æ”¯æŒå‘é‡æ•¸é‡: {len(svc_linear.support_)}")
print(f"æ”¯æŒå‘é‡æ¯”ä¾‹: {len(svc_linear.support_)/len(X_train):.2%}")
```

**åŸ·è¡Œçµæœ**ï¼š

```
============================================================
ç·šæ€§æ ¸ SVC è¨“ç·´
============================================================
è¨“ç·´é›†æº–ç¢ºç‡: 0.8225
æ¸¬è©¦é›†æº–ç¢ºç‡: 0.8000
æ”¯æŒå‘é‡æ•¸é‡: 173
æ”¯æŒå‘é‡æ¯”ä¾‹: 43.25%
```

**çµæœåˆ†æ**ï¼š

- **æº–ç¢ºç‡**ï¼šæ¸¬è©¦é›† 80.0%ï¼Œè¨“ç·´é›† 82.25%ï¼Œå·®ç•°å° (2.25%)ï¼Œç„¡æ˜é¡¯éæ“¬åˆ
- **æ”¯æŒå‘é‡æ•¸é‡**ï¼š173 å€‹ï¼Œä½”è¨“ç·´é›† 43.25%
  - é€™å€‹æ¯”ä¾‹ç›¸å°è¼ƒé«˜ï¼Œè¡¨ç¤ºæ•¸æ“šä¸æ˜¯å®Œå…¨ç·šæ€§å¯åˆ†
  - è¨±å¤šæ¨£æœ¬é è¿‘æ±ºç­–é‚Šç•Œï¼Œéœ€è¦åƒèˆ‡å®šç¾©è¶…å¹³é¢
- **æ¨¡å‹ç‰¹æ€§**ï¼š
  - ç·šæ€§æ ¸é©åˆä½œç‚º baselineï¼Œè¨ˆç®—é€Ÿåº¦å¿«
  - 80% æº–ç¢ºç‡è¡¨ç¤ºæ•¸æ“šå…·æœ‰ä¸€å®šç·šæ€§å¯åˆ†æ€§ï¼Œä½†ä»æœ‰æ”¹é€²ç©ºé–“
  - æ¥ä¸‹ä¾†ä½¿ç”¨éç·šæ€§æ ¸ï¼ˆRBFï¼‰ä¾†æ•æ‰æ›´è¤‡é›œçš„æ±ºç­–é‚Šç•Œ

### 5.5 æ¨¡å‹è¨“ç·´ï¼šRBF æ ¸

```python
print("\n" + "="*60)
print("RBF æ ¸ SVC è¨“ç·´")
print("="*60)

# å‰µå»º RBF æ ¸ SVC
svc_rbf = SVC(
    kernel='rbf',
    C=1.0,
    gamma='scale',
    random_state=42
)

# è¨“ç·´æ¨¡å‹
svc_rbf.fit(X_train_scaled, y_train)

# é æ¸¬
y_train_pred_rbf = svc_rbf.predict(X_train_scaled)
y_test_pred_rbf = svc_rbf.predict(X_test_scaled)

# è©•ä¼°
train_acc_rbf = accuracy_score(y_train, y_train_pred_rbf)
test_acc_rbf = accuracy_score(y_test, y_test_pred_rbf)

print(f"è¨“ç·´é›†æº–ç¢ºç‡: {train_acc_rbf:.4f}")
print(f"æ¸¬è©¦é›†æº–ç¢ºç‡: {test_acc_rbf:.4f}")
print(f"æ”¯æŒå‘é‡æ•¸é‡: {len(svc_rbf.support_)}")
print(f"æ”¯æŒå‘é‡æ¯”ä¾‹: {len(svc_rbf.support_)/len(X_train):.2%}")
```

**åŸ·è¡Œçµæœ**ï¼š

```
============================================================
RBF æ ¸ SVC è¨“ç·´
============================================================
è¨“ç·´é›†æº–ç¢ºç‡: 0.8525
æ¸¬è©¦é›†æº–ç¢ºç‡: 0.8100
æ”¯æŒå‘é‡æ•¸é‡: 190
æ”¯æŒå‘é‡æ¯”ä¾‹: 47.50%
```

**çµæœåˆ†æ**ï¼š

- **æº–ç¢ºç‡æå‡**ï¼š
  - æ¸¬è©¦é›†å¾ 80.0% (ç·šæ€§æ ¸) â†’ **81.0% (RBFæ ¸)**ï¼Œæå‡ 1%
  - è¨“ç·´é›†å¾ 82.25% â†’ 85.25%ï¼Œæå‡ 3%
- **æ”¯æŒå‘é‡æ•¸é‡**ï¼š190 å€‹ (47.5%)ï¼Œæ¯”ç·šæ€§æ ¸å¤š 17 å€‹
  - RBF æ ¸ä½¿ç”¨æ›´å¤šæ”¯æŒå‘é‡ä¾†æ§‹å»ºè¤‡é›œçš„éç·šæ€§æ±ºç­–é‚Šç•Œ
  - é€™è¡¨ç¤º RBF æ ¸æˆåŠŸæ•æ‰åˆ°æ•¸æ“šä¸­çš„éç·šæ€§æ¨¡å¼
- **æ³›åŒ–èƒ½åŠ›**ï¼šè¨“ç·´é›†èˆ‡æ¸¬è©¦é›†å·®ç•° 4.25%ï¼Œç•¥å¤§æ–¼ç·šæ€§æ ¸ä½†ä»åœ¨åˆç†ç¯„åœ
- **åŒ–å·¥æ„ç¾©**ï¼š
  - RBF æ ¸èƒ½å¤ æ¨¡æ“¬åŒ–å­¸åæ‡‰æ¢ä»¶ä¹‹é–“çš„éç·šæ€§äº¤äº’ä½œç”¨
  - ä¾‹å¦‚ï¼šæº«åº¦èˆ‡å£“åŠ›çš„å”åŒæ•ˆæ‡‰ã€å‚¬åŒ–åŠ‘æ¿ƒåº¦çš„æœ€å„ªçª—å£ç­‰

### 5.6 æ¨¡å‹è¨“ç·´ï¼šå¤šé …å¼æ ¸

```python
print("\n" + "="*60)
print("å¤šé …å¼æ ¸ (degree=2) SVC è¨“ç·´")
print("="*60)

# å‰µå»ºå¤šé …å¼æ ¸ SVC
svc_poly = SVC(
    kernel='poly',
    degree=2,
    C=1.0,
    gamma='scale',
    random_state=42
)

# è¨“ç·´æ¨¡å‹
svc_poly.fit(X_train_scaled, y_train)

# é æ¸¬
y_train_pred_poly = svc_poly.predict(X_train_scaled)
y_test_pred_poly = svc_poly.predict(X_test_scaled)

# è©•ä¼°
train_acc_poly = accuracy_score(y_train, y_train_pred_poly)
test_acc_poly = accuracy_score(y_test, y_test_pred_poly)

print(f"è¨“ç·´é›†æº–ç¢ºç‡: {train_acc_poly:.4f}")
print(f"æ¸¬è©¦é›†æº–ç¢ºç‡: {test_acc_poly:.4f}")
print(f"æ”¯æŒå‘é‡æ•¸é‡: {len(svc_poly.support_)}")
print(f"æ”¯æŒå‘é‡æ¯”ä¾‹: {len(svc_poly.support_)/len(X_train):.2%}")
```

**åŸ·è¡Œçµæœ**ï¼š

```
============================================================
å¤šé …å¼æ ¸ (degree=2) SVC è¨“ç·´
============================================================
è¨“ç·´é›†æº–ç¢ºç‡: 0.7300
æ¸¬è©¦é›†æº–ç¢ºç‡: 0.7300
æ”¯æŒå‘é‡æ•¸é‡: 246
æ”¯æŒå‘é‡æ¯”ä¾‹: 61.50%
```

**çµæœåˆ†æ**ï¼š

- **æº–ç¢ºç‡**ï¼š73.0%ï¼Œé¡¯è‘—ä½æ–¼ç·šæ€§æ ¸ (80%) å’Œ RBF æ ¸ (81%)
  - è¨“ç·´é›†èˆ‡æ¸¬è©¦é›†å®Œå…¨ç›¸åŒï¼Œè¡¨ç¤ºæ¨¡å‹å¯èƒ½éæ–¼ç°¡å–®ï¼ˆæ¬ æ“¬åˆï¼‰
  - äºŒæ¬¡å¤šé …å¼æ ¸ç„¡æ³•å……åˆ†æ•æ‰æ•¸æ“šçš„è¤‡é›œæ¨¡å¼
- **æ”¯æŒå‘é‡æ•¸é‡**ï¼š246 å€‹ (61.5%)ï¼Œæ˜¯ä¸‰å€‹æ ¸ä¸­æœ€é«˜çš„
  - é«˜é” 61.5% çš„æ¨£æœ¬æˆç‚ºæ”¯æŒå‘é‡ï¼Œè¡¨ç¤ºæ¨¡å‹é›£ä»¥æ‰¾åˆ°æ¸…æ™°çš„æ±ºç­–é‚Šç•Œ
  - æ¨¡å‹åœ¨åŠªåŠ›æ“¬åˆæ•¸æ“šä½†æ•ˆæœä¸ä½³
- **ç‚ºä½•è¡¨ç¾ä¸ä½³ï¼Ÿ**
  - æ•¸æ“šçš„éç·šæ€§æ¨¡å¼å¯èƒ½ä¸ç¬¦åˆäºŒæ¬¡å¤šé …å¼å½¢å¼
  - RBF æ ¸çš„éˆæ´»æ€§æ›´é©åˆæ•æ‰ä»»æ„éç·šæ€§é—œä¿‚
  - å¤šé …å¼æ ¸å°ç‰¹å¾µå°ºåº¦æ›´æ•æ„Ÿï¼Œå¯èƒ½éœ€è¦æ›´ç´°ç·»çš„æ¨™æº–åŒ–
- **å¯¦å‹™å»ºè­°**ï¼šåœ¨åŒ–å·¥æ‡‰ç”¨ä¸­ï¼Œå¦‚ç„¡ç‰¹å®šç†ç”±ï¼ˆå¦‚å·²çŸ¥ç‰©ç†æ¨¡å‹ç‚ºå¤šé …å¼å½¢å¼ï¼‰ï¼Œ**å„ªå…ˆé¸æ“‡ RBF æ ¸**

### 5.7 æ¨¡å‹æ¯”è¼ƒ

```python
# æ¯”è¼ƒä¸åŒæ ¸å‡½æ•¸çš„è¡¨ç¾
results = pd.DataFrame({
    'Kernel': ['Linear', 'RBF', 'Polynomial'],
    'Train Accuracy': [train_acc_linear, train_acc_rbf, train_acc_poly],
    'Test Accuracy': [test_acc_linear, test_acc_rbf, test_acc_poly],
    'Support Vectors': [
        len(svc_linear.support_),
        len(svc_rbf.support_),
        len(svc_poly.support_)
    ]
})

print("\n" + "="*60)
print("ä¸åŒæ ¸å‡½æ•¸æ€§èƒ½æ¯”è¼ƒ")
print("="*60)
print(results)

# å¯è¦–åŒ–æ¯”è¼ƒ
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# æº–ç¢ºç‡æ¯”è¼ƒ
results.plot(x='Kernel', y=['Train Accuracy', 'Test Accuracy'], 
             kind='bar', ax=axes[0], rot=0)
axes[0].set_ylabel('Accuracy')
axes[0].set_title('Accuracy Comparison')
axes[0].legend(['Train', 'Test'])
axes[0].set_ylim([0.7, 1.0])
axes[0].grid(alpha=0.3)

# æ”¯æŒå‘é‡æ•¸é‡æ¯”è¼ƒ
results.plot(x='Kernel', y='Support Vectors', kind='bar', ax=axes[1], 
             rot=0, legend=False, color='orange')
axes[1].set_ylabel('Number of Support Vectors')
axes[1].set_title('Support Vectors Comparison')
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.savefig('svc_kernel_comparison.png', dpi=300, bbox_inches='tight')
plt.show()
```

**åŸ·è¡Œçµæœ**ï¼š

```
============================================================
æ¨¡å‹æ€§èƒ½æ¯”è¼ƒï¼ˆå«é¡åˆ¥æ¬Šé‡å¹³è¡¡ï¼‰
============================================================
            Model  Test Accuracy  Support Vectors
0          Linear           0.80              173
1             RBF           0.81              190
2  RBF (Balanced)           0.79              209
3      Polynomial           0.73              246

ã€Failure é¡åˆ¥å¬å›ç‡æ¯”è¼ƒã€‘
RBF (åŸå§‹):   40.74%
RBF (å¹³è¡¡):   59.26%
æ”¹å–„å¹…åº¦:     18.52%
```

**é—œéµç™¼ç¾**ï¼š

1. **æº–ç¢ºç‡æ’å**ï¼š
   - **RBF æ ¸è¡¨ç¾æœ€ä½³** (81.0%)
   - ç·šæ€§æ ¸æ¬¡ä¹‹ (80.0%)
   - RBF Balanced ç•¥ä½ (79.0%)
   - å¤šé …å¼æ ¸æœ€å·® (73.0%)

2. **é¡åˆ¥ä¸å¹³è¡¡è™•ç†çš„é‡è¦æ€§**ï¼š
   - ä½¿ç”¨ `class_weight='balanced'` å¾Œï¼Œ**Failure é¡åˆ¥å¬å›ç‡**å¾ 40.74% â†’ **59.26%**
   - **æå‡ 18.52 å€‹ç™¾åˆ†é»**ï¼Œæ¥è¿‘ 50% çš„ç›¸å°æ”¹å–„
   - åƒ…çŠ§ç‰² 2% æ•´é«”æº–ç¢ºç‡ (81% â†’ 79%)

3. **æ”¯æŒå‘é‡æ•¸é‡è®ŠåŒ–**ï¼š
   - ç·šæ€§æ ¸ï¼š173 (43.25%)
   - RBF æ ¸ï¼š190 (47.50%)
   - RBF Balancedï¼š209 (52.25%)
   - å¤šé …å¼æ ¸ï¼š246 (61.50%)
   - è¶Šè¤‡é›œçš„æ¨¡å‹éœ€è¦è¶Šå¤šæ”¯æŒå‘é‡ä¾†å®šç¾©æ±ºç­–é‚Šç•Œ

![æ ¸å‡½æ•¸æ€§èƒ½æ¯”è¼ƒ](outputs/P3_Unit12_Support_Vector_Classification/figs/kernel_comparison.png)

**åœ–è¡¨è§£é‡‹**ï¼š

- **å·¦åœ– - æº–ç¢ºç‡æ¯”è¼ƒ**ï¼š
  - RBF æ ¸åœ¨è¨“ç·´é›†å’Œæ¸¬è©¦é›†ä¸Šéƒ½è¡¨ç¾æœ€ä½³
  - æ‰€æœ‰æ¨¡å‹çš„æ¸¬è©¦é›†æº–ç¢ºç‡éƒ½æ¥è¿‘è¨“ç·´é›†ï¼Œé¡¯ç¤ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›
  - å¤šé …å¼æ ¸è¡¨ç¾æœ€å·®ï¼Œä¸é©åˆæœ¬æ•¸æ“šé›†

- **å³åœ– - æ”¯æŒå‘é‡æ•¸é‡**ï¼š
  - æ”¯æŒå‘é‡æ•¸é‡å¾å°‘åˆ°å¤šï¼šLinear < RBF < RBF Balanced < Polynomial
  - æ”¯æŒå‘é‡è¶Šå¤šè¡¨ç¤ºæ±ºç­–é‚Šç•Œè¶Šè¤‡é›œæˆ–æ•¸æ“šæ›´é›£åˆ†é›¢
  - **æœ€ä½³å¹³è¡¡**ï¼šRBF æ ¸ä½¿ç”¨ä¸­ç­‰æ•¸é‡çš„æ”¯æŒå‘é‡ (47.5%) é”åˆ°æœ€é«˜æº–ç¢ºç‡

**åŒ–å·¥é ˜åŸŸå¯¦å‹™å»ºè­°**ï¼š

1. **æ¨¡å‹é¸æ“‡**ï¼š
   - åœ¨è¿½æ±‚æ•´é«”æº–ç¢ºç‡æ™‚ï¼Œé¸æ“‡ **RBF æ ¸** (81%)
   - åœ¨éœ€è¦é¿å…æ¼æª¢ Failure æ¡ˆä¾‹æ™‚ï¼Œé¸æ“‡ **RBF Balanced** (Failure å¬å›ç‡ 59.26%)

2. **æˆæœ¬è€ƒé‡**ï¼š
   - å¦‚æœåæ‡‰å¤±æ•—çš„æˆæœ¬é é«˜æ–¼åŸæ–™è³‡æ–™ï¼Œå„ªå…ˆä½¿ç”¨ Balanced æ¨¡å‹
   - å¦‚æœèª¤åˆ¤æˆåŠŸç‚ºå¤±æ•—çš„æˆæœ¬å¾ˆé«˜ï¼Œä½¿ç”¨æ¨™æº– RBF æ¨¡å‹

3. **å¢åŠ  Failure å¬å›ç‡çš„æ–¹æ³•**ï¼š
   - ä½¿ç”¨ `class_weight='balanced'` åƒæ•¸
   - èª¿æ•´æ±ºç­–é–¾å€¼ï¼ˆå¦‚æœä½¿ç”¨æ©Ÿç‡é æ¸¬ï¼‰
   - æ”¶é›†æ›´å¤š Failure æ¨£æœ¬é€²è¡Œè¨“ç·´

### 5.8 è©³ç´°è©•ä¼°ï¼ˆä½¿ç”¨æœ€ä½³æ¨¡å‹ - RBF æ ¸ï¼‰

```python
print("\n" + "="*60)
print("RBF æ ¸ SVC è©³ç´°è©•ä¼°")
print("="*60)

# åˆ†é¡å ±å‘Š
print("\næ¸¬è©¦é›†åˆ†é¡å ±å‘Š:")
print(classification_report(y_test, y_test_pred_rbf, 
                           target_names=['Failure', 'Success']))

# æ··æ·†çŸ©é™£
cm = confusion_matrix(y_test, y_test_pred_rbf)
print("\næ··æ·†çŸ©é™£:")
print(cm)

# å¯è¦–åŒ–æ··æ·†çŸ©é™£
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],
            xticklabels=['Failure', 'Success'],
            yticklabels=['Failure', 'Success'])
axes[0].set_xlabel('Predicted Label')
axes[0].set_ylabel('True Label')
axes[0].set_title('Confusion Matrix')

cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', ax=axes[1],
            xticklabels=['Failure', 'Success'],
            yticklabels=['Failure', 'Success'])
axes[1].set_xlabel('Predicted Label')
axes[1].set_ylabel('True Label')
axes[1].set_title('Normalized Confusion Matrix')

plt.tight_layout()
plt.savefig('svc_confusion_matrix.png', dpi=300, bbox_inches='tight')
plt.show()
```

**åŸ·è¡Œçµæœ**ï¼š

```
============================================================
RBF æ ¸ SVC è©³ç´°è©•ä¼°
============================================================

æ¸¬è©¦é›†åˆ†é¡å ±å‘Š:
              precision    recall  f1-score   support

     Failure       0.79      0.41      0.54        27
     Success       0.81      0.96      0.88        73

    accuracy                           0.81       100
   macro avg       0.80      0.68      0.71       100
weighted avg       0.81      0.81      0.79       100


æ··æ·†çŸ©é™£:
[[11 16]
 [ 3 70]]
```

![æ··æ·†çŸ©é™£](outputs/P3_Unit12_Support_Vector_Classification/figs/confusion_matrix.png)

**è©³ç´°åˆ†æ**ï¼š

1. **æ··æ·†çŸ©é™£è®€æ³•** (å·¦åœ– - åŸå§‹æ•¸é‡)ï¼š
   ```
   é æ¸¬çµæœ      Failure    Success
   å¯¦éš›ç‹€æ³
   Failure (27)      11 (TN)    16 (FP)
   Success (73)       3 (FN)    70 (TP)
   ```
   - **True Negative (TN)**: 11 - æ­£ç¢ºé æ¸¬ç‚ºå¤±æ•—
   - **False Positive (FP)**: 16 - èª¤åˆ¤æˆåŠŸç‚ºå¤±æ•— (å‹ I éŒ¯èª¤)
   - **False Negative (FN)**: 3 - æ¼æª¢å¤±æ•— (å‹ II éŒ¯èª¤) â† **åŒ–å·¥ä¸Šæœ€å±éšª**
   - **True Positive (TP)**: 70 - æ­£ç¢ºé æ¸¬ç‚ºæˆåŠŸ

2. **æ€§èƒ½æŒ‡æ¨™è©³ç´°è§£é‡‹**ï¼š

   **Failure é¡åˆ¥** (å°‘æ•¸é¡ï¼Œæ›´é‡è¦)ï¼š
   - **Precision (Failure)**: 0.79 (79%)
     - åœ¨é æ¸¬ç‚º Failure çš„ 14 å€‹æ¨£æœ¬ä¸­ï¼Œæœ‰ 11 å€‹æ˜¯æ­£ç¢ºçš„ (11/(11+3))
     - é«˜ç²¾ç¢ºåº¦è¡¨ç¤ºæ¨¡å‹å° Failure çš„åˆ¤æ–·å¾ˆè¬¹æ…ï¼Œä¸æœƒäº‚å ±è­¦
   - **Recall (Failure)**: 0.41 (41%)
     - åœ¨ 27 å€‹å¯¦éš› Failure ä¸­ï¼Œåªæœ‰ 11 å€‹è¢«æ­£ç¢ºæª¢å‡º (11/27)
     - ä½å¬å›ç‡è¡¨ç¤º **æœ‰ 59% çš„ Failure æ¡ˆä¾‹è¢«æ¼æª¢**ï¼Œé€™åœ¨åŒ–å·¥æ‡‰ç”¨ä¸­å¾ˆå±éšª
   - **F1-score (Failure)**: 0.54 - Precision å’Œ Recall çš„èª¿å’Œå¹³å‡

   **Success é¡åˆ¥** (å¤šæ•¸é¡)ï¼š
   - **Precision (Success)**: 0.81 (81%)
   - **Recall (Success)**: 0.96 (96%) - å¹¾ä¹æ‰€æœ‰æˆåŠŸæ¡ˆä¾‹éƒ½è¢«æª¢å‡º
   - **F1-score (Success)**: 0.88 - è¡¨ç¾å„ªç•°

3. **æ··æ·†çŸ©é™£æ¯”ä¾‹** (å³åœ– - ç™¾åˆ†æ¯”)ï¼š
   - Failure å¬å›ç‡ï¼š40.74% (11/27)
   - Success å¬å›ç‡ï¼š95.89% (70/73)
   - æ˜é¡¯çš„ä¸å¹³è¡¡ï¼šæ¨¡å‹åå¥½é æ¸¬ Success

**åŒ–å·¥å¯¦å‹™å½±éŸ¿**ï¼š

1. **FN (False Negative) = 3 å€‹**ï¼š
   - **æ„ç¾©**ï¼šå¯¦éš›æœƒå¤±æ•—çš„åæ‡‰æ¢ä»¶è¢«é æ¸¬ç‚ºæˆåŠŸ
   - **æˆæœ¬**ï¼š
     - æµªè²»åŸæ–™ã€èƒ½æºã€äººåŠ›é€²è¡Œå¯¦éš›ç”Ÿç”¢
     - å¯èƒ½é€ æˆè¨­å‚™æå£æˆ–å®‰å…¨å•é¡Œ
     - å½±éŸ¿å¾ŒçºŒçš„ç”Ÿç”¢è¨ˆåŠƒ
   - **å»ºè­°**ï¼šä½¿ç”¨ `class_weight='balanced'` æˆ–èª¿æ•´æ±ºç­–é–¾å€¼é™ä½ FN

2. **FP (False Positive) = 16 å€‹**ï¼š
   - **æ„ç¾©**ï¼šå¯¦éš›æœƒæˆåŠŸçš„åæ‡‰æ¢ä»¶è¢«é æ¸¬ç‚ºå¤±æ•—
   - **æˆæœ¬**ï¼š
     - æ”¾æ£„å„ªè‰¯çš„åæ‡‰æ¢ä»¶ï¼Œéºå¤±ç”Ÿç”¢æ©Ÿæœƒ
     - ç”¢èƒ½ä¸‹é™ã€æ”¶ç›Šæ¸›å°‘
   - **ç›¸å°å½±éŸ¿**ï¼šé€šå¸¸æ¯” FN å°ï¼Œå› ç‚ºåªæ˜¯ä¸ä½¿ç”¨æŸäº›æ¢ä»¶ï¼Œä¸æœƒé€ æˆç›´æ¥æå¤±

3. **æ±ºç­–å»ºè­°**ï¼š
   - å¦‚æœ **FN æˆæœ¬ >> FP æˆæœ¬**ï¼šå„ªå…ˆæå‡ Recallï¼Œæ¥å—è¼ƒä½çš„ Precision
   - å¦‚æœ **FP æˆæœ¬ >> FN æˆæœ¬**ï¼šå„ªå…ˆæå‡ Precisionï¼Œæ¥å—è¼ƒä½çš„ Recall
   - åœ¨å¤§å¤šæ•¸åŒ–å·¥å ´æ™¯ä¸­ï¼Œ**Recall æ›´é‡è¦**ï¼ˆé¿å…å®‰å…¨é¢¨éšªã€è¨­å‚™æå£ï¼‰

### 5.9 æ±ºç­–å‡½æ•¸å¯è¦–åŒ–

```python
# ç²å–æ±ºç­–å‡½æ•¸å€¼
decision_scores = svc_rbf.decision_function(X_test_scaled)

# å¯è¦–åŒ–æ±ºç­–åˆ†æ•¸åˆ†ä½ˆ
plt.figure(figsize=(10, 6))

plt.hist(decision_scores[y_test==0], bins=30, alpha=0.6, 
         label='Actual Failure', color='red', edgecolor='black')
plt.hist(decision_scores[y_test==1], bins=30, alpha=0.6, 
         label='Actual Success', color='green', edgecolor='black')

plt.axvline(x=0, color='black', linestyle='--', linewidth=2, 
            label='Decision Boundary')
plt.xlabel('Decision Function Score')
plt.ylabel('Frequency')
plt.title('Distribution of Decision Scores (RBF Kernel)')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig('svc_decision_scores.png', dpi=300, bbox_inches='tight')
plt.show()
```

![æ±ºç­–å‡½æ•¸åˆ†æ•¸åˆ†ä½ˆ](outputs/P3_Unit12_Support_Vector_Classification/figs/decision_scores.png)

**æ±ºç­–å‡½æ•¸åˆ†æ**ï¼š

1. **åˆ†æ•¸åˆ†ä½ˆç‰¹å¾µ**ï¼š
   - **Failure (ç´…è‰²)**: ä¸»è¦åˆ†ä½ˆåœ¨è² åˆ†å€åŸŸï¼Œä½†æœ‰æ˜é¡¯é‡ç–Šåˆ°æ­£åˆ†å€
   - **Success (ç¶ è‰²)**: é›†ä¸­åœ¨æ­£åˆ†å€åŸŸï¼Œåªæœ‰å°‘é‡å»¶ä¼¸åˆ°è² åˆ†å€
   - **æ±ºç­–é‚Šç•Œ** (é»‘è‰²è™›ç·š): x = 0

2. **æ¨¡å‹ä¿¡å¿ƒåº¦è®€æ³•**ï¼š
   - **åˆ†æ•¸ > 1**: æ¨¡å‹å¾ˆç¢ºä¿¡æ˜¯ Success
   - **0 < åˆ†æ•¸ < 1**: å‚¾å‘ Successï¼Œä½†ä¿¡å¿ƒåº¦ä¸é«˜
   - **-1 < åˆ†æ•¸ < 0**: å‚¾å‘ Failureï¼Œä½†ä¿¡å¿ƒåº¦ä¸é«˜
   - **åˆ†æ•¸ < -1**: æ¨¡å‹å¾ˆç¢ºä¿¡æ˜¯ Failure

3. **é‡ç–Šå€åŸŸåˆ†æ** (ç´„ -1 åˆ° +1 ç¯„åœ)ï¼š
   - å…©é¡åˆ¥æœ‰æ˜é¡¯é‡ç–Šï¼Œè¡¨ç¤ºå­˜åœ¨é›£ä»¥åˆ†é¡çš„é‚Šç•Œæ¡ˆä¾‹
   - é€™äº›æ¡ˆä¾‹æ˜¯æ¨¡å‹ä¸ç¢ºå®šæ€§çš„ä¸»è¦ä¾†æº
   - å°æ‡‰åˆ°åŒ–å­¸åæ‡‰ï¼šæŸäº›æ¢ä»¶ä»‹æ–¼æˆåŠŸèˆ‡å¤±æ•—çš„è‡¨ç•Œé»

4. **å¯¦å‹™æ‡‰ç”¨å»ºè­°**ï¼š
   - **é«˜ä¿¡å¿ƒåº¦å€åŸŸ** (|score| > 1)ï¼šå¯ä»¥ç›´æ¥ä½¿ç”¨æ¨¡å‹é æ¸¬
   - **ä½ä¿¡å¿ƒåº¦å€åŸŸ** (|score| < 1)ï¼šå»ºè­°é€²è¡Œå¯¦é©—é©—è­‰æˆ–å°ˆå®¶å¯©æŸ¥
   - **æ±ºç­–é–¾å€¼èª¿æ•´**ï¼šå¯ä»¥é€éèª¿æ•´æ±ºç­–é–¾å€¼ä¾†å¹³è¡¡ Precision å’Œ Recall

---

## 5.9.1 æ©Ÿç‡é æ¸¬èˆ‡é–¾å€¼èª¿æ•´

åœ¨å¯¦å‹™æ‡‰ç”¨ä¸­ï¼Œæœ‰æ™‚æˆ‘å€‘éœ€è¦ç²å–æ¨¡å‹çš„**æ©Ÿç‡é æ¸¬**è€Œéåƒ…åƒ…æ˜¯é¡åˆ¥æ¨™ç±¤ã€‚SVC é è¨­ä¸è¼¸å‡ºæ©Ÿç‡ï¼Œä½†å¯ä»¥é€éè¨­ç½® `probability=True` ä¾†å•Ÿç”¨æ©Ÿç‡ä¼°è¨ˆï¼ˆä½¿ç”¨ Platt Scaling æ–¹æ³•ï¼‰ã€‚

### æ©Ÿç‡é æ¸¬

```python
# è¨“ç·´æ”¯æŒæ©Ÿç‡é æ¸¬çš„ SVC
svc_proba = SVC(
    kernel='rbf',
    C=1.0,
    gamma='scale',
    probability=True,  # å•Ÿç”¨æ©Ÿç‡é æ¸¬
    random_state=42
)

svc_proba.fit(X_train_scaled, y_train)

# ç²å–æ©Ÿç‡é æ¸¬
y_proba = svc_proba.predict_proba(X_test_scaled)
print("æ©Ÿç‡é æ¸¬ç¯„ä¾‹ï¼ˆå‰5å€‹æ¨£æœ¬ï¼‰:")
print("æ¨£æœ¬  | P(Failure) | P(Success)")
print("-" * 35)
for i in range(5):
    print(f"{i+1:3d}   | {y_proba[i,0]:.4f}     | {y_proba[i,1]:.4f}")
```

### é–¾å€¼èª¿æ•´å¯¦é©—

é è¨­æƒ…æ³ä¸‹ï¼ŒSVC ä½¿ç”¨ 0.5 ä½œç‚ºæ±ºç­–é–¾å€¼ã€‚åœ¨é¡åˆ¥ä¸å¹³è¡¡æˆ–æˆæœ¬ä¸å°ç¨±çš„æƒ…æ³ä¸‹ï¼Œæˆ‘å€‘å¯ä»¥èª¿æ•´é–¾å€¼ä¾†å„ªåŒ–ç‰¹å®šæŒ‡æ¨™ã€‚

```python
print("\n" + "="*60)
print("æ©Ÿç‡é æ¸¬èˆ‡é–¾å€¼èª¿æ•´")
print("="*60)

# æ¸¬è©¦ä¸åŒé–¾å€¼
thresholds = [0.3, 0.5, 0.7]

print("\nä¸åŒé–¾å€¼ä¸‹çš„ Failure å¬å›ç‡:\n")
for threshold in thresholds:
    # æ ¹æ“šæ©Ÿç‡å’Œé–¾å€¼é€²è¡Œé æ¸¬
    y_pred_threshold = (y_proba[:, 1] >= threshold).astype(int)
    
    # è¨ˆç®—æ··æ·†çŸ©é™£
    cm_threshold = confusion_matrix(y_test, y_pred_threshold)
    
    # è¨ˆç®—å¬å›ç‡
    accuracy = accuracy_score(y_test, y_pred_threshold)
    failure_recall = cm_threshold[0,0] / cm_threshold[0,:].sum() if cm_threshold[0,:].sum() > 0 else 0
    success_recall = cm_threshold[1,1] / cm_threshold[1,:].sum() if cm_threshold[1,:].sum() > 0 else 0
    
    print(f"é–¾å€¼ {threshold}:")
    print(f"  æ•´é«”æº–ç¢ºç‡:     {accuracy:.2%}")
    print(f"  Failure å¬å›ç‡: {failure_recall:.2%}")
    print(f"  Success å¬å›ç‡: {success_recall:.2%}\n")
```

**åŸ·è¡Œçµæœ**ï¼š

```
============================================================
æ©Ÿç‡é æ¸¬èˆ‡é–¾å€¼èª¿æ•´
============================================================

ä¸åŒé–¾å€¼ä¸‹çš„ Failure å¬å›ç‡:

é–¾å€¼ 0.3:
  æ•´é«”æº–ç¢ºç‡:     77.00%
  Failure å¬å›ç‡: 18.52%
  Success å¬å›ç‡: 98.63%

é–¾å€¼ 0.5:
  æ•´é«”æº–ç¢ºç‡:     81.00%
  Failure å¬å›ç‡: 44.44%
  Success å¬å›ç‡: 94.52%

é–¾å€¼ 0.7:
  æ•´é«”æº–ç¢ºç‡:     80.00%
  Failure å¬å›ç‡: 62.96%
  Success å¬å›ç‡: 86.30%
```

**é–¾å€¼èª¿æ•´åˆ†æ**ï¼š

1. **é–¾å€¼ = 0.3** (é™ä½é–¾å€¼ï¼Œæ›´å®¹æ˜“é æ¸¬ç‚º Success)ï¼š
   - **æ•ˆæœ**ï¼šSuccess å¬å›ç‡æ¥µé«˜ (98.63%)ï¼Œä½† Failure å¬å›ç‡æ¥µä½ (18.52%)
   - **æ„ç¾©**ï¼šå¹¾ä¹æ‰€æœ‰æ¨£æœ¬éƒ½è¢«é æ¸¬ç‚º Success
   - **é©ç”¨å ´æ™¯**ï¼šç•¶èª¤åˆ¤ Success ç‚º Failure çš„æˆæœ¬æ¥µé«˜æ™‚ä½¿ç”¨

2. **é–¾å€¼ = 0.5** (é è¨­å€¼)ï¼š
   - **æ•ˆæœ**ï¼šå¹³è¡¡çš„æ€§èƒ½ï¼Œæ•´é«”æº–ç¢ºç‡æœ€é«˜ (81%)
   - **Failure å¬å›ç‡**ï¼š44.44%ï¼Œä»ç„¶åä½
   - **é©ç”¨å ´æ™¯**ï¼šæ¨™æº–æ‡‰ç”¨ï¼Œç„¡ç‰¹æ®Šæˆæœ¬è€ƒé‡

3. **é–¾å€¼ = 0.7** (æé«˜é–¾å€¼ï¼Œæ›´å®¹æ˜“é æ¸¬ç‚º Failure)ï¼š
   - **æ•ˆæœ**ï¼šFailure å¬å›ç‡é¡¯è‘—æå‡ (62.96%)
   - **ä»£åƒ¹**ï¼šSuccess å¬å›ç‡ä¸‹é™ (86.30%)ï¼Œæ•´é«”æº–ç¢ºç‡ç•¥é™ (80%)
   - **é©ç”¨å ´æ™¯**ï¼šç•¶æ¼æª¢ Failure çš„æˆæœ¬å¾ˆé«˜æ™‚ä½¿ç”¨ï¼ˆåŒ–å·¥å®‰å…¨æ‡‰ç”¨ï¼‰

**åŒ–å·¥å¯¦å‹™å»ºè­°**ï¼š

1. **å®‰å…¨å„ªå…ˆå ´æ™¯** (é–¾å€¼ = 0.7 æˆ–æ›´é«˜)ï¼š
   - åæ‡‰å¤±æ•—å¯èƒ½å°è‡´è¨­å‚™æå£ã€å®‰å…¨äº‹æ•…
   - æ¥å—è¼ƒä½çš„æ•´é«”æº–ç¢ºç‡ï¼Œå„ªå…ˆé¿å…æ¼æª¢å¤±æ•—æ¡ˆä¾‹
   - å¯§å¯å¤šåšå¯¦é©—é©—è­‰ï¼Œä¹Ÿä¸è¦å†’éšªåŸ·è¡Œå¯èƒ½å¤±æ•—çš„æ¢ä»¶

2. **æ•ˆç‡å„ªå…ˆå ´æ™¯** (é–¾å€¼ = 0.3 æˆ–æ›´ä½)ï¼š
   - åæ‡‰å¤±æ•—çš„æˆæœ¬ç›¸å°è¼ƒä½ï¼Œå¯å¿«é€Ÿé‡è©¦
   - å„ªå…ˆç¢ºä¿æˆåŠŸæ¡ˆä¾‹ä¸è¢«éŒ¯é
   - é©åˆé«˜é€šé‡ç¯©é¸ã€åˆæ­¥æ¢ç´¢éšæ®µ

3. **å¹³è¡¡å ´æ™¯** (é–¾å€¼ = 0.5)ï¼š
   - å¤±æ•—å’Œèª¤åˆ¤çš„æˆæœ¬ç›¸ç•¶
   - è¿½æ±‚æ•´é«”æº–ç¢ºç‡æœ€å¤§åŒ–
   - é©åˆæˆç†Ÿè£½ç¨‹çš„å„ªåŒ–æ”¹é€²

4. **é–¾å€¼é¸æ“‡ç­–ç•¥**ï¼š
   ```python
   # åŸºæ–¼æˆæœ¬çš„é–¾å€¼é¸æ“‡
   cost_FN = 10000  # æ¼æª¢ Failure çš„æˆæœ¬ï¼ˆè¨­å‚™æå£ï¼‰
   cost_FP = 1000   # èª¤åˆ¤ Success çš„æˆæœ¬ï¼ˆæ©Ÿæœƒæˆæœ¬ï¼‰
   
   # å¦‚æœ cost_FN >> cost_FPï¼Œæé«˜é–¾å€¼ï¼ˆå¦‚ 0.7ï¼‰
   # å¦‚æœ cost_FP >> cost_FNï¼Œé™ä½é–¾å€¼ï¼ˆå¦‚ 0.3ï¼‰
   # å¦‚æœ cost_FN â‰ˆ cost_FPï¼Œä½¿ç”¨é è¨­é–¾å€¼ï¼ˆ0.5ï¼‰
   ```

5. **èˆ‡ class_weight='balanced' çš„æ¯”è¼ƒ**ï¼š
   - **é–¾å€¼èª¿æ•´**ï¼šåœ¨é æ¸¬éšæ®µèª¿æ•´ï¼Œéˆæ´»ä½†éœ€è¦æ©Ÿç‡é æ¸¬ï¼ˆå¢åŠ è¨“ç·´æ™‚é–“ï¼‰
   - **class_weight**ï¼šåœ¨è¨“ç·´éšæ®µèª¿æ•´ï¼Œæ¨¡å‹ç›´æ¥å­¸ç¿’ä¸å¹³è¡¡ï¼Œä½†ä¸å¯å¾ŒçºŒèª¿æ•´
   - **å»ºè­°**ï¼šå…ˆä½¿ç”¨ `class_weight='balanced'` è¨“ç·´ï¼Œå†æ ¹æ“šå¯¦éš›éœ€æ±‚å¾®èª¿é–¾å€¼

### 5.10 è¶…åƒæ•¸èª¿æ•´

```python
print("\n" + "="*60)
print("è¶…åƒæ•¸èª¿æ•´ (Grid Search)")
print("="*60)

# å®šç¾©åƒæ•¸ç¶²æ ¼
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],
    'kernel': ['rbf']
}

# å‰µå»º Grid Search
grid_search = GridSearchCV(
    SVC(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

# åŸ·è¡Œ Grid Search
grid_search.fit(X_train_scaled, y_train)

# æœ€ä½³åƒæ•¸
print(f"\næœ€ä½³åƒæ•¸: {grid_search.best_params_}")
print(f"æœ€ä½³äº¤å‰é©—è­‰æº–ç¢ºç‡: {grid_search.best_score_:.4f}")

# ä½¿ç”¨æœ€ä½³æ¨¡å‹è©•ä¼°
best_model = grid_search.best_estimator_
y_test_pred_best = best_model.predict(X_test_scaled)

test_accuracy_best = accuracy_score(y_test, y_test_pred_best)
print(f"\næœ€ä½³æ¨¡å‹åœ¨æ¸¬è©¦é›†ä¸Šçš„è¡¨ç¾:")
print(f"æº–ç¢ºç‡: {test_accuracy_best:.4f}")

# æ¯”è¼ƒ C å’Œ gamma çš„å½±éŸ¿
cv_results = pd.DataFrame(grid_search.cv_results_)
pivot_table = cv_results.pivot_table(
    values='mean_test_score',
    index='param_gamma',
    columns='param_C'
)

plt.figure(figsize=(10, 6))
sns.heatmap(pivot_table, annot=True, fmt='.3f', cmap='YlGnBu')
plt.title('Grid Search: Accuracy Heatmap (C vs gamma)')
plt.xlabel('C')
plt.ylabel('gamma')
plt.tight_layout()
plt.savefig('svc_grid_search_heatmap.png', dpi=300, bbox_inches='tight')
plt.show()
```

**åŸ·è¡Œçµæœ**ï¼š

```
============================================================
è¶…åƒæ•¸èª¿æ•´ (Grid Search)
============================================================
Fitting 5 folds for each of 24 candidates, totalling 120 fits

æœ€ä½³åƒæ•¸: {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}
æœ€ä½³äº¤å‰é©—è­‰æº–ç¢ºç‡: 0.8200

æœ€ä½³æ¨¡å‹åœ¨æ¸¬è©¦é›†ä¸Šçš„è¡¨ç¾:
æº–ç¢ºç‡: 0.8100
```

![è¶…åƒæ•¸èª¿æ•´ç†±åœ–](outputs/P3_Unit12_Support_Vector_Classification/figs/grid_search_heatmap.png)

**è¶…åƒæ•¸åˆ†æ**ï¼š

1. **æœ€ä½³åƒæ•¸çµ„åˆ**ï¼š
   - **C = 10**: ä¸­ç­‰æ‡²ç½°å¼·åº¦ï¼Œå¹³è¡¡é–“éš”æœ€å¤§åŒ–èˆ‡èª¤åˆ†é¡
   - **gamma = 0.01**: ä¸­ç­‰å½±éŸ¿ç¯„åœï¼Œé¿å…éæ–¼å±€éƒ¨æˆ–éæ–¼å…¨åŸŸ
   - **äº¤å‰é©—è­‰æº–ç¢ºç‡**: 82.0%
   - **æ¸¬è©¦é›†æº–ç¢ºç‡**: 81.0%
   - CV èˆ‡æ¸¬è©¦é›†æ¥è¿‘ï¼Œè¡¨ç¤ºæ¨¡å‹æ³›åŒ–èƒ½åŠ›è‰¯å¥½

2. **ç†±åœ–è®€æ³•** (C vs gamma)ï¼š
   - **é¡è‰²æ·± (è—è‰²)**ï¼šæ€§èƒ½è¼ƒå¥½ (0.80-0.82)
   - **é¡è‰²æ·º (é»ƒè‰²)**ï¼šæ€§èƒ½è¼ƒå·® (0.73-0.76)
   - **æœ€ä½³å€åŸŸ**ï¼šä¸­å¿ƒå€åŸŸ (C=1-10, gamma=0.01-0.1)
   - **éæ“¬åˆå€åŸŸ**ï¼šå³ä¸Šè§’ (Cå¤§, gammaå¤§)
   - **æ¬ æ“¬åˆå€åŸŸ**ï¼šå·¦ä¸‹è§’ (Cå°, gammaå°)

3. **C åƒæ•¸å½±éŸ¿**ï¼š
   - **C = 0.1**: æ€§èƒ½è¼ƒå·® (0.74-0.76)ï¼Œæ¨¡å‹éæ–¼ç°¡å–®
   - **C = 1**: æ€§èƒ½è‰¯å¥½ (0.79-0.81)ï¼Œæ˜¯é è¨­å€¼
   - **C = 10**: æœ€ä½³æ€§èƒ½ (0.81-0.82)
   - **C = 100**: æ€§èƒ½ä¸‹é™ (0.79-0.81)ï¼Œå¯èƒ½éæ“¬åˆ
   - **çµè«–**ï¼šæœ¬æ•¸æ“šé›†éœ€è¦ä¸­ç­‰æ‡²ç½°å¼·åº¦ (C=10)

4. **gamma åƒæ•¸å½±éŸ¿**ï¼š
   - **gamma = 'scale' / 'auto'**: æ€§èƒ½ä¸­ç­‰ (0.78-0.80)
   - **gamma = 0.001**: æ€§èƒ½è¼ƒå·® (0.75-0.77)ï¼Œå½±éŸ¿ç¯„åœéå¤§ï¼Œæ±ºç­–é‚Šç•Œéæ–¼å¹³æ»‘
   - **gamma = 0.01**: æœ€ä½³æ€§èƒ½ (0.81-0.82)
   - **gamma = 0.1**: æ€§èƒ½è‰¯å¥½ (0.79-0.81)
   - **gamma = 1.0**: æ€§èƒ½ä¸‹é™ (0.74-0.78)ï¼Œå½±éŸ¿ç¯„åœéå°ï¼Œéæ“¬åˆ
   - **çµè«–**ï¼šgamma=0.01 æä¾›æœ€ä½³çš„å±€éƒ¨-å…¨åŸŸå¹³è¡¡

5. **åƒæ•¸äº¤äº’ä½œç”¨**ï¼š
   - **C èˆ‡ gamma ä¹‹é–“å­˜åœ¨äº¤äº’ä½œç”¨**ï¼š
     - é«˜ C + é«˜ gamma â†’ éæ“¬åˆ
     - ä½ C + ä½ gamma â†’ æ¬ æ“¬åˆ
     - ä¸­ C + ä¸­ gamma â†’ æœ€ä½³è¡¨ç¾
   - éœ€è¦åŒæ™‚èª¿æ•´å…©å€‹åƒæ•¸ä»¥é”åˆ°æœ€ä½³å¹³è¡¡

6. **å¯¦å‹™å»ºè­°**ï¼š
   - **ç²—èª¿ç­–ç•¥**ï¼šå…ˆåœ¨å¤§ç¯„åœå…§å¿«é€Ÿæœç´¢ (C: [0.1, 1, 10, 100], gamma: ['scale', 0.001, 0.01, 0.1, 1])
   - **ç´°èª¿ç­–ç•¥**ï¼šåœ¨æœ€ä½³å€åŸŸé™„è¿‘ç²¾ç´°æœç´¢ (C: [5, 10, 20], gamma: [0.005, 0.01, 0.02])
   - **æ™‚é–“è€ƒé‡**ï¼šGrid Search éœ€è¦ 120 æ¬¡è¨“ç·´ (24 çµ„åˆ Ã— 5 folds)ï¼Œå¤§æ•¸æ“šé›†å¯ä½¿ç”¨ RandomizedSearchCV
   - **åŒ–å·¥æ‡‰ç”¨**ï¼šå»ºç«‹åƒæ•¸è³‡æ–™åº«ï¼Œè¨˜éŒ„ä¸åŒåŒ–å­¸ç³»çµ±çš„æœ€ä½³åƒæ•¸ï¼ŒåŠ é€Ÿæœªä¾†å»ºæ¨¡

---

## 6. SVC çš„å„ªå‹¢èˆ‡é™åˆ¶

### 6.1 å„ªå‹¢

1. **å¼·å¤§çš„éç·šæ€§èƒ½åŠ›**
   - é€šéæ ¸å‡½æ•¸è™•ç†è¤‡é›œçš„éç·šæ€§é—œä¿‚
   - RBF æ ¸å¯ä»¥é€¼è¿‘ä»»æ„è¤‡é›œçš„æ±ºç­–é‚Šç•Œ

2. **é«˜ç¶­ç©ºé–“è¡¨ç¾å„ªç•°**
   - åœ¨ç‰¹å¾µæ•¸ >> æ¨£æœ¬æ•¸çš„æƒ…æ³ä¸‹ä»ç„¶æœ‰æ•ˆ
   - é©åˆè™•ç†é«˜ç¶­æ•¸æ“šï¼ˆå¦‚å…‰è­œã€å½±åƒç‰¹å¾µï¼‰

3. **å°ç•°å¸¸å€¼ç›¸å°ç©©å¥**
   - è»Ÿé–“éš”å…è¨±éƒ¨åˆ†æ¨£æœ¬é•åé–“éš”ç´„æŸ
   - åªæœ‰æ”¯æŒå‘é‡å½±éŸ¿æ±ºç­–é‚Šç•Œ

4. **ç†è«–åŸºç¤å®Œå–„**
   - åŸºæ–¼çµ±è¨ˆå­¸ç¿’ç†è«–ï¼ˆVCç¶­ã€çµæ§‹é¢¨éšªæœ€å°åŒ–ï¼‰
   - å‡¸å„ªåŒ–å•é¡Œï¼Œä¿è­‰æ‰¾åˆ°å…¨å±€æœ€å„ªè§£

5. **è¨˜æ†¶é«˜æ•ˆ**
   - åªéœ€å„²å­˜æ”¯æŒå‘é‡ï¼ˆé€šå¸¸é å°‘æ–¼è¨“ç·´æ¨£æœ¬ï¼‰

### 6.2 é™åˆ¶

1. **è¨ˆç®—æˆæœ¬é«˜**
   - è¨“ç·´æ™‚é–“è¤‡é›œåº¦ï¼š$O(n^2)$ åˆ° $O(n^3)$ 
   - å¤§æ•¸æ“šé›†ï¼ˆn > 10,000ï¼‰è¨“ç·´ç·©æ…¢
   - è§£æ±ºæ–¹æ¡ˆï¼šä½¿ç”¨ `LinearSVC` æˆ– SGD å„ªåŒ–

2. **åƒæ•¸èª¿æ•´æ•æ„Ÿ**
   - C å’Œ gamma éœ€è¦ä»”ç´°èª¿æ•´
   - ç¶²æ ¼æœç´¢è€—æ™‚ï¼ˆå°¤å…¶å¤§æ•¸æ“šé›†ï¼‰
   - éœ€è¦é ˜åŸŸçŸ¥è­˜é¸æ“‡åˆé©çš„æ ¸å‡½æ•¸

3. **æ©Ÿç‡è¼¸å‡ºä¸ç›´æ¥**
   - é è¨­ä¸è¼¸å‡ºæ©Ÿç‡ï¼Œéœ€è¦ `probability=True`
   - æ©Ÿç‡ä¼°è¨ˆä½¿ç”¨äº¤å‰é©—è­‰æ ¡æº–ï¼Œå¢åŠ è¨ˆç®—æˆæœ¬
   - æ©Ÿç‡æº–ç¢ºæ€§ä¸å¦‚ç›´æ¥è¼¸å‡ºæ©Ÿç‡çš„æ¨¡å‹ï¼ˆå¦‚é‚è¼¯è¿´æ­¸ï¼‰

4. **å°ç‰¹å¾µå°ºåº¦æ•æ„Ÿ**
   - å¿…é ˆé€²è¡Œç‰¹å¾µæ¨™æº–åŒ–
   - æœªæ¨™æº–åŒ–æœƒå°è‡´æŸäº›ç‰¹å¾µä¸»å°æ±ºç­–

5. **è§£é‡‹æ€§ç›¸å°è¼ƒå¼±**
   - é›£ä»¥åƒç·šæ€§æ¨¡å‹é‚£æ¨£ç›´æ¥è§£é‡‹ç‰¹å¾µé‡è¦æ€§
   - æ”¯æŒå‘é‡çš„æ„ç¾©ä¸å¦‚æ±ºç­–æ¨¹æ¸…æ™°

6. **å¤šåˆ†é¡æ•ˆç‡è¼ƒä½**
   - One-vs-One ç­–ç•¥éœ€è¦ $\frac{k(k-1)}{2}$ å€‹åˆ†é¡å™¨
   - k é¡åˆ¥è¼ƒå¤šæ™‚è¨ˆç®—é‡å¤§

### 6.3 é©ç”¨å ´æ™¯

**å»ºè­°ä½¿ç”¨ SVC çš„æƒ…æ³**ï¼š
- æ•¸æ“šå…·æœ‰æ˜é¡¯çš„éç·šæ€§é—œä¿‚
- ç‰¹å¾µæ•¸é‡å¤šä½†æ¨£æœ¬æ•¸é©ä¸­ï¼ˆ100 ~ 10,000ï¼‰
- éœ€è¦é«˜æº–ç¢ºç‡çš„äºŒå…ƒæˆ–å°è¦æ¨¡å¤šåˆ†é¡å•é¡Œ
- å°æ¨¡å‹å¯è§£é‡‹æ€§è¦æ±‚ä¸é«˜
- æœ‰è¶³å¤ çš„æ™‚é–“é€²è¡Œåƒæ•¸èª¿æ•´

**è€ƒæ…®å…¶ä»–æ¨¡å‹çš„æƒ…æ³**ï¼š
- è¶…å¤§æ•¸æ“šé›†ï¼ˆn > 100,000ï¼‰â†’ ä½¿ç”¨ SGDClassifierã€é‚è¼¯è¿´æ­¸
- éœ€è¦æ©Ÿç‡è¼¸å‡ºä¸”å¯¦æ™‚é æ¸¬ â†’ é‚è¼¯è¿´æ­¸ã€éš¨æ©Ÿæ£®æ—
- éœ€è¦æ¨¡å‹å¯è§£é‡‹æ€§ â†’ æ±ºç­–æ¨¹ã€é‚è¼¯è¿´æ­¸
- è¨ˆç®—è³‡æºæœ‰é™ â†’ ç·šæ€§æ¨¡å‹ã€æ¨¸ç´ è²è‘‰æ–¯

---

## 7. å¯¦å‹™å»ºè­°

### 7.1 ç‰¹å¾µå·¥ç¨‹

```python
# 1. ç‰¹å¾µæ¨™æº–åŒ–ï¼ˆå¿…é ˆï¼‰
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 2. ç§»é™¤ä½æ–¹å·®ç‰¹å¾µ
from sklearn.feature_selection import VarianceThreshold
selector = VarianceThreshold(threshold=0.01)
X_selected = selector.fit_transform(X_scaled)

# 3. é™ç¶­ï¼ˆé«˜ç¶­æ•¸æ“šï¼‰
from sklearn.decomposition import PCA
pca = PCA(n_components=0.95)  # ä¿ç•™95%æ–¹å·®
X_pca = pca.fit_transform(X_scaled)
```

### 7.2 æ ¸å‡½æ•¸é¸æ“‡ç­–ç•¥

```python
# ç­–ç•¥1ï¼šå…ˆå˜—è©¦ç·šæ€§æ ¸
svc_linear = SVC(kernel='linear')
svc_linear.fit(X_train, y_train)
score_linear = svc_linear.score(X_test, y_test)

# ç­–ç•¥2ï¼šå¦‚æœç·šæ€§æ ¸æ•ˆæœä¸ä½³ï¼Œå˜—è©¦ RBF æ ¸
if score_linear < 0.80:
    svc_rbf = SVC(kernel='rbf', gamma='scale')
    svc_rbf.fit(X_train, y_train)
    score_rbf = svc_rbf.score(X_test, y_test)
    print(f"Linear: {score_linear:.3f}, RBF: {score_rbf:.3f}")
```

### 7.3 åƒæ•¸èª¿æ•´å»ºè­°

```python
# ç²—èª¿ï¼šå¤§ç¯„åœå¿«é€Ÿæœç´¢
param_grid_coarse = {
    'C': [0.1, 1, 10, 100],
    'gamma': ['scale', 0.001, 0.01, 0.1, 1]
}

# ç´°èª¿ï¼šç¸®å°ç¯„åœç²¾ç´°æœç´¢
param_grid_fine = {
    'C': [0.5, 1, 2, 5],
    'gamma': [0.005, 0.01, 0.02, 0.05]
}

# ä½¿ç”¨ RandomizedSearchCV åŠ é€Ÿ
from sklearn.model_selection import RandomizedSearchCV
random_search = RandomizedSearchCV(
    SVC(),
    param_distributions={
        'C': np.logspace(-2, 3, 20),
        'gamma': np.logspace(-4, 1, 20)
    },
    n_iter=50,  # éš¨æ©Ÿå˜—è©¦50çµ„åƒæ•¸
    cv=5,
    random_state=42
)
```

### 7.4 é¡åˆ¥ä¸å¹³è¡¡è™•ç†

```python
# æ–¹æ³•1ï¼šè¨­ç½® class_weight
svc_balanced = SVC(kernel='rbf', class_weight='balanced')

# æ–¹æ³•2ï¼šæ‰‹å‹•è¨­ç½®æ¬Šé‡
class_weights = {0: 1, 1: 3}  # é¡åˆ¥1çš„æ¬Šé‡æ˜¯é¡åˆ¥0çš„3å€
svc_weighted = SVC(kernel='rbf', class_weight=class_weights)

# æ–¹æ³•3ï¼šé‡æ¡æ¨£ï¼ˆé…åˆ imbalanced-learnï¼‰
from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
```

### 7.5 åŠ é€Ÿè¨“ç·´æŠ€å·§

```python
# 1. ä½¿ç”¨ LinearSVC (å¤§æ•¸æ“šé›†)
from sklearn.svm import LinearSVC
linear_svc = LinearSVC(max_iter=10000)

# 2. æ¸›å°‘æ¨£æœ¬æ•¸é‡ï¼ˆå­æ¡æ¨£ï¼‰
from sklearn.model_selection import StratifiedShuffleSplit
splitter = StratifiedShuffleSplit(n_splits=1, train_size=0.1, random_state=42)
for train_idx, _ in splitter.split(X, y):
    X_subset = X[train_idx]
    y_subset = y[train_idx]

# 3. å¢åŠ ç·©å­˜å¤§å°
svc = SVC(kernel='rbf', cache_size=500)  # 500 MB

# 4. ä¸¦è¡ŒåŒ–ï¼ˆå°æ–¼ one-vs-rest å¤šåˆ†é¡ï¼‰
from sklearn.multiclass import OneVsRestClassifier
ovr = OneVsRestClassifier(SVC(kernel='rbf'), n_jobs=-1)
```

---

## 8. SVC vs é‚è¼¯è¿´æ­¸æ¯”è¼ƒ

| ç‰¹æ€§ | SVC | é‚è¼¯è¿´æ­¸ |
|-----|-----|---------|
| **æ±ºç­–é‚Šç•Œ** | ç·šæ€§æˆ–éç·šæ€§ï¼ˆæ ¸å‡½æ•¸ï¼‰ | åƒ…ç·šæ€§ |
| **è¨“ç·´æ™‚é–“** | $O(n^2) \sim O(n^3)$ | $O(n \times m)$ |
| **é©åˆæ•¸æ“šé‡** | å°åˆ°ä¸­å‹ (< 10,000) | å°åˆ°å¤§å‹ |
| **æ©Ÿç‡è¼¸å‡º** | é–“æ¥ï¼ˆéœ€è¦æ ¡æº–ï¼‰ | ç›´æ¥è¼¸å‡º |
| **å¯è§£é‡‹æ€§** | å¼±ï¼ˆéç·šæ€§æ ¸ï¼‰ | å¼·ï¼ˆä¿‚æ•¸æ„ç¾©æ˜ç¢ºï¼‰ |
| **ç‰¹å¾µç¸®æ”¾** | å¿…é ˆæ¨™æº–åŒ– | å»ºè­°æ¨™æº–åŒ– |
| **æ­£å‰‡åŒ–** | C åƒæ•¸ï¼ˆè»Ÿé–“éš”ï¼‰ | L1/L2 æ­£å‰‡åŒ– |
| **é«˜ç¶­æ•¸æ“š** | è¡¨ç¾å„ªç§€ | è¡¨ç¾è‰¯å¥½ |
| **éæ“¬åˆæ§åˆ¶** | èª¿æ•´ C å’Œ gamma | èª¿æ•´æ­£å‰‡åŒ–åƒæ•¸ |
| **åœ¨ç·šå­¸ç¿’** | ä¸æ”¯æŒ | æ”¯æŒï¼ˆSGDï¼‰ |

**é¸æ“‡å»ºè­°**ï¼š
- **SVC**ï¼šæ•¸æ“šæœ‰éç·šæ€§æ¨¡å¼ï¼Œæ¨£æœ¬æ•¸é©ä¸­ï¼Œè¿½æ±‚é«˜æº–ç¢ºç‡
- **é‚è¼¯è¿´æ­¸**ï¼šéœ€è¦æ©Ÿç‡è¼¸å‡ºï¼Œå¤§æ•¸æ“šé›†ï¼Œéœ€è¦æ¨¡å‹å¯è§£é‡‹æ€§

---

## 9. ç¸½çµ

æœ¬ç¯€èª²æˆ‘å€‘æ·±å…¥å­¸ç¿’äº†**æ”¯æŒå‘é‡åˆ†é¡ (SVC)**ï¼š

### æ ¸å¿ƒæ¦‚å¿µå›é¡§

1. **é–“éš”æœ€å¤§åŒ–**ï¼š
   - SVC å°‹æ‰¾æœ€å¤§é–“éš”çš„è¶…å¹³é¢åˆ†éš”é¡åˆ¥
   - æ”¯æŒå‘é‡æ±ºå®šæ±ºç­–é‚Šç•Œ

2. **æ ¸å‡½æ•¸æŠ€å·§**ï¼š
   - ç·šæ€§æ ¸ã€RBF æ ¸ã€å¤šé …å¼æ ¸ã€Sigmoid æ ¸
   - å°‡ä½ç¶­éç·šæ€§å•é¡Œè½‰æ›ç‚ºé«˜ç¶­ç·šæ€§å•é¡Œ

3. **è»Ÿé–“éš”**ï¼š
   - C åƒæ•¸æ§åˆ¶é–“éš”èˆ‡èª¤åˆ†é¡çš„æ¬Šè¡¡
   - å…è¨±éƒ¨åˆ†æ¨£æœ¬é•åé–“éš”ç´„æŸ

4. **sklearn å¯¦ç¾**ï¼š
   - `SVC` é¡æä¾›å®Œæ•´åŠŸèƒ½
   - æ”¯æŒå¤šç¨®æ ¸å‡½æ•¸å’Œåƒæ•¸èª¿æ•´

5. **åŒ–å·¥æ‡‰ç”¨**ï¼š
   - ç”¢å“å“è³ªåˆ†é¡
   - ç›¸æ…‹è­˜åˆ¥
   - è¨­å‚™æ•…éšœè¨ºæ–·
   - åæ‡‰è·¯å¾‘é¸æ“‡

### é—œéµè¦é»

âœ… **å„ªå‹¢**ï¼šå¼·å¤§çš„éç·šæ€§èƒ½åŠ›ã€é«˜ç¶­è¡¨ç¾å„ªç•°ã€ç†è«–åŸºç¤å®Œå–„  
âš ï¸ **é™åˆ¶**ï¼šè¨ˆç®—æˆæœ¬é«˜ã€åƒæ•¸èª¿æ•´æ•æ„Ÿã€å¤§æ•¸æ“šé›†ç·©æ…¢  
ğŸ¯ **é©ç”¨**ï¼šéç·šæ€§å•é¡Œã€ä¸­å°å‹æ•¸æ“šé›†ã€é«˜æº–ç¢ºç‡éœ€æ±‚

### ä¸‹ä¸€æ­¥å­¸ç¿’

å®Œæˆ SVC çš„å­¸ç¿’å¾Œï¼Œå»ºè­°ç¹¼çºŒå­¸ç¿’ï¼š

- **Unit12_Decision_Tree_Classifier**ï¼šå¯è§£é‡‹çš„éç·šæ€§åˆ†é¡
- **Unit12_Random_Forest_Classifier**ï¼šé›†æˆå­¸ç¿’æå‡æ€§èƒ½
- **Unit12_Gradient_Boosting_Classifier**ï¼šå¼·å¤§çš„æ¢¯åº¦æå‡æ–¹æ³•

---

**èª²ç¨‹è³‡è¨Š**
- èª²ç¨‹åç¨±ï¼šAIåœ¨åŒ–å·¥ä¸Šä¹‹æ‡‰ç”¨
- èª²ç¨‹å–®å…ƒï¼šUnit12 Support Vector Classification æ”¯æŒå‘é‡åˆ†é¡
- èª²ç¨‹è£½ä½œï¼šé€¢ç”²å¤§å­¸ åŒ–å·¥ç³» æ™ºæ…§ç¨‹åºç³»çµ±å·¥ç¨‹å¯¦é©—å®¤
- æˆèª²æ•™å¸«ï¼šèŠæ›œç¦ åŠ©ç†æ•™æˆ
- æ›´æ–°æ—¥æœŸï¼š2026-01-28

**èª²ç¨‹æˆæ¬Š [CC BY-NC-SA 4.0]**
 - æœ¬æ•™æéµå¾ª [å‰µç”¨CC å§“åæ¨™ç¤º-éå•†æ¥­æ€§-ç›¸åŒæ–¹å¼åˆ†äº« 4.0 åœ‹éš› (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh) æˆæ¬Šã€‚

---
