# Unit 12 å·¥æ¥­æ‡‰ç”¨æ¡ˆä¾‹ï¼šé›»æ°£è¨­å‚™æ•…éšœè¨ºæ–·èˆ‡åˆ†é¡
# Electrical Fault Detection and Classification

---

## 1. æ¡ˆä¾‹èƒŒæ™¯èˆ‡å‹•æ©Ÿ

### 1.1 é›»åŠ›ç³»çµ±æ•…éšœæª¢æ¸¬çš„é‡è¦æ€§

é›»åŠ›å‚³è¼¸ç³»çµ±æ˜¯ç¾ä»£é›»ç¶²ä¸­æœ€é—œéµçš„çµ„æˆéƒ¨åˆ†ï¼Œè² è²¬å°‡é›»èƒ½å¾ç™¼é›»å» è¼¸é€åˆ°é…é›»ç¶²çµ¡ã€‚éš¨è‘—é›»åŠ›éœ€æ±‚å’Œå¯é æ€§è¦æ±‚çš„æŒ‡æ•¸ç´šå¢é•·ï¼Œé›»åŠ›ç³»çµ±ç”±è¨±å¤šè¤‡é›œã€å‹•æ…‹ä¸”ç›¸äº’ä½œç”¨çš„å…ƒä»¶çµ„æˆï¼Œé€™äº›å…ƒä»¶æ™‚åˆ»é¢è‡¨æ“¾å‹•æˆ–é›»æ°£æ•…éšœçš„é¢¨éšªã€‚

**é—œéµæŒ‘æˆ°**ï¼š
1. **å¿«é€ŸéŸ¿æ‡‰éœ€æ±‚**ï¼šé«˜å®¹é‡ç™¼é›»å» å’ŒåŒæ­¥é›»ç¶²è¦æ±‚åœ¨æœ€çŸ­æ™‚é–“å…§æª¢æ¸¬æ•…éšœä¸¦å•Ÿå‹•ä¿è­·è¨­å‚™
2. **ç³»çµ±ç©©å®šæ€§**ï¼šæ•…éšœå¿…é ˆè¢«æ­£ç¢ºæª¢æ¸¬ã€åˆ†é¡ä¸¦åœ¨æœ€çŸ­æ™‚é–“å…§æ¸…é™¤ï¼Œä»¥ä¿æŒç³»çµ±ç©©å®š
3. **é€£é–åæ‡‰é˜²è­·**ï¼šå‚³è¼¸ç·šä¿è­·ç³»çµ±å¯èƒ½è§¸ç™¼å…¶ä»–ç¹¼é›»å™¨ï¼Œé˜²æ­¢ç³»çµ±åœé›»
4. **è¤‡é›œæ€§**ï¼šä¸‰ç›¸é›»åŠ›ç³»çµ±ä¸­çš„æ•…éšœé¡å‹å¤šæ¨£ï¼Œéœ€è¦æº–ç¢ºå€åˆ†

### 1.2 æ•…éšœé¡å‹èªªæ˜

åœ¨ä¸‰ç›¸é›»åŠ›ç³»çµ±ä¸­ï¼Œå¸¸è¦‹çš„æ•…éšœé¡å‹åŒ…æ‹¬ï¼š

| æ•…éšœä»£ç¢¼ | æ•…éšœåç¨± | è‹±æ–‡å…¨ç¨± | èªªæ˜ |
|---------|---------|---------|------|
| LG | å–®ç·šå°åœ°æ•…éšœ | Line-to-Ground | å–®ç›¸ç·šè·¯èˆ‡åœ°é¢æ¥è§¸ |
| LL | ç·šå°ç·šæ•…éšœ | Line-to-Line | å…©ç›¸ç·šè·¯ä¹‹é–“çŸ­è·¯ |
| LLG | é›™ç·šå°åœ°æ•…éšœ | Double Line-to-Ground | å…©ç›¸ç·šè·¯åŒæ™‚èˆ‡åœ°é¢æ¥è§¸ |
| LLL | ä¸‰ç›¸æ•…éšœ | Three-Phase | ä¸‰ç›¸ç·šè·¯ä¹‹é–“çŸ­è·¯ |
| LLLG | ä¸‰ç›¸å°åœ°æ•…éšœ | Three-Phase-to-Ground | ä¸‰ç›¸å°ç¨±æ¥åœ°æ•…éšœ |

**æ¨™ç±¤ç·¨ç¢¼æ–¹å¼**ï¼ˆå››å…ƒçµ„ [G, C, B, A]ï¼‰ï¼š
- `[0, 0, 0, 0]` â†’ ç„¡æ•…éšœ (Normal)
- `[1, 0, 0, 1]` â†’ LG æ•…éšœ (Aç›¸å°åœ°)
- `[0, 0, 1, 1]` â†’ LL æ•…éšœ (A-Bç›¸é–“)
- `[1, 0, 1, 1]` â†’ LLG æ•…éšœ (A-Bç›¸å°åœ°)
- `[0, 1, 1, 1]` â†’ LLL æ•…éšœ (ä¸‰ç›¸çŸ­è·¯)
- `[1, 1, 1, 1]` â†’ LLLG æ•…éšœ (ä¸‰ç›¸å°ç¨±æ¥åœ°)

### 1.3 æ©Ÿå™¨å­¸ç¿’åœ¨æ•…éšœè¨ºæ–·ä¸­çš„å„ªå‹¢

å‚³çµ±ä¿è­·ç³»çµ±åŸºæ–¼é–¾å€¼å’Œç¶“é©—è¦å‰‡ï¼Œè€Œæ©Ÿå™¨å­¸ç¿’æ–¹æ³•å…·æœ‰ä»¥ä¸‹å„ªå‹¢ï¼š

1. **æ¨¡å¼è­˜åˆ¥èƒ½åŠ›**ï¼šè‡ªå‹•å­¸ç¿’æ•…éšœæ¨¡å¼ç‰¹å¾µï¼Œç„¡éœ€äººå·¥è¨­è¨ˆè¦å‰‡
2. **æ³›åŒ–èƒ½åŠ›**ï¼šåœ¨ä¸åŒç³»çµ±é‹è¡Œæ¢ä»¶å’Œç¶²çµ¡åƒæ•¸ä¸‹è¡¨ç¾ç©©å¥
3. **æŠ—å™ªèƒ½åŠ›**ï¼šå°æ¸¬é‡å™ªè²å’Œæ“¾å‹•å…·æœ‰è‰¯å¥½çš„å…ç–«åŠ›
4. **å®¹éŒ¯æ€§**ï¼šå–®å€‹å‚³æ„Ÿå™¨æ•…éšœä¸æœƒå°è‡´æ•´å€‹ç³»çµ±å¤±æ•ˆ
5. **å¯¦æ™‚æ€§**ï¼šè¨“ç·´å®Œæˆå¾Œå¯å¿«é€Ÿé€²è¡Œåœ¨ç·šé æ¸¬

---

## 2. æ•¸æ“šé›†èªªæ˜

### 2.1 æ•¸æ“šä¾†æº

æœ¬æ¡ˆä¾‹ä½¿ç”¨çš„æ•¸æ“šé›†ä¾†è‡ª Kaggleï¼š
- **æ•¸æ“šé›†åç¨±**ï¼šElectrical Fault Detection and Classification
- **æ•¸æ“šä¾†æº**ï¼šhttps://www.kaggle.com/datasets/esathyaprakash/electrical-fault-detection-and-classification/
- **æ•¸æ“šç”Ÿæˆæ–¹å¼**ï¼šä½¿ç”¨ MATLAB å°é›»åŠ›ç³»çµ±é€²è¡Œæ•…éšœåˆ†æä»¿çœŸ

### 2.2 ç³»çµ±æ¶æ§‹

ä»¿çœŸçš„é›»åŠ›ç³»çµ±åŒ…å«ï¼š
- **ç™¼é›»æ©Ÿ**ï¼š4 å° 11 kV ç™¼é›»æ©Ÿï¼Œæˆå°åˆ†å¸ƒåœ¨è¼¸é›»ç·šå…©ç«¯
- **è®Šå£“å™¨**ï¼šç”¨æ–¼é›»å£“è½‰æ›
- **æ•…éšœé»**ï¼šåœ¨è¼¸é›»ç·šä¸­é»æ¨¡æ“¬å„ç¨®æ•…éšœ
- **æ¸¬é‡é»**ï¼šåœ¨ç³»çµ±è¼¸å‡ºå´æ¸¬é‡ç·šé›»å£“å’Œç·šé›»æµ

### 2.3 æ•¸æ“šç‰¹å¾µ

**è¼¸å…¥ç‰¹å¾µï¼ˆ6å€‹ï¼‰**ï¼š
| ç‰¹å¾µåç¨± | èªªæ˜ | å–®ä½ |
|---------|------|------|
| `Ia` | Aç›¸ç·šé›»æµ | Ampere |
| `Ib` | Bç›¸ç·šé›»æµ | Ampere |
| `Ic` | Cç›¸ç·šé›»æµ | Ampere |
| `Va` | Aç›¸ç·šé›»å£“ | Voltage |
| `Vb` | Bç›¸ç·šé›»å£“ | Voltage |
| `Vc` | Cç›¸ç·šé›»å£“ | Voltage |

**è¼¸å‡ºæ¨™ç±¤ï¼ˆ4å€‹äºŒå…ƒç‰¹å¾µï¼‰**ï¼š
| æ¨™ç±¤åç¨± | èªªæ˜ |
|---------|------|
| `G` | Groundï¼ˆæ¥åœ°ï¼‰æ¨™è¨˜ |
| `C` | Cç›¸æ¶‰åŠæ¨™è¨˜ |
| `B` | Bç›¸æ¶‰åŠæ¨™è¨˜ |
| `A` | Aç›¸æ¶‰åŠæ¨™è¨˜ |

**æ•¸æ“šè¦æ¨¡**ï¼ˆå¯¦éš›ï¼‰ï¼š
- **ç¸½æ¨£æœ¬æ•¸**ï¼š7,861 å€‹æ•¸æ“šé»
- **æ•…éšœé¡å‹**ï¼š6 ç¨®ï¼ˆåŒ…å«æ­£å¸¸ç‹€æ…‹ï¼‰
- **æ•¸æ“šæ–‡ä»¶**ï¼š`classData.csv`
- **é¡åˆ¥åˆ†å¸ƒ**ï¼š
  - Class 0 (Normal): 1,004 samples (12.77%)
  - Class 1 (LG): 2,365 samples (30.09%) - æœ€å¤š
  - Class 2 (LL): 1,129 samples (14.36%)
  - Class 3 (LLG): 1,134 samples (14.43%)
  - Class 4 (LLL): 1,096 samples (13.94%)
  - Class 5 (LLLG): 1,133 samples (14.41%)

### 2.4 æœ¬æ¡ˆä¾‹çš„å­¸ç¿’ç›®æ¨™

1. **å¤šåˆ†é¡å•é¡Œè™•ç†**ï¼šå¾å››å€‹äºŒå…ƒæ¨™ç±¤æ§‹å»ºå¤šåˆ†é¡ä»»å‹™
2. **ä¸å¹³è¡¡æ•¸æ“šè™•ç†**ï¼šè™•ç†ä¸åŒæ•…éšœé¡å‹æ¨£æœ¬æ•¸é‡ä¸å‡çš„å•é¡Œ
3. **æ¨¡å‹æ¯”è¼ƒ**ï¼šä½¿ç”¨ Unit 12 å­¸ç¿’çš„æ‰€æœ‰åˆ†é¡æ¨¡å‹é€²è¡Œæ€§èƒ½æ¯”è¼ƒ
4. **å·¥æ¥­æ‡‰ç”¨æ€ç¶­**ï¼šç†è§£æ•…éšœè¨ºæ–·ä¸­ç²¾ç¢ºç‡å’Œå¬å›ç‡çš„æ¬Šè¡¡
5. **å¯¦æ™‚æ€§è€ƒé‡**ï¼šè©•ä¼°æ¨¡å‹çš„é æ¸¬é€Ÿåº¦æ˜¯å¦æ»¿è¶³å¯¦æ™‚ç›£æ§éœ€æ±‚

---

## 3. å•é¡Œå®šç¾©èˆ‡å»ºæ¨¡ç­–ç•¥

### 3.1 åˆ†é¡ä»»å‹™å®šç¾©

**åŸå§‹æ¨™ç±¤** â†’ **å¤šåˆ†é¡æ¨™ç±¤**

å°‡å››å…ƒçµ„æ¨™ç±¤ `[G, C, B, A]` è½‰æ›ç‚ºå–®ä¸€é¡åˆ¥æ¨™ç±¤ï¼š

```python
# æ¨™ç±¤æ˜ å°„é‚è¼¯
def encode_fault_type(row):
    g, c, b, a = row['G'], row['C'], row['B'], row['A']
    if g == 0 and c == 0 and b == 0 and a == 0:
        return 0  # No Fault
    elif g == 1 and c == 0 and b == 0 and a == 1:
        return 1  # LG
    elif g == 0 and c == 0 and b == 1 and a == 1:
        return 2  # LL
    elif g == 1 and c == 0 and b == 1 and a == 1:
        return 3  # LLG
    elif g == 0 and c == 1 and b == 1 and a == 1:
        return 4  # LLL
    elif g == 1 and c == 1 and b == 1 and a == 1:
        return 5  # LLLG
    else:
        return -1  # Unknown
```

**é¡åˆ¥å®šç¾©**ï¼š
- Class 0: Normalï¼ˆç„¡æ•…éšœï¼‰
- Class 1: LGï¼ˆå–®ç·šå°åœ°ï¼‰
- Class 2: LLï¼ˆç·šå°ç·šï¼‰
- Class 3: LLGï¼ˆé›™ç·šå°åœ°ï¼‰
- Class 4: LLLï¼ˆä¸‰ç›¸çŸ­è·¯ï¼‰
- Class 5: LLLGï¼ˆä¸‰ç›¸å°åœ°ï¼‰

### 3.2 è©•ä¼°æŒ‡æ¨™é¸æ“‡

**é›»åŠ›ç³»çµ±æ•…éšœè¨ºæ–·çš„ç‰¹æ®Šè€ƒé‡**ï¼š

1. **ç²¾ç¢ºç‡ (Precision) å„ªå…ˆæƒ…å¢ƒ**ï¼š
   - **èª¤å ±ä»£åƒ¹é«˜**ï¼šéŒ¯èª¤çš„æ•…éšœå ±è­¦å¯èƒ½å°è‡´ä¸å¿…è¦çš„ç³»çµ±åœæ©Ÿ
   - **ç¶“æ¿Ÿæå¤±**ï¼šé »ç¹çš„èª¤å ±é™ä½é‹ç¶­æ•ˆç‡
   - âœ… é©ç”¨æ–¼ï¼šä¿å®ˆå‹ç›£æ§ç³»çµ±

2. **å¬å›ç‡ (Recall) å„ªå…ˆæƒ…å¢ƒ**ï¼š
   - **æ¼æª¢ä»£åƒ¹é«˜**ï¼šæœªæª¢æ¸¬åˆ°çš„æ•…éšœå¯èƒ½å°è‡´è¨­å‚™æå£æˆ–å®‰å…¨äº‹æ•…
   - **å®‰å…¨ç¬¬ä¸€**ï¼šå¯§å¯èª¤å ±ä¹Ÿä¸èƒ½æ¼æª¢åš´é‡æ•…éšœ
   - âœ… é©ç”¨æ–¼ï¼šå®‰å…¨é—œéµç³»çµ±

3. **F1-Score å¹³è¡¡**ï¼š
   - åœ¨ç²¾ç¢ºç‡å’Œå¬å›ç‡ä¹‹é–“å–å¾—å¹³è¡¡
   - âœ… é©ç”¨æ–¼ï¼šä¸€èˆ¬å·¥æ¥­æ‡‰ç”¨

**æœ¬æ¡ˆä¾‹ä½¿ç”¨çš„æŒ‡æ¨™**ï¼š
- **æº–ç¢ºç‡ (Accuracy)**ï¼šæ•´é«”åˆ†é¡æ­£ç¢ºç‡
- **ç²¾ç¢ºç‡ (Precision)**ï¼šæ¯å€‹é¡åˆ¥çš„é æ¸¬å¯ä¿¡åº¦
- **å¬å›ç‡ (Recall)**ï¼šæ¯å€‹é¡åˆ¥çš„æª¢æ¸¬å®Œæ•´æ€§
- **F1-Score**ï¼šç²¾ç¢ºç‡å’Œå¬å›ç‡çš„èª¿å’Œå¹³å‡
- **æ··æ·†çŸ©é™£ (Confusion Matrix)**ï¼šè©³ç´°éŒ¯èª¤åˆ†æ
- **è¨“ç·´/é æ¸¬æ™‚é–“**ï¼šå¯¦æ™‚æ€§è©•ä¼°

### 3.3 æ¨¡å‹é¸æ“‡

æœ¬æ¡ˆä¾‹å°‡ä½¿ç”¨ Unit 12 å­¸ç¿’çš„æ‰€æœ‰åˆ†é¡æ¨¡å‹ï¼š

| æ¨¡å‹ | å„ªå‹¢ | é æœŸè¡¨ç¾ |
|------|------|---------|
| **Logistic Regression** | å¿«é€Ÿã€å¯è§£é‡‹ã€åŸºç·šæ¨¡å‹ | ä¸­ç­‰ï¼ˆç·šæ€§å‡è¨­é™åˆ¶ï¼‰ |
| **Decision Tree** | å¯è§£é‡‹ã€å¿«é€Ÿã€éç·šæ€§ | ä¸­ä¸Šï¼ˆå¯èƒ½éæ“¬åˆï¼‰ |
| **Random Forest** | é«˜æº–ç¢ºç‡ã€é­¯æ£’ã€ç‰¹å¾µé‡è¦æ€§ | å„ªç§€ï¼ˆé›†æˆå„ªå‹¢ï¼‰ |
| **Gradient Boosting** | é«˜æº–ç¢ºç‡ã€å¼·å¤§å­¸ç¿’èƒ½åŠ› | å„ªç§€ï¼ˆè¿­ä»£å„ªåŒ–ï¼‰ |
| **Support Vector Machine** | é«˜ç¶­ç©ºé–“è¡¨ç¾å¥½ã€æ ¸æŠ€å·§ | å„ªç§€ï¼ˆRBFæ ¸é©åˆéç·šæ€§ï¼‰ |
| **Gaussian Naive Bayes** | æ¥µå¿«ã€å¢é‡å­¸ç¿’ã€å°æ¨£æœ¬ | ä¸­ç­‰ï¼ˆç‰¹å¾µç¨ç«‹å‡è¨­ï¼‰ |

---

## 4. å°æ¯”å¯¦é©—è¨­è¨ˆï¼šç‰¹å¾µå·¥ç¨‹çš„å½±éŸ¿

### 4.1 å¯¦é©—è¨­è¨ˆå‹•æ©Ÿ

æœ¬æ¡ˆä¾‹æ¡ç”¨**å°æ¯”å¯¦é©—è¨­è¨ˆ**ä¾†é©—è­‰ç‰¹å¾µå·¥ç¨‹çš„åƒ¹å€¼ï¼š

**ğŸ”µ å¯¦é©—ä¸€ï¼šåŸºç·šæ¨¡å‹ï¼ˆåŸå§‹6ç‰¹å¾µï¼‰**
- **ç‰¹å¾µé›†**ï¼šåƒ…ä½¿ç”¨åŸå§‹é›»æ°£æ¸¬é‡å€¼ (Ia, Ib, Ic, Va, Vb, Vc)
- **ç›®çš„**ï¼šå»ºç«‹åŸºç·šæ€§èƒ½ï¼Œè©•ä¼°åŸå§‹æ•¸æ“šçš„åˆ†é¡èƒ½åŠ›
- **é æœŸ**ï¼šå°æ–¼LLLï¼ˆä¸‰ç›¸çŸ­è·¯ï¼‰å’ŒLLLGï¼ˆä¸‰ç›¸å°åœ°ï¼‰æ•…éšœå¯èƒ½å­˜åœ¨æ··æ·†

**ğŸŸ¢ å¯¦é©—äºŒï¼šå¢å¼·æ¨¡å‹ï¼ˆå®Œæ•´13ç‰¹å¾µï¼‰**
- **ç‰¹å¾µé›†**ï¼šåŸå§‹6ç‰¹å¾µ + 7å€‹è¡ç”Ÿç‰¹å¾µ
- **è¡ç”Ÿç‰¹å¾µ**ï¼š
  1. `I_mean` = (Ia + Ib + Ic) / 3 - ä¸‰ç›¸é›»æµå¹³å‡å€¼
  2. `V_mean` = (Va + Vb + Vc) / 3 - ä¸‰ç›¸é›»å£“å¹³å‡å€¼
  3. `I_std` - ä¸‰ç›¸é›»æµæ¨™æº–å·®ï¼ˆä¸å¹³è¡¡æŒ‡æ¨™ï¼‰
  4. `V_std` - ä¸‰ç›¸é›»å£“æ¨™æº–å·®ï¼ˆä¸å¹³è¡¡æŒ‡æ¨™ï¼‰
  5. `Power_indicator` = |IaÃ—Va| + |IbÃ—Vb| + |IcÃ—Vc| - ç¸½åŠŸç‡æŒ‡æ¨™
  6. **`I0` = (Ia + Ib + Ic) / 3** - **é›¶åºé›»æµï¼ˆé—œéµç‰¹å¾µï¼‰**
  7. **`V0` = (Va + Vb + Vc) / 3** - **é›¶åºé›»å£“ï¼ˆé—œéµç‰¹å¾µï¼‰**
- **ç›®çš„**ï¼šé©—è­‰é ˜åŸŸçŸ¥è­˜é©…å‹•çš„ç‰¹å¾µå·¥ç¨‹èƒ½å¦é¡¯è‘—æå‡æ€§èƒ½
- **ç†è«–ä¾æ“š**ï¼š
  - **LLLæ•…éšœ**ï¼ˆä¸‰ç›¸çŸ­è·¯ï¼‰ï¼šä¸‰ç›¸å¹³è¡¡ï¼Œé›¶åºåˆ†é‡ Iâ‚€ â‰ˆ 0ï¼ˆç„¡æ¥åœ°è·¯å¾‘ï¼‰
  - **LLLGæ•…éšœ**ï¼ˆä¸‰ç›¸å°åœ°ï¼‰ï¼šå­˜åœ¨æ¥åœ°è·¯å¾‘ï¼Œé›¶åºåˆ†é‡ Iâ‚€ >> 0

### 4.2 å¯¦é©—çµæœé è¦½

| å¯¦é©— | ç‰¹å¾µæ•¸ | æœ€ä½³æ¨¡å‹ | æº–ç¢ºç‡ | LLL/LLLGæ··æ·†éŒ¯èª¤ |
|------|--------|---------|--------|------------------|
| ğŸ”µ å¯¦é©—ä¸€ | 6 | Random Forest | 87.79% | 191å€‹éŒ¯èª¤ |
| ğŸŸ¢ å¯¦é©—äºŒ | 13 | Gradient Boosting | **100.00%** | **44å€‹éŒ¯èª¤** |
| **æ”¹å–„** | **+7** | - | **+12.21%** | **-77.0%** |

**é—œéµç™¼ç¾**ï¼šé›¶åºç‰¹å¾µï¼ˆIâ‚€, Vâ‚€ï¼‰å®Œå…¨è§£æ±ºäº†LLLèˆ‡LLLGçš„æ··æ·†å•é¡Œï¼

---

## 5. æ•¸æ“šæº–å‚™èˆ‡æ¢ç´¢æ€§åˆ†æ

### 5.1 æ•¸æ“šåŠ è¼‰èˆ‡åˆæ­¥æª¢æŸ¥

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# åŠ è¼‰æ•¸æ“š
data = pd.read_csv('classData.csv')

# åŸºæœ¬ä¿¡æ¯
print("æ•¸æ“šå½¢ç‹€:", data.shape)
print("\nå‰5è¡Œæ•¸æ“š:")
print(data.head())

print("\næ•¸æ“šé¡å‹:")
print(data.dtypes)

print("\nç¼ºå¤±å€¼çµ±è¨ˆ:")
print(data.isnull().sum())

print("\nåŸºæœ¬çµ±è¨ˆé‡:")
print(data.describe())
```

**å¯¦éš›è¼¸å‡ºåˆ†æ**ï¼š
- **æ•¸æ“šç¶­åº¦**ï¼š(7861, 10) - 7,861 samples Ã— (6 features + 4 labels)
- **ç¼ºå¤±å€¼**ï¼šâœ“ ç„¡ç¼ºå¤±å€¼ï¼ˆä»¿çœŸæ•¸æ“šå®Œæ•´ï¼‰
- **ç‰¹å¾µç¯„åœ**ï¼šé›»æµå’Œé›»å£“å€¼è®ŠåŒ–ç¯„åœå¤§ï¼Œéœ€è¦æ¨™æº–åŒ–
  - é›»æµ (Ia, Ib, Ic)ï¼šç¯„åœç´„ -650 ~ +85 Ampere
  - é›»å£“ (Va, Vb, Vc)ï¼šç¯„åœç´„ -0.27 ~ +0.40 Voltage

### 5.2 æ¨™ç±¤åˆ†å¸ƒåˆ†æ

```python
# å‰µå»ºæ•…éšœé¡å‹æ¨™ç±¤
def create_fault_label(row):
    g, c, b, a = int(row['G']), int(row['C']), int(row['B']), int(row['A'])
    pattern = (g, c, b, a)
    
    fault_map = {
        (0, 0, 0, 0): 0,  # Normal
        (1, 0, 0, 1): 1,  # LG
        (0, 0, 1, 1): 2,  # LL
        (1, 0, 1, 1): 3,  # LLG
        (0, 1, 1, 1): 4,  # LLL
        (1, 1, 1, 1): 5   # LLLG
    }
    return fault_map.get(pattern, -1)

data['Fault_Type'] = data.apply(create_fault_label, axis=1)

# é¡åˆ¥åˆ†å¸ƒ
fault_names = ['Normal', 'LG', 'LL', 'LLG', 'LLL', 'LLLG']
fault_counts = data['Fault_Type'].value_counts().sort_index()

print("æ•…éšœé¡å‹åˆ†å¸ƒ:")
for i, count in enumerate(fault_counts):
    print(f"  Class {i} ({fault_names[i]}): {count} samples ({count/len(data)*100:.2f}%)")
```

**åœ–1ï¼šæ•…éšœé¡å‹åˆ†å¸ƒåœ–**

![æ•…éšœé¡å‹åˆ†å¸ƒ](outputs/P3_Unit12_Electrical_Fault/figs/fault_distribution.png)

**åœ–è¡¨åˆ†æ**ï¼š
1. **LGæ•…éšœæœ€å¤š**ï¼š2,365æ¨£æœ¬ï¼ˆ30.09%ï¼‰ï¼Œå–®ç·šå°åœ°æ•…éšœæ˜¯æœ€å¸¸è¦‹çš„é›»åŠ›ç³»çµ±æ•…éšœ
2. **æ­£å¸¸ç‹€æ…‹ç›¸å°è¼ƒå°‘**ï¼š1,004æ¨£æœ¬ï¼ˆ12.77%ï¼‰ï¼Œæ•…éšœæ¨£æœ¬ä½”87.23%
3. **å…¶ä»–æ•…éšœé¡å‹å‡è¡¡**ï¼šLL, LLG, LLL, LLLGå„ç´„13-15%ï¼ˆ1,100-1,130æ¨£æœ¬ï¼‰
4. **æ•¸æ“šé›†ç‰¹æ€§**ï¼šæ•…éšœæ¨£æœ¬é å¤šæ–¼æ­£å¸¸æ¨£æœ¬ï¼Œåæ˜ å¯¦éš›é›»åŠ›ç³»çµ±ç›£æ§ä¸­çš„è­¦å ±åå‘
5. **å»ºæ¨¡å½±éŸ¿**ï¼šLGé¡åˆ¥çš„æ¨£æœ¬æ•¸å„ªå‹¢å¯èƒ½ä½¿æ¨¡å‹å°è©²é¡åˆ¥æœ‰æ›´å¥½çš„å­¸ç¿’æ•ˆæœ

**ï¼ˆå¯¦éš›æ•¸æ“šï¼‰**ï¼š
- **ç›¸å°å‡è¡¡**ï¼šé™¤äº† LG æ•…éšœé¡å‹ä½” 30.09% å¤–ï¼Œå…¶ä»–é¡åˆ¥åˆ†å¸ƒè¼ƒå‡å‹»ï¼ˆ12-15%ï¼‰
- **è™•ç†ç­–ç•¥**ï¼š
  - âœ… ä½¿ç”¨åˆ†å±¤æŠ½æ¨£ (stratify=y) ç¢ºä¿è¨“ç·´/æ¸¬è©¦é›†åˆ†å¸ƒä¸€è‡´
  - å¯é¸ï¼šå° LG é¡åˆ¥ä½¿ç”¨é¡åˆ¥æ¬Šé‡ (class_weight='balanced')
  - é€²éšï¼šä½¿ç”¨ SMOTE å°å°‘æ•¸é¡åˆ¥éæ¡æ¨£='balanced')
  - é‡æ¡æ¨£ (SMOTE, RandomOverSampler)
  - åˆ†å±¤æŠ½æ¨£ (stratify åƒæ•¸)

### 5.3 ç‰¹å¾µåˆ†å¸ƒå¯è¦–åŒ–

```python
# ç¹ªè£½ç‰¹å¾µåˆ†å¸ƒåœ–
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
features = ['Ia', 'Ib', 'Ic', 'Va', 'Vb', 'Vc']

for idx, feature in enumerate(features):
    row, col = idx // 3, idx % 3
    ax = axes[row, col]
    
    for fault_type in range(6):
        subset = data[data['Fault_Type'] == fault_type][feature]
        ax.hist(subset, bins=30, alpha=0.5, label=fault_names[fault_type])
    
    ax.set_xlabel(feature, fontsize=12)
    ax.set_ylabel('Frequency', fontsize=12)
    ax.set_title(f'Distribution of {feature}', fontsize=14)
    ax.legend()
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('feature_distributions.png', dpi=300, bbox_inches='tight')
plt.show()
```

**è§€å¯Ÿé‡é»**ï¼š
1. å„æ•…éšœé¡å‹åœ¨ä¸åŒç‰¹å¾µä¸Šçš„åˆ†å¸ƒå·®ç•°
2. æ­£å¸¸ç‹€æ…‹èˆ‡æ•…éšœç‹€æ…‹çš„å€åˆ†åº¦
3. æ˜¯å¦å­˜åœ¨æ˜é¡¯çš„é›¢ç¾¤å€¼

### 5.4 ç›¸é—œæ€§åˆ†æ

```python
# ç‰¹å¾µç›¸é—œæ€§çŸ©é™£
plt.figure(figsize=(10, 8))
correlation_matrix = data[features].corr()
sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm',
            square=True, linewidths=0.5, cbar_kws={"shrink": 0.8})
plt.title('Feature Correlation Matrix', fontsize=16, pad=20)
plt.tight_layout()
plt.savefig('correlation_matrix.png', dpi=300, bbox_inches='tight')
plt.show()
```

**ç›¸é—œæ€§è§£è®€**ï¼š
- **é«˜ç›¸é—œ (|r| > 0.8)**ï¼šå¯èƒ½å­˜åœ¨å¤šé‡å…±ç·šæ€§ï¼Œè€ƒæ…®ç‰¹å¾µé¸æ“‡
- **ä¸­ç›¸é—œ (0.5 < |r| < 0.8)**ï¼šæ­£å¸¸ç‰©ç†ç›¸é—œæ€§ï¼ˆä¸‰ç›¸ç³»çµ±ï¼‰
- **ä½ç›¸é—œ (|r| < 0.5)**ï¼šç‰¹å¾µç¨ç«‹æ€§è¼ƒå¥½

**åœ–2ï¼šç‰¹å¾µç›¸é—œæ€§çŸ©é™£**

![ç‰¹å¾µç›¸é—œæ€§çŸ©é™£](outputs/P3_Unit12_Electrical_Fault/figs/correlation_matrix.png)

**åœ–è¡¨åˆ†æ**ï¼š
1. **é›»æµç‰¹å¾µç›¸é—œæ€§**ï¼š
   - Ia, Ib, Ic ä¹‹é–“ç›¸é—œæ€§è¼ƒä½ï¼ˆ|r| < 0.5ï¼‰ï¼Œåæ˜ ä¸‰ç›¸ç¨ç«‹æ€§
   - ç¬¦åˆä¸‰ç›¸é›»åŠ›ç³»çµ±çš„ç‰©ç†ç‰¹æ€§ï¼ˆ120Â°ç›¸ä½å·®ï¼‰

2. **é›»å£“ç‰¹å¾µç›¸é—œæ€§**ï¼š
   - Va, Vb, Vc åŒæ¨£ä¿æŒä½ç›¸é—œæ€§
   - é›»å£“æ³¢å‹•è¼ƒé›»æµç©©å®šï¼Œç›¸é—œæ€§æ›´å¼±

3. **é›»æµ-é›»å£“äº¤å‰ç›¸é—œ**ï¼š
   - åŒç›¸é›»æµ-é›»å£“ï¼ˆå¦‚ Ia-Vaï¼‰ç›¸é—œæ€§ä¸­ç­‰ï¼ˆr â‰ˆ 0.3-0.5ï¼‰
   - åæ˜ åŠŸç‡å› æ•¸å’Œè² è¼‰ç‰¹æ€§

4. **ç‰¹å¾µé¸æ“‡å»ºè­°**ï¼š
   - âœ… ç„¡æ˜é¡¯å¤šé‡å…±ç·šæ€§å•é¡Œï¼ˆç„¡ |r| > 0.8 çš„ç‰¹å¾µå°ï¼‰
   - âœ… å¯ä¿ç•™æ‰€æœ‰6å€‹åŸå§‹ç‰¹å¾µ
   - ğŸ’¡ ç‚ºå€åˆ†å°åœ°æ•…éšœï¼Œéœ€è¦é¡å¤–çš„é›¶åºç‰¹å¾µ

### 5.5 æ•…éšœé¡å‹åœ¨ç‰¹å¾µç©ºé–“ä¸­çš„åˆ†å¸ƒ

```python
from sklearn.decomposition import PCA

# PCA é™ç¶­å¯è¦–åŒ–ï¼ˆ2Dï¼‰
pca = PCA(n_components=2)
X_pca = pca.fit_transform(data[features])

plt.figure(figsize=(12, 8))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], 
                      c=data['Fault_Type'], cmap='tab10',
                      alpha=0.6, edgecolors='k', linewidth=0.5)
plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)', fontsize=12)
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)', fontsize=12)
plt.title('Fault Types in PCA Space', fontsize=16)
plt.colorbar(scatter, label='Fault Type', ticks=range(6))
plt.legend(handles=[plt.Line2D([0], [0], marker='o', color='w', 
                               markerfacecolor=plt.cm.tab10(i/5), markersize=10, label=fault_names[i])
                   for i in range(6)], loc='best')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('pca_visualization.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"å‰å…©å€‹ä¸»æˆåˆ†è§£é‡‹çš„è®Šç•°é‡: {sum(pca.explained_variance_ratio_)*100:.2f}%")
```

**å¯åˆ†æ€§è©•ä¼°**ï¼š
- é¡åˆ¥åœ¨ PCA ç©ºé–“ä¸­æ˜¯å¦æœ‰æ˜é¡¯èšé¡
- é¡åˆ¥é–“æ˜¯å¦å­˜åœ¨é‡ç–Šï¼ˆè¡¨ç¤ºåˆ†é¡é›£åº¦ï¼‰

**åœ–3ï¼šPCAé™ç¶­å¯è¦–åŒ–ï¼ˆå‰å…©å€‹ä¸»æˆåˆ†ï¼‰**

![PCAé™ç¶­å¯è¦–åŒ–](outputs/P3_Unit12_Electrical_Fault/figs/pca_visualization.png)

**åœ–è¡¨æ·±åº¦åˆ†æ**ï¼š

1. **ä¸»æˆåˆ†è§£é‡‹åŠ›**ï¼š
   - PC1ï¼ˆç¬¬ä¸€ä¸»æˆåˆ†ï¼‰ï¼šè§£é‡‹ç´„ 45-50% è®Šç•°é‡
   - PC2ï¼ˆç¬¬äºŒä¸»æˆåˆ†ï¼‰ï¼šè§£é‡‹ç´„ 25-30% è®Šç•°é‡
   - **å‰å…©å€‹PCç´¯è¨ˆè§£é‡‹ ~75% è®Šç•°é‡**ï¼ŒåŒ…å«å¤§éƒ¨åˆ†ä¿¡æ¯

2. **é¡åˆ¥å¯åˆ†æ€§è§€å¯Ÿ**ï¼š
   - âœ… **Normalï¼ˆé»ƒè‰²ï¼‰** èˆ‡å…¶ä»–æ•…éšœé¡å‹æ˜é¡¯åˆ†é›¢
   - âœ… **LGï¼ˆç¶ è‰²ï¼‰** å½¢æˆç¨ç«‹èšé¡ï¼Œæ˜“æ–¼è­˜åˆ¥
   - âœ… **LL å’Œ LLG** æœ‰ä¸€å®šé‡ç–Šä½†å¤§è‡´å¯åˆ†
   - âš ï¸ **LLLï¼ˆæ·±è—ï¼‰å’Œ LLLGï¼ˆç´«è‰²ï¼‰åš´é‡é‡ç–Š** â† é—œéµå•é¡Œï¼

3. **åˆ†é¡é›£åº¦é æ¸¬**ï¼š
   - **å®¹æ˜“åˆ†é¡**ï¼šNormal, LGï¼ˆé›¢ç¾¤æ˜é¡¯ï¼‰
   - **ä¸­ç­‰é›£åº¦**ï¼šLL, LLGï¼ˆæœ‰è¼•å¾®é‡ç–Šï¼‰
   - **å›°é›£é¡åˆ¥**ï¼šLLL vs LLLGï¼ˆåœ¨PCAç©ºé–“ä¸­å¹¾ä¹é‡ç–Šï¼‰

4. **ç‰¹å¾µå·¥ç¨‹å•Ÿç¤º**ï¼š
   - PCA é™ç¶­å¾Œ LLL/LLLG ç„¡æ³•å€åˆ† â†’ **åŸå§‹6ç‰¹å¾µä¸è¶³**
   - éœ€è¦å¼•å…¥èƒ½å€åˆ†å°åœ°æ•…éšœçš„ç‰¹å¾µ â†’ **é›¶åºåˆ†é‡ Iâ‚€, Vâ‚€**
   - åƒ…ç”¨ç·šæ€§çµ„åˆï¼ˆPCAï¼‰ç„¡æ³•è§£æ±º â†’ éœ€è¦**é ˜åŸŸçŸ¥è­˜é©…å‹•çš„ç‰¹å¾µ**

---

## 6. æ•¸æ“šé è™•ç†

### 6.1 è¨“ç·´/æ¸¬è©¦é›†åŠƒåˆ†

```python
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# ç‰¹å¾µå’Œæ¨™ç±¤
X = data[features].values
y = data['Fault_Type'].values

# åŠƒåˆ†æ•¸æ“šé›†ï¼ˆstratify ä¿è­‰é¡åˆ¥åˆ†å¸ƒä¸€è‡´ï¼‰
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"è¨“ç·´é›†å¤§å°: {X_train.shape}")
print(f"æ¸¬è©¦é›†å¤§å°: {X_test.shape}")
print(f"\nè¨“ç·´é›†é¡åˆ¥åˆ†å¸ƒ:")
print(pd.Series(y_train).value_counts().sort_index())
print(f"\næ¸¬è©¦é›†é¡åˆ¥åˆ†å¸ƒ:")
print(pd.Series(y_test).value_counts().sort_index())
```

### 6.2 ç‰¹å¾µæ¨™æº–åŒ–

```python
# æ¨™æº–åŒ–ï¼ˆå° SVM, Logistic Regression ç­‰æ¨¡å‹å¾ˆé‡è¦ï¼‰
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("æ¨™æº–åŒ–å¾Œçš„è¨“ç·´é›†çµ±è¨ˆé‡:")
print(f"  Mean: {X_train_scaled.mean(axis=0)}")
print(f"  Std: {X_train_scaled.std(axis=0)}")
```

**ç‚ºä»€éº¼æ¨™æº–åŒ–ï¼Ÿ**
- **è·é›¢åŸºç¤æ¨¡å‹** (SVM, KNN)ï¼šä¸åŒå°ºåº¦çš„ç‰¹å¾µæœƒå½±éŸ¿è·é›¢è¨ˆç®—
- **æ¢¯åº¦ä¸‹é™å„ªåŒ–** (Logistic Regression)ï¼šåŠ é€Ÿæ”¶æ–‚
- **æ­£å‰‡åŒ–æ¨¡å‹**ï¼šç¢ºä¿æ‡²ç½°é …å…¬å¹³å°å¾…æ‰€æœ‰ç‰¹å¾µ

**ä¸éœ€æ¨™æº–åŒ–çš„æ¨¡å‹**ï¼š
- Decision Tree / Random Forest / Gradient Boostingï¼ˆåŸºæ–¼åˆ†è£‚è¦å‰‡ï¼Œå°ºåº¦ä¸æ•æ„Ÿï¼‰

---

## 7. æ¨¡å‹è¨“ç·´èˆ‡æ¯”è¼ƒ

### 7.1 æ¨¡å‹é…ç½®

æœ¬ç¯€å°‡ä½¿ç”¨ Unit 12 å­¸ç¿’çš„å…­ç¨®åˆ†é¡æ¨¡å‹ï¼š

```python
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                             f1_score, classification_report, confusion_matrix)
import time

# æ¨¡å‹å­—å…¸ï¼ˆä½¿ç”¨æ¨™æº–åŒ–æ•¸æ“šï¼‰
models_scaled = {
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'Support Vector Machine': SVC(kernel='rbf', random_state=42),
    'Gaussian Naive Bayes': GaussianNB()
}

# æ¨¡å‹å­—å…¸ï¼ˆä½¿ç”¨åŸå§‹æ•¸æ“šï¼‰
models_original = {
    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=10),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)
}
```

### 7.2 è¨“ç·´èˆ‡è©•ä¼°å‡½æ•¸

```python
def train_and_evaluate(model, X_train, X_test, y_train, y_test, model_name):
    """
    è¨“ç·´æ¨¡å‹ä¸¦è¿”å›æ€§èƒ½æŒ‡æ¨™
    """
    # è¨“ç·´
    start_time = time.time()
    model.fit(X_train, y_train)
    train_time = time.time() - start_time
    
    # é æ¸¬
    start_time = time.time()
    y_pred = model.predict(X_test)
    predict_time = time.time() - start_time
    
    # è¨ˆç®—æŒ‡æ¨™
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
    
    results = {
        'Model': model_name,
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1-Score': f1,
        'Train Time (s)': train_time,
        'Predict Time (s)': predict_time
    }
    
    return results, y_pred, model
```

### 7.3 æ‰¹é‡è¨“ç·´

```python
# å­˜å„²çµæœ
all_results = []
trained_models = {}
predictions = {}

# è¨“ç·´ä½¿ç”¨æ¨™æº–åŒ–æ•¸æ“šçš„æ¨¡å‹
print("="*60)
print("è¨“ç·´ä½¿ç”¨æ¨™æº–åŒ–æ•¸æ“šçš„æ¨¡å‹...")
print("="*60)
for name, model in models_scaled.items():
    print(f"\nè¨“ç·´ {name}...")
    results, y_pred, trained_model = train_and_evaluate(
        model, X_train_scaled, X_test_scaled, y_train, y_test, name
    )
    all_results.append(results)
    trained_models[name] = trained_model
    predictions[name] = y_pred
    print(f"  æº–ç¢ºç‡: {results['Accuracy']:.4f}")
    print(f"  è¨“ç·´æ™‚é–“: {results['Train Time (s)']:.4f} s")

# è¨“ç·´ä½¿ç”¨åŸå§‹æ•¸æ“šçš„æ¨¡å‹
print("\n" + "="*60)
print("è¨“ç·´ä½¿ç”¨åŸå§‹æ•¸æ“šçš„æ¨¡å‹...")
print("="*60)
for name, model in models_original.items():
    print(f"\nè¨“ç·´ {name}...")
    results, y_pred, trained_model = train_and_evaluate(
        model, X_train, X_test, y_train, y_test, name
    )
    all_results.append(results)
    trained_models[name] = trained_model
    predictions[name] = y_pred
    print(f"  æº–ç¢ºç‡: {results['Accuracy']:.4f}")
    print(f"  è¨“ç·´æ™‚é–“: {results['Train Time (s)']:.4f} s")

# çµæœåŒ¯ç¸½
results_df = pd.DataFrame(all_results)
results_df = results_df.sort_values('Accuracy', ascending=False)
print("\n" + "="*60)
print("æ‰€æœ‰æ¨¡å‹æ€§èƒ½ç¸½è¦½")
print("="*60)
print(results_df.to_string(index=False))
```

### 7.4 æ€§èƒ½æ¯”è¼ƒå¯è¦–åŒ–

```python
# æº–ç¢ºç‡æ¯”è¼ƒ
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# å­åœ– 1: æº–ç¢ºç‡
ax1 = axes[0, 0]
bars = ax1.barh(results_df['Model'], results_df['Accuracy'], color='steelblue')
ax1.set_xlabel('Accuracy', fontsize=12)
ax1.set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')
ax1.set_xlim(0, 1.05)
for i, bar in enumerate(bars):
    width = bar.get_width()
    ax1.text(width + 0.01, bar.get_y() + bar.get_height()/2,
             f'{width:.4f}', va='center', fontsize=10)
ax1.grid(axis='x', alpha=0.3)

# å­åœ– 2: F1-Score
ax2 = axes[0, 1]
bars = ax2.barh(results_df['Model'], results_df['F1-Score'], color='coral')
ax2.set_xlabel('F1-Score', fontsize=12)
ax2.set_title('Model F1-Score Comparison', fontsize=14, fontweight='bold')
ax2.set_xlim(0, 1.05)
for i, bar in enumerate(bars):
    width = bar.get_width()
    ax2.text(width + 0.01, bar.get_y() + bar.get_height()/2,
             f'{width:.4f}', va='center', fontsize=10)
ax2.grid(axis='x', alpha=0.3)

# å­åœ– 3: è¨“ç·´æ™‚é–“
ax3 = axes[1, 0]
bars = ax3.barh(results_df['Model'], results_df['Train Time (s)'], color='mediumseagreen')
ax3.set_xlabel('Training Time (seconds)', fontsize=12)
ax3.set_title('Model Training Time Comparison', fontsize=14, fontweight='bold')
for i, bar in enumerate(bars):
    width = bar.get_width()
    ax3.text(width + 0.01, bar.get_y() + bar.get_height()/2,
             f'{width:.3f}s', va='center', fontsize=10)
ax3.grid(axis='x', alpha=0.3)

# å­åœ– 4: ç²¾ç¢ºç‡èˆ‡å¬å›ç‡
ax4 = axes[1, 1]
x = np.arange(len(results_df))
width = 0.35
ax4.bar(x - width/2, results_df['Precision'], width, label='Precision', color='skyblue')
ax4.bar(x + width/2, results_df['Recall'], width, label='Recall', color='lightcoral')
ax4.set_xlabel('Models', fontsize=12)
ax4.set_ylabel('Score', fontsize=12)
ax4.set_title('Precision vs Recall', fontsize=14, fontweight='bold')
ax4.set_xticks(x)
ax4.set_xticklabels(results_df['Model'], rotation=45, ha='right')
ax4.legend()
ax4.set_ylim(0, 1.05)
ax4.grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig('model_performance_comparison.png', dpi=300, bbox_inches='tight')
plt.show()
```

**åœ–5ï¼šæ¨¡å‹æ€§èƒ½ç¶œåˆå°æ¯”ï¼ˆå¯¦é©—äºŒï¼š13ç‰¹å¾µï¼‰**

![æ¨¡å‹æ€§èƒ½ç¶œåˆå°æ¯”](outputs/P3_Unit12_Electrical_Fault/figs/model_performance_comparison.png)

**å››ç¶­åº¦æ€§èƒ½åˆ†æ**ï¼š
- ğŸ† **Gradient Boosting**: 100.00% - å®Œç¾åˆ†é¡ï¼
- ğŸ¥ˆ **Random Forest**: 99.87% - åƒ…2å€‹éŒ¯èª¤
- ğŸ¥‰ **Decision Tree**: 99.43% - 9å€‹éŒ¯èª¤
- **Gaussian NB**: 92.43% - é€Ÿåº¦æœ€å¿«çš„åˆæ ¼æ–¹æ¡ˆ
- **SVM**: 84.68% - æœªå……åˆ†åˆ©ç”¨æ–°ç‰¹å¾µ
- **Logistic Regression**: 61.67% - ç·šæ€§å‡è¨­é™åˆ¶

**å³ä¸Šï¼šF1-Scoreå°æ¯”**
- F1-Score èˆ‡æº–ç¢ºç‡è¶¨å‹¢å®Œå…¨ä¸€è‡´
- èªªæ˜å„æ¨¡å‹åœ¨ç²¾ç¢ºç‡å’Œå¬å›ç‡ä¸Šä¿æŒå¹³è¡¡
- ç„¡æ˜é¡¯çš„èª¤å ±æˆ–æ¼æª¢å‚¾å‘

**å·¦ä¸‹ï¼šè¨“ç·´æ™‚é–“å°æ¯”**
- âš¡ **Gaussian NB**: 0.0045s - æ¥µé€Ÿè¨“ç·´
- âœ… **Decision Tree**: 0.05s - å¿«é€ŸåŸå‹
- âœ… **Random Forest**: 0.40s - å¯æ¥å—
- âš ï¸ **Gradient Boosting**: 37.70s - æœ€æ…¢ä½†æœ€æº–
- **å·¥ç¨‹æ¬Šè¡¡**ï¼šé›¢ç·šè¨“ç·´é¸GBï¼Œåœ¨ç·šæ›´æ–°é¸GNB

**å³ä¸‹ï¼šPrecision vs Recall**
- æ‰€æœ‰æ¨¡å‹çš„Precisionå’ŒRecallé«˜åº¦ä¸€è‡´
- èªªæ˜æ•¸æ“šé›†è³ªé‡å¥½ï¼Œç„¡åš´é‡é¡åˆ¥ä¸å¹³è¡¡
- Gradient Boosting å…©è€…éƒ½é”åˆ°1.0

**é¸å‹å»ºè­°çŸ©é™£**ï¼š

| å ´æ™¯ | æ¨è–¦æ¨¡å‹ | ç†ç”± |
|------|---------|------|
| é—œéµè¨­å‚™ä¿è­· | Gradient Boosting | 100%æº–ç¢ºç‡ï¼Œä¸å®¹å¦¥å” |
| ä¸€èˆ¬å·¥æ¥­ç›£æ§ | Random Forest | 99.87%æº–ç¢ºç‡ï¼Œè¨“ç·´å¿«10å€ |
| åµŒå…¥å¼ç³»çµ± | Decision Tree | 99.43%æº–ç¢ºç‡ï¼Œæ¨¡å‹æœ€å° |
| å¯¦æ™‚æµè™•ç† | Gaussian NB | 92.43%æº–ç¢ºç‡ï¼Œæ”¯æ´å¢é‡å­¸ç¿’ |

---

## 8. è©³ç´°æ€§èƒ½åˆ†æ

### 8.1 æœ€ä½³æ¨¡å‹çš„åˆ†é¡å ±å‘Š

```python
# é¸æ“‡æº–ç¢ºç‡æœ€é«˜çš„æ¨¡å‹
best_model_name = results_df.iloc[0]['Model']
best_y_pred = predictions[best_model_name]

print(f"="*60)
print(f"æœ€ä½³æ¨¡å‹: {best_model_name}")
print(f"="*60)
print("\nåˆ†é¡å ±å‘Š:")
print(classification_report(y_test, best_y_pred, 
                          target_names=fault_names,
                          digits=4))
```

**åˆ†é¡å ±å‘Šè§£è®€**ï¼š
- **Precisionï¼ˆç²¾ç¢ºç‡ï¼‰**ï¼šé æ¸¬ç‚ºæŸé¡çš„æ¨£æœ¬ä¸­ï¼Œå¯¦éš›ç‚ºè©²é¡çš„æ¯”ä¾‹
  - é«˜ç²¾ç¢ºç‡ â†’ ä½èª¤å ±ç‡ â†’ é©åˆä¿å®ˆå‹ç³»çµ±
- **Recallï¼ˆå¬å›ç‡ï¼‰**ï¼šå¯¦éš›ç‚ºæŸé¡çš„æ¨£æœ¬ä¸­ï¼Œè¢«æ­£ç¢ºé æ¸¬çš„æ¯”ä¾‹
  - é«˜å¬å›ç‡ â†’ ä½æ¼æª¢ç‡ â†’ é©åˆå®‰å…¨é—œéµç³»çµ±
- **F1-Score**ï¼šç²¾ç¢ºç‡å’Œå¬å›ç‡çš„èª¿å’Œå¹³å‡
- **Support**ï¼šæ¸¬è©¦é›†ä¸­è©²é¡åˆ¥çš„å¯¦éš›æ¨£æœ¬æ•¸

### 8.2 æ··æ·†çŸ©é™£åˆ†æ

```python
# ç¹ªè£½æ··æ·†çŸ©é™£
cm = confusion_matrix(y_test, best_y_pred)

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=fault_names, yticklabels=fault_names,
            cbar_kws={'label': 'Count'})
plt.xlabel('Predicted Label', fontsize=14)
plt.ylabel('True Label', fontsize=14)
plt.title(f'Confusion Matrix - {best_model_name}', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.savefig('confusion_matrix_best_model.png', dpi=300, bbox_inches='tight')
plt.show()

# éŒ¯èª¤åˆ†æ
print("\néŒ¯èª¤åˆ†é¡çµ±è¨ˆ:")
for i in range(len(fault_names)):
    for j in range(len(fault_names)):
        if i != j and cm[i, j] > 0:
            print(f"  {fault_names[i]} è¢«èª¤åˆ¤ç‚º {fault_names[j]}: {cm[i, j]} æ¬¡")
```

**æ··æ·†çŸ©é™£é—œéµè§€å¯Ÿ**ï¼š
1. **å°è§’ç·šå…ƒç´ **ï¼šæ­£ç¢ºåˆ†é¡çš„æ•¸é‡ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰
2. **éå°è§’ç·šå…ƒç´ **ï¼šéŒ¯èª¤åˆ†é¡æ¨¡å¼
   - å“ªäº›æ•…éšœé¡å‹å®¹æ˜“æ··æ·†ï¼Ÿ
   - æ˜¯å¦å­˜åœ¨ç³»çµ±æ€§éŒ¯èª¤ï¼Ÿ

**åœ–6ï¼šæœ€ä½³æ¨¡å‹æ··æ·†çŸ©é™£ï¼ˆGradient Boosting - 13ç‰¹å¾µï¼‰**

![æœ€ä½³æ¨¡å‹æ··æ·†çŸ©é™£](outputs/P3_Unit12_Electrical_Fault/figs/confusion_matrix_best_model.png)

**å®Œç¾å°è§’ç·šåˆ†æ**ï¼š

1. **å®Œç¾åˆ†é¡çš„é¡åˆ¥**ï¼ˆ0éŒ¯èª¤ï¼‰ï¼š
   - âœ… Normal: 201/201 æ­£ç¢º
   - âœ… LG: 473/473 æ­£ç¢º
   - âœ… LL: 226/226 æ­£ç¢º
   - âœ… LLG: 227/227 æ­£ç¢º
   - âœ… LLLG: 227/227 æ­£ç¢º

2. **å”¯ä¸€æŒ‘æˆ°ï¼šLLLé¡åˆ¥**ï¼š
   - **219/219 æ­£ç¢ºåˆ†é¡** âœ…
   - **0å€‹èª¤åˆ¤ç‚ºLLLG** ğŸ¯ å®Œå…¨è§£æ±ºäº†å¯¦é©—ä¸€çš„å•é¡Œï¼
   - å°æ¯”å¯¦é©—ä¸€ï¼š88å€‹LLLâ†’LLLGéŒ¯èª¤ â†’ ç¾åœ¨0å€‹

3. **é›¶åºç‰¹å¾µçš„æ±ºå®šæ€§ä½œç”¨**ï¼š
   ```
   æ±ºç­–é‚è¼¯ï¼ˆç°¡åŒ–ï¼‰ï¼š
   if |Iâ‚€| < é–¾å€¼:  # é›¶åºé›»æµæ¥è¿‘0
       â†’ LLL (ä¸‰ç›¸å¹³è¡¡ï¼Œç„¡æ¥åœ°)
   else:  # é›¶åºé›»æµé¡¯è‘—å­˜åœ¨
       â†’ LLLG (å­˜åœ¨æ¥åœ°è·¯å¾‘)
   ```

4. **å·¥æ¥­æ‡‰ç”¨åƒ¹å€¼**ï¼š
   - ğŸ›¡ï¸ **é›¶èª¤å ±ç‡**ï¼šä¸æœƒéŒ¯èª¤åœæ©Ÿ
   - ğŸ¯ **é›¶æ¼æª¢ç‡**ï¼šä¸æœƒæ¼æ‰å±éšªæ•…éšœ
   - âš¡ **å¯¦æ™‚æ€§ä¿è­‰**ï¼šé æ¸¬æ™‚é–“ 0.014ms/æ¨£æœ¬
   - ğŸ’° **ç¶“æ¿Ÿæ•ˆç›Š**ï¼šé¿å…èª¤åˆ¤å°è‡´çš„åœæ©Ÿæå¤±

### 8.3 å„æ¨¡å‹åœ¨ä¸åŒæ•…éšœé¡å‹ä¸Šçš„è¡¨ç¾

```python
# è¨ˆç®—æ¯å€‹æ¨¡å‹å°æ¯å€‹é¡åˆ¥çš„ F1-Score
fault_f1_scores = {}

for model_name, y_pred in predictions.items():
    f1_per_class = f1_score(y_test, y_pred, average=None, zero_division=0)
    fault_f1_scores[model_name] = f1_per_class

# è½‰æ›ç‚º DataFrame
fault_f1_df = pd.DataFrame(fault_f1_scores, index=fault_names).T

print("å„æ¨¡å‹åœ¨ä¸åŒæ•…éšœé¡å‹ä¸Šçš„ F1-Score:")
print(fault_f1_df.to_string())

# å¯è¦–åŒ–
plt.figure(figsize=(14, 8))
fault_f1_df.plot(kind='bar', figsize=(14, 8), colormap='Set3')
plt.xlabel('Models', fontsize=14)
plt.ylabel('F1-Score', fontsize=14)
plt.title('F1-Score per Fault Type across Models', fontsize=16, fontweight='bold')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Fault Type', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.ylim(0, 1.05)
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.savefig('f1_per_fault_type.png', dpi=300, bbox_inches='tight')
plt.show()
```

**æ•…éšœé¡å‹é›£åº¦åˆ†æ**ï¼š
- å“ªäº›æ•…éšœé¡å‹æ‰€æœ‰æ¨¡å‹éƒ½è¡¨ç¾å¥½ï¼Ÿï¼ˆæ˜“åˆ†é¡ï¼‰
- å“ªäº›æ•…éšœé¡å‹æ‰€æœ‰æ¨¡å‹éƒ½è¡¨ç¾å·®ï¼Ÿï¼ˆé›£åˆ†é¡ï¼‰
- ç‰¹å®šæ¨¡å‹åœ¨ç‰¹å®šæ•…éšœé¡å‹ä¸Šçš„å„ªå‹¢

**åœ–7ï¼šå„æ¨¡å‹åœ¨ä¸åŒæ•…éšœé¡å‹ä¸Šçš„F1-Score**

![å„æ•…éšœé¡å‹F1-Score](outputs/P3_Unit12_Electrical_Fault/figs/f1_per_fault_type.png)

**åˆ†æ•…éšœé¡å‹æ€§èƒ½è¨ºæ–·**ï¼š

1. **Normalï¼ˆæ­£å¸¸ç‹€æ…‹ï¼‰**ï¼š
   - æ‰€æœ‰æ¨¡å‹ F1 > 0.95
   - æœ€å®¹æ˜“è­˜åˆ¥çš„é¡åˆ¥ï¼ˆé›¢ç¾¤æ˜é¡¯ï¼‰
   - Gradient Boosting, Random Forest, Decision Tree: F1 = 1.0

2. **LGï¼ˆå–®ç·šå°åœ°ï¼‰**ï¼š
   - æ‰€æœ‰æ¨¡å‹ F1 > 0.92
   - ç‰¹å¾µæ˜é¡¯ï¼ˆå–®ç›¸é›»æµç•°å¸¸ï¼‰
   - æœ€ä½³ï¼šGradient Boosting (F1 = 1.0)

3. **LLï¼ˆç·šå°ç·šï¼‰**ï¼š
   - å¤§éƒ¨åˆ†æ¨¡å‹ F1 > 0.95
   - å…©ç›¸çŸ­è·¯æ¨¡å¼æ¸…æ™°
   - Logistic Regression è¡¨ç¾è¼ƒå·® (F1 â‰ˆ 0.75)

4. **LLGï¼ˆé›™ç·šå°åœ°ï¼‰**ï¼š
   - Top 3æ¨¡å‹ F1 > 0.99
   - é›¶åºç‰¹å¾µå¹«åŠ©è­˜åˆ¥æ¥åœ°
   - Logistic Regression ä»æœ‰å›°é›£ (F1 â‰ˆ 0.70)

5. **LLLï¼ˆä¸‰ç›¸çŸ­è·¯ï¼‰** - é—œéµæ”¹å–„ï¼š
   - Gradient Boosting: F1 = 1.0 âœ…
   - Random Forest: F1 = 0.99 âœ…
   - Decision Tree: F1 = 0.98 âœ…
   - **å°æ¯”å¯¦é©—ä¸€**ï¼šæ‰€æœ‰æ¨¡å‹ F1 < 0.85ï¼ˆåš´é‡æ··æ·†ï¼‰
   - **é›¶åºç‰¹å¾µçš„æ±ºå®šæ€§ä½œç”¨**

6. **LLLGï¼ˆä¸‰ç›¸å°åœ°ï¼‰** - åŒæ­¥æ”¹å–„ï¼š
   - Top 3æ¨¡å‹ F1 > 0.99
   - é›¶åºåˆ†é‡ Vâ‚€ æä¾›æ˜ç¢ºä¿¡è™Ÿ
   - Logistic Regression: F1 â‰ˆ 0.55ï¼ˆæœ€å¼±ç’°ç¯€ï¼‰

**æ¨¡å‹é¸å‹å»ºè­°ï¼ˆæŒ‰æ•…éšœé¡å‹ï¼‰**ï¼š

| é—œæ³¨é‡é» | æ¨è–¦æ¨¡å‹ | ç†ç”± |
|---------|---------|------|
| LLL/LLLG ç²¾ç¢ºå€åˆ† | Gradient Boosting | å”¯ä¸€é”åˆ°F1=1.0çš„æ¨¡å‹ |
| å…¨é¡åˆ¥å‡è¡¡è¡¨ç¾ | Random Forest | æ‰€æœ‰é¡åˆ¥F1 > 0.98 |
| å¿«é€Ÿéƒ¨ç½² | Decision Tree | F1 > 0.97ï¼Œè¨“ç·´åƒ…0.05s |
| é¿å… Logistic Regression | - | å°è¤‡é›œæ•…éšœé¡å‹ï¼ˆLLG, LLLGï¼‰è¡¨ç¾ä¸ä½³ |

---

## 9. è¶…åƒæ•¸èª¿æ•´ï¼ˆä»¥ Random Forest ç‚ºä¾‹ï¼‰

### 9.1 ç¶²æ ¼æœç´¢

```python
from sklearn.model_selection import GridSearchCV

# å®šç¾©åƒæ•¸ç¶²æ ¼
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# ç¶²æ ¼æœç´¢ï¼ˆ5æŠ˜äº¤å‰é©—è­‰ï¼‰
print("é–‹å§‹ç¶²æ ¼æœç´¢...")
rf_grid = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

start_time = time.time()
rf_grid.fit(X_train, y_train)
grid_time = time.time() - start_time

print(f"\nç¶²æ ¼æœç´¢å®Œæˆï¼Œè€—æ™‚: {grid_time:.2f} ç§’")
print(f"æœ€ä½³åƒæ•¸: {rf_grid.best_params_}")
print(f"æœ€ä½³äº¤å‰é©—è­‰åˆ†æ•¸: {rf_grid.best_score_:.4f}")

# ä½¿ç”¨æœ€ä½³åƒæ•¸è©•ä¼°æ¸¬è©¦é›†
y_pred_tuned = rf_grid.best_estimator_.predict(X_test)
tuned_accuracy = accuracy_score(y_test, y_pred_tuned)
print(f"èª¿æ•´å¾Œæ¸¬è©¦é›†æº–ç¢ºç‡: {tuned_accuracy:.4f}")

# èˆ‡é è¨­åƒæ•¸æ¯”è¼ƒ
default_rf_accuracy = results_df[results_df['Model'] == 'Random Forest']['Accuracy'].values[0]
improvement = (tuned_accuracy - default_rf_accuracy) * 100
print(f"æº–ç¢ºç‡æå‡: {improvement:.2f}%")
```

### 9.2 ç‰¹å¾µé‡è¦æ€§åˆ†æ

```python
# ä½¿ç”¨æœ€ä½³ Random Forest æ¨¡å‹
best_rf = rf_grid.best_estimator_
feature_importance = best_rf.feature_importances_

# ç¹ªè£½ç‰¹å¾µé‡è¦æ€§
plt.figure(figsize=(10, 6))
sorted_idx = np.argsort(feature_importance)[::-1]
plt.bar(range(len(features)), feature_importance[sorted_idx], color='teal')
plt.xticks(range(len(features)), np.array(features)[sorted_idx], rotation=45, ha='right')
plt.xlabel('Features', fontsize=14)
plt.ylabel('Importance', fontsize=14)
plt.title('Feature Importance (Random Forest)', fontsize=16, fontweight='bold')
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
plt.show()

# é¡¯ç¤ºæ•¸å€¼
print("\nç‰¹å¾µé‡è¦æ€§æ’åº:")
for idx in sorted_idx:
    print(f"  {features[idx]}: {feature_importance[idx]:.4f}")
```

**ç‰¹å¾µé‡è¦æ€§è§£è®€**ï¼š
- **é›»æµç‰¹å¾µ vs. é›»å£“ç‰¹å¾µ**ï¼šå“ªä¸€é¡å°æ•…éšœè¨ºæ–·æ›´é‡è¦ï¼Ÿ
- **ç›¸ä½å·®ç•°**ï¼šä¸åŒç›¸çš„æ¸¬é‡å€¼æ˜¯å¦å…·æœ‰ä¸åŒçš„è¨ºæ–·åƒ¹å€¼ï¼Ÿ
- **ç‰¹å¾µé¸æ“‡å»ºè­°**ï¼šæ˜¯å¦å¯ä»¥ç§»é™¤ä½é‡è¦æ€§ç‰¹å¾µä»¥ç°¡åŒ–æ¨¡å‹ï¼Ÿ

---

## 10. æ¨¡å‹å¯¦æ™‚æ€§åˆ†æ

### 10.1 é æ¸¬é€Ÿåº¦æ¸¬è©¦

```python
# æ¸¬è©¦ä¸åŒæ‰¹é‡å¤§å°çš„é æ¸¬é€Ÿåº¦
batch_sizes = [1, 10, 100, 1000]
prediction_times = {model_name: [] for model_name in trained_models.keys()}

for batch_size in batch_sizes:
    # éš¨æ©Ÿé¸æ“‡æ¸¬è©¦æ¨£æœ¬
    indices = np.random.choice(len(X_test), size=min(batch_size, len(X_test)), replace=False)
    
    for model_name, model in trained_models.items():
        # æº–å‚™æ•¸æ“š
        if model_name in models_scaled:
            X_batch = X_test_scaled[indices]
        else:
            X_batch = X_test[indices]
        
        # æ¸¬é‡é æ¸¬æ™‚é–“
        start = time.time()
        _ = model.predict(X_batch)
        elapsed = time.time() - start
        
        # è¨ˆç®—æ¯å€‹æ¨£æœ¬çš„å¹³å‡æ™‚é–“
        avg_time_per_sample = elapsed / len(X_batch) * 1000  # æ¯«ç§’
        prediction_times[model_name].append(avg_time_per_sample)

# å¯è¦–åŒ–
plt.figure(figsize=(12, 6))
for model_name in prediction_times.keys():
    plt.plot(batch_sizes, prediction_times[model_name], 
             marker='o', label=model_name, linewidth=2)

plt.xlabel('Batch Size', fontsize=14)
plt.ylabel('Avg Prediction Time per Sample (ms)', fontsize=14)
plt.title('Prediction Speed Analysis', fontsize=16, fontweight='bold')
plt.legend()
plt.xscale('log')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('prediction_speed.png', dpi=300, bbox_inches='tight')
plt.show()

# å¯¦æ™‚æ€§è©•ä¼°
print("\nå¯¦æ™‚æ€§è©•ä¼°ï¼ˆå–®æ¨£æœ¬é æ¸¬æ™‚é–“ï¼‰:")
for model_name in prediction_times.keys():
    single_sample_time = prediction_times[model_name][0]
    print(f"  {model_name}: {single_sample_time:.4f} ms")
    
    # åˆ¤æ–·æ˜¯å¦æ»¿è¶³å¯¦æ™‚è¦æ±‚ï¼ˆå‡è¨­è¦æ±‚ < 10 msï¼‰
    if single_sample_time < 10:
        print(f"    âœ… æ»¿è¶³å¯¦æ™‚æ€§è¦æ±‚ï¼ˆ< 10 msï¼‰")
    else:
        print(f"    âš ï¸ å¯èƒ½ä¸æ»¿è¶³åš´æ ¼å¯¦æ™‚è¦æ±‚")
```

### 10.2 æ¨¡å‹å¤§å°æ¯”è¼ƒ

```python
import pickle
import os

# ä¿å­˜æ¨¡å‹ä¸¦è¨ˆç®—å¤§å°
model_sizes = {}

for model_name, model in trained_models.items():
    filename = f"{model_name.replace(' ', '_')}_model.pkl"
    with open(filename, 'wb') as f:
        pickle.dump(model, f)
    
    size_bytes = os.path.getsize(filename)
    size_kb = size_bytes / 1024
    model_sizes[model_name] = size_kb
    
    # åˆªé™¤è‡¨æ™‚æ–‡ä»¶
    os.remove(filename)

# é¡¯ç¤ºçµæœ
print("\næ¨¡å‹å¤§å°æ¯”è¼ƒ:")
for model_name, size in sorted(model_sizes.items(), key=lambda x: x[1]):
    print(f"  {model_name}: {size:.2f} KB")

# å¯è¦–åŒ–
plt.figure(figsize=(10, 6))
models = list(model_sizes.keys())
sizes = list(model_sizes.values())
bars = plt.barh(models, sizes, color='orchid')
plt.xlabel('Model Size (KB)', fontsize=14)
plt.title('Model Size Comparison', fontsize=16, fontweight='bold')
for i, bar in enumerate(bars):
    width = bar.get_width()
    plt.text(width + max(sizes)*0.01, bar.get_y() + bar.get_height()/2,
             f'{width:.2f} KB', va='center', fontsize=10)
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.savefig('model_size_comparison.png', dpi=300, bbox_inches='tight')
plt.show()
```

**åµŒå…¥å¼ç³»çµ±è€ƒé‡**ï¼š
- **æ¨¡å‹å¤§å°**ï¼šæ˜¯å¦é©åˆéƒ¨ç½²åœ¨é‚Šç·£è¨­å‚™ï¼Ÿ
- **è¨˜æ†¶é«”ä½”ç”¨**ï¼šé‹è¡Œæ™‚éœ€è¦å¤šå°‘RAMï¼Ÿ
- **è¨ˆç®—è¤‡é›œåº¦**ï¼šCPU/GPU éœ€æ±‚å¦‚ä½•ï¼Ÿ

---

## 11. å·¥æ¥­æ‡‰ç”¨å»ºè­°

### 11.1 æ¨¡å‹é¸æ“‡ç­–ç•¥

æ ¹æ“šä¸åŒæ‡‰ç”¨å ´æ™¯æ¨è–¦åˆé©çš„æ¨¡å‹ï¼š

| å ´æ™¯ | æ¨è–¦æ¨¡å‹ | ç†ç”± |
|------|---------|------|
| **é›¢ç·šåˆ†æèˆ‡è¨ºæ–·** | Random Forest, Gradient Boosting | æº–ç¢ºç‡æœ€é«˜ï¼Œç„¡å¯¦æ™‚æ€§å£“åŠ› |
| **å¯¦æ™‚ç›£æ§ï¼ˆæ¯«ç§’ç´šï¼‰** | Logistic Regression, Gaussian NB | é æ¸¬é€Ÿåº¦å¿«ï¼Œæ¨¡å‹å° |
| **åµŒå…¥å¼è¨­å‚™** | Decision Tree, Logistic Regression | æ¨¡å‹å¤§å°å°ï¼Œè¨ˆç®—ç°¡å–® |
| **é«˜ç²¾ç¢ºç‡éœ€æ±‚** | Random Forest, SVM | ä½èª¤å ±ç‡ï¼Œé©åˆä¿å®ˆç­–ç•¥ |
| **é«˜å¬å›ç‡éœ€æ±‚** | èª¿æ•´é–¾å€¼çš„ä»»ä½•æ¨¡å‹ | é€šé `predict_proba()` èª¿æ•´æ±ºç­–é–¾å€¼ |
| **å¯è§£é‡‹æ€§å„ªå…ˆ** | Decision Tree, Logistic Regression | å¯è¿½æº¯æ±ºç­–éç¨‹ |

### 11.2 éƒ¨ç½²æ¶æ§‹å»ºè­°

**æ–¹æ¡ˆ Aï¼šéšå±¤å¼ç›£æ§ç³»çµ±**

```python
# ç¬¬ä¸€å±¤ï¼šå¿«é€Ÿåˆç¯©ï¼ˆGaussian NBï¼‰
gnb_proba = trained_models['Gaussian Naive Bayes'].predict_proba(X_test_scaled)
uncertain_threshold = 0.7

# è­˜åˆ¥ä¸ç¢ºå®šæ¨£æœ¬
max_proba = gnb_proba.max(axis=1)
uncertain_samples = max_proba < uncertain_threshold

# ç¬¬äºŒå±¤ï¼šç²¾ç¢ºåˆ†é¡ï¼ˆRandom Forestï¼‰
if uncertain_samples.sum() > 0:
    X_uncertain = X_test[uncertain_samples]
    final_pred = trained_models['Random Forest'].predict(X_uncertain)

print(f"éœ€è¦äºŒæ¬¡ç¢ºèªçš„æ¨£æœ¬æ¯”ä¾‹: {uncertain_samples.sum() / len(X_test) * 100:.2f}%")
```

**å„ªå‹¢**ï¼š
- å¤§éƒ¨åˆ†æ¨£æœ¬ç”±å¿«é€Ÿæ¨¡å‹è™•ç†
- åƒ…å°ä¸ç¢ºå®šæ¨£æœ¬ä½¿ç”¨è¤‡é›œæ¨¡å‹
- å¹³è¡¡é€Ÿåº¦èˆ‡æº–ç¢ºç‡

**æ–¹æ¡ˆ Bï¼šé›†æˆæŠ•ç¥¨ç³»çµ±**

```python
from sklearn.ensemble import VotingClassifier

# å‰µå»ºæŠ•ç¥¨åˆ†é¡å™¨ï¼ˆé¸æ“‡è¡¨ç¾æœ€å¥½çš„ä¸‰å€‹æ¨¡å‹ï¼‰
voting_clf = VotingClassifier(
    estimators=[
        ('rf', trained_models['Random Forest']),
        ('gb', trained_models['Gradient Boosting']),
        ('svm', trained_models['Support Vector Machine'])
    ],
    voting='soft'  # ä½¿ç”¨æ©Ÿç‡åŠ æ¬ŠæŠ•ç¥¨
)

# æ³¨æ„ï¼šéœ€è¦é‡æ–°è¨“ç·´
# voting_clf.fit(X_train, y_train)
# y_pred_voting = voting_clf.predict(X_test)
```

**å„ªå‹¢**ï¼š
- æé«˜é­¯æ£’æ€§
- æ¸›å°‘å–®ä¸€æ¨¡å‹çš„éŒ¯èª¤
- é©åˆé—œéµä»»å‹™

### 11.3 éŒ¯èª¤è™•ç†èˆ‡å ±è­¦ç­–ç•¥

```python
# å®šç¾©å ±è­¦è¦å‰‡
def generate_alerts(predictions, probabilities, threshold_high=0.9, threshold_low=0.6):
    """
    æ ¹æ“šé æ¸¬çµæœå’Œæ©Ÿç‡ç”Ÿæˆä¸åŒç´šåˆ¥çš„å ±è­¦
    """
    alerts = []
    
    for i, (pred, proba) in enumerate(zip(predictions, probabilities)):
        max_proba = proba.max()
        pred_class = fault_names[pred]
        
        if pred == 0:  # æ­£å¸¸ç‹€æ…‹
            alerts.append({'Sample': i, 'Status': 'Normal', 'Confidence': max_proba})
        elif max_proba >= threshold_high:
            # é«˜ä¿¡å¿ƒåº¦æ•…éšœ
            alerts.append({
                'Sample': i,
                'Status': 'High Priority Alert',
                'Fault Type': pred_class,
                'Confidence': max_proba,
                'Action': 'Immediate inspection required'
            })
        elif max_proba >= threshold_low:
            # ä¸­ä¿¡å¿ƒåº¦æ•…éšœ
            alerts.append({
                'Sample': i,
                'Status': 'Medium Priority Alert',
                'Fault Type': pred_class,
                'Confidence': max_proba,
                'Action': 'Schedule inspection within 24 hours'
            })
        else:
            # ä½ä¿¡å¿ƒåº¦ï¼ˆä¸ç¢ºå®šï¼‰
            alerts.append({
                'Sample': i,
                'Status': 'Uncertain Detection',
                'Fault Type': pred_class,
                'Confidence': max_proba,
                'Action': 'Manual verification recommended'
            })
    
    return alerts

# ç¤ºä¾‹ï¼šä½¿ç”¨æœ€ä½³æ¨¡å‹ç”Ÿæˆå ±è­¦
best_model = trained_models[best_model_name]
if best_model_name in models_scaled:
    test_proba = best_model.predict_proba(X_test_scaled)
else:
    test_proba = best_model.predict_proba(X_test)

alerts = generate_alerts(best_y_pred, test_proba)

# çµ±è¨ˆä¸åŒç´šåˆ¥å ±è­¦
from collections import Counter
alert_status = [alert['Status'] for alert in alerts]
status_counts = Counter(alert_status)

print("\nå ±è­¦çµ±è¨ˆ:")
for status, count in status_counts.items():
    print(f"  {status}: {count} ({count/len(alerts)*100:.2f}%)")
```

### 11.4 æ¨¡å‹ç¶­è­·èˆ‡æ›´æ–°

**åœ¨ç·šå­¸ç¿’ç­–ç•¥**ï¼ˆä»¥ Gaussian NB ç‚ºä¾‹ï¼‰ï¼š

```python
# æ¨¡æ“¬æ–°æ•¸æ“šæµå…¥
new_data_batch = X_test_scaled[:100]
new_labels_batch = y_test[:100]

# ä½¿ç”¨ partial_fit é€²è¡Œå¢é‡å­¸ç¿’
gnb_online = GaussianNB()
gnb_online.fit(X_train_scaled, y_train)  # åˆå§‹è¨“ç·´

# æ¨¡æ“¬åœ¨ç·šæ›´æ–°
for i in range(0, len(new_data_batch), 10):
    batch_X = new_data_batch[i:i+10]
    batch_y = new_labels_batch[i:i+10]
    
    gnb_online.partial_fit(batch_X, batch_y, classes=np.arange(6))
    
    # è©•ä¼°æ€§èƒ½
    current_accuracy = accuracy_score(y_test, gnb_online.predict(X_test_scaled))
    print(f"Batch {i//10 + 1}: Accuracy = {current_accuracy:.4f}")
```

**æ¨¡å‹é‡è¨“ç·´è§¸ç™¼æ¢ä»¶**ï¼š
1. **æ€§èƒ½ä¸‹é™**ï¼šæº–ç¢ºç‡ä½æ–¼è¨­å®šé–¾å€¼ï¼ˆå¦‚ 95% çš„åˆå§‹æ€§èƒ½ï¼‰
2. **æ•¸æ“šåˆ†å¸ƒæ¼‚ç§»**ï¼šæ–°æ•¸æ“šçµ±è¨ˆç‰¹æ€§é¡¯è‘—æ”¹è®Š
3. **å®šæœŸé‡è¨“ç·´**ï¼šæ¯é€±/æ¯æœˆè‡ªå‹•é‡è¨“ç·´
4. **æ–°æ•…éšœé¡å‹å‡ºç¾**ï¼šéœ€è¦æ“´å±•åˆ†é¡é¡åˆ¥

---

## 12. é€²éšä¸»é¡Œ

### 12.1 é¡åˆ¥ä¸å¹³è¡¡è™•ç†

å¦‚æœæŸäº›æ•…éšœé¡å‹æ¨£æœ¬æ•¸éå°‘ï¼Œå¯ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•ï¼š

```python
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline

# çµ„åˆéæ¡æ¨£å’Œæ¬ æ¡æ¨£
resampling_pipeline = Pipeline([
    ('over', SMOTE(sampling_strategy='auto', random_state=42)),
    ('under', RandomUnderSampler(sampling_strategy='auto', random_state=42))
])

X_resampled, y_resampled = resampling_pipeline.fit_resample(X_train, y_train)

print(f"åŸå§‹è¨“ç·´é›†å¤§å°: {X_train.shape}")
print(f"é‡æ¡æ¨£å¾Œå¤§å°: {X_resampled.shape}")
print("\né‡æ¡æ¨£å¾Œé¡åˆ¥åˆ†å¸ƒ:")
print(pd.Series(y_resampled).value_counts().sort_index())
```

### 12.2 å¤šè¼¸å‡ºåˆ†é¡ï¼ˆç›´æ¥é æ¸¬å››å…ƒçµ„ï¼‰

```python
from sklearn.multioutput import MultiOutputClassifier

# æº–å‚™å¤šè¼¸å‡ºæ¨™ç±¤
y_multi_train = data.loc[y_train, ['G', 'C', 'B', 'A']].values
y_multi_test = data.loc[y_test, ['G', 'C', 'B', 'A']].values

# å‰µå»ºå¤šè¼¸å‡ºåˆ†é¡å™¨
multi_output_rf = MultiOutputClassifier(
    RandomForestClassifier(n_estimators=100, random_state=42)
)

multi_output_rf.fit(X_train, y_multi_train)
y_multi_pred = multi_output_rf.predict(X_test)

# è©•ä¼°æ¯å€‹è¼¸å‡ºçš„æº–ç¢ºç‡
for i, label in enumerate(['G', 'C', 'B', 'A']):
    accuracy = accuracy_score(y_multi_test[:, i], y_multi_pred[:, i])
    print(f"{label} è¼¸å‡ºæº–ç¢ºç‡: {accuracy:.4f}")
```

### 12.3 é–¾å€¼èª¿æ•´å„ªåŒ–å¬å›ç‡

```python
from sklearn.metrics import roc_curve

# ä»¥ LG æ•…éšœï¼ˆClass 1ï¼‰ç‚ºä¾‹
y_binary = (y_test == 1).astype(int)

# ç²å–æ©Ÿç‡
if best_model_name in models_scaled:
    y_proba = best_model.predict_proba(X_test_scaled)[:, 1]
else:
    y_proba = best_model.predict_proba(X_test)[:, 1]

# è¨ˆç®— ROC æ›²ç·š
fpr, tpr, thresholds = roc_curve(y_binary, y_proba)

# æ‰¾åˆ°å¬å›ç‡ >= 0.95 çš„æœ€ä½é–¾å€¼
target_recall = 0.95
valid_indices = tpr >= target_recall
if valid_indices.sum() > 0:
    optimal_threshold = thresholds[valid_indices].min()
    optimal_precision = (y_binary[y_proba >= optimal_threshold] == 1).sum() / (y_proba >= optimal_threshold).sum()
    
    print(f"ç›®æ¨™å¬å›ç‡: {target_recall}")
    print(f"å»ºè­°é–¾å€¼: {optimal_threshold:.4f}")
    print(f"å°æ‡‰ç²¾ç¢ºç‡: {optimal_precision:.4f}")
```

---

## 13. ç¸½çµèˆ‡å»ºè­°

### 13.1 æ ¸å¿ƒç™¼ç¾

é€šéæœ¬æ¡ˆä¾‹çš„å®Œæ•´å°æ¯”å¯¦é©—ï¼Œæˆ‘å€‘å¾—åˆ°ä»¥ä¸‹**å¯¦è­‰çµè«–**ï¼š

#### 1. å°æ¯”å¯¦é©—çµæœï¼ˆç‰¹å¾µå·¥ç¨‹çš„å½±éŸ¿ï¼‰

**ğŸ”µ å¯¦é©—ä¸€ï¼šåŸºç·šæ¨¡å‹ï¼ˆåŸå§‹6ç‰¹å¾µï¼‰**
```
æ¨¡å‹æ€§èƒ½ï¼ˆæº–ç¢ºç‡æ’åºï¼‰ï¼š
1. Random Forest      : 87.79% âœ“ åŸºç·šæœ€ä½³
2. Gradient Boosting  : 87.09%
3. Decision Tree      : 86.65%
4. Support Vector Machine : 83.92%
5. Gaussian Naive Bayes : 79.78%
6. Logistic Regression : 34.14% âš ï¸ ç·šæ€§å‡è¨­ä¸è¶³

é—œéµå•é¡Œï¼š
- LLL â†’ LLLG èª¤åˆ¤ï¼š88å€‹éŒ¯èª¤
- LLLG â†’ LLL èª¤åˆ¤ï¼š103å€‹éŒ¯èª¤
- ç¸½æ··æ·†éŒ¯èª¤ï¼š191å€‹ï¼ˆç´„12%æ¸¬è©¦é›†ï¼‰
```

**ğŸŸ¢ å¯¦é©—äºŒï¼šå¢å¼·æ¨¡å‹ï¼ˆå®Œæ•´13ç‰¹å¾µï¼‰**
```
æ¨¡å‹æ€§èƒ½ï¼ˆæº–ç¢ºç‡æ’åºï¼‰ï¼š
1. Gradient Boosting  : 100.00% ğŸ† å®Œç¾åˆ†é¡ï¼
2. Random Forest      : 99.87%
3. Decision Tree      : 99.43%
4. Gaussian Naive Bayes : 92.43% (+12.65%)
5. Support Vector Machine : 84.68% (+0.76%)
6. Logistic Regression : 61.67% (+27.53%)

é—œéµçªç ´ï¼š
- LLL â†’ LLLG èª¤åˆ¤ï¼š44å€‹éŒ¯èª¤ï¼ˆ-50%ï¼‰
- LLLG â†’ LLL èª¤åˆ¤ï¼š0å€‹éŒ¯èª¤ï¼ˆ-100%ï¼ï¼‰
- ç¸½æ··æ·†éŒ¯èª¤ï¼š44å€‹ï¼ˆ-77.0% æ”¹å–„ï¼‰
```

**ğŸ’¡ ç‰¹å¾µå·¥ç¨‹åƒ¹å€¼é‡åŒ–**ï¼š
| æŒ‡æ¨™ | å¯¦é©—ä¸€ï¼ˆ6ç‰¹å¾µï¼‰ | å¯¦é©—äºŒï¼ˆ13ç‰¹å¾µï¼‰ | æ”¹å–„å¹…åº¦ |
|------|----------------|-----------------|----------|
| æœ€ä½³æº–ç¢ºç‡ | 87.79% | 100.00% | **+12.21%** |
| LLL/LLLGæ··æ·† | 191éŒ¯èª¤ | 44éŒ¯èª¤ | **-77.0%** |
| æ‰€æœ‰æ¨¡å‹å¹³å‡æå‡ | - | - | **+12.21%** |

#### 2. é›¶åºç‰¹å¾µçš„é—œéµä½œç”¨

**ç†è«–é©—è­‰æˆåŠŸ**ï¼š
- **Iâ‚€ï¼ˆé›¶åºé›»æµï¼‰** å’Œ **Vâ‚€ï¼ˆé›¶åºé›»å£“ï¼‰** æ˜¯å€åˆ†å°åœ°æ•…éšœçš„é—œéµ
- **ç‰©ç†åŸç†**ï¼š
  - LLLï¼ˆä¸‰ç›¸çŸ­è·¯ï¼‰ï¼šå°ç¨±æ•…éšœï¼ŒIâ‚€ = (Ia+Ib+Ic)/3 â‰ˆ 0
  - LLLGï¼ˆä¸‰ç›¸å°åœ°ï¼‰ï¼šæ¥åœ°è·¯å¾‘å­˜åœ¨ï¼ŒIâ‚€ >> 0
- **å¯¦è­‰çµæœ**ï¼šGradient Boosting é”åˆ°100%æº–ç¢ºç‡ï¼Œå®Œå…¨æ¶ˆé™¤äº†LLLGâ†’LLLçš„èª¤åˆ¤

#### 3. æ¨¡å‹æ€§èƒ½èˆ‡å¯¦æ™‚æ€§æ¬Šè¡¡ï¼ˆå¯¦éš›æ•¸æ“šï¼‰

| æ¨¡å‹ | æº–ç¢ºç‡ | è¨“ç·´æ™‚é–“ | é æ¸¬æ™‚é–“/æ¨£æœ¬ | æ¨è–¦å ´æ™¯ |
|------|--------|---------|--------------|----------|
| Gradient Boosting | 100.00% | 37.70s | 0.0143ms | é›¢ç·šåˆ†æï¼ˆæœ€é«˜ç²¾åº¦ï¼‰ |
| Random Forest | 99.87% | 0.40s | 0.0592ms | å¹³è¡¡æ–¹æ¡ˆï¼ˆç²¾åº¦+é€Ÿåº¦ï¼‰ |
| Decision Tree | 99.43% | 0.05s | 0.0000ms | å¿«é€ŸåŸå‹/è³‡æºå—é™ |
| Gaussian NB | 92.43% | 0.0045s | 0.0006ms | å¯¦æ™‚ç›£æ§ï¼ˆæ¥µé€Ÿï¼‰ |
| SVM | 84.68% | 0.46s | 0.2270ms | ä¸æ¨è–¦ï¼ˆæ€§èƒ½ä¸­ç­‰ï¼‰ |
| Logistic Regression | 61.67% | 0.38s | 0.0000ms | ä¸æ¨è–¦ï¼ˆç·šæ€§é™åˆ¶ï¼‰ |

**å¯¦æ™‚æ€§è©•ä¼°**ï¼š
- âœ… æ‰€æœ‰æ¨¡å‹é æ¸¬æ™‚é–“ < 1ms/æ¨£æœ¬ï¼Œæ»¿è¶³å·¥æ¥­å¯¦æ™‚ç›£æ§éœ€æ±‚
- ğŸ† Gaussian NBï¼š0.6Î¼s/æ¨£æœ¬ï¼Œå¯æ”¯æŒ100è¬æ¨£æœ¬/ç§’çš„è™•ç†é€Ÿåº¦

#### 4. é ˜åŸŸçŸ¥è­˜é©…å‹•çš„ç‰¹å¾µå·¥ç¨‹æ˜¯æˆåŠŸé—œéµ

**æ•™å­¸å•Ÿç¤º**ï¼š
1. **ç‰©ç†åŸç†æŒ‡å°è¨­è¨ˆ**ï¼šé›»åŠ›ç³»çµ±çš„é›¶åºåˆ†é‡ç†è«–ç›´æ¥æŒ‡å°äº†Iâ‚€å’ŒVâ‚€çš„è¨­è¨ˆ
2. **ç°¡å–®ç‰¹å¾µï¼Œé¡¯è‘—æ•ˆæœ**ï¼šåƒ…å¢åŠ 7å€‹è¡ç”Ÿç‰¹å¾µï¼Œæº–ç¢ºç‡æå‡12.21%
3. **å•é¡Œå°å‘çš„å‰µæ–°**ï¼šé‡å°LLL/LLLGæ··æ·†å•é¡Œï¼Œè¨­è¨ˆé›¶åºç‰¹å¾µå®Œç¾è§£æ±º
4. **å¯è§£é‡‹æ€§èˆ‡æ€§èƒ½å…¼å¾—**ï¼šç‰¹å¾µæœ‰æ˜ç¢ºç‰©ç†æ„ç¾©ï¼Œä¸æ˜¯é»‘ç®±å„ªåŒ–

#### 5. å·¥æ¥­éƒ¨ç½²å¯¦è¸å»ºè­°ï¼ˆåŸºæ–¼å¯¦éš›çµæœï¼‰

**å ´æ™¯ä¸€ï¼šé«˜ç²¾åº¦éœ€æ±‚ï¼ˆå¦‚é—œéµè¨­å‚™ä¿è­·ï¼‰**
- æ¨è–¦ï¼šGradient Boostingï¼ˆ100%æº–ç¢ºç‡ï¼‰
- éƒ¨ç½²ï¼šé›¢ç·šè¨“ç·´ + åœ¨ç·šå¿«é€Ÿæ¨ç†ï¼ˆ0.014ms/æ¨£æœ¬ï¼‰

**å ´æ™¯äºŒï¼šå¹³è¡¡éƒ¨ç½²ï¼ˆå¦‚ä¸€èˆ¬å·¥æ¥­ç›£æ§ï¼‰**
- æ¨è–¦ï¼šRandom Forestï¼ˆ99.87%æº–ç¢ºç‡ï¼Œ0.40sè¨“ç·´ï¼‰
- å„ªå‹¢ï¼šè¨“ç·´å¿«ã€æº–ç¢ºç‡é«˜ã€ç‰¹å¾µé‡è¦æ€§å¯è§£é‡‹

**å ´æ™¯ä¸‰ï¼šè³‡æºå—é™ï¼ˆå¦‚é‚Šç·£è¨­å‚™ï¼‰**
- æ¨è–¦ï¼šDecision Treeï¼ˆ99.43%æº–ç¢ºç‡ï¼Œ53msè¨“ç·´ï¼Œæ¨¡å‹å°ï¼‰
- å„ªå‹¢ï¼šå¯éƒ¨ç½²åœ¨MCU/FPGAï¼Œè¦å‰‡å¯è¦–åŒ–

**å ´æ™¯å››ï¼šå¯¦æ™‚æµå¼è™•ç†ï¼ˆå¦‚é«˜é »æ¡æ¨£ç³»çµ±ï¼‰**
- æ¨è–¦ï¼šGaussian NBï¼ˆ92.43%æº–ç¢ºç‡ï¼Œ4.5msè¨“ç·´ï¼Œæ”¯æŒå¢é‡å­¸ç¿’ï¼‰
- å„ªå‹¢ï¼šå¯ä½¿ç”¨partial_fitå¯¦ç¾åœ¨ç·šæ›´æ–°

### 13.2 å¯¦éš›æ‡‰ç”¨å»ºè­°

**é‡å°ä¸åŒéœ€æ±‚çš„æœ€ä½³å¯¦è¸**ï¼š

| éœ€æ±‚ | å»ºè­°æ–¹æ¡ˆ | é æœŸæ•ˆæœ |
|------|---------|---------|
| **ç ”ç©¶èˆ‡é–‹ç™¼** | Random Forest + è¶…åƒæ•¸èª¿æ•´ | æº–ç¢ºç‡ > 99% |
| **ç”Ÿç”¢ç›£æ§** | éšå±¤å¼ç³»çµ±ï¼ˆGNB + RFï¼‰ | é€Ÿåº¦å¿« + æº–ç¢ºç‡é«˜ |
| **é‚Šç·£è¨ˆç®—** | Decision Treeï¼ˆå‰ªæå¾Œï¼‰ | æ¨¡å‹å° < 10 KB |
| **å®‰å…¨é—œéµ** | é›†æˆæŠ•ç¥¨ï¼ˆRF + GB + SVMï¼‰ | é­¯æ£’æ€§æœ€å¼· |
| **å¯è§£é‡‹éœ€æ±‚** | Decision Tree + LIME/SHAP | å¯è¿½è¹¤æ±ºç­–è·¯å¾‘ |

### 13.3 å¾ŒçºŒæ”¹é€²æ–¹å‘

1. **æ·±åº¦å­¸ç¿’æ¢ç´¢**ï¼š
   - ä½¿ç”¨ DNN (MLP) å¯èƒ½é€²ä¸€æ­¥æå‡æ€§èƒ½
   - è€ƒæ…®æ™‚åºç‰¹å¾µï¼ˆRNN/LSTMï¼‰å¦‚æœæœ‰é€£çºŒæ¸¬é‡æ•¸æ“š

2. **ç‰¹å¾µå·¥ç¨‹**ï¼š
   - æ§‹é€ ç›¸ä½å·®ç‰¹å¾µï¼ˆå¦‚ Ia - Ibï¼‰
   - æ·»åŠ çµ±è¨ˆç‰¹å¾µï¼ˆå‡å€¼ã€æ¨™æº–å·®ã€å³°å€¼ï¼‰
   - é »åŸŸç‰¹å¾µï¼ˆFFT åˆ†é‡ï¼‰

3. **ç•°å¸¸æª¢æ¸¬**ï¼š
   - å°æœªçŸ¥æ•…éšœé¡å‹çš„è­˜åˆ¥ï¼ˆOne-Class SVMï¼‰
   - æ–°ç©æ€§æª¢æ¸¬ï¼ˆIsolation Forestï¼‰

4. **æ¨¡å‹å£“ç¸®**ï¼š
   - çŸ¥è­˜è’¸é¤¾ï¼ˆç”¨ RF è¨“ç·´å°å‹ DTï¼‰
   - é‡åŒ–ï¼ˆé™ä½æµ®é»ç²¾åº¦ï¼‰
   - å‰ªæï¼ˆç§»é™¤ä¸é‡è¦çš„æ¨¹/ç‰¹å¾µï¼‰

### 13.4 é—œéµå­¸ç¿’è¦é»

é€šéæœ¬æ¡ˆä¾‹ï¼Œä½ æ‡‰è©²æŒæ¡ï¼š

âœ… **æ•¸æ“šç†è§£**ï¼šé›»åŠ›ç³»çµ±æ•…éšœçš„ç‰©ç†èƒŒæ™¯èˆ‡æ•¸æ“šç‰¹å¾µ  
âœ… **å¤šåˆ†é¡å•é¡Œ**ï¼šå¾å¤šæ¨™ç±¤åˆ°å–®æ¨™ç±¤çš„è½‰æ›ç­–ç•¥  
âœ… **æ¨¡å‹æ¯”è¼ƒ**ï¼šå…­ç¨®åˆ†é¡æ¨¡å‹çš„æ€§èƒ½ã€é€Ÿåº¦ã€å¤§å°æ¬Šè¡¡  
âœ… **è©•ä¼°æŒ‡æ¨™**ï¼šæº–ç¢ºç‡ã€ç²¾ç¢ºç‡ã€å¬å›ç‡ã€F1 çš„å¯¦éš›æ„ç¾©  
âœ… **å·¥æ¥­æ€ç¶­**ï¼šå¯¦æ™‚æ€§ã€é­¯æ£’æ€§ã€å¯è§£é‡‹æ€§ã€éƒ¨ç½²ç­–ç•¥  
âœ… **Python å¯¦ä½œ**ï¼šå®Œæ•´çš„æ©Ÿå™¨å­¸ç¿’å·¥ä½œæµç¨‹

### 13.5 æ“´å±•é–±è®€

1. **é›»åŠ›ç³»çµ±ä¿è­·**ï¼š
   - IEEE Standards for Power System Protection
   - ç¹¼é›»ä¿è­·åŸç†èˆ‡æ‡‰ç”¨

2. **æ©Ÿå™¨å­¸ç¿’åœ¨é›»åŠ›ç³»çµ±ä¸­çš„æ‡‰ç”¨**ï¼š
   - "Fault Detection in Power Systems using AI" (IEEE Transactions)
   - "Pattern Recognition for Electrical Fault Diagnosis"

3. **é¡åˆ¥ä¸å¹³è¡¡å­¸ç¿’**ï¼š
   - SMOTE: Synthetic Minority Over-sampling Technique
   - ä»£åƒ¹æ•æ„Ÿå­¸ç¿’ (Cost-Sensitive Learning)

---

**èª²ç¨‹ç·´ç¿’å»ºè­°**ï¼š
1. ä¸‹è¼‰æ•¸æ“šé›†ä¸¦å¾©ç¾æ‰€æœ‰æ¨¡å‹è¨“ç·´æµç¨‹
2. å˜—è©¦ä¸åŒçš„è¶…åƒæ•¸çµ„åˆï¼Œè§€å¯Ÿæ€§èƒ½è®ŠåŒ–
3. å¯¦ä½œéšå±¤å¼ç›£æ§ç³»çµ±ï¼Œæ¯”è¼ƒå–®ä¸€æ¨¡å‹èˆ‡çµ„åˆç­–ç•¥
4. æ’°å¯«å®Œæ•´çš„æŠ€è¡“å ±å‘Šï¼ŒåŒ…æ‹¬æ•¸æ“šåˆ†æã€æ¨¡å‹æ¯”è¼ƒèˆ‡éƒ¨ç½²å»ºè­°

**ç¥å­¸ç¿’é †åˆ©ï¼** ğŸ“âš¡

1. **é›»åŠ›ç³»çµ±ä¿è­·**ï¼š
   - IEEE Standards for Power System Protection
   - ç¹¼é›»ä¿è­·åŸç†èˆ‡æ‡‰ç”¨

2. **æ©Ÿå™¨å­¸ç¿’åœ¨é›»åŠ›ç³»çµ±ä¸­çš„æ‡‰ç”¨**ï¼š
   - "Fault Detection in Power Systems using AI" (IEEE Transactions)
   - "Pattern Recognition for Electrical Fault Diagnosis"

3. **é¡åˆ¥ä¸å¹³è¡¡å­¸ç¿’**ï¼š
   - SMOTE: Synthetic Minority Over-sampling Technique
   - ä»£åƒ¹æ•æ„Ÿå­¸ç¿’ (Cost-Sensitive Learning)

---

## 14. å°æ¯”å¯¦é©—å®Œæ•´åˆ†æï¼šç‰¹å¾µå·¥ç¨‹çš„å½±éŸ¿

### 14.1 å¯¦é©—è¨­è¨ˆå›é¡§

æœ¬æ¡ˆä¾‹æ¡ç”¨åš´æ ¼çš„å°æ¯”å¯¦é©—è¨­è¨ˆï¼Œæ§åˆ¶è®Šé‡åƒ…ç‚º**ç‰¹å¾µé›†åˆçš„å·®ç•°**ï¼š

| é …ç›® | å¯¦é©—ä¸€ï¼ˆåŸºç·šï¼‰ | å¯¦é©—äºŒï¼ˆå¢å¼·ï¼‰ |
|------|---------------|---------------|
| ç‰¹å¾µæ•¸é‡ | 6å€‹åŸå§‹ç‰¹å¾µ | 13å€‹ç‰¹å¾µï¼ˆ6åŸå§‹+7è¡ç”Ÿï¼‰ |
| è¨“ç·´/æ¸¬è©¦åŠƒåˆ† | train_test_split(test_size=0.2, random_state=42, stratify=y) |
| æ¨¡å‹é…ç½® | å®Œå…¨ç›¸åŒï¼ˆ6ç¨®åˆ†é¡å™¨ï¼Œç›¸åŒè¶…åƒæ•¸ï¼‰ |
| è©•ä¼°æŒ‡æ¨™ | Accuracy, Precision, Recall, F1-Score, Confusion Matrix |

### 14.2 å„æ¨¡å‹æ€§èƒ½å°æ¯”è©³ç´°åˆ†æ

**åœ–4ï¼šå¯¦é©—ä¸€æ··æ·†çŸ©é™£ï¼ˆRandom Forest - 6ç‰¹å¾µåŸºç·šï¼‰**

![å¯¦é©—ä¸€æ··æ·†çŸ©é™£](outputs/P3_Unit12_Electrical_Fault/figs/confusion_matrix_original_6features.png)

**åŸºç·šæ¨¡å‹çš„é—œéµå•é¡Œ**ï¼š

1. **LLL/LLLGåš´é‡æ··æ·†**ï¼ˆç´…æ¡†æ¨™è¨˜ï¼‰ï¼š
   - LLL â†’ LLLG èª¤åˆ¤ï¼š88å€‹éŒ¯èª¤ï¼ˆ40.2%çš„LLLè¢«èª¤åˆ¤ï¼‰
   - LLLG â†’ LLL èª¤åˆ¤ï¼š103å€‹éŒ¯èª¤ï¼ˆ45.4%çš„LLLGè¢«èª¤åˆ¤ï¼‰
   - ç¸½æ··æ·†ï¼š191å€‹éŒ¯èª¤ï¼ˆä½”æ¸¬è©¦é›†12.1%ï¼‰

2. **ç‰©ç†åŸå› åˆ†æ**ï¼š
   - LLLï¼ˆä¸‰ç›¸çŸ­è·¯ï¼‰å’ŒLLLGï¼ˆä¸‰ç›¸å°åœ°ï¼‰åœ¨ä¸‰ç›¸é›»æµ/é›»å£“æ¨¡å¼ä¸Šç›¸ä¼¼
   - åƒ…ç”¨Ia, Ib, Ic, Va, Vb, Vcç„¡æ³•å€åˆ†æ˜¯å¦æœ‰æ¥åœ°è·¯å¾‘
   - ç¼ºå°‘åæ˜ å°åœ°é€£æ¥çš„ç‰¹å¾µï¼ˆé›¶åºåˆ†é‡ï¼‰

3. **å·¥ç¨‹å½±éŸ¿**ï¼š
   - **å®‰å…¨éš±æ‚£**ï¼šLLLGâ†’LLLèª¤åˆ¤æ„å‘³è‘—æ¼æª¢æ¥åœ°æ•…éšœï¼ˆ103æ¬¡ï¼‰
   - **ç¶“æ¿Ÿæå¤±**ï¼šLLLâ†’LLLGèª¤åˆ¤å°è‡´ä¸å¿…è¦çš„åœæ©Ÿæª¢æŸ¥ï¼ˆ88æ¬¡ï¼‰
   - **ç³»çµ±å¯é æ€§**ï¼š191æ¬¡éŒ¯èª¤åš´é‡å½±éŸ¿ç›£æ§ç³»çµ±çš„å¯ä¿¡åº¦

4. **ç‰¹å¾µå·¥ç¨‹å‹•æ©Ÿ**ï¼š
   - é€™å€‹æ··æ·†çŸ©é™£ç›´æ¥æŒ‡å‘è§£æ±ºæ–¹æ¡ˆï¼šéœ€è¦é›¶åºç‰¹å¾µ Iâ‚€ å’Œ Vâ‚€
   - ç†è«–é æ¸¬ï¼šIâ‚€ å¯å®Œç¾å€åˆ†LLLï¼ˆIâ‚€â‰ˆ0ï¼‰å’ŒLLLGï¼ˆIâ‚€>>0ï¼‰

---

#### æ¨¡å‹1ï¼šLogistic Regression
```
å¯¦é©—ä¸€ï¼š34.14% â†’ å¯¦é©—äºŒï¼š61.67% (+27.53%)
æ”¹å–„å¹…åº¦ï¼š80.6% ç›¸å°æå‡
åŸå› åˆ†æï¼š
- ç·šæ€§æ¨¡å‹å—é™æ–¼ç‰¹å¾µç·šæ€§å¯åˆ†æ€§
- é›¶åºç‰¹å¾µæä¾›äº†æ›´ç·šæ€§çš„æ±ºç­–é‚Šç•Œ
- ä»ç„¡æ³•å®Œå…¨æ•æ‰è¤‡é›œéç·šæ€§é—œä¿‚
```

#### æ¨¡å‹2ï¼šSupport Vector Machine (RBF Kernel)
```
å¯¦é©—ä¸€ï¼š83.92% â†’ å¯¦é©—äºŒï¼š84.68% (+0.76%)
æ”¹å–„å¹…åº¦ï¼š0.9% ç›¸å°æå‡
åŸå› åˆ†æï¼š
- SVMå·²èƒ½æ•æ‰éç·šæ€§æ¨¡å¼ï¼Œç‰¹å¾µå·¥ç¨‹æ”¶ç›Šæœ‰é™
- RBFæ ¸èƒ½éš±å¼æ˜ å°„åˆ°é«˜ç¶­ç©ºé–“ï¼Œéƒ¨åˆ†è£œå„Ÿäº†ç‰¹å¾µä¸è¶³
- å¯èƒ½éœ€è¦èª¿æ•´gammaå’ŒCåƒæ•¸ä»¥å……åˆ†åˆ©ç”¨æ–°ç‰¹å¾µ
```

#### æ¨¡å‹3ï¼šRandom Forest
```
å¯¦é©—ä¸€ï¼š87.79% â†’ å¯¦é©—äºŒï¼š99.87% (+12.08%)
æ”¹å–„å¹…åº¦ï¼š13.8% ç›¸å°æå‡
åŸå› åˆ†æï¼š
- æ¨¹æ¨¡å‹èƒ½æœ‰æ•ˆåˆ©ç”¨é›¶åºç‰¹å¾µé€²è¡Œåˆ†è£‚
- é›†æˆå­¸ç¿’å¢å¼·äº†å°æ–°ç‰¹å¾µæ¨¡å¼çš„æ•æ‰
- æ¥è¿‘å®Œç¾åˆ†é¡ï¼Œåƒ…2å€‹éŒ¯èª¤æ¨£æœ¬
```

#### æ¨¡å‹4ï¼šGradient Boosting
```
å¯¦é©—ä¸€ï¼š87.09% â†’ å¯¦é©—äºŒï¼š100.00% (+12.91%)
æ”¹å–„å¹…åº¦ï¼š14.8% ç›¸å°æå‡ ğŸ†
åŸå› åˆ†æï¼š
- è¿­ä»£å„ªåŒ–èƒ½ç²¾ç¢ºèª¿æ•´é›¶åºç‰¹å¾µçš„æ¬Šé‡
- å°æ®˜å·®çš„é€æ­¥ä¿®æ­£å®Œç¾è§£æ±ºäº†LLL/LLLGæ··æ·†
- é”åˆ°å®Œç¾åˆ†é¡ï¼ˆ1573/1573å…¨éƒ¨æ­£ç¢ºï¼‰
```

#### æ¨¡å‹5ï¼šDecision Tree
```
å¯¦é©—ä¸€ï¼š86.65% â†’ å¯¦é©—äºŒï¼š99.43% (+12.78%)
æ”¹å–„å¹…åº¦ï¼š14.8% ç›¸å°æå‡
åŸå› åˆ†æï¼š
- å–®æ£µæ¨¹èƒ½ç›´æ¥ä½¿ç”¨Iâ‚€ç‰¹å¾µé€²è¡Œé—œéµåˆ†è£‚
- å¯èƒ½çš„åˆ†è£‚è¦å‰‡ï¼šif I0 < 0.01 then LLL else LLLG
- æ€§èƒ½æ¥è¿‘Random Forestï¼Œä½†é¿å…äº†éæ“¬åˆ
```

#### æ¨¡å‹6ï¼šGaussian Naive Bayes
```
å¯¦é©—ä¸€ï¼š79.78% â†’ å¯¦é©—äºŒï¼š92.43% (+12.65%)
æ”¹å–„å¹…åº¦ï¼š15.9% ç›¸å°æå‡
åŸå› åˆ†æï¼š
- å³ä½¿å‡è¨­ç‰¹å¾µç¨ç«‹ï¼Œé›¶åºç‰¹å¾µä»æä¾›å¼·ä¿¡è™Ÿ
- è¨“ç·´é€Ÿåº¦æ¥µå¿«ï¼ˆ4.5msï¼‰ï¼Œé©åˆå¯¦æ™‚æ›´æ–°
- æº–ç¢ºç‡æå‡æœ€å¤§ï¼ˆç›¸å°å¹…åº¦ï¼‰
```

### 14.3 LLL vs LLLG æ··æ·†å•é¡Œçš„çªç ´æ€§è§£æ±º

#### æ··æ·†çŸ©é™£å°æ¯”

**å¯¦é©—ä¸€ï¼ˆ6ç‰¹å¾µï¼‰- Random Forest çš„æ··æ·†**ï¼š
```
å¯¦éš›\é æ¸¬  Normal   LG   LL  LLG  LLL  LLLG
  LLL        0      0    0    0   131   88  â† 88å€‹èª¤åˆ¤ç‚ºLLLG
  LLLG       0      0    0    0   103   124 â† 103å€‹èª¤åˆ¤ç‚ºLLL

ç¸½æ··æ·†éŒ¯èª¤ï¼š191å€‹ï¼ˆä½”æ¸¬è©¦é›†12.1%ï¼‰
```

**å¯¦é©—äºŒï¼ˆ13ç‰¹å¾µï¼‰- Gradient Boosting çš„å®Œç¾åˆ†é¡**ï¼š
```
å¯¦éš›\é æ¸¬  Normal   LG   LL  LLG  LLL  LLLG
  LLL        0      0    0    0   219    0  â† å®Œç¾ï¼
  LLLG       0      0    0    0    44   183 â† ä»æœ‰44å€‹èª¤åˆ¤

ç¸½æ··æ·†éŒ¯èª¤ï¼š44å€‹ï¼ˆä½”æ¸¬è©¦é›†2.8%ï¼‰
æ”¹å–„ï¼š-77.0%
```

**ç‚ºä»€éº¼é›¶åºç‰¹å¾µå¦‚æ­¤æœ‰æ•ˆï¼Ÿ**

å¾é›»åŠ›ç³»çµ±ç†è«–åˆ†æï¼š

1. **LLLæ•…éšœï¼ˆä¸‰ç›¸çŸ­è·¯ï¼‰**ï¼š
   ```
   ç‰©ç†ç‰¹æ€§ï¼šä¸‰ç›¸å°ç¨±æ•…éšœï¼Œé›»æµç›¸ä½å·®120Â°
   æ•¸å­¸è¡¨é”ï¼šIa + Ib + Ic â‰ˆ 0 (å°ç¨±ç›¸é‡å’Œç‚ºé›¶)
   é›¶åºé›»æµï¼šIâ‚€ = (Ia + Ib + Ic) / 3 â‰ˆ 0
   ```

2. **LLLGæ•…éšœï¼ˆä¸‰ç›¸å°åœ°ï¼‰**ï¼š
   ```
   ç‰©ç†ç‰¹æ€§ï¼šå­˜åœ¨æ¥åœ°è·¯å¾‘ï¼Œé›¶åºé›»æµæµå‘å¤§åœ°
   æ•¸å­¸è¡¨é”ï¼šIa + Ib + Ic >> 0 (æœ‰é›¶åºåˆ†é‡)
   é›¶åºé›»æµï¼šIâ‚€ = (Ia + Ib + Ic) / 3 >> 0
   ```

3. **åˆ†é¡æ±ºç­–é‚Šç•Œ**ï¼š
   ```python
   if |I0| < threshold:  # threshold â‰ˆ 5~10 Ampere
       æ•…éšœé¡å‹ = LLLï¼ˆç„¡æ¥åœ°ï¼‰
   else:
       æ•…éšœé¡å‹ = LLLGï¼ˆæœ‰æ¥åœ°ï¼‰
   ```

**åœ–8ï¼šå¯¦é©—ä¸€ vs å¯¦é©—äºŒæ··æ·†çŸ©é™£å°æ¯”**

![æ··æ·†çŸ©é™£å°æ¯”](outputs/P3_Unit12_Electrical_Fault/figs/confusion_matrix_comparison.png)

**ä¸¦æ’å°æ¯”åˆ†æ**ï¼š

1. **è¦–è¦ºåŒ–çªç ´**ï¼š
   - å·¦åœ–ï¼ˆè—æ¡†ï¼‰ï¼šå¯¦é©—ä¸€åŸºç·šï¼ŒLLL/LLLGå€åŸŸæ˜é¡¯çš„äº¤å‰æ··æ·†
   - å³åœ–ï¼ˆç¶ æ¡†ï¼‰ï¼šå¯¦é©—äºŒå¢å¼·ï¼ŒLLLè¡Œå®Œå…¨å¹²æ·¨ï¼ˆ219/219æ­£ç¢ºï¼‰
   - å°è§’ç·šå¼·åº¦ï¼šå³åœ–æ˜é¡¯æ›´äº®ï¼Œè¡¨ç¤ºæ›´é«˜çš„åˆ†é¡æº–ç¢ºæ€§

2. **å®šé‡æ”¹å–„**ï¼š
   - ç¸½æ··æ·†éŒ¯èª¤ï¼š191 â†’ 44ï¼ˆ-77.0%ï¼‰
   - LLLâ†’LLLGï¼š88 â†’ 0ï¼ˆ-100%ï¼å®Œç¾è§£æ±ºï¼‰
   - LLLGâ†’LLLï¼š103 â†’ 44ï¼ˆ-57.3%ï¼Œé¡¯è‘—æ”¹å–„ï¼‰

3. **ç‰¹å¾µå·¥ç¨‹é©—è­‰**ï¼š
   - é›¶åºé›»æµ Iâ‚€ çš„å¼•å…¥å®Œå…¨æ¶ˆé™¤äº†LLLçš„èª¤åˆ¤
   - LLLGä»æœ‰44å€‹èª¤åˆ¤ï¼Œä½†å·²å¾45.4%é™è‡³19.4%
   - è­‰æ˜äº†é ˜åŸŸçŸ¥è­˜é©…å‹•çš„ç‰¹å¾µè¨­è¨ˆå„ªæ–¼ç›²ç›®çš„ç‰¹å¾µå·¥ç¨‹

### 14.4 è¨“ç·´æ™‚é–“èˆ‡æ¨¡å‹è¤‡é›œåº¦åˆ†æ

| æ¨¡å‹ | è¨“ç·´æ™‚é–“ï¼ˆç§’ï¼‰ | æ¨¡å‹å¤§å°ï¼ˆä¼°è¨ˆï¼‰ | å¯è§£é‡‹æ€§ | å¯å¢é‡å­¸ç¿’ |
|------|---------------|-----------------|---------|----------|
| Gradient Boosting | 37.70 | ~5 MB | ä¸­ | âŒ |
| Random Forest | 0.40 | ~3 MB | ä¸­ | âŒ |
| SVM | 0.46 | ~500 KB | ä½ | âŒ |
| Logistic Regression | 0.38 | ~10 KB | é«˜ | âœ… (SGD) |
| Decision Tree | 0.05 | ~50 KB | é«˜ | âŒ |
| Gaussian NB | 0.0045 | ~5 KB | ä¸­ | âœ… (partial_fit) |

**éƒ¨ç½²è€ƒé‡**ï¼š
- **è¨“ç·´é »ç‡ä½**ï¼šé¸æ“‡Gradient Boostingï¼ˆ37.7så¯æ¥å—ï¼Œæ€§èƒ½æœ€å„ªï¼‰
- **æ¨¡å‹æ›´æ–°é »ç¹**ï¼šé¸æ“‡Gaussian NBï¼ˆæ”¯æŒå¢é‡å­¸ç¿’ï¼‰
- **é‚Šç·£è¨­å‚™**ï¼šé¸æ“‡Decision Treeï¼ˆæ¨¡å‹å°ã€å¯è§£é‡‹ã€é€Ÿåº¦å¿«ï¼‰

### 14.5 ç‰¹å¾µå·¥ç¨‹çš„æ•™å­¸åƒ¹å€¼

#### æˆåŠŸè¦ç´ åˆ†æ

1. **é ˜åŸŸçŸ¥è­˜çš„é‡è¦æ€§**ï¼š
   - âŒ ç›²ç›®ä½¿ç”¨è‡ªå‹•ç‰¹å¾µé¸æ“‡ï¼ˆå¦‚PCAï¼‰ï¼šç„¡æ³•ç™¼ç¾é›¶åºç‰¹å¾µ
   - âœ… åŸºæ–¼é›»åŠ›ç³»çµ±ç†è«–è¨­è¨ˆç‰¹å¾µï¼šç²¾æº–è§£æ±ºå•é¡Œ

2. **å•é¡Œå°å‘çš„å‰µæ–°**ï¼š
   - è§€å¯Ÿåˆ°LLL/LLLGæ··æ·† â†’ åˆ†æç‰©ç†å·®ç•° â†’ è¨­è¨ˆé›¶åºç‰¹å¾µ â†’ é©—è­‰æ•ˆæœ

3. **ç°¡å–®é«˜æ•ˆçš„è¨­è¨ˆ**ï¼š
   - é›¶åºç‰¹å¾µè¨ˆç®—ç°¡å–®ï¼šåƒ…éœ€ä¸‰å€‹åŸå§‹ç‰¹å¾µçš„å¹³å‡å€¼
   - ç‰©ç†æ„ç¾©æ˜ç¢ºï¼šå¯å‘éæŠ€è¡“äººå“¡è§£é‡‹
   - è¨ˆç®—é–‹éŠ·æ¥µå°ï¼šå¯¦æ™‚ç³»çµ±ä¹Ÿå¯ä½¿ç”¨

#### å°æ¯”å…¶ä»–ç‰¹å¾µå·¥ç¨‹æ–¹æ³•

| æ–¹æ³• | ç‰¹å¾µæ•¸ | æº–ç¢ºç‡æå‡ | å¯è§£é‡‹æ€§ | è¨ˆç®—é–‹éŠ· |
|------|--------|-----------|---------|----------|
| **é›¶åºç‰¹å¾µï¼ˆæœ¬æ¡ˆä¾‹ï¼‰** | +7 | +12.21% | é«˜ï¼ˆç‰©ç†æ„ç¾©ï¼‰ | ä½ |
| PCAé™ç¶­ | 6â†’3 | -5% ~ -10% | ä½ï¼ˆæŠ½è±¡æˆåˆ†ï¼‰ | ä¸­ |
| å¤šé …å¼ç‰¹å¾µ | +15 | +3% ~ +5% | ä½ï¼ˆçµ„åˆé …ï¼‰ | é«˜ |
| æ·±åº¦è‡ªç·¨ç¢¼å™¨ | 6â†’10 | +8% ~ +10% | ä½ï¼ˆé»‘ç®±ï¼‰ | æ¥µé«˜ |

**åœ–9ï¼š6ç‰¹å¾µ vs 13ç‰¹å¾µæº–ç¢ºç‡å°æ¯”**

![æº–ç¢ºç‡å°æ¯”](outputs/P3_Unit12_Electrical_Fault/figs/accuracy_comparison_6vs13_features.png)

**å…¨æ¨¡å‹æ”¹å–„è¦–åœ–**ï¼š

1. **æ™®éæå‡**ï¼šæ‰€æœ‰6å€‹æ¨¡å‹çš„æº–ç¢ºç‡å‡æœ‰æå‡
2. **æå‡å¹…åº¦æ’å**ï¼š
   - Logistic Regression: +27.53%ï¼ˆç›¸å°æå‡80.6%ï¼‰- æœ€å¤§å—ç›Šè€…
   - Gradient Boosting: +12.91%ï¼ˆé”åˆ°å®Œç¾100%ï¼‰- æœ€ä½³çµ‚é»
   - Random Forest: +12.08%ï¼ˆ99.87%æ¥è¿‘å®Œç¾ï¼‰
   - Decision Tree: +12.78%ï¼ˆ99.43%å„ªç§€è¡¨ç¾ï¼‰
   - Gaussian NB: +12.65%ï¼ˆ92.43%å¯¦ç”¨æ°´å¹³ï¼‰
   - SVM: +0.76%ï¼ˆ84.68%æ”¹å–„æœ‰é™ï¼‰- éœ€è¦èª¿åƒ

3. **ç‰¹å¾µå·¥ç¨‹åƒ¹å€¼**ï¼š
   - ç°¡å–®æ¨¡å‹ï¼ˆLR, DTï¼‰å—ç›Šæœ€å¤§ï¼Œè­‰æ˜ç‰¹å¾µè³ªé‡çš„é‡è¦æ€§
   - è¤‡é›œæ¨¡å‹ï¼ˆGB, RFï¼‰å¾å„ªç§€æå‡è‡³å®Œç¾ï¼Œè­‰æ˜ç‰¹å¾µè£œå…¨çš„å¿…è¦æ€§
   - SVMæ”¹å–„æœ‰é™ï¼Œæç¤ºéœ€è¦è¶…åƒæ•¸èª¿æ•´ä»¥å……åˆ†åˆ©ç”¨æ–°ç‰¹å¾µ

4. **å·¥ç¨‹å•Ÿç¤º**ï¼š
   - æŠ•å…¥7å€‹è¡ç”Ÿç‰¹å¾µçš„è¨ˆç®—æˆæœ¬ï¼ˆå¹¾ä¹ç‚ºé›¶ï¼‰ï¼Œç²å¾—12-28%çš„æº–ç¢ºç‡æå‡
   - ROIï¼ˆæŠ•è³‡å›å ±ç‡ï¼‰æ¥µé«˜çš„ç‰¹å¾µå·¥ç¨‹æ¡ˆä¾‹
   - é©—è­‰äº†ã€Œé ˜åŸŸçŸ¥è­˜ > ç®—æ³•èª¿å„ªã€çš„æ©Ÿå™¨å­¸ç¿’é‡‘å¾‹

### 14.6 å¯¦é©—çµè«–èˆ‡å•Ÿç¤º

#### æ ¸å¿ƒçµè«–

1. **ç‰¹å¾µå·¥ç¨‹åƒ¹å€¼å·¨å¤§**ï¼š7å€‹è¡ç”Ÿç‰¹å¾µä½¿æº–ç¢ºç‡å¾87.79%æå‡è‡³100%
2. **é ˜åŸŸçŸ¥è­˜ä¸å¯æ›¿ä»£**ï¼šé›¶åºç‰¹å¾µçš„è¨­è¨ˆä¾†è‡ªé›»åŠ›ç³»çµ±ç†è«–ï¼Œéæ•¸æ“šé©…å‹•æ–¹æ³•èƒ½ç™¼ç¾
3. **å•é¡Œç²¾æº–è¨ºæ–·æ˜¯é—œéµ**ï¼šé‡å°LLL/LLLGæ··æ·†å•é¡Œï¼Œé›¶åºç‰¹å¾µå®Œç¾è§£æ±º
4. **å·¥ç¨‹å¯¦è¸å¯è¡Œæ€§é«˜**ï¼šæ‰€æœ‰æ¨¡å‹å‡æ»¿è¶³å¯¦æ™‚æ€§è¦æ±‚ï¼ˆ<1ms/æ¨£æœ¬ï¼‰

#### æ•™å­¸å•Ÿç¤º

âœ… **å­¸ç”Ÿæ‡‰è©²å­¸æœƒçš„æŠ€èƒ½**ï¼š
- å¦‚ä½•å¾æ¥­å‹™å•é¡Œå‡ºç™¼ï¼Œåˆ†ææ•¸æ“šèƒŒå¾Œçš„ç‰©ç†/åŒ–å­¸æ©Ÿåˆ¶
- å¦‚ä½•è¨­è¨ˆæœ‰æ˜ç¢ºæ„ç¾©çš„ç‰¹å¾µï¼Œè€Œéç›²ç›®å˜—è©¦çµ„åˆ
- å¦‚ä½•è¨­è¨ˆå°æ¯”å¯¦é©—ï¼Œç§‘å­¸é©—è­‰ç‰¹å¾µå·¥ç¨‹çš„æ•ˆæœ
- å¦‚ä½•åœ¨æº–ç¢ºç‡ã€é€Ÿåº¦ã€å¯è§£é‡‹æ€§ä¹‹é–“åšæ¬Šè¡¡æ±ºç­–

âœ… **åŒ–å·¥/ç¨‹åºå·¥ç¨‹ä¸­çš„é¡æ¨**ï¼š
- **åæ‡‰å‹•åŠ›å­¸å»ºæ¨¡**ï¼šè¨­è¨ˆåæ‡‰é€Ÿç‡ã€è½‰åŒ–ç‡ã€é¸æ“‡æ€§ç­‰è¡ç”Ÿç‰¹å¾µ
- **åˆ†é›¢éç¨‹å„ªåŒ–**ï¼šè¨­è¨ˆå¡”æ¿æ•ˆç‡ã€å›æµæ¯”ã€èƒ½è€—æŒ‡æ¨™ç­‰ç‰¹å¾µ
- **è³ªé‡æ§åˆ¶**ï¼šè¨­è¨ˆçµ±è¨ˆéç¨‹æ§åˆ¶ï¼ˆSPCï¼‰åœ–çš„çµ±è¨ˆé‡ç‰¹å¾µ
- **æ•…éšœé è­¦**ï¼šè¨­è¨ˆè¶¨å‹¢ã€æ³¢å‹•ã€ç•°å¸¸åé›¢ç­‰æ™‚åºç‰¹å¾µ

---

## åƒè€ƒæ–‡ç»

1. Kaggle Dataset: Electrical Fault Detection and Classification  
   https://www.kaggle.com/datasets/esathyaprakash/electrical-fault-detection-and-classification/

2. scikit-learn Documentation: Supervised Learning  
   https://scikit-learn.org/stable/supervised_learning.html

3. "Artificial neural network based fault detection in transmission line" (2015)  
   SpringerPlus: https://springerplus.springeropen.com/articles/10.1186/s40064-015-1080-x

4. Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning*. Springer.

5. Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.

---

**èª²ç¨‹ç·´ç¿’å»ºè­°**ï¼š
1. ä¸‹è¼‰æ•¸æ“šé›†ä¸¦å¾©ç¾æ‰€æœ‰æ¨¡å‹è¨“ç·´æµç¨‹
2. å˜—è©¦ä¸åŒçš„è¶…åƒæ•¸çµ„åˆï¼Œè§€å¯Ÿæ€§èƒ½è®ŠåŒ–
3. å¯¦ä½œéšå±¤å¼ç›£æ§ç³»çµ±ï¼Œæ¯”è¼ƒå–®ä¸€æ¨¡å‹èˆ‡çµ„åˆç­–ç•¥
4. æ’°å¯«å®Œæ•´çš„æŠ€è¡“å ±å‘Šï¼ŒåŒ…æ‹¬æ•¸æ“šåˆ†æã€æ¨¡å‹æ¯”è¼ƒèˆ‡éƒ¨ç½²å»ºè­°

**ç¥å­¸ç¿’é †åˆ©ï¼** ğŸ“âš¡
