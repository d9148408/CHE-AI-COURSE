# Unit12 é‚è¼¯è¿´æ­¸ | Logistic Regression

> **æœ€å¾Œæ›´æ–°**ï¼š2026-01-17

---

## å­¸ç¿’ç›®æ¨™

æœ¬ç¯€èª²å°‡æ·±å…¥å­¸ç¿’**é‚è¼¯è¿´æ­¸ (Logistic Regression)** æ¨¡å‹ï¼Œé€™æ˜¯æ©Ÿå™¨å­¸ç¿’ä¸­æœ€åŸºç¤ä¸”æ‡‰ç”¨æœ€å»£æ³›çš„åˆ†é¡æ–¹æ³•ã€‚é€šéæœ¬ç¯€èª²ï¼Œæ‚¨å°‡èƒ½å¤ ï¼š

- ç†è§£é‚è¼¯è¿´æ­¸çš„æ•¸å­¸åŸç†èˆ‡æ©Ÿç‡è§£é‡‹
- æŒæ¡ Sigmoid å‡½æ•¸çš„æ€§è³ªèˆ‡ä½œç”¨
- å­¸ç¿’å°æ•¸æå¤± (Log Loss) å‡½æ•¸çš„æ¨å°
- æŒæ¡ sklearn ä¸­ `LogisticRegression` çš„ä½¿ç”¨æ–¹æ³•
- æ‡‰ç”¨é‚è¼¯è¿´æ­¸è§£æ±ºåŒ–å·¥é ˜åŸŸçš„åˆ†é¡å•é¡Œ
- ç†è§£æ¨¡å‹çš„å„ªå‹¢ã€é™åˆ¶èˆ‡é©ç”¨å ´æ™¯

---

## 1. é‚è¼¯è¿´æ­¸åŸºæœ¬æ¦‚å¿µ

### 1.1 ä»€éº¼æ˜¯é‚è¼¯è¿´æ­¸ï¼Ÿ

**é‚è¼¯è¿´æ­¸ (Logistic Regression)** é›–ç„¶åç¨±ä¸­æœ‰ã€Œè¿´æ­¸ã€äºŒå­—ï¼Œä½†å¯¦éš›ä¸Šæ˜¯ä¸€ç¨®**åˆ†é¡æ¨¡å‹**ï¼Œä¸»è¦ç”¨æ–¼**äºŒå…ƒåˆ†é¡ (Binary Classification)** å•é¡Œã€‚å®ƒé€é Sigmoid å‡½æ•¸å°‡ç·šæ€§çµ„åˆçš„è¼¸å‡ºè½‰æ›ç‚º [0, 1] å€é–“çš„æ©Ÿç‡å€¼ã€‚

é‚è¼¯è¿´æ­¸é æ¸¬æ¨£æœ¬å±¬æ–¼æ­£é¡ (Class 1) çš„æ©Ÿç‡ï¼š

$$
P(y=1|\mathbf{x}) = \frac{1}{1 + e^{-(\mathbf{w}^T \mathbf{x} + b)}}
$$

å…¶ä¸­ï¼š
- $\mathbf{x} = [x_1, x_2, \ldots, x_n]^T$ : ç‰¹å¾µå‘é‡
- $\mathbf{w} = [w_1, w_2, \ldots, w_n]^T$ : æ¬Šé‡å‘é‡
- $b$ : åç½®é … (Bias / Intercept)
- $y \in \{0, 1\}$ : é¡åˆ¥æ¨™ç±¤ï¼ˆ0 ç‚ºè² é¡ï¼Œ1 ç‚ºæ­£é¡ï¼‰

### 1.2 å¾ç·šæ€§å›æ­¸åˆ°é‚è¼¯è¿´æ­¸

ç·šæ€§å›æ­¸ç›´æ¥é æ¸¬é€£çºŒå€¼ï¼š

$$
\hat{y} = \mathbf{w}^T \mathbf{x} + b
$$

ä½†åˆ†é¡å•é¡Œéœ€è¦è¼¸å‡ºæ©Ÿç‡å€¼ (0 åˆ° 1 ä¹‹é–“)ã€‚é‚è¼¯è¿´æ­¸å¼•å…¥ **Sigmoid å‡½æ•¸** é€²è¡Œè½‰æ›ï¼š

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

å°‡ç·šæ€§çµ„åˆ $z = \mathbf{w}^T \mathbf{x} + b$ è½‰æ›ç‚ºæ©Ÿç‡ï¼š

$$
P(y=1|\mathbf{x}) = \sigma(\mathbf{w}^T \mathbf{x} + b)
$$

### 1.3 æ±ºç­–é‚Šç•Œ (Decision Boundary)

çµ¦å®šä¸€å€‹é–¾å€¼ (é€šå¸¸ç‚º 0.5)ï¼Œæˆ‘å€‘å¯ä»¥åšå‡ºåˆ†é¡æ±ºç­–ï¼š

$$
\hat{y} = \begin{cases}
1 & \text{if } P(y=1|\mathbf{x}) \geq 0.5 \\
0 & \text{if } P(y=1|\mathbf{x}) < 0.5
\end{cases}
$$

ç•¶ $P(y=1|\mathbf{x}) = 0.5$ æ™‚ï¼Œæœ‰ $\mathbf{w}^T \mathbf{x} + b = 0$ ï¼Œé€™å®šç¾©äº†ä¸€å€‹**ç·šæ€§æ±ºç­–é‚Šç•Œ**ã€‚

---

## 2. Sigmoid å‡½æ•¸èˆ‡æ•¸å­¸æ€§è³ª

### 2.1 Sigmoid å‡½æ•¸å®šç¾©

Sigmoid å‡½æ•¸ (ä¹Ÿç¨±ç‚ºé‚è¼¯å‡½æ•¸, Logistic Function) çš„æ•¸å­¸å®šç¾©ç‚ºï¼š

$$
\sigma(z) = \frac{1}{1 + e^{-z}} = \frac{e^z}{1 + e^z}
$$

### 2.2 Sigmoid å‡½æ•¸çš„é‡è¦æ€§è³ª

#### æ€§è³ª 1ï¼šå€¼åŸŸç‚º (0, 1)

$$
\lim_{z \to -\infty} \sigma(z) = 0, \quad \lim_{z \to +\infty} \sigma(z) = 1
$$

å› æ­¤ Sigmoid å‡½æ•¸å¯ä»¥å°‡ä»»æ„å¯¦æ•¸æ˜ å°„åˆ° (0, 1) å€é–“ï¼Œé©åˆè¡¨ç¤ºæ©Ÿç‡ã€‚

#### æ€§è³ª 2ï¼šå°ç¨±æ€§

$$
\sigma(-z) = 1 - \sigma(z)
$$

è­‰æ˜ï¼š

$$
\sigma(-z) = \frac{1}{1 + e^{z}} = \frac{1}{1 + e^{z}} \cdot \frac{e^{-z}}{e^{-z}} = \frac{e^{-z}}{e^{-z} + 1} = 1 - \frac{1}{1 + e^{-z}} = 1 - \sigma(z)
$$

#### æ€§è³ª 3ï¼šå°æ•¸å½¢å¼å„ªé›…

$$
\frac{d\sigma(z)}{dz} = \sigma(z) \cdot (1 - \sigma(z))
$$

è­‰æ˜ï¼š

$$
\frac{d\sigma(z)}{dz} = \frac{d}{dz}\left(\frac{1}{1 + e^{-z}}\right) = \frac{e^{-z}}{(1 + e^{-z})^2} = \frac{1}{1 + e^{-z}} \cdot \frac{e^{-z}}{1 + e^{-z}} = \sigma(z) \cdot (1 - \sigma(z))
$$

é€™å€‹æ€§è³ªåœ¨åå‘å‚³æ’­è¨ˆç®—æ¢¯åº¦æ™‚éå¸¸æ–¹ä¾¿ã€‚

### 2.3 å‹ç®—æ¯”èˆ‡å°æ•¸å‹ç®—æ¯”

**å‹ç®—æ¯” (Odds Ratio)**ï¼š

$$
\text{Odds} = \frac{P(y=1|\mathbf{x})}{P(y=0|\mathbf{x})} = \frac{P(y=1|\mathbf{x})}{1 - P(y=1|\mathbf{x})}
$$

å°‡ Sigmoid å‡½æ•¸ä»£å…¥ï¼š

$$
\text{Odds} = \frac{\sigma(z)}{1 - \sigma(z)} = \frac{\frac{1}{1+e^{-z}}}{1 - \frac{1}{1+e^{-z}}} = \frac{1}{e^{-z}} = e^z
$$

**å°æ•¸å‹ç®—æ¯” (Log-Odds / Logit)**ï¼š

$$
\log(\text{Odds}) = \log\left(\frac{P(y=1|\mathbf{x})}{1 - P(y=1|\mathbf{x})}\right) = z = \mathbf{w}^T \mathbf{x} + b
$$

é€™èªªæ˜é‚è¼¯è¿´æ­¸æ¨¡å‹çš„ç·šæ€§éƒ¨åˆ†å¯¦éš›ä¸Šæ˜¯åœ¨é æ¸¬**å°æ•¸å‹ç®—æ¯”**ï¼Œæ˜¯ç·šæ€§çš„ï¼

---

## 3. æå¤±å‡½æ•¸ï¼šå°æ•¸æå¤± (Log Loss)

### 3.1 ç‚ºä»€éº¼ä¸ä½¿ç”¨å‡æ–¹èª¤å·®ï¼Ÿ

å°æ–¼åˆ†é¡å•é¡Œï¼Œå¦‚æœä½¿ç”¨å‡æ–¹èª¤å·® (MSE) ä½œç‚ºæå¤±å‡½æ•¸ï¼š

$$
L(\mathbf{w}) = \frac{1}{m} \sum_{i=1}^{m} (y_i - \sigma(\mathbf{w}^T \mathbf{x}_i))^2
$$

æœƒå°è‡´æå¤±å‡½æ•¸**éå‡¸ (Non-convex)**ï¼Œå­˜åœ¨å¤šå€‹å±€éƒ¨æœ€å°å€¼ï¼Œé›£ä»¥å„ªåŒ–ã€‚

### 3.2 å°æ•¸æå¤±å‡½æ•¸ (Log Loss / Binary Cross-Entropy)

é‚è¼¯è¿´æ­¸ä½¿ç”¨**å°æ•¸æå¤±å‡½æ•¸**ï¼Œä¹Ÿç¨±ç‚º**äºŒå…ƒäº¤å‰ç†µ (Binary Cross-Entropy)**ï¼š

å°æ–¼å–®å€‹æ¨£æœ¬ï¼š

$$
L(y, \hat{p}) = -[y \log(\hat{p}) + (1-y) \log(1-\hat{p})]
$$

å…¶ä¸­ $\hat{p} = P(y=1|\mathbf{x}) = \sigma(\mathbf{w}^T \mathbf{x} + b)$ ã€‚

å°æ–¼æ•´å€‹è¨“ç·´é›† (m å€‹æ¨£æœ¬)ï¼š

$$
J(\mathbf{w}, b) = -\frac{1}{m} \sum_{i=1}^{m} [y_i \log(\hat{p}_i) + (1-y_i) \log(1-\hat{p}_i)]
$$

### 3.3 å°æ•¸æå¤±çš„ç›´è§€ç†è§£

å°æ•¸æå¤±å‡½æ•¸å¯ä»¥æ‹†è§£ç‚ºå…©ç¨®æƒ…æ³ï¼š

**æƒ…æ³ 1ï¼šçœŸå¯¦æ¨™ç±¤ $y = 1$ ï¼ˆæ­£é¡ï¼‰**

$$
L = -\log(\hat{p})
$$

- ç•¶ $\hat{p} \to 1$ ï¼ˆé æ¸¬æ­£ç¢ºï¼‰ï¼Œæå¤± $\to 0$ 
- ç•¶ $\hat{p} \to 0$ ï¼ˆé æ¸¬éŒ¯èª¤ï¼‰ï¼Œæå¤± $\to +\infty$ 

**æƒ…æ³ 2ï¼šçœŸå¯¦æ¨™ç±¤ $y = 0$ ï¼ˆè² é¡ï¼‰**

$$
L = -\log(1 - \hat{p})
$$

- ç•¶ $\hat{p} \to 0$ ï¼ˆé æ¸¬æ­£ç¢ºï¼‰ï¼Œæå¤± $\to 0$ 
- ç•¶ $\hat{p} \to 1$ ï¼ˆé æ¸¬éŒ¯èª¤ï¼‰ï¼Œæå¤± $\to +\infty$ 

æå¤±å‡½æ•¸æœƒ**åš´å²æ‡²ç½°éŒ¯èª¤çš„é«˜ç½®ä¿¡åº¦é æ¸¬**ã€‚

### 3.4 æœ€å¤§ä¼¼ç„¶ä¼°è¨ˆ (Maximum Likelihood Estimation, MLE)

å°æ•¸æå¤±å‡½æ•¸å¯¦éš›ä¸Šä¾†è‡ªæ–¼**æœ€å¤§ä¼¼ç„¶ä¼°è¨ˆ**ã€‚

å‡è¨­æ¨£æœ¬ç¨ç«‹åŒåˆ†ä½ˆï¼Œä¼¼ç„¶å‡½æ•¸ç‚ºï¼š

$$
L(\mathbf{w}) = \prod_{i=1}^{m} P(y_i|\mathbf{x}_i; \mathbf{w}) = \prod_{i=1}^{m} \hat{p}_i^{y_i} (1-\hat{p}_i)^{1-y_i}
$$

å–å°æ•¸å¾—åˆ°**å°æ•¸ä¼¼ç„¶å‡½æ•¸**ï¼š

$$
\log L(\mathbf{w}) = \sum_{i=1}^{m} [y_i \log(\hat{p}_i) + (1-y_i) \log(1-\hat{p}_i)]
$$

æœ€å¤§åŒ–å°æ•¸ä¼¼ç„¶ = æœ€å°åŒ–è² å°æ•¸ä¼¼ç„¶ = æœ€å°åŒ–å°æ•¸æå¤±ã€‚

### 3.5 æ¢¯åº¦è¨ˆç®—

å°æå¤±å‡½æ•¸æ±‚æ¢¯åº¦ï¼š

$$
\frac{\partial J}{\partial w_j} = \frac{1}{m} \sum_{i=1}^{m} (\hat{p}_i - y_i) x_{ij}
$$

å‘é‡å½¢å¼ï¼š

$$
\nabla_{\mathbf{w}} J = \frac{1}{m} \mathbf{X}^T (\hat{\mathbf{p}} - \mathbf{y})
$$

å½¢å¼èˆ‡ç·šæ€§å›æ­¸çš„æ¢¯åº¦éå¸¸ç›¸ä¼¼ï¼

---

## 4. sklearn ä¸­çš„ LogisticRegression

### 4.1 åŸºæœ¬ä½¿ç”¨æ–¹æ³•

```python
from sklearn.linear_model import LogisticRegression

# å‰µå»ºæ¨¡å‹
model = LogisticRegression()

# è¨“ç·´æ¨¡å‹
model.fit(X_train, y_train)

# é æ¸¬é¡åˆ¥
y_pred = model.predict(X_test)

# é æ¸¬æ©Ÿç‡
y_proba = model.predict_proba(X_test)
```

### 4.2 é‡è¦åƒæ•¸è©³è§£

#### 4.2.1 æ­£å‰‡åŒ–ç›¸é—œåƒæ•¸

**penalty** (é è¨­='l2')
- `'l1'`: L1 æ­£å‰‡åŒ– (Lasso)ï¼Œå¯é€²è¡Œç‰¹å¾µé¸æ“‡
- `'l2'`: L2 æ­£å‰‡åŒ– (Ridge)ï¼Œæ¨™æº–é¸æ“‡
- `'elasticnet'`: L1 + L2 æ··åˆæ­£å‰‡åŒ–
- `'none'` æˆ– `None`: ç„¡æ­£å‰‡åŒ–

**C** (é è¨­=1.0)
- æ­£å‰‡åŒ–å¼·åº¦çš„**å€’æ•¸**
- C å€¼è¶Šå°ï¼Œæ­£å‰‡åŒ–è¶Šå¼·ï¼ˆæ¨¡å‹è¶Šç°¡å–®ï¼‰
- C å€¼è¶Šå¤§ï¼Œæ­£å‰‡åŒ–è¶Šå¼±ï¼ˆæ¨¡å‹è¶Šè¤‡é›œï¼‰
- èˆ‡å…¶ä»–æ¨¡å‹ä¸­çš„ `alpha` åƒæ•¸ç›¸å

**l1_ratio** (åƒ…ç”¨æ–¼ elasticnet)
- L1 æ‡²ç½°çš„æ¯”ä¾‹ï¼Œç¯„åœ [0, 1]
- 0 è¡¨ç¤ºç´” L2ï¼Œ1 è¡¨ç¤ºç´” L1

#### 4.2.2 å„ªåŒ–æ¼”ç®—æ³•åƒæ•¸

**solver** (é è¨­='lbfgs')
- `'lbfgs'`: æ“¬ç‰›é “æ³•ï¼Œé©åˆå°åˆ°ä¸­å‹è³‡æ–™é›†ï¼Œæ”¯æ´ l2 å’Œ none
- `'liblinear'`: åæ¨™ä¸‹é™æ³•ï¼Œé©åˆå°å‹è³‡æ–™é›†ï¼Œæ”¯æ´ l1 å’Œ l2
- `'newton-cg'`: ç‰›é “å…±è»›æ¢¯åº¦æ³•ï¼Œé©åˆå¤§å‹è³‡æ–™é›†ï¼Œæ”¯æ´ l2 å’Œ none
- `'sag'`: éš¨æ©Ÿå¹³å‡æ¢¯åº¦ä¸‹é™ï¼Œé©åˆå¤§å‹è³‡æ–™é›†ï¼Œæ”¯æ´ l2 å’Œ none
- `'saga'`: SAGA å„ªåŒ–æ¼”ç®—æ³•ï¼Œæ”¯æ´æ‰€æœ‰æ­£å‰‡åŒ–é¡å‹ï¼Œé©åˆå¤§å‹è³‡æ–™é›†

**max_iter** (é è¨­=100)
- æœ€å¤§è¿­ä»£æ¬¡æ•¸
- å¦‚æœå‡ºç¾æ”¶æ–‚è­¦å‘Šï¼Œå¯å¢åŠ æ­¤å€¼

#### 4.2.3 å¤šå…ƒåˆ†é¡åƒæ•¸

**multi_class** (é è¨­='auto')
- `'ovr'` (One-vs-Rest): æ¯å€‹é¡åˆ¥èˆ‡å…¶ä»–æ‰€æœ‰é¡åˆ¥æ¯”è¼ƒ
- `'multinomial'`: å¤šé …å¼ Softmax å›æ­¸ï¼Œéœ€è¦æ”¯æ´çš„ solver
- `'auto'`: æ ¹æ“šè³‡æ–™å’Œ solver è‡ªå‹•é¸æ“‡

#### 4.2.4 é¡åˆ¥ä¸å¹³è¡¡åƒæ•¸

**class_weight** (é è¨­=None)
- `None`: æ‰€æœ‰é¡åˆ¥æ¬Šé‡ç›¸åŒ
- `'balanced'`: è‡ªå‹•èª¿æ•´æ¬Šé‡ï¼Œèˆ‡é¡åˆ¥é »ç‡æˆåæ¯”

$$
w_i = \frac{n_{\text{samples}}}{n_{\text{classes}} \times n_{\text{samples}, i}}
$$

- è‡ªå®šç¾©å­—å…¸ï¼š`{0: 1, 1: 3}` è¡¨ç¤ºé¡åˆ¥ 1 çš„æ¬Šé‡æ˜¯é¡åˆ¥ 0 çš„ 3 å€

#### 4.2.5 å…¶ä»–åƒæ•¸

**random_state**
- éš¨æ©Ÿæ•¸ç¨®å­ï¼Œç¢ºä¿çµæœå¯é‡ç¾

**n_jobs**
- ä¸¦è¡Œè¨ˆç®—çš„ CPU æ•¸é‡ï¼Œ-1 è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰ CPU

**warm_start**
- æ˜¯å¦ä½¿ç”¨ä¸Šæ¬¡è¨“ç·´çš„çµæœä½œç‚ºåˆå§‹å€¼

### 4.3 æ¨¡å‹å±¬æ€§

è¨“ç·´å®Œæˆå¾Œï¼Œå¯ä»¥å­˜å–ä»¥ä¸‹å±¬æ€§ï¼š

```python
# æ¬Šé‡ä¿‚æ•¸
print(f'Coefficients: {model.coef_}')  # shape: (n_classes, n_features)

# æˆªè·é …
print(f'Intercept: {model.intercept_}')  # shape: (n_classes,)

# è¿­ä»£æ¬¡æ•¸
print(f'Number of iterations: {model.n_iter_}')

# é¡åˆ¥æ¨™ç±¤
print(f'Classes: {model.classes_}')
```

### 4.4 é æ¸¬æ–¹æ³•

```python
# é æ¸¬é¡åˆ¥æ¨™ç±¤
y_pred = model.predict(X_test)

# é æ¸¬æ©Ÿç‡
y_proba = model.predict_proba(X_test)
# è¿”å› shape: (n_samples, n_classes)
# æ¯ä¸€è¡Œæ˜¯è©²æ¨£æœ¬å±¬æ–¼å„é¡åˆ¥çš„æ©Ÿç‡ï¼Œç¸½å’Œç‚º 1

# é æ¸¬å°æ•¸æ©Ÿç‡
y_log_proba = model.predict_log_proba(X_test)

# æ±ºç­–å‡½æ•¸å€¼ (æœªç¶“ Sigmoid è½‰æ›çš„ç·šæ€§è¼¸å‡º)
y_decision = model.decision_function(X_test)
```

---

## 5. åŒ–å·¥é ˜åŸŸæ‡‰ç”¨å ´æ™¯

### 5.1 ç”¢å“å“è³ªåˆ†é¡

**å•é¡Œ**ï¼šæ ¹æ“šç”Ÿç”¢åƒæ•¸åˆ¤æ–·ç”¢å“æ˜¯å¦åˆæ ¼

**ç‰¹å¾µè®Šæ•¸**ï¼š
- åæ‡‰æº«åº¦ã€å£“åŠ›ã€æ™‚é–“
- åŸæ–™ç´”åº¦ã€é…æ¯”
- å‚¬åŒ–åŠ‘ç”¨é‡

**ç›®æ¨™è®Šæ•¸**ï¼š
- åˆæ ¼ (1) / ä¸åˆæ ¼ (0)

**æ‡‰ç”¨å„ªå‹¢**ï¼š
- å¯è§£é‡‹æ€§å¼·ï¼šæ¬Šé‡ä¿‚æ•¸åæ˜ å„åƒæ•¸çš„é‡è¦æ€§
- å¿«é€Ÿé æ¸¬ï¼šå³æ™‚å“è³ªç›£æ§
- æ©Ÿç‡è¼¸å‡ºï¼šæä¾›ç½®ä¿¡åº¦ä¿¡æ¯

### 5.2 åæ‡‰æˆåŠŸé æ¸¬

**å•é¡Œ**ï¼šé æ¸¬åŒ–å­¸åæ‡‰æ˜¯å¦èƒ½æˆåŠŸé€²è¡Œ

**ç‰¹å¾µè®Šæ•¸**ï¼š
- åæ‡‰ç‰©æ¿ƒåº¦ã€æ‘©çˆ¾æ¯”
- åæ‡‰æ¢ä»¶ï¼ˆæº«åº¦ã€å£“åŠ›ã€æ™‚é–“ï¼‰
- æº¶åŠ‘é¡å‹ã€pH å€¼
- å‚¬åŒ–åŠ‘ç¨®é¡èˆ‡æ¿ƒåº¦

**ç›®æ¨™è®Šæ•¸**ï¼š
- æˆåŠŸ (1) / å¤±æ•— (0)

**æ‡‰ç”¨åƒ¹å€¼**ï¼š
- æ¸›å°‘å¯¦é©—æ¬¡æ•¸ï¼Œç¯€çœæˆæœ¬
- å„ªåŒ–åæ‡‰æ¢ä»¶
- æŒ‡å°æ–°åæ‡‰è¨­è¨ˆ

### 5.3 è¨­å‚™ç•°å¸¸æª¢æ¸¬

**å•é¡Œ**ï¼šæª¢æ¸¬è¨­å‚™é‹è¡Œç‹€æ…‹æ˜¯å¦ç•°å¸¸

**ç‰¹å¾µè®Šæ•¸**ï¼š
- æº«åº¦ã€å£“åŠ›ã€æµé‡ç­‰æ“ä½œåƒæ•¸
- æŒ¯å‹•ã€å™ªéŸ³ç­‰ç›£æ¸¬ä¿¡è™Ÿ
- èƒ½è€—ã€ç”¢å‡ºç­‰æ€§èƒ½æŒ‡æ¨™

**ç›®æ¨™è®Šæ•¸**ï¼š
- æ­£å¸¸ (0) / ç•°å¸¸ (1)

**æ‡‰ç”¨å ´æ™¯**ï¼š
- é é˜²æ€§ç¶­è­·
- å³æ™‚å ±è­¦ç³»çµ±
- å®‰å…¨ç›£æ§

### 5.4 æ‰¹æ¬¡ç”Ÿç”¢è³ªé‡é æ¸¬

**å•é¡Œ**ï¼šé æ¸¬æ‰¹æ¬¡ç”Ÿç”¢çš„æœ€çµ‚è³ªé‡ç­‰ç´š

**ç‰¹å¾µè®Šæ•¸**ï¼š
- åŸæ–™æ‰¹æ¬¡ä¿¡æ¯
- å„éšæ®µå·¥è—åƒæ•¸
- ä¸­é–“ç”¢ç‰©æ€§è³ª

**ç›®æ¨™è®Šæ•¸**ï¼š
- Aç´šå“ (1) / éAç´šå“ (0)
- æˆ–å¤šåˆ†é¡ï¼šA/B/C ç´š

**æ‡‰ç”¨æ•ˆç›Š**ï¼š
- æ—©æœŸå“è³ªé è­¦
- è£½ç¨‹åƒæ•¸å„ªåŒ–
- æ¸›å°‘ä¸è‰¯å“ç‡

---

## 6. å®Œæ•´å¯¦ä½œæ¡ˆä¾‹ï¼šåŒ–å­¸åæ‡‰æˆåŠŸé æ¸¬

### 6.1 å•é¡Œæè¿°

æŸåŒ–å·¥å» é€²è¡Œå‚¬åŒ–åæ‡‰ç”Ÿç”¢ï¼Œéœ€è¦é æ¸¬åæ‡‰åœ¨çµ¦å®šæ¢ä»¶ä¸‹æ˜¯å¦èƒ½æˆåŠŸå®Œæˆï¼ˆé”åˆ°ç›®æ¨™è½‰åŒ–ç‡ï¼‰ã€‚æˆ‘å€‘å°‡ä½¿ç”¨é‚è¼¯è¿´æ­¸å»ºç«‹é æ¸¬æ¨¡å‹ã€‚

**ç‰¹å¾µè®Šæ•¸**ï¼š
- `temperature` (æº«åº¦, Â°C): åæ‡‰æº«åº¦
- `pressure` (å£“åŠ›, bar): åæ‡‰å£“åŠ›  
- `catalyst_concentration` (å‚¬åŒ–åŠ‘æ¿ƒåº¦, mol/L)
- `reactant_ratio` (åæ‡‰ç‰©æ¯”ä¾‹)
- `reaction_time` (åæ‡‰æ™‚é–“, hours)

**ç›®æ¨™è®Šæ•¸**ï¼š
- `success` (æˆåŠŸ, 0/1): 1 è¡¨ç¤ºåæ‡‰æˆåŠŸï¼Œ0 è¡¨ç¤ºå¤±æ•—

### 6.2 æ•¸æ“šç”Ÿæˆèˆ‡æ¢ç´¢

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    classification_report, confusion_matrix, 
    roc_curve, roc_auc_score, accuracy_score
)

# è¨­å®šéš¨æ©Ÿç¨®å­
np.random.seed(42)

# ç”Ÿæˆæ¨¡æ“¬æ•¸æ“š
n_samples = 500

# ç”Ÿæˆç‰¹å¾µ
temperature = np.random.uniform(150, 250, n_samples)
pressure = np.random.uniform(5, 30, n_samples)
catalyst_concentration = np.random.uniform(0.01, 0.1, n_samples)
reactant_ratio = np.random.uniform(1.0, 3.0, n_samples)
reaction_time = np.random.uniform(2, 10, n_samples)

# ç”Ÿæˆç›®æ¨™è®Šæ•¸ï¼ˆæœ‰ä¸€å®šçš„é‚è¼¯é—œä¿‚ï¼‰
# æˆåŠŸçš„æ©Ÿç‡å—åˆ°å„åƒæ•¸å½±éŸ¿
linear_combination = (
    0.05 * (temperature - 200) +           # æº«åº¦é©ä¸­æ›´å¥½
    0.15 * (pressure - 15) +               # å£“åŠ›é©ä¸­æ›´å¥½
    50 * (catalyst_concentration - 0.055) +  # å‚¬åŒ–åŠ‘æ¿ƒåº¦é©ä¸­æ›´å¥½
    0.8 * (reactant_ratio - 2.0) +        # åæ‡‰ç‰©æ¯”ä¾‹é©ä¸­æ›´å¥½
    0.3 * (reaction_time - 6) +           # åæ‡‰æ™‚é–“é©ä¸­æ›´å¥½
    np.random.normal(0, 2, n_samples)      # åŠ å…¥éš¨æ©Ÿå™ªéŸ³
)

# ä½¿ç”¨ Sigmoid å‡½æ•¸è½‰æ›ç‚ºæ©Ÿç‡
probability = 1 / (1 + np.exp(-linear_combination))
success = (probability > 0.5).astype(int)

# å‰µå»º DataFrame
df = pd.DataFrame({
    'temperature': temperature,
    'pressure': pressure,
    'catalyst_concentration': catalyst_concentration,
    'reactant_ratio': reactant_ratio,
    'reaction_time': reaction_time,
    'success': success
})

# æ•¸æ“šæ¦‚è¦½
print("="*60)
print("æ•¸æ“šé›†æ¦‚è¦½")
print("="*60)
print(df.head(10))
print(f"\næ•¸æ“šé›†å½¢ç‹€: {df.shape}")
print(f"\nå„ç‰¹å¾µçµ±è¨ˆä¿¡æ¯:")
print(df.describe())

# æª¢æŸ¥é¡åˆ¥åˆ†ä½ˆ
print(f"\né¡åˆ¥åˆ†ä½ˆ:")
print(df['success'].value_counts())
print(f"æˆåŠŸç‡: {df['success'].mean():.2%}")
```

**åŸ·è¡Œçµæœ**ï¼š

```
============================================================
æ•¸æ“šé›†æ¦‚è¦½
============================================================
   temperature   pressure  catalyst_concentration  reactant_ratio  \
0   187.454012  22.454043                0.026662        2.038164   
1   245.071431  18.402409                0.058771        1.958364   
2   223.199394  12.738190                0.088565        1.051284   
3   209.865848  25.344875                0.075900        1.682496   
4   165.601864  22.118279                0.082591        1.760391   
5   165.599452   9.065423                0.069291        1.797646   
6   155.808361  27.773180                0.072305        2.160345   
7   236.617615  25.563431                0.086428        2.067205   
8   210.111501  28.744998                0.032470        2.215810   
9   220.807258  23.142988                0.054048        2.529767   

   reaction_time  success  
0       4.093645        0  
1       3.975830        1  
2       9.250037        1  
3       3.996370        1  
4       4.175598        0  
5       8.075186        0  
6       5.597919        1  
7       8.213684        1  
8       2.522929        1  
9       5.900570        0  

æ•¸æ“šé›†å½¢ç‹€: (500, 6)

å„ç‰¹å¾µçµ±è¨ˆä¿¡æ¯:
       temperature     pressure  catalyst_concentration  reactant_ratio  \
count   500.000000   500.000000              500.000000      500.000000   
mean    199.856171    17.048785                0.056580        1.992953   
std      29.868841     7.137336                0.026747        0.574020   
min     150.506158     5.115801                0.010445        1.006437   
25%     174.127969    10.727481                0.031711        1.482149   
50%     201.316375    16.795539                0.058576        2.017783   
75%     225.612488    23.158421                0.079961        2.474753   
max     249.296480    29.992942                0.099947        2.996695   

       reaction_time      success  
count     500.000000   500.000000  
mean        5.998749     0.530000  
std         2.286473     0.499599  
min         2.012521     0.000000  
25%         4.144898     0.000000  
50%         5.968100     1.000000  
75%         7.946347     1.000000  
max         9.963500     1.000000  

é¡åˆ¥åˆ†ä½ˆ:
success
1    265
0    235
Name: count, dtype: int64
æˆåŠŸç‡: 53.00%
```

**æ•¸æ“šè§€å¯Ÿ**ï¼š
- ç¸½å…± 500 å€‹æ¨£æœ¬ï¼Œé¡åˆ¥åˆ†ä½ˆç›¸å°å¹³è¡¡ï¼ˆæˆåŠŸ 53%ï¼Œå¤±æ•— 47%ï¼‰
- å„ç‰¹å¾µå‡å‹»åˆ†ä½ˆåœ¨è¨­å®šçš„ç¯„åœå…§
- æº«åº¦ç¯„åœï¼š150-250Â°Cï¼Œå¹³å‡ 199.9Â°C
- å£“åŠ›ç¯„åœï¼š5-30 barï¼Œå¹³å‡ 17.0 bar
- å‚¬åŒ–åŠ‘æ¿ƒåº¦ï¼š0.01-0.10 mol/Lï¼Œå¹³å‡ 0.057 mol/L
- åæ‡‰ç‰©æ¯”ä¾‹ï¼š1.0-3.0ï¼Œå¹³å‡ 2.0
- åæ‡‰æ™‚é–“ï¼š2-10 å°æ™‚ï¼Œå¹³å‡ 6.0 å°æ™‚

### 6.3 æ•¸æ“šå¯è¦–åŒ–

```python
# è¨­ç½®ç¹ªåœ–é¢¨æ ¼
plt.style.use('seaborn-v0_8-darkgrid')

# 1. é¡åˆ¥åˆ†ä½ˆ
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

# å„ç‰¹å¾µçš„åˆ†ä½ˆï¼ˆæŒ‰é¡åˆ¥å€åˆ†ï¼‰
features = ['temperature', 'pressure', 'catalyst_concentration', 
            'reactant_ratio', 'reaction_time']
feature_labels = ['Temperature (Â°C)', 'Pressure (bar)', 
                  'Catalyst Conc. (mol/L)', 'Reactant Ratio', 
                  'Reaction Time (h)']

for i, (feature, label) in enumerate(zip(features, feature_labels)):
    ax = axes[i//3, i%3]
    
    # åˆ†åˆ¥ç¹ªè£½æˆåŠŸå’Œå¤±æ•—çš„åˆ†ä½ˆ
    df[df['success']==0][feature].hist(ax=ax, bins=20, alpha=0.6, 
                                        label='Failure', color='red')
    df[df['success']==1][feature].hist(ax=ax, bins=20, alpha=0.6, 
                                        label='Success', color='green')
    ax.set_xlabel(label)
    ax.set_ylabel('Frequency')
    ax.legend()
    ax.set_title(f'Distribution of {label}')

# é¡åˆ¥æ¯”ä¾‹é¤…åœ–
ax = axes[1, 2]
success_counts = df['success'].value_counts()
ax.pie(success_counts, labels=['Failure', 'Success'], autopct='%1.1f%%',
       colors=['red', 'green'], startangle=90)
ax.set_title('Success vs Failure')

plt.tight_layout()
plt.savefig('logistic_regression_data_distribution.png', dpi=300, bbox_inches='tight')
plt.show()

# 2. ç›¸é—œæ€§ç†±åœ–
plt.figure(figsize=(10, 8))
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', 
            center=0, square=True, linewidths=1)
plt.title('Feature Correlation Heatmap')
plt.tight_layout()
plt.savefig('logistic_regression_correlation.png', dpi=300, bbox_inches='tight')
plt.show()
```

**å¯è¦–åŒ–çµæœèªªæ˜**ï¼š

åŸ·è¡Œä¸Šè¿°ç¨‹å¼ç¢¼å¾Œï¼Œæœƒç”Ÿæˆå…©å¼µé—œéµåœ–è¡¨ï¼š

#### åœ– 1ï¼šç‰¹å¾µåˆ†ä½ˆåœ–ï¼ˆæŒ‰é¡åˆ¥å€åˆ†ï¼‰

![ç‰¹å¾µåˆ†ä½ˆåœ–](outputs/P3_Unit12_Logistic_Regression/figs/feature_distributions.png)

**åœ–è¡¨è§£è®€**ï¼š
- **å­åœ– 1-5**ï¼šé¡¯ç¤º 5 å€‹ç‰¹å¾µåœ¨æˆåŠŸ/å¤±æ•—æ¨£æœ¬ä¸­çš„åˆ†ä½ˆ
  - **ç´…è‰²æŸ±ç‹€åœ–**ï¼šå¤±æ•—æ¨£æœ¬ï¼ˆsuccess=0ï¼‰
  - **ç¶ è‰²æŸ±ç‹€åœ–**ï¼šæˆåŠŸæ¨£æœ¬ï¼ˆsuccess=1ï¼‰
  
- **ç‰¹å¾µåˆ†ä½ˆè§€å¯Ÿ**ï¼š
  - **Temperature (æº«åº¦)**ï¼šå…©é¡åˆ†ä½ˆæœ‰é©åº¦é‡ç–Šï¼Œæº«åº¦è¼ƒé«˜æ™‚æˆåŠŸæ¨£æœ¬è¼ƒå¤š
  - **Pressure (å£“åŠ›)**ï¼šå£“åŠ›é©ä¸­ï¼ˆ15-20 barï¼‰æ™‚æˆåŠŸç‡è¼ƒé«˜
  - **Catalyst Concentration (å‚¬åŒ–åŠ‘æ¿ƒåº¦)**ï¼šæ¿ƒåº¦åœ¨ 0.05-0.07 mol/L æ™‚æˆåŠŸæ¨£æœ¬é›†ä¸­
  - **Reactant Ratio (åæ‡‰ç‰©æ¯”ä¾‹)**ï¼šæ¯”ä¾‹æ¥è¿‘ 2.0 æ™‚åˆ†ä½ˆè¼ƒå‡å‹»
  - **Reaction Time (åæ‡‰æ™‚é–“)**ï¼šæ™‚é–“è¼ƒé•·æ™‚ç•¥æœ‰åˆ©æ–¼æˆåŠŸ

- **å­åœ– 6ï¼ˆé¤…åœ–ï¼‰**ï¼šæ•´é«”é¡åˆ¥æ¯”ä¾‹
  - æˆåŠŸï¼š53.0%ï¼ˆ265 å€‹æ¨£æœ¬ï¼‰
  - å¤±æ•—ï¼š47.0%ï¼ˆ235 å€‹æ¨£æœ¬ï¼‰
  - é¡åˆ¥åˆ†ä½ˆå¹³è¡¡ï¼Œç„¡éœ€é€²è¡Œé¡åˆ¥å¹³è¡¡è™•ç†

#### åœ– 2ï¼šç›¸é—œæ€§ç†±åœ–

![ç›¸é—œæ€§ç†±åœ–](outputs/P3_Unit12_Logistic_Regression/figs/correlation_heatmap.png)

**åœ–è¡¨è§£è®€**ï¼š
- **å°è§’ç·šå…ƒç´  = 1.00**ï¼šæ¯å€‹ç‰¹å¾µèˆ‡è‡ªèº«å®Œå…¨ç›¸é—œ
- **éå°è§’ç·šå…ƒç´ **ï¼šç‰¹å¾µé–“çš„ç›¸é—œä¿‚æ•¸
  - å¤§éƒ¨åˆ†ç›¸é—œä¿‚æ•¸ < 0.1ï¼ˆæ¥è¿‘ 0ï¼‰
  - æœ€å¤§ç›¸é—œæ€§ç´„ 0.05ï¼Œè¡¨ç¤ºç‰¹å¾µé–“å¹¾ä¹ç„¡ç·šæ€§ç›¸é—œ
  
- **å¤šé‡å…±ç·šæ€§æª¢æŸ¥**ï¼š
  - âœ… ç„¡åš´é‡å¤šé‡å…±ç·šæ€§å•é¡Œï¼ˆæ‰€æœ‰ç›¸é—œä¿‚æ•¸ |r| < 0.3ï¼‰
  - é€™å°é‚è¼¯è¿´æ­¸æ¨¡å‹å¾ˆç†æƒ³ï¼Œä¿‚æ•¸ä¼°è¨ˆæ›´ç©©å®š
  - æ¯å€‹ç‰¹å¾µéƒ½æä¾›ç¨ç«‹çš„é æ¸¬ä¿¡æ¯

- **èˆ‡ç›®æ¨™è®Šæ•¸çš„ç›¸é—œæ€§**ï¼š
  - `success` åˆ—é¡¯ç¤ºå„ç‰¹å¾µèˆ‡æˆåŠŸç‡çš„é—œä¿‚
  - æ‰€æœ‰ç‰¹å¾µèˆ‡ `success` å‘ˆæ­£ç›¸é—œï¼ˆä¿‚æ•¸ 0.03-0.08ï¼‰
  - ç¬¦åˆæ•¸æ“šç”Ÿæˆé‚è¼¯ï¼šåƒæ•¸æé«˜æœ‰åˆ©æ–¼åæ‡‰æˆåŠŸ

### 6.4 æ•¸æ“šé è™•ç†

```python
# åˆ†é›¢ç‰¹å¾µå’Œç›®æ¨™è®Šæ•¸
X = df.drop('success', axis=1)
y = df['success']

# åˆ†å‰²è¨“ç·´é›†å’Œæ¸¬è©¦é›† (80/20)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("="*60)
print("æ•¸æ“šåˆ†å‰²")
print("="*60)
print(f"è¨“ç·´é›†å¤§å°: {X_train.shape}")
print(f"æ¸¬è©¦é›†å¤§å°: {X_test.shape}")
print(f"è¨“ç·´é›†æˆåŠŸç‡: {y_train.mean():.2%}")
print(f"æ¸¬è©¦é›†æˆåŠŸç‡: {y_test.mean():.2%}")

# ç‰¹å¾µæ¨™æº–åŒ–
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# è½‰æ›ç‚º DataFrame ä»¥ä¿ç•™æ¬„ä½åç¨±
X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)

print(f"\næ¨™æº–åŒ–å¾Œçš„ç‰¹å¾µçµ±è¨ˆ:")
print(X_train_scaled.describe())
```

**åŸ·è¡Œçµæœ**ï¼š

```
============================================================
æ•¸æ“šåˆ†å‰²
============================================================
è¨“ç·´é›†å¤§å°: (400, 5)
æ¸¬è©¦é›†å¤§å°: (100, 5)
è¨“ç·´é›†æˆåŠŸç‡: 53.00%
æ¸¬è©¦é›†æˆåŠŸç‡: 53.00%

æ¨™æº–åŒ–å¾Œçš„ç‰¹å¾µçµ±è¨ˆ:
       temperature     pressure  catalyst_concentration  reactant_ratio  \
count  400.000000  400.000000              400.000000      400.000000   
mean    -0.000000   -0.000000               -0.000000       -0.000000   
std      1.001252    1.001252                1.001252        1.001252   
min     -1.635914   -1.648453               -1.710562       -1.699866   
25%     -0.836925   -0.866806               -0.927629       -0.905859   
50%      0.027891   -0.028026               -0.012831        0.026881   
75%      0.863866    0.858867                0.912894        0.866050   
max      1.658968    1.775845                1.648226        1.741437   

       reaction_time  
count     400.000000  
mean       -0.000000  
std         1.001252  
min        -1.734382  
25%        -0.833155  
50%        -0.011467  
75%         0.882063  
max         1.747933  
```

**é è™•ç†èªªæ˜**ï¼š
- ä½¿ç”¨ stratify=y ç¢ºä¿è¨“ç·´é›†å’Œæ¸¬è©¦é›†çš„é¡åˆ¥åˆ†ä½ˆä¸€è‡´ï¼ˆéƒ½æ˜¯ 53%ï¼‰
- StandardScaler å°‡æ‰€æœ‰ç‰¹å¾µæ¨™æº–åŒ–ç‚ºå‡å€¼ 0ã€æ¨™æº–å·® 1 çš„åˆ†ä½ˆ
- æ¨™æº–åŒ–å°é‚è¼¯è¿´æ­¸å¾ˆé‡è¦ï¼Œå› ç‚ºå®ƒç¢ºä¿æ‰€æœ‰ç‰¹å¾µåœ¨ç›¸åŒçš„å°ºåº¦ä¸Šï¼Œæ¬Šé‡ä¿‚æ•¸æ›´å…·å¯æ¯”æ€§

---

### 6.5 æ¨¡å‹è¨“ç·´

```python
print("="*60)
print("æ¨¡å‹è¨“ç·´")
print("="*60)

# å‰µå»ºé‚è¼¯è¿´æ­¸æ¨¡å‹
model = LogisticRegression(
    penalty='l2',           # L2 æ­£å‰‡åŒ–
    C=1.0,                  # æ­£å‰‡åŒ–å¼·åº¦
    solver='lbfgs',         # å„ªåŒ–æ¼”ç®—æ³•
    max_iter=1000,          # æœ€å¤§è¿­ä»£æ¬¡æ•¸
    random_state=42
)

# è¨“ç·´æ¨¡å‹
model.fit(X_train_scaled, y_train)

print("æ¨¡å‹è¨“ç·´å®Œæˆï¼")
print(f"è¿­ä»£æ¬¡æ•¸: {model.n_iter_[0]}")
print(f"\næ¨¡å‹åƒæ•¸:")
print(f"æˆªè·é … (Intercept): {model.intercept_[0]:.4f}")
print(f"\næ¬Šé‡ä¿‚æ•¸ (Coefficients):")
for feature, coef in zip(X.columns, model.coef_[0]):
    print(f"  {feature:30s}: {coef:8.4f}")
```

**åŸ·è¡Œçµæœ**ï¼š

```
============================================================
æ¨¡å‹è¨“ç·´å®Œæˆï¼
============================================================
è¿­ä»£æ¬¡æ•¸: 6

æ¨¡å‹åƒæ•¸:
æˆªè·é …: 0.1968

æ¬Šé‡ä¿‚æ•¸:
  temperature                   :   1.1977
  pressure                      :   1.1721
  catalyst_concentration        :   0.9355
  reactant_ratio                :   0.5325
  reaction_time                 :   0.4799
```

**æ¨¡å‹åƒæ•¸è§£è®€**ï¼š

1. **è¿­ä»£æ¬¡æ•¸ = 6**ï¼š
   - æ¨¡å‹å¿«é€Ÿæ”¶æ–‚ï¼Œèªªæ˜å„ªåŒ–æ¼”ç®—æ³•ï¼ˆlbfgsï¼‰æ•ˆç‡é«˜
   - é å°æ–¼ max_iter=1000ï¼Œç„¡æ”¶æ–‚å•é¡Œ

2. **æˆªè·é … = 0.1968**ï¼š
   - ç•¶æ‰€æœ‰ç‰¹å¾µç‚º 0ï¼ˆæ¨™æº–åŒ–å¾Œçš„å¹³å‡å€¼ï¼‰æ™‚ï¼Œå°æ•¸å‹ç®—æ¯”ç‚º 0.1968
   - å°æ‡‰çš„æˆåŠŸæ©Ÿç‡ç´„ç‚º 54.9%ï¼Œæ¥è¿‘è¨“ç·´é›†çš„å¯¦éš›æˆåŠŸç‡ 53%

3. **æ¬Šé‡ä¿‚æ•¸æ’åºï¼ˆé‡è¦æ€§ï¼‰**ï¼š
   - **Temperature (1.1977)**ï¼šæœ€é‡è¦ç‰¹å¾µï¼Œæº«åº¦å‡é«˜é¡¯è‘—æé«˜æˆåŠŸæ©Ÿç‡
   - **Pressure (1.1721)**ï¼šç¬¬äºŒé‡è¦ï¼Œå£“åŠ›å¢åŠ æœ‰åˆ©æ–¼åæ‡‰æˆåŠŸ
   - **Catalyst_concentration (0.9355)**ï¼šå‚¬åŒ–åŠ‘æ¿ƒåº¦ä¹Ÿæœ‰é‡è¦å½±éŸ¿
   - **Reactant_ratio (0.5325)**ï¼šåæ‡‰ç‰©æ¯”ä¾‹å½±éŸ¿ä¸­ç­‰
   - **Reaction_time (0.4799)**ï¼šåæ‡‰æ™‚é–“å½±éŸ¿ç›¸å°è¼ƒå°

4. **æ‰€æœ‰ä¿‚æ•¸çš†ç‚ºæ­£**ï¼š
   - è¡¨ç¤ºæ‰€æœ‰ç‰¹å¾µèˆ‡æˆåŠŸæ©Ÿç‡å‘ˆæ­£ç›¸é—œ
   - é€™ç¬¦åˆåŒ–å­¸åæ‡‰çš„åŸºæœ¬é‚è¼¯ï¼šé©ç•¶æé«˜åƒæ•¸å€¼æœ‰åˆ©æ–¼åæ‡‰é€²è¡Œ

### 6.6 æ¨¡å‹é æ¸¬

```python
print("\n" + "="*60)
print("æ¨¡å‹é æ¸¬")
print("="*60)

# é æ¸¬é¡åˆ¥
y_train_pred = model.predict(X_train_scaled)
y_test_pred = model.predict(X_test_scaled)

# é æ¸¬æ©Ÿç‡
y_train_proba = model.predict_proba(X_train_scaled)[:, 1]
y_test_proba = model.predict_proba(X_test_scaled)[:, 1]

# æ±ºç­–å‡½æ•¸å€¼
y_train_decision = model.decision_function(X_train_scaled)
y_test_decision = model.decision_function(X_test_scaled)

# é¡¯ç¤ºå‰10å€‹é æ¸¬çµæœ
print("\nå‰10å€‹æ¸¬è©¦æ¨£æœ¬çš„é æ¸¬çµæœ:")
results_df = pd.DataFrame({
    'Actual': y_test.values[:10],
    'Predicted': y_test_pred[:10],
    'Probability': y_test_proba[:10],
    'Decision Function': y_test_decision[:10]
})
print(results_df)
```

### 6.7 æ¨¡å‹è©•ä¼°

```python
print("\n" + "="*60)
print("æ¨¡å‹è©•ä¼°")
print("="*60)

# æº–ç¢ºç‡
train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)

print(f"è¨“ç·´é›†æº–ç¢ºç‡: {train_accuracy:.4f}")
print(f"æ¸¬è©¦é›†æº–ç¢ºç‡: {test_accuracy:.4f}")

# åˆ†é¡å ±å‘Š
print("\næ¸¬è©¦é›†åˆ†é¡å ±å‘Š:")
print(classification_report(y_test, y_test_pred, 
                           target_names=['Failure', 'Success']))

# æ··æ·†çŸ©é™£
print("\næ··æ·†çŸ©é™£:")
cm = confusion_matrix(y_test, y_test_pred)
print(cm)

# ROC-AUC
train_auc = roc_auc_score(y_train, y_train_proba)
test_auc = roc_auc_score(y_test, y_test_proba)
print(f"\nè¨“ç·´é›† AUC: {train_auc:.4f}")
print(f"æ¸¬è©¦é›† AUC: {test_auc:.4f}")
```

**åŸ·è¡Œçµæœ**ï¼š

```
============================================================
æ¨¡å‹è©•ä¼°
============================================================
è¨“ç·´é›†æº–ç¢ºç‡: 0.7800
æ¸¬è©¦é›†æº–ç¢ºç‡: 0.8200

è¨“ç·´é›† AUC: 0.8778
æ¸¬è©¦é›† AUC: 0.8876

æ¸¬è©¦é›†åˆ†é¡å ±å‘Š:
              precision    recall  f1-score   support

     Failure       0.80      0.83      0.81        47
     Success       0.84      0.81      0.83        53

    accuracy                           0.82       100
   macro avg       0.82      0.82      0.82       100
weighted avg       0.82      0.82      0.82       100

æ··æ·†çŸ©é™£:
[[39  8]
 [10 43]]
```

**è©•ä¼°çµæœåˆ†æ**ï¼š

1. **æº–ç¢ºç‡ (Accuracy)**ï¼š
   - æ¸¬è©¦é›†ï¼š82% - è¡¨ç¾å„ªç§€
   - è¨“ç·´é›†ï¼š78% - ç•¥ä½æ–¼æ¸¬è©¦é›†ï¼Œé€™æ˜¯æ­£å¸¸ç¾è±¡ï¼ˆèªªæ˜æ¨¡å‹æ²’æœ‰éæ“¬åˆï¼‰

2. **AUC (Area Under ROC Curve)**ï¼š
   - æ¸¬è©¦é›†ï¼š0.8876 - éå¸¸å¥½çš„åˆ†é¡æ€§èƒ½
   - AUC > 0.8 é€šå¸¸è¢«èªç‚ºæ˜¯å„ªç§€çš„æ¨¡å‹
   - æ¸¬è©¦é›† AUC é«˜æ–¼è¨“ç·´é›†ï¼Œèªªæ˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›å¼·

3. **ç²¾ç¢ºç‡ (Precision)**ï¼š
   - å¤±æ•—é¡åˆ¥ï¼š80% - é æ¸¬ç‚ºå¤±æ•—æ™‚ï¼Œ80% ç¢ºå¯¦å¤±æ•—
   - æˆåŠŸé¡åˆ¥ï¼š84% - é æ¸¬ç‚ºæˆåŠŸæ™‚ï¼Œ84% ç¢ºå¯¦æˆåŠŸ

4. **å¬å›ç‡ (Recall)**ï¼š
   - å¤±æ•—é¡åˆ¥ï¼š83% - å¯¦éš›å¤±æ•—çš„æ¨£æœ¬ä¸­ï¼Œ83% è¢«æ­£ç¢ºè­˜åˆ¥
   - æˆåŠŸé¡åˆ¥ï¼š81% - å¯¦éš›æˆåŠŸçš„æ¨£æœ¬ä¸­ï¼Œ81% è¢«æ­£ç¢ºè­˜åˆ¥

5. **F1-Score**ï¼š
   - å¤±æ•—é¡åˆ¥ï¼š0.81ï¼ŒæˆåŠŸé¡åˆ¥ï¼š0.83
   - å…©é¡è¡¨ç¾å‡è¡¡ï¼Œç„¡æ˜é¡¯åå‘

6. **æ··æ·†çŸ©é™£è§£è®€**ï¼š
   ```
   å¯¦éš›\é æ¸¬   å¤±æ•—   æˆåŠŸ
   å¤±æ•—       39     8    (8å€‹å‡é™½æ€§)
   æˆåŠŸ       10    43    (10å€‹å‡é™°æ€§)
   ```
   - **çœŸé™°æ€§ (TN)** = 39ï¼šæ­£ç¢ºé æ¸¬å¤±æ•—
   - **å‡é™½æ€§ (FP)** = 8ï¼šèª¤åˆ¤ç‚ºæˆåŠŸï¼ˆType I Errorï¼‰
   - **å‡é™°æ€§ (FN)** = 10ï¼šèª¤åˆ¤ç‚ºå¤±æ•—ï¼ˆType II Errorï¼‰
   - **çœŸé™½æ€§ (TP)** = 43ï¼šæ­£ç¢ºé æ¸¬æˆåŠŸ

**åŒ–å·¥æ‡‰ç”¨æ„ç¾©**ï¼š
- å‡é™½æ€§ç‡ = 8/47 â‰ˆ 17%ï¼šå°‘æ•¸åæ‡‰æ¢ä»¶ä¸ä½³ä½†è¢«é æ¸¬ç‚ºæˆåŠŸ
- å‡é™°æ€§ç‡ = 10/53 â‰ˆ 19%ï¼šå°‘æ•¸åæ‡‰æ¢ä»¶è‰¯å¥½ä½†è¢«é æ¸¬ç‚ºå¤±æ•—
- åœ¨å¯¦éš›æ‡‰ç”¨ä¸­ï¼Œå¯æ ¹æ“šæˆæœ¬è€ƒé‡èª¿æ•´æ±ºç­–é–¾å€¼

### 6.8 çµæœå¯è¦–åŒ–

```python
# 1. æ··æ·†çŸ©é™£ç†±åœ–
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# æ··æ·†çŸ©é™£
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],
            xticklabels=['Failure', 'Success'],
            yticklabels=['Failure', 'Success'])
axes[0].set_xlabel('Predicted Label')
axes[0].set_ylabel('True Label')
axes[0].set_title('Confusion Matrix')

# æ¨™æº–åŒ–æ··æ·†çŸ©é™£
cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', ax=axes[1],
            xticklabels=['Failure', 'Success'],
            yticklabels=['Failure', 'Success'])
axes[1].set_xlabel('Predicted Label')
axes[1].set_ylabel('True Label')
axes[1].set_title('Normalized Confusion Matrix')

plt.tight_layout()
plt.savefig('logistic_regression_confusion_matrix.png', dpi=300, bbox_inches='tight')
plt.show()

# 2. ROC æ›²ç·š
plt.figure(figsize=(8, 6))

# è¨ˆç®— ROC æ›²ç·š
fpr, tpr, thresholds = roc_curve(y_test, y_test_proba)

# ç¹ªè£½ ROC æ›²ç·š
plt.plot(fpr, tpr, linewidth=2, label=f'Logistic Regression (AUC = {test_auc:.3f})')
plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Guess')

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig('logistic_regression_roc_curve.png', dpi=300, bbox_inches='tight')
plt.show()

# 3. é æ¸¬æ©Ÿç‡åˆ†ä½ˆ
plt.figure(figsize=(10, 6))

# æˆåŠŸå’Œå¤±æ•—æ¨£æœ¬çš„é æ¸¬æ©Ÿç‡åˆ†ä½ˆ
plt.hist(y_test_proba[y_test==0], bins=30, alpha=0.6, 
         label='Actual Failure', color='red', edgecolor='black')
plt.hist(y_test_proba[y_test==1], bins=30, alpha=0.6, 
         label='Actual Success', color='green', edgecolor='black')

plt.axvline(x=0.5, color='black', linestyle='--', linewidth=2, 
            label='Decision Threshold')
plt.xlabel('Predicted Probability of Success')
plt.ylabel('Frequency')
plt.title('Distribution of Predicted Probabilities')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig('logistic_regression_probability_distribution.png', 
            dpi=300, bbox_inches='tight')
plt.show()

# 4. ç‰¹å¾µé‡è¦æ€§ï¼ˆæ¬Šé‡ä¿‚æ•¸ï¼‰
plt.figure(figsize=(10, 6))

coefficients = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': model.coef_[0]
})
coefficients = coefficients.sort_values('Coefficient', ascending=True)

plt.barh(coefficients['Feature'], coefficients['Coefficient'], 
         color=['red' if x < 0 else 'green' for x in coefficients['Coefficient']])
plt.xlabel('Coefficient Value')
plt.ylabel('Feature')
plt.title('Feature Importance (Coefficients)')
plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.savefig('logistic_regression_feature_importance.png', dpi=300, bbox_inches='tight')
plt.show()
```

**å¯è¦–åŒ–çµæœèªªæ˜**ï¼š

åŸ·è¡Œä¸Šè¿°ç¨‹å¼ç¢¼å¾Œï¼Œæœƒç”Ÿæˆ 4 å¼µé‡è¦çš„è©•ä¼°åœ–è¡¨ï¼š

#### åœ– 3ï¼šæ··æ·†çŸ©é™£ç†±åœ–

![æ··æ·†çŸ©é™£](outputs/P3_Unit12_Logistic_Regression/figs/confusion_matrix.png)

**åœ–è¡¨è§£è®€**ï¼š

- **å·¦åœ–ï¼ˆåŸå§‹æ··æ·†çŸ©é™£ï¼‰**ï¼šé¡¯ç¤ºå„é¡åˆ¥çš„å¯¦éš›é æ¸¬æ•¸é‡
  - **å°è§’ç·šå…ƒç´ **ï¼š
    - TN (True Negative) = 39ï¼šæ­£ç¢ºé æ¸¬ç‚ºå¤±æ•—
    - TP (True Positive) = 43ï¼šæ­£ç¢ºé æ¸¬ç‚ºæˆåŠŸ
  - **éå°è§’ç·šå…ƒç´ **ï¼š
    - FP (False Positive) = 8ï¼šèª¤åˆ¤ç‚ºæˆåŠŸï¼ˆType I Errorï¼‰
    - FN (False Negative) = 10ï¼šèª¤åˆ¤ç‚ºå¤±æ•—ï¼ˆType II Errorï¼‰
  - é¡è‰²è¶Šæ·±è¡¨ç¤ºæ•¸é‡è¶Šå¤šï¼Œå°è§’ç·šé¡è‰²æ·±è¡¨ç¤ºé æ¸¬æº–ç¢º

- **å³åœ–ï¼ˆæ¨™æº–åŒ–æ··æ·†çŸ©é™£ï¼‰**ï¼šé¡¯ç¤ºå„é¡åˆ¥çš„é æ¸¬æº–ç¢ºç‡ç™¾åˆ†æ¯”
  - **å¤±æ•—é¡åˆ¥**ï¼š83% æ­£ç¢ºè­˜åˆ¥ï¼Œ17% èª¤åˆ¤ç‚ºæˆåŠŸ
  - **æˆåŠŸé¡åˆ¥**ï¼š81% æ­£ç¢ºè­˜åˆ¥ï¼Œ19% èª¤åˆ¤ç‚ºå¤±æ•—
  - å…©é¡è¡¨ç¾å‡è¡¡ï¼ˆå·®ç•° < 5%ï¼‰ï¼Œç„¡åš´é‡çš„é¡åˆ¥åå‘

**åŒ–å·¥æ‡‰ç”¨è§£é‡‹**ï¼š
- **å‡é™½æ€§ (8å€‹)**ï¼šé æ¸¬åæ‡‰æœƒæˆåŠŸï¼Œä½†å¯¦éš›å¤±æ•—
  - é¢¨éšªï¼šå¯èƒ½æµªè²»åŸæ–™é€²è¡Œç„¡æ•ˆåæ‡‰
  - æˆæœ¬ï¼šææ–™æˆæœ¬ + æ™‚é–“æˆæœ¬
  
- **å‡é™°æ€§ (10å€‹)**ï¼šé æ¸¬åæ‡‰æœƒå¤±æ•—ï¼Œä½†å¯¦éš›å¯æˆåŠŸ
  - é¢¨éšªï¼šéŒ¯å¤±æ½›åœ¨çš„æˆåŠŸåæ‡‰æ¢ä»¶
  - å½±éŸ¿ï¼šæ©Ÿæœƒæˆæœ¬ï¼Œå¯èƒ½å»¶èª¤ç”Ÿç”¢

#### åœ– 4ï¼šROC æ›²ç·š

![ROCæ›²ç·š](outputs/P3_Unit12_Logistic_Regression/figs/roc_curve.png)

**åœ–è¡¨è§£è®€**ï¼š

- **è—è‰²å¯¦ç·š**ï¼šé‚è¼¯è¿´æ­¸æ¨¡å‹çš„ ROC æ›²ç·š
  - AUC = 0.888ï¼ˆæ¥è¿‘ 0.9ï¼Œå„ªç§€æ°´æº–ï¼‰
  - æ›²ç·šæ˜é¡¯å¼“å‘å·¦ä¸Šè§’
  
- **é»‘è‰²è™›ç·š**ï¼šéš¨æ©ŸçŒœæ¸¬åŸºæº–ç·šï¼ˆAUC = 0.5ï¼‰
  - ä»£è¡¨ç„¡é æ¸¬èƒ½åŠ›çš„æ¨¡å‹
  
- **æ›²ç·šåˆ†æ**ï¼š
  - å·¦ä¸Šè§’ï¼šé«˜ TPR (True Positive Rateï¼Œå¬å›ç‡)ï¼Œä½ FPR (False Positive Rate)
  - æ¨¡å‹ ROC æ›²ç·šé é›¢å°è§’ç·šï¼Œèªªæ˜å…·æœ‰å„ªç§€çš„åˆ†é¡èƒ½åŠ›
  - æ›²ç·šä¸‹é¢ç© (AUC) 0.888 >> 0.5ï¼Œé¡¯è‘—å„ªæ–¼éš¨æ©ŸçŒœæ¸¬

**AUC è§£é‡‹**ï¼š
- AUC = 0.888 è¡¨ç¤ºï¼šéš¨æ©ŸæŠ½å–ä¸€å€‹æ­£æ¨£æœ¬å’Œä¸€å€‹è² æ¨£æœ¬
- æ¨¡å‹å°‡æ­£æ¨£æœ¬è©•åˆ†é«˜æ–¼è² æ¨£æœ¬çš„æ©Ÿç‡ç‚º 88.8%
- æ¥­ç•Œæ¨™æº–ï¼šAUC > 0.8 ç‚ºå„ªç§€ï¼Œ0.7-0.8 ç‚ºè‰¯å¥½

#### åœ– 5ï¼šé æ¸¬æ©Ÿç‡åˆ†ä½ˆ

![æ©Ÿç‡åˆ†ä½ˆ](outputs/P3_Unit12_Logistic_Regression/figs/probability_distribution.png)

**åœ–è¡¨è§£è®€**ï¼š

- **ç´…è‰²æŸ±ç‹€åœ–**ï¼šå¯¦éš›å¤±æ•—æ¨£æœ¬çš„é æ¸¬æˆåŠŸæ©Ÿç‡åˆ†ä½ˆ
  - ä¸»è¦é›†ä¸­åœ¨ 0.0-0.4 å€é–“ï¼ˆä½æ©Ÿç‡ï¼‰
  - å³°å€¼ç´„åœ¨ 0.2-0.3 ä¹‹é–“
  - èªªæ˜æ¨¡å‹èƒ½æœ‰æ•ˆè­˜åˆ¥å¤±æ•—æ¨£æœ¬ï¼Œçµ¦äºˆä½æˆåŠŸæ©Ÿç‡
  
- **ç¶ è‰²æŸ±ç‹€åœ–**ï¼šå¯¦éš›æˆåŠŸæ¨£æœ¬çš„é æ¸¬æˆåŠŸæ©Ÿç‡åˆ†ä½ˆ
  - ä¸»è¦é›†ä¸­åœ¨ 0.6-1.0 å€é–“ï¼ˆé«˜æ©Ÿç‡ï¼‰
  - å³°å€¼ç´„åœ¨ 0.7-0.8 ä¹‹é–“
  - èªªæ˜æ¨¡å‹èƒ½æœ‰æ•ˆè­˜åˆ¥æˆåŠŸæ¨£æœ¬ï¼Œçµ¦äºˆé«˜æˆåŠŸæ©Ÿç‡

- **é»‘è‰²è™›ç·š**ï¼šæ±ºç­–é–¾å€¼ = 0.5
  - æ©Ÿç‡ > 0.5 é æ¸¬ç‚ºæˆåŠŸï¼Œ< 0.5 é æ¸¬ç‚ºå¤±æ•—
  - å…©å€‹åˆ†ä½ˆåœ¨é–¾å€¼è™•æœ‰é©åº¦é‡ç–Š
  
- **é‡ç–Šå€åŸŸåˆ†æ**ï¼š
  - 0.4-0.6 å€é–“æœ‰éƒ¨åˆ†é‡ç–Š
  - é€™æ˜¯æ¨¡å‹ä¸ç¢ºå®šçš„æ¨£æœ¬ï¼ˆé‚Šç•Œæ¡ˆä¾‹ï¼‰
  - å¯¦å‹™ä¸­å¯é‡å°æ­¤å€é–“æ¨£æœ¬é€²è¡Œäººå·¥è¤‡æŸ¥

**æ©Ÿç‡æ ¡æº–å“è³ª**ï¼š
- å…©å€‹åˆ†ä½ˆåˆ†é›¢è‰¯å¥½ï¼Œé‡ç–Šä¸å¤š
- èªªæ˜æ¨¡å‹çš„æ©Ÿç‡è¼¸å‡ºå…·æœ‰å€åˆ†åº¦
- å¯æ ¹æ“šæ¥­å‹™éœ€æ±‚èª¿æ•´é–¾å€¼ï¼ˆå¦‚æé«˜åˆ° 0.6 å¢åŠ ç²¾ç¢ºç‡ï¼‰

#### åœ– 6ï¼šç‰¹å¾µé‡è¦æ€§ï¼ˆæ¬Šé‡ä¿‚æ•¸ï¼‰

![ç‰¹å¾µé‡è¦æ€§](outputs/P3_Unit12_Logistic_Regression/figs/feature_importance.png)

**åœ–è¡¨è§£è®€**ï¼š

- **ç¶ è‰²æŸ±**ï¼šæ­£å‘å½±éŸ¿ï¼ˆå¢åŠ æˆåŠŸæ©Ÿç‡ï¼‰
  - æœ¬æ¡ˆä¾‹ä¸­æ‰€æœ‰ç‰¹å¾µéƒ½æ˜¯ç¶ è‰²
  - è¡¨ç¤ºæé«˜ä»»ä½•ç‰¹å¾µå€¼éƒ½æœ‰åˆ©æ–¼åæ‡‰æˆåŠŸ
  
- **æŸ±é•·åº¦**ï¼šè¡¨ç¤ºå½±éŸ¿ç¨‹åº¦ï¼ˆæ¨™æº–åŒ–å¾Œçš„æ¬Šé‡ï¼‰

**ç‰¹å¾µé‡è¦æ€§æ’åº**ï¼š
1. **Temperature (1.20)**ï¼šæº«åº¦æ˜¯æœ€é‡è¦çš„å› ç´ 
   - æ¨™æº–åŒ–å¾Œæ¯å¢åŠ  1 å€‹æ¨™æº–å·®ï¼ˆç´„ 30Â°Cï¼‰
   - å°æ•¸å‹ç®—æ¯”å¢åŠ  1.20
   - æˆåŠŸæ©Ÿç‡é¡¯è‘—æå‡

2. **Pressure (1.17)**ï¼šå£“åŠ›æ¬¡ä¹‹
   - æ¯å¢åŠ  1 å€‹æ¨™æº–å·®ï¼ˆç´„ 7 barï¼‰
   - å°æ•¸å‹ç®—æ¯”å¢åŠ  1.17
   - å½±éŸ¿ç¨‹åº¦èˆ‡æº«åº¦æ¥è¿‘

3. **Catalyst_concentration (0.94)**ï¼šå‚¬åŒ–åŠ‘æ¿ƒåº¦é‡è¦
   - æ¯å¢åŠ  1 å€‹æ¨™æº–å·®ï¼ˆç´„ 0.027 mol/Lï¼‰
   - å°æ•¸å‹ç®—æ¯”å¢åŠ  0.94
   - é¡¯è‘—å½±éŸ¿åæ‡‰æˆåŠŸç‡

4. **Reactant_ratio (0.53)**ï¼šåæ‡‰ç‰©æ¯”ä¾‹å½±éŸ¿ä¸­ç­‰
   - æ¯å¢åŠ  1 å€‹æ¨™æº–å·®ï¼ˆç´„ 0.57ï¼‰
   - å°æ•¸å‹ç®—æ¯”å¢åŠ  0.53

5. **Reaction_time (0.48)**ï¼šåæ‡‰æ™‚é–“å½±éŸ¿ç›¸å°æœ€å°
   - æ¯å¢åŠ  1 å€‹æ¨™æº–å·®ï¼ˆç´„ 2.3 å°æ™‚ï¼‰
   - å°æ•¸å‹ç®—æ¯”å¢åŠ  0.48

**åŒ–å·¥æ„ç¾©è§£é‡‹**ï¼š
- **æ‰€æœ‰ä¿‚æ•¸ç‚ºæ­£**ï¼šç¬¦åˆåŒ–å­¸åæ‡‰è¦å¾‹
  - æé«˜æº«åº¦åŠ é€Ÿåæ‡‰å‹•åŠ›å­¸
  - å¢åŠ å£“åŠ›æœ‰åˆ©æ–¼é«”ç©ç¸®å°çš„åæ‡‰
  - å‚¬åŒ–åŠ‘æ¿ƒåº¦æé«˜å¢åŠ æ´»æ€§ä½é»
  
- **æº«åº¦å’Œå£“åŠ›æœ€é‡è¦**ï¼š
  - èˆ‡è¨±å¤šåŒ–å­¸åæ‡‰å°æº«å£“æ•æ„Ÿçš„ç‰¹æ€§ä¸€è‡´
  - é€™å…©å€‹åƒæ•¸é€šå¸¸æ˜¯åæ‡‰å·¥ç¨‹çš„é—œéµæ§åˆ¶è®Šæ•¸
  
- **æ™‚é–“å½±éŸ¿æœ€å°**ï¼š
  - åœ¨è¨­å®šç¯„åœå…§ï¼ˆ2-10å°æ™‚ï¼‰ï¼Œæ™‚é–“å»¶é•·çš„é‚Šéš›æ•ˆç›Šéæ¸›
  - å¯èƒ½å­˜åœ¨åæ‡‰å¹³è¡¡æˆ–å‰¯åæ‡‰ç«¶çˆ­

**å¯¦å‹™æ‡‰ç”¨å»ºè­°**ï¼š
- å„ªå…ˆå„ªåŒ–æº«åº¦å’Œå£“åŠ›æ¢ä»¶
- å‚¬åŒ–åŠ‘æ¿ƒåº¦éœ€åœ¨æˆæœ¬èˆ‡æ•ˆæœé–“å¹³è¡¡
- åæ‡‰æ™‚é–“å¯é©åº¦ç¸®çŸ­ä»¥æé«˜ç”Ÿç”¢æ•ˆç‡

---

### 6.9 æ¨¡å‹è§£é‡‹èˆ‡åˆ†æ

```python
print("="*60)
print("æ¨¡å‹è§£é‡‹èˆ‡åˆ†æ")
print("="*60)

# 1. ä¿‚æ•¸è§£é‡‹
print("\n1. ç‰¹å¾µå½±éŸ¿åˆ†æï¼ˆæ¨™æº–åŒ–å¾Œçš„ä¿‚æ•¸ï¼‰:")
print("-" * 60)
for feature, coef in zip(X.columns, model.coef_[0]):
    direction = "æ­£å‘å½±éŸ¿" if coef > 0 else "è² å‘å½±éŸ¿"
    print(f"{feature:30s}: {coef:8.4f} ({direction})")
    
print("\nä¿‚æ•¸è§£é‡‹:")
print("- æ­£ä¿‚æ•¸ï¼šç‰¹å¾µå€¼å¢åŠ æœƒæé«˜æˆåŠŸæ©Ÿç‡")
print("- è² ä¿‚æ•¸ï¼šç‰¹å¾µå€¼å¢åŠ æœƒé™ä½æˆåŠŸæ©Ÿç‡")
print("- ä¿‚æ•¸çµ•å°å€¼ï¼šè¡¨ç¤ºç‰¹å¾µçš„é‡è¦ç¨‹åº¦")

# 2. é æ¸¬ç¤ºä¾‹ï¼šä¸åŒæ¢ä»¶ä¸‹çš„æˆåŠŸæ©Ÿç‡
print("\n2. é æ¸¬ç¤ºä¾‹:")
print("-" * 60)

# å‰µå»ºå¹¾å€‹æ¸¬è©¦æ¡ˆä¾‹
test_cases = pd.DataFrame({
    'temperature': [180, 200, 220],
    'pressure': [10, 15, 20],
    'catalyst_concentration': [0.03, 0.055, 0.08],
    'reactant_ratio': [1.5, 2.0, 2.5],
    'reaction_time': [4, 6, 8]
})

test_cases_scaled = scaler.transform(test_cases)
predictions = model.predict(test_cases_scaled)
probabilities = model.predict_proba(test_cases_scaled)[:, 1]

print("\næ¸¬è©¦æ¡ˆä¾‹é æ¸¬çµæœ:")
for i in range(len(test_cases)):
    print(f"\næ¡ˆä¾‹ {i+1}:")
    print(f"  æ¢ä»¶: T={test_cases.iloc[i, 0]:.1f}Â°C, "
          f"P={test_cases.iloc[i, 1]:.1f}bar, "
          f"C={test_cases.iloc[i, 2]:.3f}mol/L")
    print(f"  é æ¸¬: {'æˆåŠŸ' if predictions[i]==1 else 'å¤±æ•—'} "
          f"(æˆåŠŸæ©Ÿç‡: {probabilities[i]:.2%})")

# 3. æ±ºç­–é‚Šç•Œåˆ†æ
print("\n3. æ±ºç­–é‚Šç•Œåˆ†æ:")
print("-" * 60)
print(f"æ±ºç­–é–¾å€¼: 0.5")
print(f"ç•¶ P(success) > 0.5 æ™‚ï¼Œé æ¸¬ç‚ºæˆåŠŸ")
print(f"ç•¶ P(success) â‰¤ 0.5 æ™‚ï¼Œé æ¸¬ç‚ºå¤±æ•—")
```

### 6.10 è¶…åƒæ•¸èª¿æ•´

```python
from sklearn.model_selection import GridSearchCV

print("\n" + "="*60)
print("è¶…åƒæ•¸èª¿æ•´ (Grid Search)")
print("="*60)

# å®šç¾©åƒæ•¸ç¶²æ ¼
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear']  # liblinear æ”¯æ´ l1 å’Œ l2
}

# å‰µå»º Grid Search
grid_search = GridSearchCV(
    LogisticRegression(max_iter=1000, random_state=42),
    param_grid,
    cv=5,                    # 5-fold äº¤å‰é©—è­‰
    scoring='roc_auc',       # ä½¿ç”¨ AUC ä½œç‚ºè©•ä¼°æŒ‡æ¨™
    n_jobs=-1,               # ä½¿ç”¨æ‰€æœ‰ CPU
    verbose=1
)

# åŸ·è¡Œ Grid Search
grid_search.fit(X_train_scaled, y_train)

# æœ€ä½³åƒæ•¸
print(f"\næœ€ä½³åƒæ•¸: {grid_search.best_params_}")
print(f"æœ€ä½³äº¤å‰é©—è­‰ AUC: {grid_search.best_score_:.4f}")

# ä½¿ç”¨æœ€ä½³æ¨¡å‹è©•ä¼°
best_model = grid_search.best_estimator_
y_test_pred_best = best_model.predict(X_test_scaled)
y_test_proba_best = best_model.predict_proba(X_test_scaled)[:, 1]

test_accuracy_best = accuracy_score(y_test, y_test_pred_best)
test_auc_best = roc_auc_score(y_test, y_test_proba_best)

print(f"\næœ€ä½³æ¨¡å‹åœ¨æ¸¬è©¦é›†ä¸Šçš„è¡¨ç¾:")
print(f"æº–ç¢ºç‡: {test_accuracy_best:.4f}")
print(f"AUC: {test_auc_best:.4f}")

# æ¯”è¼ƒä¸åŒ C å€¼çš„å½±éŸ¿
print("\n" + "-"*60)
print("ä¸åŒ C å€¼å°æ¨¡å‹çš„å½±éŸ¿:")
print("-"*60)

cv_results = pd.DataFrame(grid_search.cv_results_)
c_comparison = cv_results.groupby('param_C')['mean_test_score'].max()
print(c_comparison)

# ç¹ªè£½ C å€¼å° AUC çš„å½±éŸ¿
plt.figure(figsize=(10, 6))
for penalty in ['l1', 'l2']:
    mask = cv_results['param_penalty'] == penalty
    plt.plot(cv_results[mask]['param_C'], 
             cv_results[mask]['mean_test_score'],
             marker='o', label=f'Penalty: {penalty}')

plt.xscale('log')
plt.xlabel('C (Regularization Strength)')
plt.ylabel('Mean Cross-Validation AUC')
plt.title('Hyperparameter Tuning: C vs AUC')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig('logistic_regression_hyperparameter_tuning.png', 
            dpi=300, bbox_inches='tight')
plt.show()
```

**åŸ·è¡Œçµæœ**ï¼š

```
Fitting 5 folds for each of 12 candidates, totalling 60 fits
============================================================
è¶…åƒæ•¸èª¿æ•´çµæœ
============================================================
æœ€ä½³åƒæ•¸: {'C': 0.001, 'penalty': 'l2', 'solver': 'liblinear'}
æœ€ä½³äº¤å‰é©—è­‰ AUC: 0.8755

æœ€ä½³æ¨¡å‹åœ¨æ¸¬è©¦é›†ä¸Šçš„è¡¨ç¾:
æº–ç¢ºç‡: 0.8200
AUC: 0.8924
```

**è¶…åƒæ•¸èª¿æ•´åˆ†æ**ï¼š

1. **æœ€ä½³åƒæ•¸çµ„åˆ**ï¼š
   - **C = 0.001**ï¼šéå¸¸å¼·çš„æ­£å‰‡åŒ–
   - **penalty = 'l2'**ï¼šL2 æ­£å‰‡åŒ–ï¼ˆRidgeï¼‰
   - **solver = 'liblinear'**ï¼šé©åˆå°å‹è³‡æ–™é›†çš„å„ªåŒ–å™¨

2. **æ­£å‰‡åŒ–å¼·åº¦çš„å½±éŸ¿**ï¼š
   - C = 0.001 æ˜¯æ¸¬è©¦ç¯„åœä¸­æœ€å°çš„å€¼ï¼Œèªªæ˜æ¨¡å‹éœ€è¦è¼ƒå¼·çš„æ­£å‰‡åŒ–
   - å¼·æ­£å‰‡åŒ–å¯ä»¥é˜²æ­¢éæ“¬åˆï¼Œæé«˜æ³›åŒ–èƒ½åŠ›
   - é€™èˆ‡æˆ‘å€‘çš„è³‡æ–™é‡ï¼ˆ500æ¨£æœ¬ï¼Œ5ç‰¹å¾µï¼‰ç›¸ç¬¦

3. **æ€§èƒ½æå‡**ï¼š
   - æœ€ä½³æ¨¡å‹ AUCï¼š0.8924ï¼ˆç›¸æ¯”åŸå§‹æ¨¡å‹çš„ 0.8876ï¼Œæå‡ç´„ 0.5%ï¼‰
   - æº–ç¢ºç‡ç¶­æŒåœ¨ 82%ï¼ˆèˆ‡åŸå§‹æ¨¡å‹ç›¸åŒï¼‰
   - æå‡å¹…åº¦è¼ƒå°ï¼Œèªªæ˜åŸå§‹åƒæ•¸å·²ç¶“ç›¸ç•¶ä¸éŒ¯

4. **L1 vs L2 æ­£å‰‡åŒ–**ï¼š
   - æœ€ä½³æ¨¡å‹é¸æ“‡ L2 è€Œé L1
   - L2 æ­£å‰‡åŒ–æœƒç¸®å°æ¬Šé‡ä½†ä¸æœƒå®Œå…¨æ­¸é›¶ï¼Œä¿ç•™æ‰€æœ‰ç‰¹å¾µ
   - é©åˆæœ¬æ¡ˆä¾‹ï¼Œå› ç‚ºæ‰€æœ‰ç‰¹å¾µéƒ½å°é æ¸¬æœ‰è²¢ç»

5. **ä¸åŒ C å€¼çš„è¡¨ç¾**ï¼ˆå¾åœ–è¡¨å¯è¦‹ï¼‰ï¼š
   - C å¤ªå°ï¼ˆéå¼·æ­£å‰‡åŒ–ï¼‰ï¼šå¯èƒ½å°è‡´æ¬ æ“¬åˆ
   - C å¤ªå¤§ï¼ˆéå¼±æ­£å‰‡åŒ–ï¼‰ï¼šå¯èƒ½å°è‡´éæ“¬åˆ
   - C = 0.001 åœ¨æœ¬æ¡ˆä¾‹ä¸­é”åˆ°æœ€ä½³å¹³è¡¡

**å¯¦å‹™å»ºè­°**ï¼š
- å°æ–¼å°å‹è³‡æ–™é›†ï¼Œå»ºè­°ä½¿ç”¨è¼ƒå¼·çš„æ­£å‰‡åŒ–ï¼ˆå° C å€¼ï¼‰
- å¦‚æœéœ€è¦ç‰¹å¾µé¸æ“‡ï¼Œå¯å˜—è©¦ L1 æ­£å‰‡åŒ–ï¼ˆpenalty='l1'ï¼‰
- å¤§å‹è³‡æ–™é›†å¯è€ƒæ…®ä½¿ç”¨ 'sag' æˆ– 'saga' solver æé«˜è¨ˆç®—æ•ˆç‡

---

## 7. é‚è¼¯è¿´æ­¸çš„å„ªå‹¢èˆ‡é™åˆ¶

### 7.1 å„ªå‹¢

1. **å¯è§£é‡‹æ€§å¼·**
   - ä¿‚æ•¸ç›´æ¥åæ˜ ç‰¹å¾µå°çµæœçš„å½±éŸ¿
   - é©åˆéœ€è¦è§£é‡‹æ¨¡å‹æ±ºç­–çš„å ´æ™¯

2. **è¨ˆç®—æ•ˆç‡é«˜**
   - è¨“ç·´å’Œé æ¸¬é€Ÿåº¦å¿«
   - é©åˆå¤§è¦æ¨¡è³‡æ–™é›†

3. **æ©Ÿç‡è¼¸å‡º**
   - æä¾›é æ¸¬çš„ç½®ä¿¡åº¦
   - å¯æ ¹æ“šæ¥­å‹™éœ€æ±‚èª¿æ•´æ±ºç­–é–¾å€¼

4. **æ­£å‰‡åŒ–æ”¯æ´**
   - L1 æ­£å‰‡åŒ–å¯é€²è¡Œç‰¹å¾µé¸æ“‡
   - L2 æ­£å‰‡åŒ–é˜²æ­¢éæ“¬åˆ

5. **å¤šå…ƒåˆ†é¡æ“´å±•**
   - å¯é€é One-vs-Rest æˆ– Multinomial è™•ç†å¤šåˆ†é¡å•é¡Œ

6. **ç†è«–åŸºç¤å®Œå–„**
   - åŸºæ–¼æœ€å¤§ä¼¼ç„¶ä¼°è¨ˆ
   - çµ±è¨ˆæ¨è«–å®Œæ•´

### 7.2 é™åˆ¶

1. **ç·šæ€§æ±ºç­–é‚Šç•Œ**
   - åªèƒ½è™•ç†ç·šæ€§å¯åˆ†æˆ–è¿‘ä¼¼ç·šæ€§å¯åˆ†çš„å•é¡Œ
   - å°æ–¼è¤‡é›œéç·šæ€§é—œä¿‚æ•ˆæœæœ‰é™
   - è§£æ±ºæ–¹æ¡ˆï¼šç‰¹å¾µå·¥ç¨‹ï¼ˆå¤šé …å¼ç‰¹å¾µã€äº¤äº’é …ï¼‰

2. **å°ç•°å¸¸å€¼æ•æ„Ÿ**
   - æ¥µç«¯å€¼å¯èƒ½å½±éŸ¿æ¨¡å‹æ¬Šé‡
   - è§£æ±ºæ–¹æ¡ˆï¼šè³‡æ–™æ¸…æ´—ã€ç‰¹å¾µæ¨™æº–åŒ–

3. **ç‰¹å¾µç¨ç«‹æ€§å‡è¨­**
   - åš´é‡çš„å¤šé‡å…±ç·šæ€§æœƒå½±éŸ¿ä¿‚æ•¸è§£é‡‹
   - è§£æ±ºæ–¹æ¡ˆï¼šä½¿ç”¨æ­£å‰‡åŒ–ã€ç§»é™¤é«˜åº¦ç›¸é—œç‰¹å¾µ

4. **é¡åˆ¥ä¸å¹³è¡¡å•é¡Œ**
   - åå‘å¤šæ•¸é¡
   - è§£æ±ºæ–¹æ¡ˆï¼šclass_weight='balanced'ã€é‡æ¡æ¨£

5. **éœ€è¦å……è¶³æ¨£æœ¬**
   - ç‰¹å¾µæ•¸é‡å¤šæ™‚éœ€è¦è¶³å¤ çš„è¨“ç·´æ¨£æœ¬
   - ç¶“é©—æ³•å‰‡ï¼šæ¨£æœ¬æ•¸ > 10 Ã— ç‰¹å¾µæ•¸

### 7.3 é©ç”¨å ´æ™¯

**å»ºè­°ä½¿ç”¨é‚è¼¯è¿´æ­¸çš„æƒ…æ³**ï¼š
- éœ€è¦æ¨¡å‹å¯è§£é‡‹æ€§
- è³‡æ–™å‘ˆç¾ç·šæ€§å¯åˆ†è¶¨å‹¢
- éœ€è¦æ©Ÿç‡è¼¸å‡º
- è¨“ç·´è³‡æ–™æœ‰é™ä½†è³ªé‡é«˜
- éœ€è¦å¿«é€Ÿéƒ¨ç½²å’Œé æ¸¬

**è€ƒæ…®å…¶ä»–æ¨¡å‹çš„æƒ…æ³**ï¼š
- è³‡æ–™æœ‰è¤‡é›œçš„éç·šæ€§é—œä¿‚
- ç‰¹å¾µé–“å­˜åœ¨è¤‡é›œäº¤äº’ä½œç”¨
- å°é æ¸¬æº–ç¢ºåº¦è¦æ±‚æ¥µé«˜
- å¯æ¥å—è¼ƒä½çš„å¯è§£é‡‹æ€§

---

## 8. å¯¦å‹™å»ºè­°

### 8.1 ç‰¹å¾µå·¥ç¨‹æŠ€å·§

```python
# 1. å¤šé …å¼ç‰¹å¾µ
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X)

# 2. äº¤äº’é …
X['temp_pressure'] = X['temperature'] * X['pressure']
X['catalyst_ratio'] = X['catalyst_concentration'] * X['reactant_ratio']

# 3. å°æ•¸è½‰æ›ï¼ˆè™•ç†åæ…‹åˆ†ä½ˆï¼‰
X['log_catalyst'] = np.log(X['catalyst_concentration'] + 1e-8)

# 4. äºŒå€¼åŒ–ç‰¹å¾µ
X['high_temp'] = (X['temperature'] > 200).astype(int)
```

### 8.2 æ¨¡å‹è¨ºæ–·

```python
# 1. æª¢æŸ¥æ”¶æ–‚æ€§
if model.n_iter_[0] == model.max_iter:
    print("è­¦å‘Šï¼šæ¨¡å‹æœªæ”¶æ–‚ï¼Œå»ºè­°å¢åŠ  max_iter")

# 2. æª¢æŸ¥ç‰¹å¾µé‡è¦æ€§
importances = np.abs(model.coef_[0])
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': importances
}).sort_values('importance', ascending=False)
print(feature_importance)

# 3. æª¢æŸ¥é æ¸¬æ©Ÿç‡åˆ†ä½ˆ
# è‰¯å¥½çš„æ¨¡å‹æ‡‰è©²æœ‰è¼ƒåˆ†æ•£çš„æ©Ÿç‡åˆ†ä½ˆ
plt.hist(y_proba, bins=50)
plt.xlabel('Predicted Probability')
plt.ylabel('Frequency')
plt.show()
```

### 8.3 é–¾å€¼èª¿æ•´ç­–ç•¥

```python
# æ ¹æ“šæ¥­å‹™éœ€æ±‚èª¿æ•´æ±ºç­–é–¾å€¼
from sklearn.metrics import precision_recall_curve

# è¨ˆç®—ä¸åŒé–¾å€¼ä¸‹çš„ç²¾ç¢ºç‡å’Œå¬å›ç‡
precisions, recalls, thresholds = precision_recall_curve(y_test, y_test_proba)

# ç¹ªè£½ Precision-Recall æ›²ç·š
plt.figure(figsize=(10, 6))
plt.plot(thresholds, precisions[:-1], label='Precision')
plt.plot(thresholds, recalls[:-1], label='Recall')
plt.xlabel('Threshold')
plt.ylabel('Score')
plt.title('Precision and Recall vs Threshold')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

# æ ¹æ“šæˆæœ¬é¸æ“‡é–¾å€¼
# ä¾‹å¦‚ï¼šå‡é™½æ€§æˆæœ¬ = 100, å‡é™°æ€§æˆæœ¬ = 500
def find_optimal_threshold(y_true, y_proba, fp_cost, fn_cost):
    thresholds = np.linspace(0, 1, 100)
    costs = []
    
    for threshold in thresholds:
        y_pred = (y_proba >= threshold).astype(int)
        fp = np.sum((y_pred == 1) & (y_true == 0))
        fn = np.sum((y_pred == 0) & (y_true == 1))
        total_cost = fp * fp_cost + fn * fn_cost
        costs.append(total_cost)
    
    optimal_idx = np.argmin(costs)
    return thresholds[optimal_idx]

optimal_threshold = find_optimal_threshold(y_test, y_test_proba, 
                                           fp_cost=100, fn_cost=500)
print(f"æœ€ä½³é–¾å€¼: {optimal_threshold:.3f}")
```

---

## 9. ç¸½çµ

æœ¬ç¯€èª²æˆ‘å€‘æ·±å…¥å­¸ç¿’äº†**é‚è¼¯è¿´æ­¸ (Logistic Regression)**ï¼š

### æ ¸å¿ƒæ¦‚å¿µå›é¡§

1. **æ¨¡å‹æœ¬è³ª**ï¼š
   - é›–ç„¶åç‚ºã€Œè¿´æ­¸ã€ï¼Œå¯¦éš›æ˜¯åˆ†é¡æ¨¡å‹
   - é€é Sigmoid å‡½æ•¸å°‡ç·šæ€§è¼¸å‡ºè½‰æ›ç‚ºæ©Ÿç‡

2. **æ•¸å­¸åŸç†**ï¼š
   - Sigmoid å‡½æ•¸ï¼š$\sigma(z) = \frac{1}{1 + e^{-z}}$ 
   - å°æ•¸æå¤±å‡½æ•¸åŸºæ–¼æœ€å¤§ä¼¼ç„¶ä¼°è¨ˆ
   - æ¢¯åº¦è¨ˆç®—ç°¡æ½”é«˜æ•ˆ

3. **sklearn å¯¦ç¾**ï¼š
   - `LogisticRegression` é¡æä¾›å®Œæ•´åŠŸèƒ½
   - æ”¯æ´å¤šç¨®æ­£å‰‡åŒ–å’Œå„ªåŒ–æ¼”ç®—æ³•
   - è±å¯Œçš„åƒæ•¸å¯èª¿

4. **åŒ–å·¥æ‡‰ç”¨**ï¼š
   - ç”¢å“å“è³ªåˆ†é¡
   - åæ‡‰æˆåŠŸé æ¸¬
   - è¨­å‚™ç•°å¸¸æª¢æ¸¬
   - æ‰¹æ¬¡è³ªé‡é æ¸¬

5. **å¯¦å‹™æŠ€å·§**ï¼š
   - ç‰¹å¾µæ¨™æº–åŒ–å¾ˆé‡è¦
   - å¯é€éç‰¹å¾µå·¥ç¨‹å¢å¼·éç·šæ€§èƒ½åŠ›
   - è¶…åƒæ•¸èª¿æ•´å¯æå‡æ€§èƒ½
   - æ ¹æ“šæ¥­å‹™éœ€æ±‚èª¿æ•´æ±ºç­–é–¾å€¼

### å¯¦é©—æ¡ˆä¾‹ç¸½çµ

åœ¨åŒ–å­¸åæ‡‰æˆåŠŸé æ¸¬æ¡ˆä¾‹ä¸­ï¼Œæˆ‘å€‘ç²å¾—äº†ä»¥ä¸‹é—œéµç™¼ç¾ï¼š

**æ•¸æ“šç‰¹æ€§**ï¼š
- 500 å€‹æ¨£æœ¬ï¼Œ5 å€‹ç‰¹å¾µï¼Œé¡åˆ¥å¹³è¡¡ï¼ˆæˆåŠŸ 53%ï¼Œå¤±æ•— 47%ï¼‰
- ç‰¹å¾µé–“ç›¸é—œæ€§ä½ï¼Œæœ‰åˆ©æ–¼æ¨¡å‹è¨“ç·´

**æ¨¡å‹æ€§èƒ½**ï¼š
- âœ… æ¸¬è©¦é›†æº–ç¢ºç‡ï¼š**82%**
- âœ… æ¸¬è©¦é›† AUCï¼š**0.8876**ï¼ˆå„ªç§€æ°´æº–ï¼‰
- âœ… ç²¾ç¢ºç‡/å¬å›ç‡ï¼šå‡è¡¡è¡¨ç¾ï¼ˆ80-84%ï¼‰
- âœ… 6 æ¬¡è¿­ä»£å³æ”¶æ–‚ï¼Œè¨“ç·´é«˜æ•ˆ

**ç‰¹å¾µé‡è¦æ€§**ï¼ˆæ¨™æº–åŒ–å¾Œä¿‚æ•¸ï¼‰ï¼š
1. Temperature (1.20) - æœ€é—œéµå› ç´ 
2. Pressure (1.17) - æ¬¡è¦é—œéµå› ç´ 
3. Catalyst_concentration (0.94) - é‡è¦å› ç´ 
4. Reactant_ratio (0.53) - ä¸­ç­‰å½±éŸ¿
5. Reaction_time (0.48) - è¼ƒå°å½±éŸ¿

**è¶…åƒæ•¸å„ªåŒ–**ï¼š
- æœ€ä½³çµ„åˆï¼šC=0.001, penalty='l2'
- è¼ƒå¼·æ­£å‰‡åŒ–é˜²æ­¢éæ“¬åˆ
- AUC æå‡è‡³ 0.8924

**åŒ–å·¥æ„ç¾©**ï¼š
- æ¨¡å‹å¯æœ‰æ•ˆé æ¸¬åæ‡‰æˆåŠŸæ©Ÿç‡
- æä¾›å®šé‡çš„åƒæ•¸å½±éŸ¿åˆ†æ
- å¯ç”¨æ–¼å¯¦é©—æ¢ä»¶å„ªåŒ–å’Œæˆæœ¬ç¯€ç´„

### é—œéµè¦é»

âœ… **å„ªå‹¢**ï¼šå¯è§£é‡‹æ€§å¼·ã€è¨ˆç®—æ•ˆç‡é«˜ã€æ©Ÿç‡è¼¸å‡º  
âš ï¸ **é™åˆ¶**ï¼šç·šæ€§æ±ºç­–é‚Šç•Œã€å°ç•°å¸¸å€¼æ•æ„Ÿ  
ğŸ¯ **é©ç”¨**ï¼šç·šæ€§å¯åˆ†å•é¡Œã€éœ€è¦å¯è§£é‡‹æ€§ã€å¿«é€Ÿéƒ¨ç½²

### ä¸‹ä¸€æ­¥å­¸ç¿’

å®Œæˆé‚è¼¯è¿´æ­¸çš„å­¸ç¿’å¾Œï¼Œå»ºè­°ç¹¼çºŒå­¸ç¿’ï¼š

- **Unit12_Support_Vector_Classification**ï¼šè™•ç†éç·šæ€§åˆ†é¡å•é¡Œ
- **Unit12_Decision_Tree_Classifier**ï¼šè™•ç†è¤‡é›œæ±ºç­–è¦å‰‡
- **Unit12_Random_Forest_Classifier**ï¼šæå‡åˆ†é¡æº–ç¢ºåº¦

---

**èª²ç¨‹è³‡è¨Š**
- èª²ç¨‹åç¨±ï¼šAIåœ¨åŒ–å·¥ä¸Šä¹‹æ‡‰ç”¨
- èª²ç¨‹å–®å…ƒï¼šUnit12 Logistic Regression é‚è¼¯è¿´æ­¸
- èª²ç¨‹è£½ä½œï¼šé€¢ç”²å¤§å­¸ åŒ–å·¥ç³» æ™ºæ…§ç¨‹åºç³»çµ±å·¥ç¨‹å¯¦é©—å®¤
- æˆèª²æ•™å¸«ï¼šèŠæ›œç¦ åŠ©ç†æ•™æˆ
- æ›´æ–°æ—¥æœŸï¼š2026-01-28

**èª²ç¨‹æˆæ¬Š [CC BY-NC-SA 4.0]**
 - æœ¬æ•™æéµå¾ª [å‰µç”¨CC å§“åæ¨™ç¤º-éå•†æ¥­æ€§-ç›¸åŒæ–¹å¼åˆ†äº« 4.0 åœ‹éš› (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh) æˆæ¬Šã€‚

---

