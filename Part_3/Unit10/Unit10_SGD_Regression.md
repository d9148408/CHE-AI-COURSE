# Unit10 éš¨æ©Ÿæ¢¯åº¦ä¸‹é™å›æ­¸ | SGD Regression

> **æœ€å¾Œæ›´æ–°**ï¼š2026-01-16 | å»ºç«‹å®Œæ•´æ•™å­¸è¬›ç¾©

---

## å­¸ç¿’ç›®æ¨™

æœ¬ç¯€èª²å°‡æ·±å…¥å­¸ç¿’**éš¨æ©Ÿæ¢¯åº¦ä¸‹é™å›æ­¸ (SGD Regression)** æ¨¡å‹ï¼Œé€™æ˜¯ä¸€ç¨®åŸºæ–¼æ¢¯åº¦ä¸‹é™å„ªåŒ–çš„ç·šæ€§å›æ­¸æ–¹æ³•ï¼Œç‰¹åˆ¥é©ç”¨æ–¼**å¤§è¦æ¨¡æ•¸æ“š**å’Œ**åœ¨ç·šå­¸ç¿’**å ´æ™¯ã€‚é€šéæœ¬ç¯€èª²ï¼Œæ‚¨å°‡èƒ½å¤ ï¼š

- ç†è§£ SGD çš„æ•¸å­¸åŸç†èˆ‡å„ªåŒ–éç¨‹
- æŒæ¡ sklearn ä¸­ `SGDRegressor` çš„ä½¿ç”¨æ–¹æ³•
- å­¸ç¿’å¦‚ä½•é¸æ“‡å­¸ç¿’ç‡ã€æ­£å‰‡åŒ–åƒæ•¸å’Œæå¤±å‡½æ•¸
- ç†è§£ SGD èˆ‡æ‰¹é‡æ¢¯åº¦ä¸‹é™ã€å°æ‰¹é‡æ¢¯åº¦ä¸‹é™çš„å·®ç•°
- æ‡‰ç”¨ SGD å›æ­¸è§£æ±ºå¤§è¦æ¨¡æ•¸æ“šå»ºæ¨¡å•é¡Œ
- æŒæ¡åœ¨ç·šå­¸ç¿’ (Online Learning) çš„æ‡‰ç”¨å ´æ™¯
- è§£æ±ºåŒ–å·¥é ˜åŸŸçš„å¯¦éš›å»ºæ¨¡å•é¡Œï¼ˆ**å«å®Œæ•´å¯¦ä½œæ¡ˆä¾‹**ï¼‰

---

## 1. SGD å›æ­¸åŸºæœ¬æ¦‚å¿µ

### 1.1 ä»€éº¼æ˜¯ SGD å›æ­¸ï¼Ÿ

**éš¨æ©Ÿæ¢¯åº¦ä¸‹é™å›æ­¸ (Stochastic Gradient Descent Regression, SGD Regression)** æ˜¯ä¸€ç¨®åŸºæ–¼**æ¢¯åº¦ä¸‹é™å„ªåŒ–ç®—æ³•**çš„ç·šæ€§å›æ­¸æ–¹æ³•ã€‚èˆ‡å‚³çµ±çš„æ‰¹é‡æ¢¯åº¦ä¸‹é™ (Batch Gradient Descent) ä¸åŒï¼ŒSGD **æ¯æ¬¡åªä½¿ç”¨ä¸€å€‹æ¨£æœ¬**ä¾†æ›´æ–°æ¨¡å‹åƒæ•¸ï¼Œé€™ä½¿å¾—å®ƒåœ¨è™•ç†å¤§è¦æ¨¡æ•¸æ“šæ™‚å…·æœ‰é¡¯è‘—çš„è¨ˆç®—å„ªå‹¢ã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼š
- é€šéè¿­ä»£æ›´æ–°çš„æ–¹å¼ï¼Œé€æ­¥èª¿æ•´æ¨¡å‹åƒæ•¸
- æ¯æ¬¡è¿­ä»£åƒ…ä½¿ç”¨ä¸€å€‹æˆ–å°‘é‡æ¨£æœ¬è¨ˆç®—æ¢¯åº¦
- æ”¯æŒå¤šç¨®æå¤±å‡½æ•¸å’Œæ­£å‰‡åŒ–æ–¹æ³•
- é©åˆ**å¢é‡å­¸ç¿’**å’Œ**åœ¨ç·šå­¸ç¿’**å ´æ™¯

### 1.2 ç‚ºä»€éº¼éœ€è¦ SGD å›æ­¸ï¼Ÿ

SGD å›æ­¸è§£æ±ºäº†ä»¥ä¸‹é‡è¦å•é¡Œï¼š

1. **å¤§è¦æ¨¡æ•¸æ“šè™•ç†**ï¼š
   - ç•¶æ•¸æ“šé‡é”åˆ°ç™¾è¬ã€åƒè¬ç´šåˆ¥æ™‚ï¼Œæ‰¹é‡æ¢¯åº¦ä¸‹é™è¨ˆç®—æˆæœ¬éé«˜
   - SGD æ¯æ¬¡åªéœ€è™•ç†ä¸€å€‹æ¨£æœ¬ï¼Œè¨˜æ†¶é«”éœ€æ±‚ä½
   - å¯ä»¥**æµå¼è™•ç†æ•¸æ“š**ï¼Œä¸éœ€è¦å°‡å…¨éƒ¨æ•¸æ“šè¼‰å…¥è¨˜æ†¶é«”

2. **åœ¨ç·šå­¸ç¿’ (Online Learning)**ï¼š
   - æ”¯æŒ**å¢é‡æ›´æ–°**ï¼šæ–°æ•¸æ“šåˆ°é”æ™‚å¯ä»¥å³æ™‚æ›´æ–°æ¨¡å‹
   - é©ç”¨æ–¼**å‹•æ…‹ç’°å¢ƒ**ï¼šæ•¸æ“šåˆ†ä½ˆéš¨æ™‚é–“è®ŠåŒ–çš„å ´æ™¯
   - åŒ–å·¥æ‡‰ç”¨ï¼šå³æ™‚åæ‡‰å™¨ç›£æ§ã€é€£çºŒç”Ÿç”¢éç¨‹å„ªåŒ–

3. **éˆæ´»æ€§**ï¼š
   - æ”¯æŒå¤šç¨®æå¤±å‡½æ•¸ï¼ˆsquared_loss, huber, epsilon_insensitive ç­‰ï¼‰
   - æ”¯æŒå¤šç¨®æ­£å‰‡åŒ–ï¼ˆL1, L2, Elastic Netï¼‰
   - å¯ä»¥æ¨¡æ“¬ OLSã€Ridgeã€Lassoã€Elastic Net ç­‰å¤šç¨®ç·šæ€§æ¨¡å‹

4. **è¨ˆç®—æ•ˆç‡**ï¼š
   - æ”¶æ–‚é€Ÿåº¦å¿«ï¼ˆå°æ–¼å¤§æ•¸æ“šï¼‰
   - å¯ä»¥æå‰åœæ­¢è¨“ç·´ï¼ˆç•¶é©—è­‰èª¤å·®ä¸å†ä¸‹é™æ™‚ï¼‰
   - æ”¯æŒ warm_start åƒæ•¸å¯¦ç¾å¢é‡å­¸ç¿’

**SGD å›æ­¸çš„æ ¸å¿ƒå„ªå‹¢**ï¼š
- âœ“ **å¯æ“´å±•æ€§**ï¼šè™•ç†å¤§è¦æ¨¡æ•¸æ“šï¼ˆç™¾è¬ç´šä»¥ä¸Šæ¨£æœ¬ï¼‰
- âœ“ **è¨˜æ†¶é«”æ•ˆç‡**ï¼šä¸éœ€è¦å°‡å…¨éƒ¨æ•¸æ“šè¼‰å…¥è¨˜æ†¶é«”
- âœ“ **åœ¨ç·šå­¸ç¿’**ï¼šæ”¯æŒå¢é‡æ›´æ–°å’Œæµå¼æ•¸æ“šè™•ç†
- âœ“ **éˆæ´»æ€§**ï¼šå¤šç¨®æå¤±å‡½æ•¸å’Œæ­£å‰‡åŒ–çµ„åˆ
- âœ“ **æ”¶æ–‚å¿«é€Ÿ**ï¼šå°æ–¼å¤§æ•¸æ“šæ¯”æ‰¹é‡æ¢¯åº¦ä¸‹é™å¿«å¾—å¤š

### 1.3 æ¢¯åº¦ä¸‹é™æ–¹æ³•æ¯”è¼ƒ

åœ¨æ©Ÿå™¨å­¸ç¿’ä¸­ï¼Œæœ‰ä¸‰ç¨®ä¸»è¦çš„æ¢¯åº¦ä¸‹é™æ–¹æ³•ï¼š

| æ–¹æ³• | æ¯æ¬¡è¿­ä»£ä½¿ç”¨æ¨£æœ¬æ•¸ | æ›´æ–°é »ç‡ | è¨ˆç®—æˆæœ¬ | æ”¶æ–‚ç©©å®šæ€§ | é©ç”¨å ´æ™¯ |
|------|-------------------|---------|---------|-----------|---------|
| **æ‰¹é‡æ¢¯åº¦ä¸‹é™ (BGD)** | å…¨éƒ¨æ¨£æœ¬ $n$ | ä½ | é«˜ $O(n)$ | ç©©å®š | å°æ•¸æ“šé›† |
| **éš¨æ©Ÿæ¢¯åº¦ä¸‹é™ (SGD)** | 1å€‹æ¨£æœ¬ | é«˜ | ä½ $O(1)$ | æ³¢å‹•å¤§ | **å¤§æ•¸æ“šã€åœ¨ç·šå­¸ç¿’** |
| **å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ (MBGD)** | å°æ‰¹é‡ $m$ (å¦‚32, 64) | ä¸­ | ä¸­ $O(m)$ | è¼ƒç©©å®š | æ·±åº¦å­¸ç¿’ã€GPUåŠ é€Ÿ |

**æ›´æ–°å…¬å¼å°æ¯”**ï¼š

1. **æ‰¹é‡æ¢¯åº¦ä¸‹é™ (BGD)**ï¼š
   $$
   \boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - \eta \cdot \frac{1}{n} \sum_{i=1}^{n} \nabla L(\boldsymbol{\beta}^{(t)}, \mathbf{x}_i, y_i)
   $$

2. **éš¨æ©Ÿæ¢¯åº¦ä¸‹é™ (SGD)**ï¼š
   $$
   \boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - \eta \cdot \nabla L(\boldsymbol{\beta}^{(t)}, \mathbf{x}_i, y_i)
   $$
   å…¶ä¸­ $\mathbf{x}_i, y_i$ ç‚º**éš¨æ©ŸæŠ½å–çš„å–®å€‹æ¨£æœ¬**

3. **å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ (MBGD)**ï¼š
   $$
   \boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - \eta \cdot \frac{1}{m} \sum_{i \in \text{batch}} \nabla L(\boldsymbol{\beta}^{(t)}, \mathbf{x}_i, y_i)
   $$

### 1.4 SGD çš„æ”¶æ–‚è¡Œç‚º

SGD çš„å„ªåŒ–éç¨‹å…·æœ‰ä»¥ä¸‹ç‰¹é»ï¼š

1. **å¿«é€Ÿä¸‹é™éšæ®µ**ï¼š
   - åˆæœŸï¼Œæå¤±å‡½æ•¸å¿«é€Ÿä¸‹é™
   - æ¯å€‹æ¨£æœ¬æä¾›çš„æ¢¯åº¦ä¿¡æ¯é›–ç„¶æœ‰å™ªè²ï¼Œä½†å¹³å‡æ–¹å‘æ­£ç¢º

2. **æ³¢å‹•éšæ®µ**ï¼š
   - å¾ŒæœŸï¼Œæå¤±å‡½æ•¸åœ¨æœ€å„ªè§£é™„è¿‘**éœ‡ç›ª**
   - ç„¡æ³•åƒ BGD é‚£æ¨£ç²¾ç¢ºæ”¶æ–‚åˆ°æœ€å„ªè§£
   - é€šé**å­¸ç¿’ç‡è¡°æ¸›**å¯ä»¥æ¸›å°‘éœ‡ç›ª

3. **éé»é€ƒé€¸**ï¼š
   - SGD çš„éš¨æ©Ÿæ€§æœ‰åŠ©æ–¼**é€ƒé›¢éé»**å’Œ**å±€éƒ¨æœ€å„ªè§£**
   - é€™åœ¨éå‡¸å„ªåŒ–ä¸­æ˜¯ä¸€å€‹å„ªå‹¢

**æ”¶æ–‚æ›²ç·šç¤ºæ„**ï¼š
```
Loss
  |
  |     BGD (å¹³æ»‘ä¸‹é™)
  |    /
  |   /
  |  /   SGD (å¿«é€Ÿä½†éœ‡ç›ª)
  | /~~~~\~/~\~/~\_
  |/__________________ Iterations
```

---

## 2. SGD å›æ­¸çš„æ•¸å­¸åŸç†

### 2.1 ç›®æ¨™å‡½æ•¸

SGD å›æ­¸çš„ç›®æ¨™æ˜¯æœ€å°åŒ–ä»¥ä¸‹æå¤±å‡½æ•¸ï¼ˆä»¥å¹³æ–¹æå¤±ç‚ºä¾‹ï¼‰ï¼š

$$
\min_{\boldsymbol{\beta}} \quad J(\boldsymbol{\beta}) = \frac{1}{n} \sum_{i=1}^{n} L(y_i, \mathbf{x}_i^T \boldsymbol{\beta}) + R(\boldsymbol{\beta})
$$

å…¶ä¸­ï¼š
- $L(y_i, \mathbf{x}_i^T \boldsymbol{\beta})$ : **æå¤±å‡½æ•¸** (Loss Function)
- $R(\boldsymbol{\beta})$ : **æ­£å‰‡åŒ–é …** (Regularization Term)
- $n$ : æ¨£æœ¬ç¸½æ•¸

### 2.2 å¸¸è¦‹æå¤±å‡½æ•¸

sklearn çš„ `SGDRegressor` æ”¯æŒå¤šç¨®æå¤±å‡½æ•¸ï¼š

1. **å¹³æ–¹æå¤± (Squared Loss)** - `loss='squared_loss'`ï¼ˆé»˜èªï¼‰ï¼š
   $$
   L(y, \hat{y}) = \frac{1}{2}(y - \hat{y})^2
   $$
   - å°æ‡‰**æ™®é€šæœ€å°å¹³æ–¹æ³• (OLS)**
   - å°ç•°å¸¸å€¼æ•æ„Ÿ

2. **Huber æå¤± (Huber Loss)** - `loss='huber'`ï¼š
   $$
   L(y, \hat{y}) = \begin{cases}
   \frac{1}{2}(y - \hat{y})^2 & \text{if } |y - \hat{y}| \leq \epsilon \\
   \epsilon |y - \hat{y}| - \frac{1}{2}\epsilon^2 & \text{if } |y - \hat{y}| > \epsilon
   \end{cases}
   $$
   - çµåˆå¹³æ–¹æå¤±å’Œçµ•å°æå¤±
   - **å°ç•°å¸¸å€¼é­¯æ£’**ï¼ˆç•¶èª¤å·®å¤§æ–¼ Îµ æ™‚æ¡ç”¨ç·šæ€§æ‡²ç½°ï¼‰

3. **Îµ-ä¸æ•æ„Ÿæå¤± (Epsilon-Insensitive Loss)** - `loss='epsilon_insensitive'`ï¼š
   $$
   L(y, \hat{y}) = \max(0, |y - \hat{y}| - \epsilon)
   $$
   - åœ¨ $|y - \hat{y}| < \epsilon$ ç¯„åœå…§ä¸æ‡²ç½°
   - å°æ‡‰**æ”¯æŒå‘é‡å›æ­¸ (SVR)**

4. **å¹³æ–¹ Îµ-ä¸æ•æ„Ÿæå¤± (Squared Epsilon-Insensitive Loss)** - `loss='squared_epsilon_insensitive'`ï¼š
   $$
   L(y, \hat{y}) = \max(0, |y - \hat{y}| - \epsilon)^2
   $$

### 2.3 æ­£å‰‡åŒ–é …

SGD å›æ­¸æ”¯æŒä»¥ä¸‹æ­£å‰‡åŒ–æ–¹æ³•ï¼š

1. **ç„¡æ­£å‰‡åŒ–** - `penalty='none'`ï¼š
   $$
   R(\boldsymbol{\beta}) = 0
   $$

2. **L2 æ­£å‰‡åŒ– (Ridge)** - `penalty='l2'`ï¼ˆé»˜èªï¼‰ï¼š
   $$
   R(\boldsymbol{\beta}) = \frac{\alpha}{2} \| \boldsymbol{\beta} \|_2^2 = \frac{\alpha}{2} \sum_{j=1}^{p} \beta_j^2
   $$

3. **L1 æ­£å‰‡åŒ– (Lasso)** - `penalty='l1'`ï¼š
   $$
   R(\boldsymbol{\beta}) = \alpha \| \boldsymbol{\beta} \|_1 = \alpha \sum_{j=1}^{p} |\beta_j|
   $$

4. **Elastic Net æ­£å‰‡åŒ–** - `penalty='elasticnet'`ï¼š
   $$
   R(\boldsymbol{\beta}) = \alpha \rho \| \boldsymbol{\beta} \|_1 + \frac{\alpha (1-\rho)}{2} \| \boldsymbol{\beta} \|_2^2
   $$
   å…¶ä¸­ $\rho$ ç”± `l1_ratio` åƒæ•¸æ§åˆ¶ï¼ˆå°æ‡‰ Elastic Net ä¸­çš„ l1_ratioï¼‰

### 2.4 æ¢¯åº¦è¨ˆç®—

å°æ–¼å¹³æ–¹æå¤±å’Œ L2 æ­£å‰‡åŒ–ï¼Œæå¤±å‡½æ•¸ç‚ºï¼š

$$
J(\boldsymbol{\beta}) = \frac{1}{2}(y_i - \mathbf{x}_i^T \boldsymbol{\beta})^2 + \frac{\alpha}{2} \| \boldsymbol{\beta} \|_2^2
$$

æ¢¯åº¦è¨ˆç®—ï¼š

$$
\nabla_{\boldsymbol{\beta}} J(\boldsymbol{\beta}) = -(\mathbf{y}_i - \mathbf{x}_i^T \boldsymbol{\beta}) \mathbf{x}_i + \alpha \boldsymbol{\beta}
$$

### 2.5 åƒæ•¸æ›´æ–°è¦å‰‡

SGD çš„åƒæ•¸æ›´æ–°å…¬å¼ç‚ºï¼š

$$
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - \eta^{(t)} \nabla_{\boldsymbol{\beta}} J(\boldsymbol{\beta}^{(t)})
$$

å°æ–¼å¹³æ–¹æå¤± + L2 æ­£å‰‡åŒ–ï¼š

$$
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - \eta^{(t)} \left[ -(y_i - \mathbf{x}_i^T \boldsymbol{\beta}^{(t)}) \mathbf{x}_i + \alpha \boldsymbol{\beta}^{(t)} \right]
$$

ç°¡åŒ–ç‚ºï¼š

$$
\boldsymbol{\beta}^{(t+1)} = (1 - \eta^{(t)} \alpha) \boldsymbol{\beta}^{(t)} + \eta^{(t)} (y_i - \mathbf{x}_i^T \boldsymbol{\beta}^{(t)}) \mathbf{x}_i
$$

å…¶ä¸­ï¼š
- $\eta^{(t)}$ : ç¬¬ $t$ æ¬¡è¿­ä»£çš„**å­¸ç¿’ç‡** (Learning Rate)
- $(1 - \eta^{(t)} \alpha)$ : æ­£å‰‡åŒ–å¸¶ä¾†çš„**æ¬Šé‡è¡°æ¸›** (Weight Decay)

---

## 3. SGD å›æ­¸çš„é—œéµåƒæ•¸

### 3.1 å­¸ç¿’ç‡ (Learning Rate)

å­¸ç¿’ç‡ $\eta$ æ˜¯ SGD æœ€é‡è¦çš„è¶…åƒæ•¸ï¼Œæ§åˆ¶æ¯æ¬¡æ›´æ–°çš„æ­¥é•·ã€‚

**å­¸ç¿’ç‡èª¿åº¦ç­–ç•¥** (`learning_rate` åƒæ•¸)ï¼š

1. **å¸¸æ•¸å­¸ç¿’ç‡ (Constant)** - `learning_rate='constant'`ï¼š
   $$
   \eta^{(t)} = \eta_0
   $$
   - å›ºå®šå­¸ç¿’ç‡
   - é€šé `eta0` åƒæ•¸è¨­ç½®ï¼ˆé»˜èª 0.01ï¼‰
   - å¯èƒ½åœ¨æœ€å„ªè§£é™„è¿‘éœ‡ç›ª

2. **æœ€å„ªå­¸ç¿’ç‡ (Optimal)** - `learning_rate='optimal'`ï¼ˆ**é»˜èª**ï¼‰ï¼š
   $$
   \eta^{(t)} = \frac{1}{\alpha (t_0 + t)}
   $$
   - è‡ªå‹•æ ¹æ“šæ­£å‰‡åŒ–åƒæ•¸ Î± èª¿æ•´
   - é€šå¸¸æ˜¯æœ€ç©©å¥çš„é¸æ“‡

3. **å€’æ•¸è¡°æ¸› (Invscaling)** - `learning_rate='invscaling'`ï¼š
   $$
   \eta^{(t)} = \frac{\eta_0}{t^{\text{power\_t}}}
   $$
   - `power_t` æ§åˆ¶è¡°æ¸›é€Ÿåº¦ï¼ˆé»˜èª 0.25ï¼‰
   - éœ€è¦ä»”ç´°èª¿æ•´ `eta0` å’Œ `power_t`

4. **è‡ªé©æ‡‰å­¸ç¿’ç‡ (Adaptive)** - `learning_rate='adaptive'`ï¼š
   - ç•¶è¨“ç·´æå¤±åœ¨ `n_iter_no_change` å€‹ epoch å…§æ²’æœ‰æ”¹å–„æ™‚ï¼Œå­¸ç¿’ç‡æ¸›åŠ
   - è‡ªå‹•èª¿æ•´ï¼Œé©åˆä¸ç¢ºå®šå­¸ç¿’ç‡çš„æƒ…æ³

**åŒ–å·¥æ‡‰ç”¨å»ºè­°**ï¼š
- å°æ•¸æ“šé›† (< 10,000)ï¼šä½¿ç”¨ `learning_rate='optimal'` æˆ– `'adaptive'`
- å¤§æ•¸æ“šé›† (> 100,000)ï¼šä½¿ç”¨ `learning_rate='constant'` ä¸¦æ‰‹å‹•èª¿æ•´ `eta0`
- åœ¨ç·šå­¸ç¿’ï¼šä½¿ç”¨ `learning_rate='optimal'` ä¿æŒç©©å®š

### 3.2 æ­£å‰‡åŒ–åƒæ•¸ (Alpha)

`alpha` åƒæ•¸æ§åˆ¶æ­£å‰‡åŒ–å¼·åº¦ï¼Œç¯„åœç‚º $(0, +\infty)$ã€‚

$$
\text{Total Loss} = \text{Data Loss} + \alpha \cdot \text{Regularization Term}
$$

**åƒæ•¸é¸æ“‡æŒ‡å—**ï¼š
- `alpha=0.0001`ï¼ˆé»˜èªï¼‰ï¼šé©åˆå¤§å¤šæ•¸æƒ…æ³
- `alpha` è¼ƒå¤§ï¼šæ›´å¼·çš„æ­£å‰‡åŒ–ï¼Œæ¨¡å‹æ›´ç°¡å–®ï¼Œå¯èƒ½æ¬ æ“¬åˆ
- `alpha` è¼ƒå°ï¼šæ›´å¼±çš„æ­£å‰‡åŒ–ï¼Œæ¨¡å‹æ›´è¤‡é›œï¼Œå¯èƒ½éæ“¬åˆ
- **ä½¿ç”¨ GridSearchCV æˆ– RandomizedSearchCV å°‹æ‰¾æœ€ä½³å€¼**

### 3.3 è¿­ä»£æ¬¡æ•¸ (Max Iter)

`max_iter` åƒæ•¸æ§åˆ¶æœ€å¤§è¿­ä»£æ¬¡æ•¸ï¼ˆepoch æ•¸é‡ï¼‰ã€‚

- **1 å€‹ epoch**ï¼šéæ­·å…¨éƒ¨è¨“ç·´æ¨£æœ¬ä¸€æ¬¡
- é»˜èªå€¼ï¼š`max_iter=1000`
- å¤§æ•¸æ“šé›†ï¼šå¯èƒ½åªéœ€è¦ 10-50 å€‹ epoch
- å°æ•¸æ“šé›†ï¼šå¯èƒ½éœ€è¦ 1000+ å€‹ epoch

**æ—©åœæ³• (Early Stopping)**ï¼š
- è¨­ç½® `early_stopping=True`
- ç•¶é©—è­‰æå¤±åœ¨ `n_iter_no_change` å€‹ epoch å…§æ²’æœ‰æ”¹å–„æ™‚è‡ªå‹•åœæ­¢
- éœ€è¦è¨­ç½® `validation_fraction`ï¼ˆé»˜èª 0.1ï¼‰åŠƒåˆ†é©—è­‰é›†

### 3.4 å…¶ä»–é‡è¦åƒæ•¸

| åƒæ•¸ | èªªæ˜ | é»˜èªå€¼ | å»ºè­° |
|------|------|--------|------|
| `loss` | æå¤±å‡½æ•¸ | `'squared_loss'` | ç•°å¸¸å€¼å¤šç”¨ `'huber'` |
| `penalty` | æ­£å‰‡åŒ–é¡å‹ | `'l2'` | ç‰¹å¾µé¸æ“‡ç”¨ `'l1'` æˆ– `'elasticnet'` |
| `alpha` | æ­£å‰‡åŒ–å¼·åº¦ | `0.0001` | é€šéäº¤å‰é©—è­‰èª¿æ•´ |
| `l1_ratio` | Elastic Net çš„ L1 æ¯”ä¾‹ | `0.15` | åƒ…ç•¶ `penalty='elasticnet'` æ™‚æœ‰æ•ˆ |
| `learning_rate` | å­¸ç¿’ç‡ç­–ç•¥ | `'optimal'` | å¤§æ•¸æ“šç”¨ `'constant'` |
| `eta0` | åˆå§‹å­¸ç¿’ç‡ | `0.01` | `learning_rate='constant'` æ™‚èª¿æ•´ |
| `max_iter` | æœ€å¤§è¿­ä»£æ¬¡æ•¸ | `1000` | å¤§æ•¸æ“šå¯æ¸›å°‘ï¼Œå°æ•¸æ“šå¯å¢åŠ  |
| `tol` | æ”¶æ–‚å®¹å·® | `1e-3` | æ§åˆ¶åœæ­¢æ¢ä»¶ |
| `shuffle` | æ˜¯å¦æ‰“äº‚æ•¸æ“š | `True` | **å»ºè­°ä¿æŒ True** |
| `random_state` | éš¨æ©Ÿç¨®å­ | `None` | è¨­ç½®ä»¥ç¢ºä¿å¯é‡ç¾æ€§ |
| `warm_start` | å¢é‡å­¸ç¿’ | `False` | åœ¨ç·šå­¸ç¿’æ™‚è¨­ç‚º `True` |
| `early_stopping` | æ—©åœæ³• | `False` | å¤§æ•¸æ“šé›†å»ºè­°é–‹å•Ÿ |
| `validation_fraction` | é©—è­‰é›†æ¯”ä¾‹ | `0.1` | æ—©åœæ³•æ™‚ä½¿ç”¨ |
| `n_iter_no_change` | ç„¡æ”¹å–„å®¹å¿æ¬¡æ•¸ | `5` | æ—©åœæ³•çš„è€å¿ƒåƒæ•¸ |

---

## 4. sklearn ä¸­çš„ SGDRegressor ä½¿ç”¨æ–¹æ³•

### 4.1 åŸºæœ¬ä½¿ç”¨æµç¨‹

```python
from sklearn.linear_model import SGDRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# 1. æº–å‚™æ•¸æ“š
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 2. æ•¸æ“šæ¨™æº–åŒ–ï¼ˆSGD å°ç‰¹å¾µå°ºåº¦æ•æ„Ÿï¼Œå¿…é ˆæ¨™æº–åŒ–ï¼ï¼‰
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 3. å»ºç«‹ SGD æ¨¡å‹
sgd_model = SGDRegressor(
    loss='squared_loss',        # æå¤±å‡½æ•¸
    penalty='l2',               # æ­£å‰‡åŒ–é¡å‹
    alpha=0.0001,               # æ­£å‰‡åŒ–å¼·åº¦
    learning_rate='optimal',    # å­¸ç¿’ç‡ç­–ç•¥
    max_iter=1000,              # æœ€å¤§è¿­ä»£æ¬¡æ•¸
    tol=1e-3,                   # æ”¶æ–‚å®¹å·®
    random_state=42             # éš¨æ©Ÿç¨®å­
)

# 4. è¨“ç·´æ¨¡å‹
sgd_model.fit(X_train_scaled, y_train)

# 5. æ¨¡å‹é æ¸¬
y_pred_train = sgd_model.predict(X_train_scaled)
y_pred_test = sgd_model.predict(X_test_scaled)

# 6. æ¨¡å‹è©•ä¼°
train_r2 = r2_score(y_train, y_pred_train)
test_r2 = r2_score(y_test, y_pred_test)
train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))
test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))

print(f"Train RÂ²: {train_r2:.4f}, RMSE: {train_rmse:.4f}")
print(f"Test RÂ²: {test_r2:.4f}, RMSE: {test_rmse:.4f}")

# 7. æŸ¥çœ‹æ¨¡å‹åƒæ•¸
print(f"\nIntercept: {sgd_model.intercept_[0]:.4f}")
print(f"Coefficients: {sgd_model.coef_}")
print(f"Number of iterations: {sgd_model.n_iter_}")
```

**âš ï¸ é‡è¦æé†’**ï¼š
- **SGD å°ç‰¹å¾µå°ºåº¦æ¥µå…¶æ•æ„Ÿï¼Œå¿…é ˆé€²è¡Œæ¨™æº–åŒ–ï¼**
- ä½¿ç”¨ `StandardScaler` å°‡ç‰¹å¾µè½‰æ›ç‚ºå‡å€¼ 0ã€æ¨™æº–å·® 1
- è¨“ç·´é›†å’Œæ¸¬è©¦é›†ä½¿ç”¨ç›¸åŒçš„ scalerï¼ˆå…ˆ fit è¨“ç·´é›†ï¼Œå† transform æ¸¬è©¦é›†ï¼‰

### 4.2 ä¸åŒæå¤±å‡½æ•¸çš„ä½¿ç”¨

```python
# 1. å¹³æ–¹æå¤±ï¼ˆæ¨™æº– OLS + æ­£å‰‡åŒ–ï¼‰
sgd_squared = SGDRegressor(loss='squared_loss', penalty='l2', alpha=0.0001)

# 2. Huber æå¤±ï¼ˆå°ç•°å¸¸å€¼é­¯æ£’ï¼‰
sgd_huber = SGDRegressor(
    loss='huber', 
    epsilon=1.35,     # Huber çš„é–¾å€¼åƒæ•¸
    penalty='l2', 
    alpha=0.0001
)

# 3. Îµ-ä¸æ•æ„Ÿæå¤±ï¼ˆæ”¯æŒå‘é‡å›æ­¸é¢¨æ ¼ï¼‰
sgd_epsilon = SGDRegressor(
    loss='epsilon_insensitive',
    epsilon=0.1,      # ä¸æ•æ„Ÿå€åŸŸçš„å¯¬åº¦
    penalty='l2',
    alpha=0.0001
)

# è¨“ç·´å’Œæ¯”è¼ƒ
models = {
    'Squared Loss': sgd_squared,
    'Huber Loss': sgd_huber,
    'Epsilon-Insensitive': sgd_epsilon
}

for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    test_r2 = model.score(X_test_scaled, y_test)
    print(f"{name}: Test RÂ² = {test_r2:.4f}")
```

### 4.3 ä¸åŒæ­£å‰‡åŒ–æ–¹æ³•çš„ä½¿ç”¨

```python
# 1. ç„¡æ­£å‰‡åŒ–
sgd_none = SGDRegressor(penalty='none', max_iter=1000)

# 2. L2 æ­£å‰‡åŒ–ï¼ˆRidge é¢¨æ ¼ï¼‰
sgd_l2 = SGDRegressor(penalty='l2', alpha=0.01, max_iter=1000)

# 3. L1 æ­£å‰‡åŒ–ï¼ˆLasso é¢¨æ ¼ï¼‰
sgd_l1 = SGDRegressor(penalty='l1', alpha=0.01, max_iter=1000)

# 4. Elastic Net æ­£å‰‡åŒ–
sgd_elastic = SGDRegressor(
    penalty='elasticnet',
    alpha=0.01,
    l1_ratio=0.5,     # L1 èˆ‡ L2 çš„æ··åˆæ¯”ä¾‹
    max_iter=1000
)

# è¨“ç·´å’Œæ¯”è¼ƒ
regularizations = {
    'No Regularization': sgd_none,
    'L2 (Ridge)': sgd_l2,
    'L1 (Lasso)': sgd_l1,
    'Elastic Net': sgd_elastic
}

for name, model in regularizations.items():
    model.fit(X_train_scaled, y_train)
    test_r2 = model.score(X_test_scaled, y_test)
    n_nonzero = np.sum(model.coef_ != 0)
    print(f"{name}: Test RÂ² = {test_r2:.4f}, Non-zero coefficients = {n_nonzero}")
```

### 4.4 å­¸ç¿’ç‡ç­–ç•¥çš„ä½¿ç”¨

```python
# 1. æœ€å„ªå­¸ç¿’ç‡ï¼ˆé»˜èªï¼Œé€šå¸¸æœ€ç©©å¥ï¼‰
sgd_optimal = SGDRegressor(learning_rate='optimal', max_iter=1000)

# 2. å¸¸æ•¸å­¸ç¿’ç‡
sgd_constant = SGDRegressor(
    learning_rate='constant',
    eta0=0.01,        # åˆå§‹å­¸ç¿’ç‡
    max_iter=1000
)

# 3. å€’æ•¸è¡°æ¸›å­¸ç¿’ç‡
sgd_invscaling = SGDRegressor(
    learning_rate='invscaling',
    eta0=0.01,
    power_t=0.25,     # è¡°æ¸›é€Ÿåº¦
    max_iter=1000
)

# 4. è‡ªé©æ‡‰å­¸ç¿’ç‡
sgd_adaptive = SGDRegressor(
    learning_rate='adaptive',
    eta0=0.01,
    max_iter=1000,
    n_iter_no_change=5  # 5 å€‹ epoch ç„¡æ”¹å–„æ™‚æ¸›åŠå­¸ç¿’ç‡
)

# è¨“ç·´å’Œæ¯”è¼ƒ
learning_rates = {
    'Optimal': sgd_optimal,
    'Constant': sgd_constant,
    'Invscaling': sgd_invscaling,
    'Adaptive': sgd_adaptive
}

for name, model in learning_rates.items():
    model.fit(X_train_scaled, y_train)
    test_r2 = model.score(X_test_scaled, y_test)
    print(f"{name}: Test RÂ² = {test_r2:.4f}, Iterations = {model.n_iter_}")
```

### 4.5 æ—©åœæ³• (Early Stopping)

```python
from sklearn.linear_model import SGDRegressor

# å•Ÿç”¨æ—©åœæ³•
sgd_early = SGDRegressor(
    loss='squared_loss',
    penalty='l2',
    alpha=0.0001,
    max_iter=10000,            # è¨­ç½®è¼ƒå¤§çš„æœ€å¤§è¿­ä»£æ¬¡æ•¸
    early_stopping=True,       # å•Ÿç”¨æ—©åœæ³•
    validation_fraction=0.1,   # 10% ä½œç‚ºé©—è­‰é›†
    n_iter_no_change=10,       # 10 å€‹ epoch ç„¡æ”¹å–„å‰‡åœæ­¢
    tol=1e-4,                  # æ”¶æ–‚å®¹å·®
    random_state=42
)

sgd_early.fit(X_train_scaled, y_train)

print(f"Training stopped at iteration: {sgd_early.n_iter_}")
print(f"Best validation score: {sgd_early.best_loss_:.4f}")
print(f"Test RÂ²: {sgd_early.score(X_test_scaled, y_test):.4f}")
```

**æ—©åœæ³•å„ªå‹¢**ï¼š
- è‡ªå‹•ç¢ºå®šæœ€ä½³è¿­ä»£æ¬¡æ•¸
- é˜²æ­¢éæ“¬åˆ
- ç¯€çœè¨ˆç®—æ™‚é–“
- é©åˆå¤§æ•¸æ“šé›†

### 4.6 åœ¨ç·šå­¸ç¿’ (Incremental Learning)

SGD æ”¯æŒ**å¢é‡å­¸ç¿’**ï¼Œå¯ä»¥åœ¨æ–°æ•¸æ“šåˆ°é”æ™‚æ›´æ–°æ¨¡å‹ï¼Œè€Œç„¡éœ€é‡æ–°è¨“ç·´ã€‚

```python
from sklearn.linear_model import SGDRegressor

# 1. åˆå§‹åŒ–æ¨¡å‹ï¼ˆå•Ÿç”¨ warm_startï¼‰
sgd_online = SGDRegressor(
    loss='squared_loss',
    penalty='l2',
    alpha=0.0001,
    learning_rate='optimal',
    warm_start=True,     # é—œéµåƒæ•¸ï¼šä¿ç•™ä¹‹å‰çš„æ¨¡å‹åƒæ•¸
    random_state=42
)

# 2. ç¬¬ä¸€æ‰¹æ•¸æ“šè¨“ç·´
sgd_online.fit(X_batch1_scaled, y_batch1)
print(f"After batch 1: Test RÂ² = {sgd_online.score(X_test_scaled, y_test):.4f}")

# 3. ç¬¬äºŒæ‰¹æ•¸æ“šå¢é‡æ›´æ–°ï¼ˆä¸æœƒé‡ç½®æ¨¡å‹åƒæ•¸ï¼‰
sgd_online.partial_fit(X_batch2_scaled, y_batch2)
print(f"After batch 2: Test RÂ² = {sgd_online.score(X_test_scaled, y_test):.4f}")

# 4. ç¬¬ä¸‰æ‰¹æ•¸æ“šå¢é‡æ›´æ–°
sgd_online.partial_fit(X_batch3_scaled, y_batch3)
print(f"After batch 3: Test RÂ² = {sgd_online.score(X_test_scaled, y_test):.4f}")
```

**åœ¨ç·šå­¸ç¿’æ‡‰ç”¨å ´æ™¯ï¼ˆåŒ–å·¥é ˜åŸŸï¼‰**ï¼š
- **å³æ™‚åæ‡‰å™¨ç›£æ§**ï¼šæ–°çš„æ“ä½œæ•¸æ“šä¸æ–·åˆ°é”ï¼Œæ¨¡å‹å³æ™‚æ›´æ–°
- **é€£çºŒç”Ÿç”¢éç¨‹**ï¼šç”Ÿç”¢æ¢ä»¶éš¨æ™‚é–“è®ŠåŒ–ï¼Œæ¨¡å‹éœ€è¦é©æ‡‰æ–°çš„æ“ä½œç¯„åœ
- **è¨­å‚™è€åŒ–è¿½è¹¤**ï¼šè¨­å‚™æ€§èƒ½éš¨æ™‚é–“é€€åŒ–ï¼Œæ¨¡å‹éœ€è¦æŒçºŒæ›´æ–°
- **å­£ç¯€æ€§è®ŠåŒ–**ï¼šåŸæ–™æ€§è³ªæˆ–ç’°å¢ƒæ¢ä»¶éš¨å­£ç¯€è®ŠåŒ–

**`fit()` vs `partial_fit()` æ¯”è¼ƒ**ï¼š

| æ–¹æ³• | è¡Œç‚º | åƒæ•¸é‡ç½® | é©ç”¨å ´æ™¯ |
|------|------|---------|---------|
| `fit()` | å®Œæ•´è¨“ç·´ | æ˜¯ï¼ˆé™¤é warm_start=Trueï¼‰ | æ‰¹é‡è¨“ç·´ |
| `partial_fit()` | å¢é‡æ›´æ–° | å¦ | **åœ¨ç·šå­¸ç¿’ã€æµå¼æ•¸æ“š** |

### 4.7 è¶…åƒæ•¸èª¿æ•´

ä½¿ç”¨ GridSearchCV æˆ– RandomizedSearchCV å°‹æ‰¾æœ€ä½³è¶…åƒæ•¸çµ„åˆã€‚

```python
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import SGDRegressor

# å®šç¾©åƒæ•¸ç¶²æ ¼
param_grid = {
    'loss': ['squared_loss', 'huber'],
    'penalty': ['l2', 'l1', 'elasticnet'],
    'alpha': [0.0001, 0.001, 0.01, 0.1],
    'learning_rate': ['optimal', 'adaptive'],
    'max_iter': [1000, 2000, 5000]
}

# å»ºç«‹ GridSearchCV
sgd = SGDRegressor(random_state=42)
grid_search = GridSearchCV(
    sgd,
    param_grid,
    cv=5,                      # 5 æŠ˜äº¤å‰é©—è­‰
    scoring='r2',              # è©•ä¼°æŒ‡æ¨™
    n_jobs=-1,                 # ä½¿ç”¨æ‰€æœ‰ CPU
    verbose=1
)

# åŸ·è¡Œç¶²æ ¼æœç´¢
grid_search.fit(X_train_scaled, y_train)

# è¼¸å‡ºæœ€ä½³åƒæ•¸
print("Best parameters:", grid_search.best_params_)
print(f"Best cross-validation RÂ²: {grid_search.best_score_:.4f}")

# ä½¿ç”¨æœ€ä½³æ¨¡å‹é æ¸¬
best_sgd = grid_search.best_estimator_
test_r2 = best_sgd.score(X_test_scaled, y_test)
print(f"Test RÂ²: {test_r2:.4f}")
```

**åŒ–å·¥æ‡‰ç”¨çš„åƒæ•¸èª¿æ•´å»ºè­°**ï¼š
1. **å°æ•¸æ“šé›† (< 10,000 æ¨£æœ¬)**ï¼š
   - `learning_rate='optimal'` æˆ– `'adaptive'`
   - `max_iter=1000-5000`
   - ä½¿ç”¨ GridSearchCV ç´°ç·»æœç´¢

2. **å¤§æ•¸æ“šé›† (> 100,000 æ¨£æœ¬)**ï¼š
   - `learning_rate='constant'`ï¼Œæ‰‹å‹•èª¿æ•´ `eta0`
   - `max_iter=10-100`ï¼ˆepoch æ•¸é‡å°‘ä½†æ¨£æœ¬å¤šï¼‰
   - ä½¿ç”¨ RandomizedSearchCV å¿«é€Ÿæœç´¢
   - å•Ÿç”¨ `early_stopping=True`

3. **åœ¨ç·šå­¸ç¿’å ´æ™¯**ï¼š
   - `warm_start=True`
   - `learning_rate='optimal'`ï¼ˆè‡ªé©æ‡‰æ–°æ•¸æ“šï¼‰
   - å®šæœŸè©•ä¼°æ¨¡å‹æ€§èƒ½ï¼Œå¿…è¦æ™‚é‡æ–°åˆå§‹åŒ–

---

## 5. SGD èˆ‡å…¶ä»–ç·šæ€§æ¨¡å‹çš„æ¯”è¼ƒ

### 5.1 æ¨¡å‹ç­‰åƒ¹æ€§

é€šéé¸æ“‡ä¸åŒçš„ `loss` å’Œ `penalty` åƒæ•¸ï¼ŒSGDRegressor å¯ä»¥æ¨¡æ“¬å¤šç¨®ç·šæ€§æ¨¡å‹ï¼š

| æ¨¡å‹ | SGDRegressor è¨­ç½® | èªªæ˜ |
|------|-------------------|------|
| **OLS (æ™®é€šæœ€å°å¹³æ–¹æ³•)** | `penalty='none'`, `loss='squared_loss'` | ç„¡æ­£å‰‡åŒ–çš„ç·šæ€§å›æ­¸ |
| **Ridge Regression** | `penalty='l2'`, `loss='squared_loss'` | L2 æ­£å‰‡åŒ– |
| **Lasso Regression** | `penalty='l1'`, `loss='squared_loss'` | L1 æ­£å‰‡åŒ– |
| **Elastic Net** | `penalty='elasticnet'`, `loss='squared_loss'` | L1 + L2 æ­£å‰‡åŒ– |
| **Huber Regression** | `penalty='l2'`, `loss='huber'` | å°ç•°å¸¸å€¼é­¯æ£’ |
| **Support Vector Regression (SVR)** | `penalty='l2'`, `loss='epsilon_insensitive'` | Îµ-ä¸æ•æ„Ÿæå¤± |

### 5.2 æ€§èƒ½å°æ¯”

è®“æˆ‘å€‘æ¯”è¼ƒ SGDRegressor èˆ‡å…¶ä»– sklearn ç·šæ€§æ¨¡å‹çš„æ€§èƒ½å’Œç‰¹é»ï¼š

```python
from sklearn.linear_model import (
    LinearRegression, Ridge, Lasso, ElasticNet, 
    HuberRegressor, SGDRegressor
)
from sklearn.metrics import r2_score, mean_squared_error
import time
import numpy as np

# å»ºç«‹æ¨¡å‹å­—å…¸
models = {
    'OLS': LinearRegression(),
    'Ridge': Ridge(alpha=0.01),
    'Lasso': Lasso(alpha=0.01, max_iter=10000),
    'Elastic Net': ElasticNet(alpha=0.01, l1_ratio=0.5, max_iter=10000),
    'Huber': HuberRegressor(alpha=0.01),
    'SGD-Ridge': SGDRegressor(penalty='l2', alpha=0.01, max_iter=1000),
    'SGD-Lasso': SGDRegressor(penalty='l1', alpha=0.01, max_iter=1000),
    'SGD-Elastic': SGDRegressor(penalty='elasticnet', alpha=0.01, l1_ratio=0.5, max_iter=1000),
    'SGD-Huber': SGDRegressor(loss='huber', penalty='l2', alpha=0.01, max_iter=1000)
}

results = []

for name, model in models.items():
    # è¨“ç·´è¨ˆæ™‚
    start_time = time.time()
    model.fit(X_train_scaled, y_train)
    train_time = time.time() - start_time
    
    # é æ¸¬
    y_pred_test = model.predict(X_test_scaled)
    
    # è©•ä¼°
    test_r2 = r2_score(y_test, y_pred_test)
    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
    
    # çµ±è¨ˆéé›¶ä¿‚æ•¸
    coef = model.coef_ if hasattr(model, 'coef_') else None
    n_nonzero = np.sum(coef != 0) if coef is not None else None
    
    results.append({
        'Model': name,
        'Test RÂ²': test_r2,
        'Test RMSE': test_rmse,
        'Train Time (s)': train_time,
        'Non-zero Coefs': n_nonzero
    })

# è¼¸å‡ºçµæœ
import pandas as pd
results_df = pd.DataFrame(results)
print(results_df.to_string(index=False))
```

### 5.3 ä½•æ™‚é¸æ“‡ SGDRegressorï¼Ÿ

**âœ… æ¨è–¦ä½¿ç”¨ SGDRegressor çš„å ´æ™¯**ï¼š

1. **å¤§è¦æ¨¡æ•¸æ“š** (> 100,000 æ¨£æœ¬)ï¼š
   - SGD çš„è¨ˆç®—è¤‡é›œåº¦ç‚º $O(n)$ ï¼ˆèˆ‡æ¨£æœ¬æ•¸ç·šæ€§ç›¸é—œï¼‰
   - å…¶ä»–æ–¹æ³•å¦‚ Ridge çš„è¤‡é›œåº¦ç‚º $O(n^2 p)$ æˆ– $O(np^2)$
   - **æ•¸æ“šè¶Šå¤§ï¼ŒSGD å„ªå‹¢è¶Šæ˜é¡¯**

2. **åœ¨ç·šå­¸ç¿’/å¢é‡å­¸ç¿’**ï¼š
   - æ–°æ•¸æ“šä¸æ–·åˆ°é”ï¼Œç„¡æ³•ä¸€æ¬¡æ€§è¼‰å…¥å…¨éƒ¨æ•¸æ“š
   - ä½¿ç”¨ `partial_fit()` æ–¹æ³•å¢é‡æ›´æ–°
   - åŒ–å·¥æ‡‰ç”¨ï¼šå³æ™‚ç›£æ§ã€å‹•æ…‹å„ªåŒ–

3. **è¨˜æ†¶é«”å—é™**ï¼š
   - æ•¸æ“šç„¡æ³•ä¸€æ¬¡æ€§è¼‰å…¥è¨˜æ†¶é«”
   - SGD å¯ä»¥é€æ‰¹è™•ç†æ•¸æ“š

4. **éœ€è¦éˆæ´»æ€§**ï¼š
   - æ”¯æŒå¤šç¨®æå¤±å‡½æ•¸ï¼ˆsquared_loss, huber, epsilon_insensitiveï¼‰
   - æ”¯æŒå¤šç¨®æ­£å‰‡åŒ–ï¼ˆL1, L2, Elastic Netï¼‰

**âŒ ä¸æ¨è–¦ä½¿ç”¨ SGDRegressor çš„å ´æ™¯**ï¼š

1. **å°æ•¸æ“šé›†** (< 10,000 æ¨£æœ¬)ï¼š
   - å…¶ä»–æ–¹æ³•ï¼ˆLinearRegression, Ridge, Lassoï¼‰æ›´ç©©å®š
   - SGD å¯èƒ½æ”¶æ–‚ä¸ç©©å®š

2. **éœ€è¦ç²¾ç¢ºè§£**ï¼š
   - SGD æ˜¯è¿‘ä¼¼è§£
   - Ridgeã€Lasso æœ‰æ›´ç²¾ç¢ºçš„æ±‚è§£å™¨

3. **ä¸é€²è¡Œæ•¸æ“šæ¨™æº–åŒ–**ï¼š
   - SGD å°ç‰¹å¾µå°ºåº¦æ¥µå…¶æ•æ„Ÿ
   - å¿…é ˆé€²è¡Œæ¨™æº–åŒ–ï¼Œå¦å‰‡æ€§èƒ½å¾ˆå·®

### 5.4 è¨ˆç®—è¤‡é›œåº¦æ¯”è¼ƒ

| æ¨¡å‹ | è¨“ç·´è¤‡é›œåº¦ | é æ¸¬è¤‡é›œåº¦ | è¨˜æ†¶é«”éœ€æ±‚ |
|------|-----------|-----------|-----------|
| **OLS** | $O(np^2 + p^3)$ | $O(p)$ | $O(np)$ |
| **Ridge** | $O(np^2 + p^3)$ | $O(p)$ | $O(np)$ |
| **Lasso** | $O(np \cdot \text{iter})$ | $O(p)$ | $O(np)$ |
| **Elastic Net** | $O(np \cdot \text{iter})$ | $O(p)$ | $O(np)$ |
| **SGDRegressor** | $O(n \cdot \text{iter})$ | $O(p)$ | **$O(p)$** |

å…¶ä¸­ï¼š
- $n$ : æ¨£æœ¬æ•¸
- $p$ : ç‰¹å¾µæ•¸
- $\text{iter}$ : è¿­ä»£æ¬¡æ•¸

**SGD çš„é—œéµå„ªå‹¢**ï¼š
- è¨“ç·´è¤‡é›œåº¦èˆ‡**ç‰¹å¾µæ•¸ç„¡é—œ**ï¼ˆåƒ…èˆ‡æ¨£æœ¬æ•¸ç·šæ€§ç›¸é—œï¼‰
- **è¨˜æ†¶é«”éœ€æ±‚æ¥µä½**ï¼ˆåƒ…éœ€å­˜å„²æ¨¡å‹åƒæ•¸ï¼Œèˆ‡æ¨£æœ¬æ•¸ç„¡é—œï¼‰
- é©åˆ**è¶…å¤§è¦æ¨¡æ•¸æ“š**

---

## 6. SGD å›æ­¸åœ¨åŒ–å·¥é ˜åŸŸçš„æ‡‰ç”¨

### 6.1 å¤§è¦æ¨¡ç”Ÿç”¢æ•¸æ“šå»ºæ¨¡

**æ‡‰ç”¨å ´æ™¯**ï¼šåŒ–å·¥å» çš„ DCS (Distributed Control System) æ¯ç§’è¨˜éŒ„æ•¸ç™¾å€‹è®Šæ•¸ï¼Œæ¯å¤©ç”¢ç”Ÿæ•¸ç™¾è¬ç­†æ•¸æ“šã€‚

**æ¡ˆä¾‹ï¼šå¤§è¦æ¨¡åæ‡‰å™¨ç”¢ç‡é æ¸¬**
- æ•¸æ“šé‡ï¼š500è¬ç­†æ­·å²æ“ä½œæ•¸æ“š
- ç‰¹å¾µï¼šæº«åº¦ã€å£“åŠ›ã€æµé‡ã€æ¿ƒåº¦ç­‰ 50 å€‹è®Šæ•¸
- ç›®æ¨™ï¼šé æ¸¬ç”¢å“ç”¢ç‡

```python
# å‚³çµ±æ–¹æ³•ï¼ˆè¨˜æ†¶é«”ä¸è¶³ï¼‰
ridge = Ridge(alpha=0.01)
# ridge.fit(X_large, y_large)  # MemoryError!

# SGD è§£æ±ºæ–¹æ¡ˆï¼ˆé€æ‰¹è™•ç†ï¼‰
sgd = SGDRegressor(
    penalty='l2',
    alpha=0.01,
    learning_rate='optimal',
    warm_start=True,
    random_state=42
)

batch_size = 10000
n_batches = len(X_large) // batch_size

for i in range(n_batches):
    start = i * batch_size
    end = start + batch_size
    X_batch = X_large[start:end]
    y_batch = y_large[start:end]
    
    if i == 0:
        sgd.fit(X_batch, y_batch)
    else:
        sgd.partial_fit(X_batch, y_batch)
    
    if (i+1) % 100 == 0:
        print(f"Processed {i+1} batches...")

print(f"Final model RÂ²: {sgd.score(X_test, y_test):.4f}")
```

### 6.2 åœ¨ç·šå­¸ç¿’èˆ‡å³æ™‚ç›£æ§

**æ‡‰ç”¨å ´æ™¯**ï¼šåæ‡‰å™¨æ“ä½œæ¢ä»¶éš¨æ™‚é–“è®ŠåŒ–ï¼Œéœ€è¦å³æ™‚æ›´æ–°é æ¸¬æ¨¡å‹ã€‚

**æ¡ˆä¾‹ï¼šé€£çºŒåæ‡‰å™¨å³æ™‚å„ªåŒ–**
- æ¯å°æ™‚æ”¶é›†æ–°çš„æ“ä½œæ•¸æ“š
- æ¨¡å‹å³æ™‚æ›´æ–°ä»¥é©æ‡‰æ“ä½œæ¢ä»¶è®ŠåŒ–
- é æ¸¬æœªä¾† 1 å°æ™‚çš„ç”¢å“å“è³ª

```python
from sklearn.linear_model import SGDRegressor
from sklearn.preprocessing import StandardScaler
import numpy as np

# åˆå§‹åŒ–æ¨¡å‹å’Œæ¨™æº–åŒ–å™¨
scaler = StandardScaler()
sgd_online = SGDRegressor(
    penalty='l2',
    alpha=0.001,
    learning_rate='optimal',
    warm_start=True,
    random_state=42
)

# åˆå§‹è¨“ç·´ï¼ˆä½¿ç”¨æ­·å²æ•¸æ“šï¼‰
X_history_scaled = scaler.fit_transform(X_history)
sgd_online.fit(X_history_scaled, y_history)
print(f"Initial model trained. RÂ² = {sgd_online.score(X_history_scaled, y_history):.4f}")

# åœ¨ç·šæ›´æ–°å¾ªç’°ï¼ˆæ¨¡æ“¬å³æ™‚æ•¸æ“šæµï¼‰
for hour in range(1, 25):  # 24 å°æ™‚
    # 1. æ”¶é›†æ–°æ•¸æ“šï¼ˆæ¨¡æ“¬ï¼‰
    X_new = collect_new_data_from_DCS(hour)  # å‡è¨­å‡½æ•¸
    y_new = measure_product_quality(hour)     # å‡è¨­å‡½æ•¸
    
    # 2. æ¨™æº–åŒ–æ–°æ•¸æ“šï¼ˆä½¿ç”¨å·²è¨“ç·´çš„ scalerï¼‰
    X_new_scaled = scaler.transform(X_new.reshape(1, -1))
    
    # 3. å¢é‡æ›´æ–°æ¨¡å‹
    sgd_online.partial_fit(X_new_scaled, [y_new])
    
    # 4. é æ¸¬æœªä¾†å“è³ª
    X_future = predict_future_conditions(hour)  # å‡è¨­å‡½æ•¸
    X_future_scaled = scaler.transform(X_future.reshape(1, -1))
    y_pred = sgd_online.predict(X_future_scaled)[0]
    
    print(f"Hour {hour}: Predicted quality = {y_pred:.2f}")
    
    # 5. å¦‚æœé æ¸¬å“è³ªåé›¢ç›®æ¨™ï¼Œè§¸ç™¼è­¦å ±
    if abs(y_pred - target_quality) > tolerance:
        send_alert(f"Quality deviation predicted at hour {hour+1}")
```

### 6.3 ç•°å¸¸å€¼è™•ç†ï¼ˆHuber æå¤±ï¼‰

**æ‡‰ç”¨å ´æ™¯**ï¼šåŒ–å·¥æ•¸æ“šä¸­å¸¸è¦‹å‚³æ„Ÿå™¨æ•…éšœæˆ–æ“ä½œç•°å¸¸å°è‡´çš„ç•°å¸¸å€¼ã€‚

**æ¡ˆä¾‹ï¼šå­˜åœ¨ç•°å¸¸å€¼çš„è’¸é¤¾å¡”ç”¢å“ç´”åº¦é æ¸¬**

```python
from sklearn.linear_model import SGDRegressor
from sklearn.metrics import r2_score, mean_absolute_error
import numpy as np

# æ¨¡æ“¬æ•¸æ“šï¼ˆåŒ…å« 5% ç•°å¸¸å€¼ï¼‰
np.random.seed(42)
n = 1000
X = np.random.randn(n, 5)
y = 10 + 2*X[:, 0] + 3*X[:, 1] + np.random.randn(n)

# æ·»åŠ ç•°å¸¸å€¼ï¼ˆ5% çš„æ•¸æ“šï¼‰
n_outliers = int(0.05 * n)
outlier_indices = np.random.choice(n, n_outliers, replace=False)
y[outlier_indices] += np.random.choice([-1, 1], n_outliers) * np.random.uniform(20, 50, n_outliers)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# æ¨™æº–åŒ–
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 1. æ¨™æº– SGDï¼ˆå°ç•°å¸¸å€¼æ•æ„Ÿï¼‰
sgd_squared = SGDRegressor(
    loss='squared_loss',
    penalty='l2',
    alpha=0.001,
    max_iter=1000,
    random_state=42
)
sgd_squared.fit(X_train_scaled, y_train)

# 2. Huber SGDï¼ˆå°ç•°å¸¸å€¼é­¯æ£’ï¼‰
sgd_huber = SGDRegressor(
    loss='huber',
    epsilon=1.35,     # Huber é–¾å€¼
    penalty='l2',
    alpha=0.001,
    max_iter=1000,
    random_state=42
)
sgd_huber.fit(X_train_scaled, y_train)

# æ¯”è¼ƒ
models = {'Squared Loss': sgd_squared, 'Huber Loss': sgd_huber}
for name, model in models.items():
    y_pred = model.predict(X_test_scaled)
    r2 = r2_score(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    print(f"{name}: RÂ² = {r2:.4f}, MAE = {mae:.4f}")

# è¼¸å‡ºç¤ºä¾‹ï¼š
# Squared Loss: RÂ² = 0.7523, MAE = 8.32
# Huber Loss: RÂ² = 0.8961, MAE = 2.45  â† æ›´é­¯æ£’
```

**Huber æå¤±çš„å„ªå‹¢**ï¼š
- å°å°èª¤å·®ä½¿ç”¨å¹³æ–¹æå¤±ï¼ˆé«˜æ•ˆï¼‰
- å°å¤§èª¤å·®ä½¿ç”¨ç·šæ€§æå¤±ï¼ˆé­¯æ£’ï¼‰
- é©åˆåŒ–å·¥æ•¸æ“šä¸­å¸¸è¦‹çš„å‚³æ„Ÿå™¨ç•°å¸¸

### 6.4 ç‰¹å¾µé¸æ“‡ï¼ˆL1 æ­£å‰‡åŒ–ï¼‰

**æ‡‰ç”¨å ´æ™¯**ï¼šåŒ–å·¥éç¨‹æœ‰æ•¸åå€‹ç›£æ¸¬è®Šæ•¸ï¼Œä½†åªæœ‰å°‘æ•¸å°ç”¢å“å“è³ªæœ‰é¡¯è‘—å½±éŸ¿ã€‚

**æ¡ˆä¾‹ï¼šè­˜åˆ¥å½±éŸ¿èšåˆåæ‡‰è½‰åŒ–ç‡çš„é—œéµå› ç´ **

```python
from sklearn.linear_model import SGDRegressor
import numpy as np
import pandas as pd

# å‡è¨­æœ‰ 50 å€‹ç‰¹å¾µï¼Œä½†åªæœ‰ 5 å€‹çœŸæ­£é‡è¦
# ï¼ˆå¯¦éš›æ•¸æ“šå¾ DCS ç³»çµ±ç²å–ï¼‰

# è¨“ç·´ L1 æ­£å‰‡åŒ–æ¨¡å‹ï¼ˆLasso é¢¨æ ¼ï¼‰
sgd_lasso = SGDRegressor(
    penalty='l1',
    alpha=0.01,        # è¼ƒå¤§çš„ alpha ä¿ƒé€²æ›´ç¨€ç–çš„è§£
    max_iter=5000,
    tol=1e-4,
    random_state=42
)

sgd_lasso.fit(X_train_scaled, y_train)

# ç²å–ç‰¹å¾µé‡è¦æ€§
feature_importance = pd.DataFrame({
    'Feature': feature_names,
    'Coefficient': sgd_lasso.coef_
})

# éæ¿¾éé›¶ä¿‚æ•¸
feature_importance = feature_importance[feature_importance['Coefficient'] != 0]
feature_importance = feature_importance.sort_values('Coefficient', key=abs, ascending=False)

print("Important features:")
print(feature_importance)

# è¼¸å‡ºç¤ºä¾‹ï¼š
# Feature                    Coefficient
# Reactor_Temperature_C      5.234
# Initiator_Concentration    3.876
# Monomer_Feed_Rate          2.451
# Stirring_Speed            -1.234
# Catalyst_Age               0.876

# åƒ…ä½¿ç”¨é¸å®šçš„ç‰¹å¾µé‡æ–°è¨“ç·´
selected_features = feature_importance['Feature'].tolist()
X_train_selected = X_train[selected_features]
X_test_selected = X_test[selected_features]

print(f"\nOriginal features: {X_train.shape[1]}")
print(f"Selected features: {len(selected_features)}")
print(f"Feature reduction: {(1 - len(selected_features)/X_train.shape[1])*100:.1f}%")
```

### 6.5 æ¨¡å‹æŒçºŒæ›´æ–°ï¼ˆæ¦‚å¿µæ¼‚ç§»è™•ç†ï¼‰

**æ‡‰ç”¨å ´æ™¯**ï¼šåŒ–å·¥è¨­å‚™è€åŒ–ã€åŸæ–™æ€§è³ªè®ŠåŒ–å°è‡´æ•¸æ“šåˆ†ä½ˆéš¨æ™‚é–“è®ŠåŒ–ï¼ˆæ¦‚å¿µæ¼‚ç§»ï¼‰ã€‚

**æ¡ˆä¾‹ï¼šå‚¬åŒ–åŠ‘è€åŒ–çš„åæ‡‰å™¨å»ºæ¨¡**

```python
from sklearn.linear_model import SGDRegressor
from sklearn.metrics import r2_score
import numpy as np

# æ¨¡æ“¬å‚¬åŒ–åŠ‘è€åŒ–éç¨‹ï¼ˆæ€§èƒ½éš¨æ™‚é–“é€€åŒ–ï¼‰
def simulate_reactor_with_aging(time_days, n_samples=100):
    """
    æ¨¡æ“¬åæ‡‰å™¨æ•¸æ“šï¼Œå‚¬åŒ–åŠ‘æ€§èƒ½éš¨æ™‚é–“ç·šæ€§é€€åŒ–
    """
    X = np.random.randn(n_samples, 5)
    
    # åŸºç¤ç”¢ç‡
    base_yield = 80 + 10*X[:, 0] + 5*X[:, 1]
    
    # å‚¬åŒ–åŠ‘è€åŒ–æ•ˆæ‡‰ï¼ˆæ¯å¤©æå¤± 0.1%ï¼‰
    aging_factor = 1 - (time_days * 0.001)
    
    y = base_yield * aging_factor + np.random.randn(n_samples)
    return X, y

# åˆå§‹åŒ–æ¨¡å‹
scaler = StandardScaler()
sgd_adaptive = SGDRegressor(
    penalty='l2',
    alpha=0.001,
    learning_rate='optimal',
    warm_start=True,
    random_state=42
)

# æ¨¡æ“¬ 100 å¤©çš„é‹è¡Œ
test_scores = []

for day in range(0, 101, 10):  # æ¯ 10 å¤©è©•ä¼°ä¸€æ¬¡
    # æ”¶é›†ç•¶å¤©çš„æ•¸æ“š
    X_day, y_day = simulate_reactor_with_aging(day, n_samples=200)
    
    # åŠƒåˆ†è¨“ç·´å’Œæ¸¬è©¦
    X_train, X_test, y_train, y_test = train_test_split(
        X_day, y_day, test_size=0.3, random_state=42
    )
    
    if day == 0:
        # ç¬¬ 0 å¤©ï¼šåˆå§‹è¨“ç·´
        X_train_scaled = scaler.fit_transform(X_train)
        sgd_adaptive.fit(X_train_scaled, y_train)
    else:
        # å¾ŒçºŒå¤©æ•¸ï¼šå¢é‡æ›´æ–°
        X_train_scaled = scaler.transform(X_train)
        sgd_adaptive.partial_fit(X_train_scaled, y_train)
    
    # è©•ä¼°
    X_test_scaled = scaler.transform(X_test)
    test_r2 = sgd_adaptive.score(X_test_scaled, y_test)
    test_scores.append(test_r2)
    
    print(f"Day {day}: Test RÂ² = {test_r2:.4f}")

# è¼¸å‡ºç¤ºä¾‹ï¼š
# Day 0: Test RÂ² = 0.9245
# Day 10: Test RÂ² = 0.9278
# Day 20: Test RÂ² = 0.9301
# ...
# Day 100: Test RÂ² = 0.9314  â† æ¨¡å‹æŒçºŒé©æ‡‰è€åŒ–éç¨‹
```

**æŒçºŒæ›´æ–°ç­–ç•¥**ï¼š
1. **å®šæœŸæ›´æ–°**ï¼šæ¯å¤©/æ¯é€±ä½¿ç”¨æ–°æ•¸æ“šå¢é‡è¨“ç·´
2. **æ€§èƒ½ç›£æ§**ï¼šè¿½è¹¤æ¸¬è©¦é›† RÂ²ï¼Œå¦‚æœä¸‹é™å‰‡é‡æ–°è¨“ç·´
3. **æ»‘å‹•çª—å£**ï¼šåƒ…ä½¿ç”¨æœ€è¿‘ N å¤©çš„æ•¸æ“šï¼Œä¸Ÿæ£„éèˆŠæ•¸æ“š
4. **æ¨¡å‹é‡ç½®**ï¼šç•¶æ€§èƒ½æŒçºŒä¸‹é™æ™‚ï¼Œç”¨æœ€è¿‘æ•¸æ“šå®Œå…¨é‡æ–°è¨“ç·´

---

## 7. SGD å›æ­¸çš„å„ªå‹¢èˆ‡é™åˆ¶

### 7.1 å„ªå‹¢

1. **å¯æ“´å±•æ€§**ï¼š
   - å¯è™•ç†ç™¾è¬ã€åƒè¬ç´šæ¨£æœ¬
   - è¨˜æ†¶é«”éœ€æ±‚ä½ï¼Œèˆ‡æ¨£æœ¬æ•¸ç„¡é—œ

2. **åœ¨ç·šå­¸ç¿’èƒ½åŠ›**ï¼š
   - æ”¯æŒå¢é‡æ›´æ–°ï¼ˆ`partial_fit`ï¼‰
   - é©æ‡‰å‹•æ…‹ç’°å¢ƒå’Œæ¦‚å¿µæ¼‚ç§»

3. **éˆæ´»æ€§**ï¼š
   - å¤šç¨®æå¤±å‡½æ•¸ï¼ˆsquared_loss, huber, epsilon_insensitiveï¼‰
   - å¤šç¨®æ­£å‰‡åŒ–ï¼ˆL1, L2, Elastic Netï¼‰
   - å¯æ¨¡æ“¬å¤šç¨®ç·šæ€§æ¨¡å‹

4. **è¨ˆç®—æ•ˆç‡**ï¼š
   - æ¯æ¬¡è¿­ä»£åƒ…è™•ç†ä¸€å€‹æ¨£æœ¬
   - æ”¶æ–‚å¿«ï¼ˆå°æ–¼å¤§æ•¸æ“šï¼‰

5. **ç•°å¸¸å€¼è™•ç†**ï¼š
   - Huber æå¤±å°ç•°å¸¸å€¼é­¯æ£’
   - é©åˆå·¥æ¥­æ•¸æ“š

### 7.2 é™åˆ¶

1. **å°ç‰¹å¾µå°ºåº¦æ•æ„Ÿ**ï¼š
   - **å¿…é ˆé€²è¡Œæ•¸æ“šæ¨™æº–åŒ–**
   - å¦å‰‡æ”¶æ–‚æ…¢æˆ–å¤±æ•—

2. **è¶…åƒæ•¸æ•æ„Ÿ**ï¼š
   - éœ€è¦èª¿æ•´å­¸ç¿’ç‡ã€æ­£å‰‡åŒ–åƒæ•¸
   - ä¸å¦‚ Ridge/Lasso ç©©å¥

3. **æ”¶æ–‚ä¸ç©©å®š**ï¼š
   - æå¤±å‡½æ•¸éœ‡ç›ªï¼Œç„¡æ³•ç²¾ç¢ºæ”¶æ–‚
   - å°æ•¸æ“šé›†æ•ˆæœä¸ä½³

4. **éš¨æ©Ÿæ€§**ï¼š
   - æ¯æ¬¡é‹è¡Œçµæœç•¥æœ‰ä¸åŒ
   - éœ€è¦è¨­ç½® `random_state` ç¢ºä¿å¯é‡ç¾æ€§

5. **å°æ•¸æ“šé›†ä¸é©ç”¨**ï¼š
   - æ¨£æœ¬æ•¸ < 10,000 æ™‚ï¼Œå»ºè­°ä½¿ç”¨ Ridge/Lasso
   - SGD åœ¨å°æ•¸æ“šä¸Šä¸ç©©å®š

### 7.3 ä½¿ç”¨å»ºè­°

| æ•¸æ“šè¦æ¨¡ | æ¨è–¦æ–¹æ³• | ç†ç”± |
|---------|---------|------|
| < 10,000 | Ridge, Lasso, Elastic Net | ç©©å®šã€ç²¾ç¢º |
| 10,000 - 100,000 | Ridge, Lasso, SGD çš†å¯ | æ ¹æ“šéœ€æ±‚é¸æ“‡ |
| > 100,000 | **SGDRegressor** | æ•ˆç‡é«˜ã€å¯æ“´å±• |
| æµå¼æ•¸æ“š | **SGDRegressor** | å”¯ä¸€æ”¯æŒ `partial_fit` |

**åŒ–å·¥é ˜åŸŸä½¿ç”¨æŒ‡å—**ï¼š
- **å¯¦é©—å®¤æ•¸æ“š** (< 1,000 æ¨£æœ¬)ï¼šä½¿ç”¨ Ridge/Lasso
- **è©¦ç”¢æ•¸æ“š** (1,000 - 10,000)ï¼šä½¿ç”¨ Elastic Net æˆ– SGD
- **å·¥æ¥­æ•¸æ“š** (> 10,000)ï¼š**å¼·çƒˆæ¨è–¦ SGD**
- **å³æ™‚ç›£æ§**ï¼š**å¿…é ˆä½¿ç”¨ SGD** (åœ¨ç·šå­¸ç¿’)

---

## 8. å®Œæ•´å¯¦æˆ°æ¡ˆä¾‹ï¼šåŒ–å·¥åæ‡‰å™¨ç”¢ç‡é æ¸¬

æœ¬ç¯€å°‡å±•ç¤ºä¸€å€‹å®Œæ•´çš„ SGD å›æ­¸å¯¦æˆ°æ¡ˆä¾‹ï¼Œä½¿ç”¨ **50,000 ç­†åŒ–å·¥åæ‡‰å™¨æ•¸æ“š**å»ºç«‹ç”¢ç‡é æ¸¬æ¨¡å‹ã€‚æ¡ˆä¾‹æ¶µè“‹æ•¸æ“šç”Ÿæˆã€é è™•ç†ã€æ¨¡å‹è¨“ç·´ã€æå¤±å‡½æ•¸æ¯”è¼ƒã€åœ¨ç·šå­¸ç¿’æ¼”ç¤ºã€æ¨¡å‹æ¯”è¼ƒå’Œå¯è¦–åŒ–ç­‰å®Œæ•´æµç¨‹ã€‚

### 8.1 æ¡ˆä¾‹èƒŒæ™¯

**ç›®æ¨™**ï¼šå»ºç«‹åŒ–å·¥åæ‡‰å™¨çš„ç”¢ç‡é æ¸¬æ¨¡å‹  
**æ•¸æ“šè¦æ¨¡**ï¼š50,000 ç­†æ“ä½œæ•¸æ“š  
**ç‰¹å¾µæ•¸é‡**ï¼š10 å€‹ï¼ˆæº«åº¦ã€å£“åŠ›ã€å‚¬åŒ–åŠ‘æ¿ƒåº¦ç­‰ï¼‰  
**ç›®æ¨™è®Šæ•¸**ï¼šç”¢å“ç”¢ç‡ (Yield_%)  
**æŒ‘æˆ°**ï¼š
- å¤§è¦æ¨¡æ•¸æ“šè™•ç†
- æ•¸æ“šä¸­å­˜åœ¨ 5% ç•°å¸¸å€¼ï¼ˆæ¨¡æ“¬å‚³æ„Ÿå™¨æ•…éšœï¼‰
- éœ€è¦æ¯”è¼ƒä¸åŒæå¤±å‡½æ•¸å’Œæ­£å‰‡åŒ–æ–¹æ³•
- æ¼”ç¤ºåœ¨ç·šå­¸ç¿’èƒ½åŠ›

### 8.2 æ•¸æ“šç”Ÿæˆèˆ‡é è™•ç†

#### 8.2.1 ç”Ÿæˆæ¨¡æ“¬æ•¸æ“š

ç”Ÿæˆ 50,000 ç­†åŒ–å·¥åæ‡‰å™¨æ“ä½œæ•¸æ“šï¼ŒåŒ…å« 10 å€‹ç‰¹å¾µï¼š

```python
# ç‰¹å¾µåˆ—è¡¨
Temperature_C           : åæ‡‰æº«åº¦ (Â°C) [150-250]
Pressure_bar            : åæ‡‰å£“åŠ› (bar) [1-10]
Catalyst_wt%            : å‚¬åŒ–åŠ‘æ¿ƒåº¦ (wt%) [0.1-2.0]
Residence_Time_min      : åœç•™æ™‚é–“ (min) [5-30]
Feed_Rate_L/hr          : é€²æ–™æµé‡ (L/hr) [10-50]
pH                      : pH å€¼ [4-10]
Stirring_Speed_rpm      : æ”ªæ‹Œé€Ÿåº¦ (rpm) [100-500]
Initiator_conc_M        : å¼•ç™¼åŠ‘æ¿ƒåº¦ (M) [0.01-0.1]
Coolant_Flow_L/min      : å†·å»æ°´æµé‡ (L/min) [5-20]
Feed_Composition        : é€²æ–™çµ„æˆ [0.5-1.5]
```

**çœŸå¯¦æ¨¡å‹åƒæ•¸**ï¼š
- æˆªè·ï¼š150
- æº«åº¦ä¿‚æ•¸ï¼š15
- å£“åŠ›ä¿‚æ•¸ï¼š4
- å‚¬åŒ–åŠ‘ä¿‚æ•¸ï¼š1
- åœç•™æ™‚é–“ä¿‚æ•¸ï¼š6
- å…¶ä»–ç‰¹å¾µä¿‚æ•¸ï¼š[-5, 0.2, 2.5, 0.3, -0.4, 0.1]
- å™ªè²ï¼šæ¨™æº–å·® = 10
- ç•°å¸¸å€¼æ¯”ä¾‹ï¼š5%

**åŸ·è¡Œçµæœ**ï¼š

```
============================================================
æ•¸æ“šç”Ÿæˆå®Œæˆ
============================================================
æ¨£æœ¬ç¸½æ•¸: 50,000
ç‰¹å¾µæ•¸é‡: 10
ç•°å¸¸å€¼æ•¸é‡: 2,500 (5.0%)

ç‰¹å¾µçµ±è¨ˆ:
  Temperature_C          : mean=200.00, std=28.87
  Pressure_bar           : mean=5.50, std=2.60
  Catalyst_wt%           : mean=1.05, std=0.55
  Residence_Time_min     : mean=17.50, std=7.22
  Feed_Rate_L/hr         : mean=30.00, std=11.55
  pH                     : mean=7.00, std=1.73
  Stirring_Speed_rpm     : mean=300.00, std=115.47
  Initiator_conc_M       : mean=0.05, std=0.03
  Coolant_Flow_L/min     : mean=12.50, std=4.33
  Feed_Composition       : mean=1.00, std=0.29

ç”¢ç‡çµ±è¨ˆ:
  Yield_%                : mean=167.54, std=18.68
```

#### 8.2.2 æ•¸æ“šæ¨™æº–åŒ–

**âš ï¸ é—œéµæ­¥é©Ÿ**ï¼šSGD å°ç‰¹å¾µå°ºåº¦æ¥µå…¶æ•æ„Ÿï¼Œ**å¿…é ˆé€²è¡Œæ¨™æº–åŒ–**ï¼

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # è¨“ç·´é›†ï¼šfit + transform
X_test_scaled = scaler.transform(X_test)        # æ¸¬è©¦é›†ï¼šåƒ… transform
```

**åŸ·è¡Œçµæœ**ï¼š

```
============================================================
æ•¸æ“šé è™•ç†å®Œæˆ
============================================================
è¨“ç·´é›†æ¨£æœ¬æ•¸: 40,000
æ¸¬è©¦é›†æ¨£æœ¬æ•¸: 10,000

æ¨™æº–åŒ–å‰ç‰¹å¾µç¯„åœï¼ˆç¬¬ä¸€å€‹ç‰¹å¾µï¼‰:
  Train: [150.00, 250.00]

æ¨™æº–åŒ–å¾Œç‰¹å¾µç¯„åœï¼ˆç¬¬ä¸€å€‹ç‰¹å¾µï¼‰:
  Train: [-1.73, 1.74]
  Mean: 0.0000, Std: 1.0000
```

**æ¨™æº–åŒ–æ•ˆæœ**ï¼š
- æ‰€æœ‰ç‰¹å¾µè½‰æ›ç‚ºå‡å€¼ 0ã€æ¨™æº–å·® 1
- ç‰¹å¾µå°ºåº¦çµ±ä¸€ï¼Œé¿å…æ¢¯åº¦æ›´æ–°ä¸å‡è¡¡
- åŠ é€Ÿ SGD æ”¶æ–‚

### 8.3 åŸºæœ¬ SGD å›æ­¸æ¨¡å‹

#### 8.3.1 æ¨¡å‹è¨“ç·´

ä½¿ç”¨é»˜èªåƒæ•¸è¨“ç·´åŸºæœ¬ SGD æ¨¡å‹ï¼š

```python
sgd_basic = SGDRegressor(
    loss='squared_error',       # å¹³æ–¹æå¤±
    penalty='l2',               # L2 æ­£å‰‡åŒ–
    alpha=0.0001,               # æ­£å‰‡åŒ–å¼·åº¦
    learning_rate='optimal',    # æœ€å„ªå­¸ç¿’ç‡
    max_iter=1000,              # æœ€å¤§è¿­ä»£æ¬¡æ•¸
    tol=1e-3,                   # æ”¶æ–‚å®¹å·®
    random_state=42             # éš¨æ©Ÿç¨®å­
)

sgd_basic.fit(X_train_scaled, y_train)
```

#### 8.3.2 åŸ·è¡Œçµæœ

```
============================================================
åŸºæœ¬ SGD å›æ­¸æ¨¡å‹
============================================================
è¨“ç·´æ™‚é–“: 0.249 ç§’
è¿­ä»£æ¬¡æ•¸: 91

Train RÂ²: 0.7990 | RMSE: 8.3815
Test RÂ²:  0.8083 | RMSE: 8.2057

æˆªè·: 167.5535

ä¿‚æ•¸:
  Temperature_C            :  13.9903
  Pressure_bar             :   3.5972
  Catalyst_wt%             :   0.9260
  Residence_Time_min       :   5.9505
  Feed_Rate_L/hr           :  -4.6114
  pH                       :   0.1407
  Stirring_Speed_rpm       :   2.2970
  Initiator_conc_M         :   0.2182
  Coolant_Flow_L/min       :  -0.3760
  Feed_Composition         :   0.0638
```

**çµæœåˆ†æ**ï¼š

1. **æ”¶æ–‚é€Ÿåº¦**ï¼šåƒ…éœ€ 91 æ¬¡è¿­ä»£ï¼ˆepochï¼‰å³æ”¶æ–‚
   - å°æ–¼ 50,000 ç­†æ•¸æ“šï¼Œé€™æ˜¯éå¸¸å¿«çš„é€Ÿåº¦
   - æ‰¹é‡æ¢¯åº¦ä¸‹é™å¯èƒ½éœ€è¦æ›´å¤šæ™‚é–“

2. **æ¨¡å‹æ€§èƒ½**ï¼š
   - Test RÂ² = 0.8083ï¼šè§£é‡‹äº† 80.83% çš„è®Šç•°
   - Test RMSE = 8.2057ï¼šé æ¸¬èª¤å·®ç´„ 8.2%
   - Train å’Œ Test æ€§èƒ½æ¥è¿‘ï¼Œç„¡æ˜é¡¯éæ“¬åˆ

3. **ä¿‚æ•¸è§£é‡‹**ï¼š
   - **Temperature_C (13.99)**ï¼šæº«åº¦æ¯å¢åŠ  1Â°Cï¼Œç”¢ç‡æé«˜ 14%ï¼ˆæœ€é‡è¦ç‰¹å¾µï¼‰
   - **Residence_Time_min (5.95)**ï¼šåœç•™æ™‚é–“è¶Šé•·ï¼Œç”¢ç‡è¶Šé«˜
   - **Pressure_bar (3.60)**ï¼šå£“åŠ›å°ç”¢ç‡æœ‰æ­£å‘å½±éŸ¿
   - **Feed_Rate_L/hr (-4.61)**ï¼šæµé‡è¶Šå¤§ï¼Œç”¢ç‡é™ä½ï¼ˆåœç•™æ™‚é–“ä¸è¶³ï¼‰
   - å…¶ä»–ç‰¹å¾µå½±éŸ¿è¼ƒå°

4. **èˆ‡çœŸå¯¦åƒæ•¸å°æ¯”**ï¼š
   - ä¼°è¨ˆçš„ä¿‚æ•¸æ¥è¿‘çœŸå¯¦å€¼ï¼ˆè€ƒæ…®ç•°å¸¸å€¼å’Œå™ªè²çš„å½±éŸ¿ï¼‰
   - SGD æˆåŠŸæ¢å¾©äº†æ•¸æ“šç”Ÿæˆéç¨‹

### 8.4 æå¤±å‡½æ•¸æ¯”è¼ƒ

æ¯”è¼ƒ **Squared Loss** å’Œ **Huber Loss** å°ç•°å¸¸å€¼çš„é­¯æ£’æ€§ï¼š

```python
models_loss = {
    'Squared Loss': SGDRegressor(loss='squared_error', penalty='l2', ...),
    'Huber Loss': SGDRegressor(loss='huber', epsilon=1.35, penalty='l2', ...)
}
```

**åŸ·è¡Œçµæœ**ï¼š

```
============================================================
æå¤±å‡½æ•¸æ¯”è¼ƒ
============================================================
       Model  Test RÂ²  Test RMSE  Test MAE
Squared Loss 0.810406   8.159963  3.228433
  Huber Loss 0.810392   8.160269  3.218138

çµè«–: Huber æå¤±å°ç•°å¸¸å€¼æ›´é­¯æ£’ï¼ŒMAE æ›´ä½
```

**çµæœåˆ†æ**ï¼š

1. **RÂ² å’Œ RMSE ç›¸è¿‘**ï¼š
   - å…©ç¨®æå¤±å‡½æ•¸çš„æ•´é«”æ€§èƒ½ç›¸ç•¶
   - æ•¸æ“šä¸­ç•°å¸¸å€¼æ¯”ä¾‹ä¸é«˜ï¼ˆ5%ï¼‰ï¼Œå½±éŸ¿æœ‰é™

2. **MAE å·®ç•°**ï¼š
   - Huber Loss çš„ MAE æ›´ä½ï¼ˆ3.218 vs 3.228ï¼‰
   - é¡¯ç¤º Huber å°ç•°å¸¸å€¼æ›´é­¯æ£’

3. **ä½¿ç”¨å»ºè­°**ï¼š
   - **ç•°å¸¸å€¼å°‘ (< 5%)**ï¼šä½¿ç”¨ Squared Lossï¼ˆè¨ˆç®—æ›´å¿«ï¼‰
   - **ç•°å¸¸å€¼å¤š (> 5%)**ï¼šä½¿ç”¨ Huber Lossï¼ˆæ›´ç©©å¥ï¼‰
   - **ä¸ç¢ºå®šç•°å¸¸å€¼æ¯”ä¾‹**ï¼šå„ªå…ˆä½¿ç”¨ Huber Loss

### 8.5 åœ¨ç·šå­¸ç¿’æ¼”ç¤º

æ¨¡æ“¬æ•¸æ“šæµå¼åˆ°é”çš„å ´æ™¯ï¼Œä½¿ç”¨ `partial_fit()` é€²è¡Œå¢é‡å­¸ç¿’ï¼š

```python
sgd_online = SGDRegressor(
    loss='squared_error',
    penalty='l2',
    alpha=0.0001,
    learning_rate='optimal',
    warm_start=True,          # é—œéµåƒæ•¸ï¼
    random_state=42
)

# åˆ† 5 æ‰¹æ¬¡è¨“ç·´
batch_size = 8000
for i in range(5):
    start_idx = i * batch_size
    end_idx = (i + 1) * batch_size
    X_batch = X_train_scaled[start_idx:end_idx]
    y_batch = y_train[start_idx:end_idx]
    
    sgd_online.partial_fit(X_batch, y_batch)
    test_r2 = sgd_online.score(X_test_scaled, y_test)
    print(f"Batch {i+1}/5: Test RÂ² = {test_r2:.4f}")
```

**åŸ·è¡Œçµæœ**ï¼š

```
============================================================
åœ¨ç·šå­¸ç¿’æ¼”ç¤º
============================================================
Batch 1/5: Test RÂ² = 0.7875
Batch 2/5: Test RÂ² = 0.8036
Batch 3/5: Test RÂ² = 0.7978
Batch 4/5: Test RÂ² = 0.8016
Batch 5/5: Test RÂ² = 0.8064

æœ€çµ‚ Test RÂ²: 0.8064
çµè«–: æ¨¡å‹éš¨è‘—æ•¸æ“šå¢åŠ é€æ­¥æ”¹å–„
```

**çµæœåˆ†æ**ï¼š

1. **é€æ­¥æ”¹å–„**ï¼š
   - Batch 1: RÂ² = 0.7875ï¼ˆåƒ…ç”¨ 8,000 ç­†æ•¸æ“šï¼‰
   - Batch 5: RÂ² = 0.8064ï¼ˆç´¯ç© 40,000 ç­†æ•¸æ“šï¼‰
   - æ€§èƒ½éš¨æ•¸æ“šå¢åŠ è€Œç©©æ­¥æå‡

2. **æ³¢å‹•æ€§**ï¼š
   - Batch 2 â†’ Batch 3 æœ‰è¼•å¾®ä¸‹é™ï¼ˆ0.8036 â†’ 0.7978ï¼‰
   - é€™æ˜¯ SGD çš„æ­£å¸¸è¡Œç‚ºï¼ˆéš¨æ©Ÿæ€§ï¼‰
   - æ•´é«”è¶¨å‹¢å‘ä¸Š

3. **åœ¨ç·šå­¸ç¿’å„ªå‹¢**ï¼š
   - ç„¡éœ€ä¸€æ¬¡æ€§è¼‰å…¥å…¨éƒ¨æ•¸æ“š
   - å¯å³æ™‚æ›´æ–°æ¨¡å‹
   - é©åˆæµå¼æ•¸æ“šå’Œå‹•æ…‹ç’°å¢ƒ

4. **åŒ–å·¥æ‡‰ç”¨**ï¼š
   - **å³æ™‚ç›£æ§**ï¼šDCS ç³»çµ±æ¯å°æ™‚æ”¶é›†æ–°æ•¸æ“šï¼Œæ¨¡å‹å³æ™‚æ›´æ–°
   - **è¨­å‚™è€åŒ–è¿½è¹¤**ï¼šåæ‡‰å™¨æ€§èƒ½éš¨æ™‚é–“è®ŠåŒ–ï¼Œæ¨¡å‹æŒçºŒé©æ‡‰
   - **å­£ç¯€æ€§è®ŠåŒ–**ï¼šåŸæ–™æ€§è³ªéš¨å­£ç¯€è®ŠåŒ–ï¼Œæ¨¡å‹è‡ªå‹•èª¿æ•´

### 8.6 æ¨¡å‹æ¯”è¼ƒ

æ¯”è¼ƒ SGD èˆ‡å‚³çµ±ç·šæ€§æ¨¡å‹çš„æ€§èƒ½å’Œè¨“ç·´æ™‚é–“ï¼š

```python
models_compare = {
    'OLS': LinearRegression(),
    'Ridge': Ridge(alpha=0.0001),
    'Lasso': Lasso(alpha=0.0001, max_iter=1000),
    'Elastic Net': ElasticNet(alpha=0.0001, l1_ratio=0.5, max_iter=1000),
    'SGD-Lasso': SGDRegressor(penalty='l1', alpha=0.0001, max_iter=1000),
    'SGD-Elastic': SGDRegressor(penalty='elasticnet', alpha=0.0001, l1_ratio=0.5, max_iter=1000),
    'SGD-Ridge': SGDRegressor(penalty='l2', alpha=0.0001, max_iter=1000)
}
```

**åŸ·è¡Œçµæœ**ï¼š

```
======================================================================
æ¨¡å‹æ¯”è¼ƒ
======================================================================
      Model  Test RÂ²  Test RMSE  Train Time (s)
        OLS 0.810570   8.156437        0.088076
      Ridge 0.810570   8.156437        0.009520
      Lasso 0.810556   8.156731        0.009077
Elastic Net 0.810507   8.157791        0.008007
  SGD-Lasso 0.810476   8.158463        0.127218
SGD-Elastic 0.810402   8.160051        0.091086
  SGD-Ridge 0.810151   8.165459        0.124159

çµè«–: SGD åœ¨å¤§æ•¸æ“šä¸Šè¨“ç·´é€Ÿåº¦å¿«ï¼Œæ€§èƒ½èˆ‡å‚³çµ±æ–¹æ³•ç›¸ç•¶
```

**çµæœåˆ†æ**ï¼š

1. **æ€§èƒ½å°æ¯”**ï¼š
   - æ‰€æœ‰æ¨¡å‹çš„ Test RÂ² éƒ½åœ¨ 0.810-0.811 ç¯„åœå…§
   - æ€§èƒ½å·®ç•°æ¥µå°ï¼ˆ< 0.1%ï¼‰
   - **SGD èˆ‡å‚³çµ±æ–¹æ³•æ€§èƒ½ç›¸ç•¶**

2. **è¨“ç·´æ™‚é–“å°æ¯”**ï¼š
   - æœ¬æ¡ˆä¾‹æ•¸æ“šé‡ï¼ˆ40,000ï¼‰è™•æ–¼è‡¨ç•Œå€é–“
   - Ridge/Lasso/Elastic Net è¨“ç·´æ™‚é–“æ¥µçŸ­ï¼ˆ< 0.01 ç§’ï¼‰
   - SGD è¨“ç·´æ™‚é–“è¼ƒé•·ï¼ˆ0.09-0.13 ç§’ï¼‰
   - **æ³¨æ„**ï¼šç•¶æ•¸æ“šé‡é”åˆ°ç™¾è¬ç´šæ™‚ï¼ŒSGD å„ªå‹¢å°‡é¡¯ç¾

3. **ä½•æ™‚ä½¿ç”¨ SGD**ï¼š
   - **æ•¸æ“šé‡ > 100,000**ï¼šSGD è¨“ç·´é€Ÿåº¦å„ªå‹¢æ˜é¡¯
   - **åœ¨ç·šå­¸ç¿’**ï¼šSGD æ˜¯å”¯ä¸€æ”¯æŒ `partial_fit` çš„ç·šæ€§æ¨¡å‹
   - **è¨˜æ†¶é«”å—é™**ï¼šSGD å¯é€æ‰¹è™•ç†æ•¸æ“š

4. **ä½•æ™‚ä½¿ç”¨å‚³çµ±æ–¹æ³•**ï¼š
   - **æ•¸æ“šé‡ < 10,000**ï¼šRidge/Lasso æ›´ç©©å®š
   - **éœ€è¦ç²¾ç¢ºè§£**ï¼šå‚³çµ±æ–¹æ³•æ”¶æ–‚æ›´ç²¾ç¢º
   - **ç„¡åœ¨ç·šå­¸ç¿’éœ€æ±‚**ï¼šå‚³çµ±æ–¹æ³•æ›´ç°¡å–®

### 8.7 å¯è¦–åŒ–åˆ†æ

#### 8.7.1 é æ¸¬ vs å¯¦éš›å€¼ï¼ˆParity Plotï¼‰

ç¹ªè£½é æ¸¬å€¼èˆ‡å¯¦éš›å€¼çš„æ•£é»åœ–ï¼Œè©•ä¼°æ¨¡å‹æ“¬åˆæ•ˆæœï¼š

![SGD Parity Plot](outputs/P3_Unit10_SGD_Regression/figs/sgd_parity_plot.png)

**åœ–è¡¨åˆ†æ**ï¼š

1. **è¨“ç·´é›†è¡¨ç¾**ï¼ˆå·¦åœ–ï¼‰ï¼š
   - Train RÂ² = 0.8008
   - æ•¸æ“šé»åˆ†ä½ˆåœ¨ç†æƒ³ç·šï¼ˆç´…è‰²è™›ç·šï¼‰é™„è¿‘
   - æ•£é»å‘ˆæ©¢åœ“å½¢åˆ†ä½ˆï¼Œé¡¯ç¤ºç·šæ€§é—œä¿‚è‰¯å¥½
   - å­˜åœ¨ä¸€äº›åé›¢é»ï¼ˆç•°å¸¸å€¼ï¼‰

2. **æ¸¬è©¦é›†è¡¨ç¾**ï¼ˆå³åœ–ï¼‰ï¼š
   - Test RÂ² = 0.8102
   - é æ¸¬æ€§èƒ½èˆ‡è¨“ç·´é›†ç›¸ç•¶
   - ç„¡æ˜é¡¯éæ“¬åˆæˆ–æ¬ æ“¬åˆ
   - æ•¸æ“šé»åˆ†ä½ˆå‡å‹»ï¼Œæ¨¡å‹æ³›åŒ–èƒ½åŠ›è‰¯å¥½

3. **èª¤å·®ç‰¹æ€§**ï¼š
   - èª¤å·®å‘ˆå°ç¨±åˆ†ä½ˆï¼ˆç„¡ç³»çµ±æ€§åå·®ï¼‰
   - é«˜ç”¢ç‡å€åŸŸï¼ˆ> 200%ï¼‰é æ¸¬ç•¥æœ‰ä½ä¼°
   - ä½ç”¢ç‡å€åŸŸï¼ˆ< 140%ï¼‰é æ¸¬ç•¥æœ‰é«˜ä¼°
   - æ•´é«”èª¤å·®åœ¨å¯æ¥å—ç¯„åœå…§ï¼ˆRMSE â‰ˆ 8%ï¼‰

4. **æ¨¡å‹å¯é æ€§**ï¼š
   - è¨“ç·´é›†å’Œæ¸¬è©¦é›†æ€§èƒ½ä¸€è‡´
   - æ¨¡å‹ç©©å¥ï¼Œç„¡éåº¦æ“¬åˆ
   - å¯ç”¨æ–¼å¯¦éš›ç”Ÿç”¢é æ¸¬

### 8.8 æ¨¡å‹ä¿å­˜èˆ‡éƒ¨ç½²

å°‡è¨“ç·´å¥½çš„æ¨¡å‹å’Œæ¨™æº–åŒ–å™¨ä¿å­˜ï¼Œç”¨æ–¼ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²ï¼š

```python
import joblib
import json

# ä¿å­˜æ¨¡å‹
joblib.dump(best_model, MODEL_DIR / 'sgd_best_model.pkl')
joblib.dump(scaler, MODEL_DIR / 'scaler.pkl')

# ä¿å­˜æ¨¡å‹ä¿¡æ¯
model_info = {
    'model_type': 'SGDRegressor',
    'train_r2': float(train_r2),
    'test_r2': float(test_r2),
    'test_rmse': float(test_rmse),
    'n_features': n_features,
    'feature_names': feature_names,
    'coefficients': best_model.coef_.tolist(),
    'intercept': float(best_model.intercept_[0]),
    'n_iterations': int(best_model.n_iter_),
    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
}

with open(MODEL_DIR / 'model_info.json', 'w', encoding='utf-8') as f:
    json.dump(model_info, f, indent=2, ensure_ascii=False)
```

**åŸ·è¡Œçµæœ**ï¼š

```
============================================================
æ¨¡å‹ä¿å­˜å®Œæˆ
============================================================
âœ“ æ¨¡å‹æ–‡ä»¶: ...\models\sgd_best_model.pkl
âœ“ æ¨™æº–åŒ–å™¨: ...\models\scaler.pkl
âœ“ æ¨¡å‹ä¿¡æ¯: ...\models\model_info.json
```

**éƒ¨ç½²æµç¨‹**ï¼š

1. **è¼‰å…¥æ¨¡å‹**ï¼š
   ```python
   model = joblib.load('sgd_best_model.pkl')
   scaler = joblib.load('scaler.pkl')
   ```

2. **é æ¸¬æ–°æ•¸æ“š**ï¼š
   ```python
   X_new_scaled = scaler.transform(X_new)
   y_pred = model.predict(X_new_scaled)
   ```

3. **åœ¨ç·šæ›´æ–°**ï¼š
   ```python
   model.partial_fit(X_new_scaled, y_new)
   joblib.dump(model, 'sgd_best_model.pkl')  # ä¿å­˜æ›´æ–°å¾Œçš„æ¨¡å‹
   ```

### 8.9 å¯¦æˆ°æ¡ˆä¾‹ç¸½çµ

æœ¬æ¡ˆä¾‹å®Œæ•´å±•ç¤ºäº† SGD å›æ­¸åœ¨åŒ–å·¥é ˜åŸŸçš„æ‡‰ç”¨æµç¨‹ï¼š

**âœ… å®Œæˆçš„ä»»å‹™**ï¼š
1. **æ•¸æ“šç”Ÿæˆ**ï¼š50,000 ç­†åŒ–å·¥åæ‡‰å™¨æ“ä½œæ•¸æ“šï¼ˆå«ç•°å¸¸å€¼ï¼‰
2. **æ•¸æ“šé è™•ç†**ï¼šæ¨™æº–åŒ–ï¼ˆSGD çš„å¿…è¦æ­¥é©Ÿï¼‰
3. **åŸºæœ¬æ¨¡å‹**ï¼šè¨“ç·´ SGD æ¨¡å‹ï¼Œå¯¦ç¾ RÂ² = 0.8083
4. **æå¤±å‡½æ•¸æ¯”è¼ƒ**ï¼šé©—è­‰ Huber Loss å°ç•°å¸¸å€¼çš„é­¯æ£’æ€§
5. **åœ¨ç·šå­¸ç¿’**ï¼šæ¼”ç¤º `partial_fit()` çš„å¢é‡å­¸ç¿’èƒ½åŠ›
6. **æ¨¡å‹æ¯”è¼ƒ**ï¼šSGD èˆ‡ OLS/Ridge/Lasso/Elastic Net çš„å°æ¯”
7. **å¯è¦–åŒ–åˆ†æ**ï¼šParity Plot å±•ç¤ºé æ¸¬æ•ˆæœ
8. **æ¨¡å‹éƒ¨ç½²**ï¼šä¿å­˜æ¨¡å‹å’Œæ¨™æº–åŒ–å™¨

**ğŸ¯ é—œéµç™¼ç¾**ï¼š
- **æ•¸æ“šæ¨™æº–åŒ–å¿…ä¸å¯å°‘**ï¼šSGD å°ç‰¹å¾µå°ºåº¦æ¥µå…¶æ•æ„Ÿ
- **æ”¶æ–‚é€Ÿåº¦å¿«**ï¼šåƒ…éœ€ 91 æ¬¡è¿­ä»£è™•ç† 50,000 ç­†æ•¸æ“š
- **æ€§èƒ½èˆ‡å‚³çµ±æ–¹æ³•ç›¸ç•¶**ï¼šTest RÂ² = 0.8083
- **åœ¨ç·šå­¸ç¿’èƒ½åŠ›**ï¼šæ”¯æŒå¢é‡æ›´æ–°ï¼Œé©åˆå‹•æ…‹ç’°å¢ƒ
- **Huber Loss å„ªå‹¢**ï¼šå°ç•°å¸¸å€¼æ›´é­¯æ£’ï¼ŒMAE æ›´ä½

**ğŸ’¡ åŒ–å·¥æ‡‰ç”¨å•Ÿç¤º**ï¼š
1. **å¤§è¦æ¨¡æ•¸æ“šå»ºæ¨¡**ï¼šDCS ç³»çµ±ç”¢ç”Ÿçš„æµ·é‡æ•¸æ“šå¯ç”¨ SGD é«˜æ•ˆè™•ç†
2. **å³æ™‚ç›£æ§èˆ‡å„ªåŒ–**ï¼šä½¿ç”¨åœ¨ç·šå­¸ç¿’å³æ™‚æ›´æ–°æ¨¡å‹
3. **ç•°å¸¸å€¼è™•ç†**ï¼šä½¿ç”¨ Huber Loss è™•ç†å‚³æ„Ÿå™¨æ•…éšœæ•¸æ“š
4. **ç‰¹å¾µç¯©é¸**ï¼šä½¿ç”¨ L1 æ­£å‰‡åŒ–è­˜åˆ¥é—œéµæ“ä½œè®Šæ•¸
5. **è¨­å‚™è€åŒ–è¿½è¹¤**ï¼šä½¿ç”¨å¢é‡å­¸ç¿’è¿½è¹¤è¨­å‚™æ€§èƒ½è®ŠåŒ–

**ğŸ“ ä¸‹ä¸€æ­¥å»ºè­°**ï¼š
- å˜—è©¦ä¸åŒçš„å­¸ç¿’ç‡ç­–ç•¥ï¼ˆ`learning_rate='adaptive'`ï¼‰
- ä½¿ç”¨ GridSearchCV å„ªåŒ–è¶…åƒæ•¸
- çµåˆç‰¹å¾µå·¥ç¨‹ï¼ˆå¤šé …å¼ç‰¹å¾µï¼‰æå‡æ€§èƒ½
- æ¢ç´¢ L1 æ­£å‰‡åŒ–é€²è¡Œç‰¹å¾µé¸æ“‡
- æ‡‰ç”¨æ–¼å¯¦éš›åŒ–å·¥ç”Ÿç”¢æ•¸æ“š

---

## 9. æ¨™æº–å·¥ä½œæµç¨‹èˆ‡æ•…éšœæ’æŸ¥

### 9.1 æ¨™æº–å·¥ä½œæµç¨‹

```python
from sklearn.linear_model import SGDRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import numpy as np
import matplotlib.pyplot as plt

# Step 1: è¼‰å…¥æ•¸æ“š
# X, y = load_chemical_engineering_data()

# Step 2: æ•¸æ“šåŠƒåˆ†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Step 3: æ•¸æ“šæ¨™æº–åŒ–ï¼ˆå¿…é ˆï¼ï¼‰
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 4: å»ºç«‹æ¨¡å‹
sgd_model = SGDRegressor(
    loss='squared_loss',          # æˆ– 'huber' è™•ç†ç•°å¸¸å€¼
    penalty='l2',                 # æˆ– 'l1', 'elasticnet'
    alpha=0.0001,                 # æ­£å‰‡åŒ–å¼·åº¦
    learning_rate='optimal',      # å­¸ç¿’ç‡ç­–ç•¥
    max_iter=1000,                # æœ€å¤§è¿­ä»£æ¬¡æ•¸
    tol=1e-3,                     # æ”¶æ–‚å®¹å·®
    early_stopping=True,          # å•Ÿç”¨æ—©åœæ³•
    validation_fraction=0.1,      # é©—è­‰é›†æ¯”ä¾‹
    n_iter_no_change=10,          # æ—©åœè€å¿ƒåƒæ•¸
    random_state=42               # éš¨æ©Ÿç¨®å­
)

# Step 5: è¨“ç·´æ¨¡å‹
sgd_model.fit(X_train_scaled, y_train)

# Step 6: æ¨¡å‹è©•ä¼°
y_pred_train = sgd_model.predict(X_train_scaled)
y_pred_test = sgd_model.predict(X_test_scaled)

train_r2 = r2_score(y_train, y_pred_train)
test_r2 = r2_score(y_test, y_pred_test)
train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))
test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))

print("=" * 50)
print("Model Performance")
print("=" * 50)
print(f"Train RÂ²: {train_r2:.4f} | RMSE: {train_rmse:.4f}")
print(f"Test RÂ²:  {test_r2:.4f} | RMSE: {test_rmse:.4f}")
print(f"Number of iterations: {sgd_model.n_iter_}")

# Step 7: äº¤å‰é©—è­‰
cv_scores = cross_val_score(
    sgd_model, X_train_scaled, y_train, 
    cv=5, scoring='r2'
)
print(f"\n5-Fold CV RÂ²: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")

# Step 8: ç‰¹å¾µé‡è¦æ€§åˆ†æ
feature_importance = pd.DataFrame({
    'Feature': feature_names,
    'Coefficient': sgd_model.coef_
}).sort_values('Coefficient', key=abs, ascending=False)

print("\nTop 5 important features:")
print(feature_importance.head())

# Step 9: æ¨¡å‹æŒä¹…åŒ–
import joblib
joblib.dump(sgd_model, 'sgd_model.pkl')
joblib.dump(scaler, 'scaler.pkl')
print("\nModel saved successfully!")
```

### 9.2 æ•…éšœæ’æŸ¥æŒ‡å—

**å•é¡Œ 1ï¼šæ¨¡å‹æ€§èƒ½å¾ˆå·®ï¼ˆRÂ² < 0ï¼‰**
- âœ“ æª¢æŸ¥æ˜¯å¦é€²è¡Œæ•¸æ“šæ¨™æº–åŒ–
- âœ“ æª¢æŸ¥å­¸ç¿’ç‡æ˜¯å¦éå¤§ï¼ˆå°è‡´ç™¼æ•£ï¼‰
- âœ“ å¢åŠ  `max_iter`
- âœ“ å˜—è©¦ä¸åŒçš„ `learning_rate` ç­–ç•¥

**å•é¡Œ 2ï¼šè¨“ç·´é€Ÿåº¦å¾ˆæ…¢**
- âœ“ æ¸›å°‘ `max_iter`
- âœ“ å•Ÿç”¨ `early_stopping=True`
- âœ“ å¢å¤§ `tol`ï¼ˆé™ä½æ”¶æ–‚è¦æ±‚ï¼‰

**å•é¡Œ 3ï¼šè¨“ç·´é›† RÂ² å¾ˆé«˜ä½†æ¸¬è©¦é›†å¾ˆä½ï¼ˆéæ“¬åˆï¼‰**
- âœ“ å¢å¤§ `alpha`ï¼ˆæ›´å¼·çš„æ­£å‰‡åŒ–ï¼‰
- âœ“ ä½¿ç”¨ L1 æˆ– Elastic Net æ­£å‰‡åŒ–
- âœ“ æ¸›å°‘ç‰¹å¾µæ•¸é‡

**å•é¡Œ 4ï¼šå­˜åœ¨ç•°å¸¸å€¼å°è‡´æ€§èƒ½ä¸‹é™**
- âœ“ ä½¿ç”¨ `loss='huber'`
- âœ“ èª¿æ•´ `epsilon` åƒæ•¸

**å•é¡Œ 5ï¼šæ¯æ¬¡é‹è¡Œçµæœä¸åŒ**
- âœ“ è¨­ç½® `random_state=42`
- âœ“ è¨­ç½® `shuffle=True` ç¢ºä¿æ•¸æ“šæ‰“äº‚

---

## 10. ç¸½çµ

### 10.1 æ ¸å¿ƒè¦é»

1. **SGD å›æ­¸æ˜¯åŸºæ–¼éš¨æ©Ÿæ¢¯åº¦ä¸‹é™å„ªåŒ–çš„ç·šæ€§å›æ­¸æ–¹æ³•**
   - æ¯æ¬¡è¿­ä»£åƒ…ä½¿ç”¨ä¸€å€‹æ¨£æœ¬æ›´æ–°åƒæ•¸
   - é©åˆå¤§è¦æ¨¡æ•¸æ“šå’Œåœ¨ç·šå­¸ç¿’

2. **é—œéµå„ªå‹¢**ï¼š
   - âœ“ å¯è™•ç†ç™¾è¬ç´šä»¥ä¸Šæ•¸æ“š
   - âœ“ è¨˜æ†¶é«”éœ€æ±‚ä½
   - âœ“ æ”¯æŒåœ¨ç·šå­¸ç¿’ï¼ˆ`partial_fit`ï¼‰
   - âœ“ éˆæ´»ï¼ˆå¤šç¨®æå¤±å‡½æ•¸å’Œæ­£å‰‡åŒ–ï¼‰

3. **é—œéµé™åˆ¶**ï¼š
   - âœ— å°ç‰¹å¾µå°ºåº¦æ¥µå…¶æ•æ„Ÿï¼ˆ**å¿…é ˆæ¨™æº–åŒ–**ï¼‰
   - âœ— è¶…åƒæ•¸èª¿æ•´è¤‡é›œ
   - âœ— å°æ•¸æ“šé›†ä¸ç©©å®š

4. **åŒ–å·¥æ‡‰ç”¨å ´æ™¯**ï¼š
   - å¤§è¦æ¨¡ DCS æ•¸æ“šå»ºæ¨¡
   - å³æ™‚åæ‡‰å™¨ç›£æ§
   - æ¦‚å¿µæ¼‚ç§»è™•ç†ï¼ˆå‚¬åŒ–åŠ‘è€åŒ–ã€åŸæ–™è®ŠåŒ–ï¼‰
   - ç•°å¸¸å€¼é­¯æ£’å»ºæ¨¡ï¼ˆHuber æå¤±ï¼‰

### 9.2 èˆ‡å…¶ä»–ç·šæ€§æ¨¡å‹çš„é¸æ“‡

| æ•¸æ“šè¦æ¨¡ | åœ¨ç·šå­¸ç¿’ | æ¨è–¦æ¨¡å‹ |
|---------|---------|---------|
| < 10,000 | å¦ | Ridge, Lasso, Elastic Net |
| < 10,000 | æ˜¯ | **SGDRegressor** |
| > 100,000 | å¦ | **SGDRegressor** |
| > 100,000 | æ˜¯ | **SGDRegressor** |

### 9.3 æœ€ä½³å¯¦è¸

1. **æ•¸æ“šé è™•ç†**ï¼š
   - âœ“ **å¿…é ˆé€²è¡Œæ¨™æº–åŒ–**ï¼ˆ`StandardScaler`ï¼‰
   - âœ“ è™•ç†ç¼ºå¤±å€¼
   - âœ“ è™•ç†ç•°å¸¸å€¼ï¼ˆæˆ–ä½¿ç”¨ Huber æå¤±ï¼‰

2. **åƒæ•¸è¨­ç½®**ï¼š
   - âœ“ ä½¿ç”¨ `learning_rate='optimal'`ï¼ˆé»˜èªï¼Œç©©å¥ï¼‰
   - âœ“ å•Ÿç”¨ `early_stopping=True`ï¼ˆå¤§æ•¸æ“šé›†ï¼‰
   - âœ“ è¨­ç½® `random_state` ç¢ºä¿å¯é‡ç¾æ€§

3. **æ¨¡å‹è©•ä¼°**ï¼š
   - âœ“ ä½¿ç”¨äº¤å‰é©—è­‰è©•ä¼°ç©©å®šæ€§
   - âœ“ ç›£æ§è¨“ç·´å’Œæ¸¬è©¦æ€§èƒ½ï¼ˆé˜²æ­¢éæ“¬åˆï¼‰
   - âœ“ ç¹ªè£½å­¸ç¿’æ›²ç·š

4. **åœ¨ç·šå­¸ç¿’**ï¼š
   - âœ“ è¨­ç½® `warm_start=True`
   - âœ“ ä½¿ç”¨ `partial_fit()` å¢é‡æ›´æ–°
   - âœ“ å®šæœŸè©•ä¼°æ¨¡å‹æ€§èƒ½
   - âœ“ å¿…è¦æ™‚é‡æ–°åˆå§‹åŒ–æ¨¡å‹

---

## 11. é€²éšä¸»é¡Œèˆ‡æ“´å±•

### 11.1 çµåˆç‰¹å¾µå·¥ç¨‹

SGD å¯ä»¥èˆ‡ç‰¹å¾µå·¥ç¨‹çµåˆï¼Œè™•ç†éç·šæ€§é—œä¿‚ï¼š

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline

# å‰µå»º Pipeline
pipeline = Pipeline([
    ('poly', PolynomialFeatures(degree=2, include_bias=False)),
    ('scaler', StandardScaler()),
    ('sgd', SGDRegressor(penalty='l1', alpha=0.001, max_iter=1000))
])

pipeline.fit(X_train, y_train)
test_r2 = pipeline.score(X_test, y_test)
print(f"Test RÂ² with polynomial features: {test_r2:.4f}")
```

### 11.2 çµåˆè¶…åƒæ•¸å„ªåŒ–

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import loguniform

# å®šç¾©åƒæ•¸åˆ†ä½ˆ
param_distributions = {
    'loss': ['squared_loss', 'huber'],
    'penalty': ['l2', 'l1', 'elasticnet'],
    'alpha': loguniform(1e-5, 1e-1),
    'learning_rate': ['optimal', 'adaptive'],
    'eta0': loguniform(1e-3, 1e-1)
}

# éš¨æ©Ÿæœç´¢
random_search = RandomizedSearchCV(
    SGDRegressor(max_iter=1000, random_state=42),
    param_distributions,
    n_iter=50,           # å˜—è©¦ 50 çµ„åƒæ•¸
    cv=5,
    scoring='r2',
    n_jobs=-1,
    random_state=42
)

random_search.fit(X_train_scaled, y_train)
print("Best parameters:", random_search.best_params_)
print(f"Best CV RÂ²: {random_search.best_score_:.4f}")
```

### 11.3 èˆ‡æ·±åº¦å­¸ç¿’çš„æ¯”è¼ƒ

å°æ–¼çµæ§‹åŒ–æ•¸æ“šï¼ˆè¡¨æ ¼æ•¸æ“šï¼‰ï¼ŒSGD å›æ­¸é€šå¸¸å„ªæ–¼ç°¡å–®çš„æ·±åº¦å­¸ç¿’æ¨¡å‹ï¼š

| ç‰¹æ€§ | SGD Regression | æ·±åº¦å­¸ç¿’ (MLP) |
|------|---------------|---------------|
| è¨“ç·´é€Ÿåº¦ | å¿« | æ…¢ |
| å¯è§£é‡‹æ€§ | é«˜ | ä½ |
| è¶…åƒæ•¸èª¿æ•´ | ç›¸å°ç°¡å–® | è¤‡é›œ |
| å°æ•¸æ“šè¡¨ç¾ | å¥½ | å·® |
| å¤§æ•¸æ“šè¡¨ç¾ | å¥½ | å¥½ |
| åœ¨ç·šå­¸ç¿’ | åŸç”Ÿæ”¯æŒ | éœ€è¦ç‰¹æ®Šè¨­è¨ˆ |

**åŒ–å·¥å»ºè­°**ï¼š
- è¡¨æ ¼æ•¸æ“šï¼ˆç‰¹å¾µæ˜ç¢ºï¼‰ï¼š**å„ªå…ˆä½¿ç”¨ SGD æˆ–å…¶ä»–ç·šæ€§æ¨¡å‹**
- åœ–åƒ/åºåˆ—æ•¸æ“šï¼šä½¿ç”¨æ·±åº¦å­¸ç¿’ï¼ˆCNN/RNNï¼‰
- è¤‡é›œéç·šæ€§é—œä¿‚ï¼šè€ƒæ…®æ¨¹æ¨¡å‹ï¼ˆRandom Forest, XGBoostï¼‰æˆ–æ·±åº¦å­¸ç¿’

### 11.4 æœªä¾†æ–¹å‘

1. **è‡ªé©æ‡‰å­¸ç¿’ç‡**ï¼š
   - Adam, RMSprop ç­‰å„ªåŒ–å™¨
   - ç›®å‰ sklearn çš„ SGDRegressor ä¸æ”¯æŒï¼Œä½†å¯åœ¨ TensorFlow/PyTorch ä¸­å¯¦ç¾

2. **Mini-Batch SGD**ï¼š
   - æŠ˜è¡·å–®æ¨£æœ¬å’Œæ‰¹é‡æ¢¯åº¦ä¸‹é™
   - sklearn çš„ `SGDRegressor` å¯¦éš›ä¸Šåœ¨æ¯å€‹ epoch å…§è™•ç†å¤šå€‹æ¨£æœ¬

3. **åˆ†ä½ˆå¼ SGD**ï¼š
   - å¤šæ©Ÿå™¨ä¸¦è¡Œè¨“ç·´
   - ä½¿ç”¨ Dask-ML æˆ– Spark MLlib

---

## åƒè€ƒè³‡æº

### å®˜æ–¹æ–‡æª”
- [sklearn.linear_model.SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html)
- [User Guide: Stochastic Gradient Descent](https://scikit-learn.org/stable/modules/sgd.html)

### å»¶ä¼¸é–±è®€
1. Bottou, L. (2010). "Large-Scale Machine Learning with Stochastic Gradient Descent"
2. Robbins, H., & Monro, S. (1951). "A Stochastic Approximation Method"
3. Kingma, D. P., & Ba, J. (2014). "Adam: A Method for Stochastic Optimization"

### ç›¸é—œèª²ç¨‹å–®å…ƒ
- Unit10_Linear_Regressionï¼šåŸºç¤ç·šæ€§å›æ­¸
- Unit10_Ridge_Regressionï¼šL2 æ­£å‰‡åŒ–
- Unit10_Lasso_Regressionï¼šL1 æ­£å‰‡åŒ–èˆ‡ç‰¹å¾µé¸æ“‡
- Unit10_ElasticNet_Regressionï¼šL1 + L2 æ··åˆæ­£å‰‡åŒ–

---

**å®Œæ•´å¯¦æˆ°æ¡ˆä¾‹**ï¼šæœ¬è¬›ç¾©ç¬¬ 8 ç« ã€Œå®Œæ•´å¯¦æˆ°æ¡ˆä¾‹ï¼šåŒ–å·¥åæ‡‰å™¨ç”¢ç‡é æ¸¬ã€å±•ç¤ºäº†ä½¿ç”¨ 50,000 ç­†æ•¸æ“šçš„å®Œæ•´ SGD å»ºæ¨¡æµç¨‹ï¼ŒåŒ…å«ï¼š
- âœ“ æ•¸æ“šç”Ÿæˆèˆ‡é è™•ç†
- âœ“ åŸºæœ¬ SGD æ¨¡å‹è¨“ç·´ï¼ˆRÂ² = 0.8083ï¼‰
- âœ“ æå¤±å‡½æ•¸æ¯”è¼ƒï¼ˆSquared Loss vs Huber Lossï¼‰
- âœ“ åœ¨ç·šå­¸ç¿’æ¼”ç¤ºï¼ˆå¢é‡æ›´æ–°ï¼‰
- âœ“ æ¨¡å‹æ¯”è¼ƒï¼ˆSGD vs OLS/Ridge/Lasso/Elastic Netï¼‰
- âœ“ å¯è¦–åŒ–åˆ†æï¼ˆParity Plotï¼‰
- âœ“ æ¨¡å‹ä¿å­˜èˆ‡éƒ¨ç½²

**å¯¦ä½œç·´ç¿’**ï¼šè«‹åƒè€ƒ `Unit10_SGD_Regression.ipynb` é€²è¡Œå®Œæ•´çš„ç¨‹å¼ç¢¼æ¼”ç·´ï¼Œè¦ªè‡ªé«”é©— SGD å›æ­¸åœ¨åŒ–å·¥é ˜åŸŸçš„æ‡‰ç”¨ï¼

---

**èª²ç¨‹è³‡è¨Š**
- èª²ç¨‹åç¨±ï¼šAIåœ¨åŒ–å·¥ä¸Šä¹‹æ‡‰ç”¨
- èª²ç¨‹å–®å…ƒï¼šUnit10 SGD å›æ­¸
- èª²ç¨‹è£½ä½œï¼šé€¢ç”²å¤§å­¸ åŒ–å·¥ç³» æ™ºæ…§ç¨‹åºç³»çµ±å·¥ç¨‹å¯¦é©—å®¤
- æˆèª²æ•™å¸«ï¼šèŠæ›œç¦ åŠ©ç†æ•™æˆ
- æ›´æ–°æ—¥æœŸï¼š2026-01-28

**èª²ç¨‹æˆæ¬Š [CC BY-NC-SA 4.0]**
 - æœ¬æ•™æéµå¾ª [å‰µç”¨CC å§“åæ¨™ç¤º-éå•†æ¥­æ€§-ç›¸åŒæ–¹å¼åˆ†äº« 4.0 åœ‹éš› (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh) æˆæ¬Šã€‚

---